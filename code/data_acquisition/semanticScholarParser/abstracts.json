{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593550","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Gas pipeline requires to be inspected regularly for leakages caused by natural disaster. Robots are widely used for pipeline inspection since they are more convenient than manual inspection. Several problems, however, exist due to the restriction by complex pipe networks. The most significant one is limited inspection range caused by restriction of cable length or wireless signal attenuation. In this paper, we proposed a concept of wireless relay communication to assist robot to extend the inspection range, and we newly developed a tracked robot chain system. In this system, each robot serves as a relay communication node. Leakage information of pipes are transmitted via these relay nodes. To ensure the stability of relay communication between adjacent robots, we adopted RSSI (received signal strength indication)-based evaluation method for cooperative and coordinated movement of robot chain system. Moreover, wireless application layer communication protocol (WALCP) was used to increase the stable performance of wireless relay communication. Each robot can self-navigate based on distance measurement module, which enables robots to pass through an elbow junction. Multiple experiments to evaluate relay transmission efficiency, RSSI-based cooperative movement, and comprehensive performance were conducted. Results revealed that our proposed system could realize relatively accurate relay transmission and RSSI-based coordinated movement.","inCitations":[],"pmid":"","title":"An Automatic Tracked Robot Chain System for Gas Pipeline Inspection and Maintenance Based on Wireless Relay Communication","journalPages":"3978-3983","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593550"],"entities":[],"journalVolume":"","outCitations":[],"id":"f5c49ed58b83c813f211c77732ef827378775df4","s2Url":"https://semanticscholar.org/paper/f5c49ed58b83c813f211c77732ef827378775df4","authors":[{"name":"Wen Zhao","ids":[]},{"name":"Mitsuhiro Kamezaki","ids":[]},{"name":"Kento Yoshida","ids":[]},{"name":"Minoru Konno","ids":[]},{"name":"Akihiko Onuki","ids":[]},{"name":"Shigeki Sugano","ids":[]}],"doi":"10.1109/IROS.2018.8593550"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206424","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We propose a novel approach for safe tracking of a moving target in cluttered environments using a quadrotor. The key contribution of our work is a formulation that enables the generation of safe and dynamical feasible tracking trajectories that satisfy arbitrary relative motion patterns (circling, parallel tracking, undirectional tracking, etc.) with respect to the target. In our framework, forming the desired relative motion pattern between the quadrotor and the target only requires a generative function that specifies relative positions at different time instants. Our method generates samples to fit a piecewise-polynomial representation of the desired relative motion pattern and embeds it into a cost function for solving valid tracking trajectories via quadratic programming. Collision avoidance is achieved by squeezing the trajectory into a collision-free flight corridor, and dynamical feasibility is achieved by enforcing bounds on corresponding derivatives. Both of which can be written as linear constraints for the quadratic programming. Our approach is lightweight and can be implemented for real-time target tracking. We use a simulated cluttered environment and multiple desired relative motion patterns to demonstrate the performance of the proposed approach.","inCitations":[],"pmid":"","title":"Using a quadrotor to track a moving target with arbitrary relative motion patterns","journalPages":"5310-5317","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206424"],"entities":["Approximation algorithm","Eye tracking","Graph (discrete mathematics)","Loss function","Polynomial","Quadratic programming","Real-time clock"],"journalVolume":"","outCitations":["a7ae29a830d475416fc036db8e35766dfc33b82f","f633f1e4d0188575a67bb5eeaa3fbb278272c018","fb806eed8d070292ee767185e0f2268f9912545d","5de3ba76eeead6a6ee3295220080ee881f84bd27","d1579f6f49ec70819f7b1028a3847d6e9b3a6a7b","8a1a082e8a215978b1d33c5fbed980fdc3cd1164","2376078d13761387cabb933798b93a706c2ea7ef","9d1f17baafa1bf923839a53a3cbc83da7e25186a","1a9db2816b8b87218fff7fb4e90c79742aedc140","066d1729df9a562d0ab63ed83fe9f97e6ca434b5","6facbe854c57343710d7a4de32cb8ab8d7b1c951","2d90537b709ae92e07856b93aab70614ac5af561","9e5bbf0e95dc6c99ea8e9575987e6685305f6db6","b37cd2ed93bf22e2ab1d427ac5dc2d9b84ef0c21","0b15c987dba1db98fd16c56997c1513db2fde306","7655b41aa31e89b0381e89de79fc82b1d483182d","85955ac10bde3d0702f10e978d5714592c52b9db"],"id":"2cd443b6bfa2ec4f971ee1b768b681fb9112ecdb","s2Url":"https://semanticscholar.org/paper/2cd443b6bfa2ec4f971ee1b768b681fb9112ecdb","authors":[{"name":"Jing Chen","ids":["50762491"]},{"name":"Shaojie Shen","ids":["3225993"]}],"doi":"10.1109/IROS.2017.8206424"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593902","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper we present a data driven approach for safe and smooth autonomous navigation of a personal mobility vehicle (PMV) when facing moving obstacles such as people and bicycles in public pedestrian paths. In a period of three months, data from five different persons driving the robotic PMV in an outdoor environment while facing pedestrians were collected. 2465 clean tracks around the vehicle together with PMVs trajectories were collected. We performed an analysis of the parameters involved for human-driven smooth navigation. Relevant parameters regarding PMV-Human interaction included distance to moving objects, passing side and velocities. Moreover, data suggests the existence of a social navigational distance for the PWv. For autonomous navigation we implemented a Frenet planner to achieve safe and smooth navigation for the passenger and pedestrians around. Experimental results in real pedestrian paths show that the PMV is capable of smoothly following its path while facing pedestrians and bicycles.","inCitations":[],"pmid":"","title":"Personal Mobility Vehicle Autonomous Navigation Through Pedestrian Flow: A Data Driven Approach for Parameter Extraction","journalPages":"3438-3444","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593902"],"entities":[],"journalVolume":"","outCitations":[],"id":"b719416823d62502f061b4b034ccbca0ebaaf9b4","s2Url":"https://semanticscholar.org/paper/b719416823d62502f061b4b034ccbca0ebaaf9b4","authors":[{"name":"Luis Yoichi Morales Saiki","ids":[]},{"name":"Naoki Akai","ids":[]},{"name":"Hiroshi Murase","ids":[]}],"doi":"10.1109/IROS.2018.8593902"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594205","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We present a filtering-based method for semantic mapping to simultaneously detect objects and localize their 6 degree-of-freedom pose. For our method, called Contextual Temporal Mapping (or CT-Map), we represent the semantic map as a belief over object classes and poses across an observed scene. Inference for the semantic mapping problem is then modeled in the form of a Conditional Random Field (CRF). CT-Map is a CRF that considers two forms of relationship potentials to account for contextual relations between objects and temporal consistency of object poses, as well as a measurement potential on observations. A particle filtering algorithm is then proposed to perform inference in the CT-Map model. We demonstrate the efficacy of the CT-Map method with a Michigan Progress Fetch robot equipped with a RGB-D sensor. Our results demonstrate that the particle filtering based inference of CT-Map provides improved object detection and pose estimation with respect to baseline methods that treat observations as independent samples of a scene.","inCitations":[],"pmid":"","title":"Semantic Mapping with Simultaneous Object Detection and Localization","journalPages":"911-918","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594205","https://export.arxiv.org/pdf/1810.11525","http://arxiv.org/abs/1810.11525","https://arxiv.org/pdf/1810.11525v1.pdf"],"entities":[],"journalVolume":"","outCitations":["b175faea7defaf8b52df80c1142977058190d24a","53079196041fedeb5f1e236b1c76c7108fd8346e","65463f263387a4fb840dfb1b5dcf45eb72cad7cf","0a1f79b0c8688d0dd56161f10518d0769a0379b8","47c3c0273c010115cd1d5ee90210937f47658d4e","979f63114a30d60c5c06d4c9c18c8249c3a63099","2fce7a21421294f110a5ad6dac845b732f3785e5","35c60fafb3b492908c1d77341be3f6b0fadaf7b3","009ca995815509342c5b16f204675e7123ecf755","89bf8f9b83bf52c5a28dd6bfa9010a8eec3f6408","04a7e015135ff9c5ada06787442561a6777f3f3c","fedbfecb52d0c44671b5befb4381f9b3d957aa79","881fa7c0ba428eb4d379c7d82451f995784fab33","ef0e38a237c6159bd2547751bf0014b9afcc2d9b","20a78d3145279dcd799cd7a856ae2714f4863a16","f89c310d152f51df9e22fcb452cd644b5cfaf1a6","1cfb38376917ce1119f11a062b4c8b03b8807526","806bde5323c53aa665b9886e61f78222df721792","16a8cf8d86a4ab59831f90d31c60d52cab3b0b52","5c3e3c9eb353c2662333b2cafbb67bfce4baa371","0c665d442c8307864fe50df42ae37da0462e1c12","21a1654b856cf0c64e60e58258669b374cb05539","646a669f1dc38ae961fe41fbd3c83cab64ce9d53","64f0ab3f2c141779fa8769ea192bd40b0a9529c7","07bd30603161412a4e3d19d1e707a601341989b0","0ea59d57b9651b7aa1c47ca96bf77bd65513cbdf","0c65245bff0004961a5173709400479addbb9ee1","1c7e078611c9df412e6eb3a356f31a0da0c1f99c","1aa664cadef71e10dbf26af2e67c67e19da3d61e","ce6d34010f04afa4cf3018f51bad8f480ebc759c","974fa8ac07387f9719c05a2ad60de3cd0c979bd2","40b727bcae57f2270eb2041a690cfd891e4af9fe","009fba8df6bbca155d9e070a9bd8d0959bc693c2","5aea5f37c8d97189ad38db84d8ab201c0d1817c7","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","adc55a30b17060d68b5092dfaba52243e39c68f5","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","fb4915611ad787c74d44ebb53f886e5802204593","42bfb5e884ae0bb8e1764e5483623f9c1d624996","5b03fed2831ac56fca15d033f2cfd584ce9cb04f","30c3e410f689516983efcd780b9bea02531c387d","061356704ec86334dbbc073985375fe13cd39088","4e717f8630a0d188ca7f9b05ed4fe47ff05936e7"],"id":"701264f2ac69d7aa51499147e7c8142ffc281727","s2Url":"https://semanticscholar.org/paper/701264f2ac69d7aa51499147e7c8142ffc281727","authors":[{"name":"Zhen Zeng","ids":["48000371"]},{"name":"Yunwen Zhou","ids":["13086783"]},{"name":"Odest Chadwicke Jenkins","ids":["1792217"]},{"name":"Karthik Desingh","ids":["2760273"]}],"doi":"10.1109/IROS.2018.8594205"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593622","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Task autonomy is an important consideration for the development of future surgical robots. For robot-assisted anastomosis, suture thread detection is a prerequisite for subsequent robot manipulation. Previous works on automatic thread detection are focused on the learning of the models with specific surgical settings that are poorly generalisable to generic settings. In this paper, we propose a joint feature learning framework that caters for the foreground and background adaptation for surgical suture thread detection. The proposed method is developed in the context of semi-supervised and unsupervised domain adaptation, leveraging the labelled training data from the source domain to learn the detection model for unlabelled or partially labelled target domain, which can also be from different types of threads or organs. Based on adversarial learning, we further preserve the semantic identity and introduce curriculum adaptation to generate synthetic data. Experiments on four domain adaptation tasks for suture thread detection demonstrate the strength of the proposed method being able to generate good quality synthetic data and transfer between specific domains with limited or even no labelled data of the target domain.","inCitations":[],"pmid":"","title":"Cross-Scene Suture Thread Parsing for Robot Assisted Anastomosis based on Joint Feature Learning","journalPages":"769-776","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593622"],"entities":[],"journalVolume":"","outCitations":[],"id":"957334148d817d5b3f014b94f017b7f9432e01a9","s2Url":"https://semanticscholar.org/paper/957334148d817d5b3f014b94f017b7f9432e01a9","authors":[{"name":"Yun Gu","ids":[]},{"name":"Yang Hu","ids":[]},{"name":"Lin Zhang","ids":[]},{"name":"Jie Yang","ids":[]},{"name":"Guang-Zhong Yang","ids":[]}],"doi":"10.1109/IROS.2018.8593622"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545522","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Single image haze removal is a challenging and ill-posed problem. The existing haze removal methods in literature, including the recently introduced deep learning methods, model the problem of haze removal as that of estimating intermediate parameters, viz., scene transmission map and atmospheric light. These are used to compute the haze-free image from the hazy input image. Such an approach only focuses on accurate estimation of intermediate parameters, while the aesthetic quality of the haze-free image is unaccounted for in the optimization framework. Thus, errors in the estimation of intermediate parameters often lead to generation of inferior quality haze-free images. In this paper, we present CANDY (Conditional Adversarial Networks based Dehazing of hazY images), a fully end-to-end model which directly generates a clean haze-free image from a hazy input image. CANDY also incorporates the visual quality of haze-free image into the optimization function; thus, generating a superior quality haze-free image. This is one of the first works in literature to propose a fully end-to-end model for single image haze removal. Also, this is the first work to explore the concept of generative adversarial networks for the problem of single image haze removal. CANDY was trained on a synthetically created haze image dataset, while evaluation was performed on challenging synthetic as well as real haze image datasets. The extensive evaluation and comparison results of CANDY reveal that it significantly outperforms existing state-of-the-art haze removal methods in literature, both quantitatively as well as qualitatively.","inCitations":[],"pmid":"","title":"CANDY: Conditional Adversarial Networks based End-to-End System for Single Image Haze Removal","journalPages":"3061-3067","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545522"],"entities":[],"journalVolume":"","outCitations":["867f1a262a704ef4cabe84899310370182dd598f","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","1fe8b9f66b29e1c43c50a200dd9a172cb531f7d8","9451d0b1bfbba5f3e19c083866f1394aabf7d06c","293b47fc6c1d83e72484debf7b4faadc5d75cce7","915c4bb289b3642489e904c65a47fa56efb60658","be70454630926d0ee83877fc05bb49370b6bff59","6faf5bb5913be4079c40caef3838824c65c491d7","29895c769b0618b854cd3f30c6b34daec4a45076","6074c1108997e0c1f97dc3c199323a162ffe978d","1f59f40be27ea17b40d5d54367624e89fc3b6659","81b2b8fd43c0ac1d62dfeed2ba528298bc2d9e4e","19063a51d985a900005fb16c41ff7bb4486a6ce7","28ba3bd535327a08ffca088246447511599a7dfe","29cd8e824e047695f47954633457271ae0c09e3b","45e8ad848cf8c9e36d8107670982ae43b8e33578","211b56f6351c8377ade4348f06e7e61ef876982e","ba753286b9e2f32c5d5a7df08571262e257d2e53","920f0c070701caabf023c600f3e310f1906ca818","30416bc0760f463fa90bd8a92a388fb6710fe589","ea1907978878bbe994543bc2df7c1946d31dd88d","c1e4420ddc71c4962e0ba26287293a25a774fb6e","bf5f67ebbe41f2fb1726a7c3c0be707366d5a4fb","c246b1127d1e48211ce5f1e2f42373bac3155ab8","35756f711a97166df11202ebe46820a36704ae77","18168aea48a22f6fe2fe407c0ff70083cba225a7","41090ca30a63b4f0934c4c0f9b59c61c7dcc2433","bb0fd8cce4945317447e061d03efef674e0c132e","072fd0b8d471f183da0ca9880379b3bb29031b6a"],"id":"27ccd5200ce03a27c3cecc37f207b42a5e54574b","s2Url":"https://semanticscholar.org/paper/27ccd5200ce03a27c3cecc37f207b42a5e54574b","authors":[{"name":"Kunal Swami","ids":["3704706"]},{"name":"Saikat Kumar Das","ids":["32201084"]}],"doi":"10.1109/ICPR.2018.8545522"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545684","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"This paper presents two variational multiphase segmentation methods for recovery of segments in weakly structured images, presenting local and global intensity bias fields, as often is the case in micro-tomography. The proposed methods assume a fixed number of classes. They use local image averages as discriminative features and binary labelling for class membership and their relaxation to per pixel/voxel posterior probabilities, Hidden Markov Measure Field Models (HMMFM). The first model uses a Total Variation weighted semi-norm (wTV) for label field regularization, similar to Geodesic Active Contours, but with a different and possibly richer representation. The second model uses a weighted Dirichlet (squared gradient) regularization. Both problems are solved by alternating minimization on computation of local class averages and label fields. The quadratic problem is essentially smooth, except for HMMFM constraints. The wTV problem uses a Chambolle-Pock scheme for label field updates. We demonstrate on synthetic examples the capabilities of the approaches, and illustrate it on a real examples.","inCitations":[],"pmid":"","title":"Multiphase Local Mean Geodesic Active Regions","journalPages":"3031-3036","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545684"],"entities":[],"journalVolume":"","outCitations":["b7e8b1bf31b16b7c3376fb58f4735bfb972657bf","01b70354f274c7c5902cf79decb1d49218038a25","4f0001bdd1bc682deb35deb39d8efd9acec07853","96cb9ee65127db92c0be39985f28957359d98ed4","f532ae0e3026704395351be0614c9d3e3ffe4597","f7af021374e16c47eb995fb9cb685158f26eac56","dfa362309120df474500e7bcbe49ccabbfd06cc0","88bf799764e7fe462abf57a589591e592adf5a3e","0ec45dd6284e9161cfd62a7aca3efc0ae1aabfee","5cdcf06509513915c95ddd9c2cc2144025ede849","353ffb1f64fdd35b5fb636ddffc5845a90b2f4ee","c17bece8f9212d983dd5b41e099b5950bd2606d5","38456189bf4d689bcf778d7bcae1eaff07135195","0e6394577b5c39910777a2d826c4aa7864a4b607","0054d6bb763b29b6f21ee6aabf7cf39bfa13682b"],"id":"c29b68d5697be2e6636f889a16dcf59b81d1c503","s2Url":"https://semanticscholar.org/paper/c29b68d5697be2e6636f889a16dcf59b81d1c503","authors":[{"name":"Jacob Daniel Kirstejn Hansen","ids":["10713756"]},{"name":"François Lauze","ids":["1752860"]}],"doi":"10.1109/ICPR.2018.8545684"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594392","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Deep-sea robot operations demand a high level of safety, efficiency and reliability. As a consequence, measures within the development stage have to be implemented to extensively evaluate and benchmark system components ranging from data acquisition, perception and localization to control. We present an approach based on high-fidelity simulation that embeds spatial and environmental conditions from recorded real-world data. This simulation in the loop (SIL) methodology allows for mitigating the discrepancy between simulation and real-world conditions, e.g. regarding sensor noise. As a result, this work provides a platform to thoroughly investigate and benchmark behaviors of system components concurrently under real and simulated conditions. The conducted evaluation shows the benefit of the proposed work in tasks related to perception and self-localization under changing spatial and environmental conditions.","inCitations":[],"pmid":"","title":"Robust Continuous System Integration for Critical Deep-Sea Robot Operations Using Knowledge-Enabled Simulation in the Loop","journalPages":"1892-1899","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594392","https://arxiv.org/pdf/1803.02127v2.pdf","http://arxiv.org/abs/1803.02127"],"entities":[],"journalVolume":"","outCitations":["8033e0edb3b43c4ba3605d70d0de14efbe69c976","82b4b8a2c9bc1552abaf665ef1c6647f8b7e4ce1","56178de1001efe54792ad93f6980de5d5e91906b","0a5f208d02a17f18bcf54bdf3cc6515c2afc64fe","2b63ab1e4bc69277a705dee422b0c44d8173793c","aabb09bde7ec04ab4a3149a6304c251c097e76a1","a921c019ba5a615532dc38a6415c3d8539146452","5c3e3c9eb353c2662333b2cafbb67bfce4baa371","0e6a45d78f03c65714a35cf79fee81e5c0a914f5","1367ec152ada061fe74194921b7d9efdfc710dc1","54a1a3a77620ce8bb77482405ce53253fd20ddda","2151a214aca6e72ee2980ae8cbf7be47fed0cb7a","a9dfa06d4ab3a32dcf00e3cb3f264e11a44aa116","26f7d738df74539b8de1e158e1844e5c5cece47f","a9291aab826488eeeed46000a6d7d05d735204c4","130830f94fd7b8bcfa501fe3e853f77484022a94","44ab7327728d61cb8e385ac03a511b252be6cc8e","3545e83d22da465b9ba37a9e4549a56098f5f1c9","2d90537b709ae92e07856b93aab70614ac5af561","9c5da87411d0f3ab6726e0a7ee46f0ee23f2bbec","e089440c76b062bb1ae590704782c6002831b175","198c535c08dd81483e50a1df78a84be926132e28"],"id":"7f01afc7d816ccf6ee4f9b2263f5255ae615504b","s2Url":"https://semanticscholar.org/paper/7f01afc7d816ccf6ee4f9b2263f5255ae615504b","authors":[{"name":"Christian A. Mueller","ids":["48117831"]},{"name":"Tobias Fromm","ids":["2724306"]},{"name":"Arturo Gomez Chavez","ids":["16312430"]},{"name":"Daniel Koehntopp","ids":["40900811"]},{"name":"Andreas Birk","ids":["1708805"]}],"doi":"10.1109/IROS.2018.8594392"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545324","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Automatic pain intensity assessment has a high value in disease diagnosis applications. Inspired by the fact that many diseases and brain disorders can interrupt normal facial expression formation, we aim to develop a computational model for automatic pain intensity assessment from spontaneous and micro facial variations. For this purpose, we propose a 3D deep architecture for dynamic facial video representation. The proposed model is built by stacking several convolutional modules where each module encompasses a 3D convolution kernel with a fixed temporal depth, several parallel 3D convolutional kernels with different temporal depths, and an average pooling layer. Deploying variable temporal depths in the proposed architecture allows the model to effectively capture a wide range of spatiotemporal variations on the faces. Extensive experiments on the UNBC-McMaster Shoulder Pain Expression Archive database show that our proposed model yields in a promising performance compared to the state-of-the-art in automatic pain intensity estimation.","inCitations":[],"pmid":"","title":"Deep Spatiotemporal Representation of the Face for Automatic Pain Intensity Estimation","journalPages":"350-354","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1806.06793","https://export.arxiv.org/pdf/1806.06793","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545324","https://arxiv.org/pdf/1806.06793v1.pdf"],"entities":[],"journalVolume":"","outCitations":["d62d0505b37f3df983d97e39452137d31d44c94e","f38904f42caa7dccd71381dacb28e19734b1e23d","3c86dfdbdf37060d5adcff6c4d7d453ea5a8b08f","1c30bb689a40a895bd089e55e0cad746e343d1e2","55b81991fbb025038d98e8c71acf7dc2b78ee5e9","56ffa7d906b08d02d6d5a12c7377a57e24ef3391","ae42bb9cc18e104a59e5a1ab55cc7fc720b5e86c","c570f70459af243af9ca73709646239d82d07655","5ce2cb4c76b0cdffe135cf24b9cda7ae841c8d49","050fec788e385006b9f909b0511964e73bf81034","214eb90d0386379972cded05e9f57b884edb1675","00c05adcd5157edba775782464402c2bfb998c82","3ba179bceb9692d4d21109d0b87b120195761148","0daaa56d724c11e64338996e99a257fa69900236","53698b91709112e5bb71eeeae94607db2aefc57c","4fcd19b0cc386215b8bd0c466e42934e5baaa4b7","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","326b9c8391e89f5bd032aebd1b65e925083c269b","5f708d6ca934eff6114e29e63509c86aaaab8882","6794d73cb5f494e7f3e30e4d01acfb40a624ee17","b61a3f8b80bbd44f24544dc915f52fd30bbdf485","ab4e3ddf21ff23ab0b8354c286ca8e0a22196a96"],"id":"a377f2c99218e0eb624ba65d6711c640b1697b92","s2Url":"https://semanticscholar.org/paper/a377f2c99218e0eb624ba65d6711c640b1697b92","authors":[{"name":"Mohammad Tavakolian","ids":["2014145"]},{"name":"Abdenour Hadid","ids":["1751372"]}],"doi":"10.1109/ICPR.2018.8545324"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546021","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"This paper proposes a novel Nonnegative and Adaptive Multi-view Clustering (NAMC) method. NAMC integrates the nonnegative matrix factorization (NMF), adaptive neighborhood learning and consensus adaptive similarity matrix fusion. More specifically, NAMC performs the nonnegative weight learning over the original data and the parts-based representations of NMF for more accurate measure and representation. For nonnegative adaptive feature extraction, our model first utilizes NMF to obtain the local parts-based representation of the original high-dimensional data. To keep the local structure of parts-based representations, we minimize the adaptive neighborhood reconstruction error. Then the optimal consensus similarity matrix can be iteratively obtained according to the nonnegative adaptive similarity matrix of each view. What's more, the proposed NAMC is totally self-weighted. Once the target graph is obtained in our model, it can be partitioned into specific clusters directly. Extensive simulations show that NAMC can achieve good performance on several public databases for multi-view clustering, compared with other related methods.","inCitations":[],"pmid":"","title":"Nonnegative and Adaptive Multi-view Clustering","journalPages":"1247-1252","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546021"],"entities":[],"journalVolume":"","outCitations":["2c9f233b924fc3113ccc93e6a569018152541fb1","0547665b377691f63b8e2718617f830858cc6535","6308ac4dbd47e38103187a6780c65295532a6f07","2e37d6b9a02ec31ea22252c37beae410c6b609a8","f3e646977ab78178fe05eb64421b421171c1c6fa","0aba2f9821ca9bf1cad5cec17c5cb5afa5698f01","f88370e1207c9ee5257a895f5b587db30148454b","3d49a0840e7d2660df586dc99c8dd7289aede6ca","ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2","18ca2837d280a6b2250024b6b0e59345601064a7","14a3931d5aec9ee91fa6894dff3a61179b002b1c","ba9233dd7c81765b7afb2cc1a6e5e9a075518d8c","77c5f7da7710cd63ec193eb85f52b715b1ac6528","917e01d4be4a3417b2941ecd548186a6bf868358","774dd71f42cca20aa0eaf93fc337ab48fb123fd1","7fd700f4a010d765c506841de9884df394c1de1c","63d440eb606c7aa4ee3c7fcd94d65af3f5c92c96","2b2639ef3220e13dee1a9b477572711a3f2d06e6","521120c3907677e17708c17c5b6bab9087e61c5b","0728914a1dba0417bf2847548aa15711f3f8d4e8"],"id":"073741c9f4f40dd1cf491e7d62a3a19a4796d0fa","s2Url":"https://semanticscholar.org/paper/073741c9f4f40dd1cf491e7d62a3a19a4796d0fa","authors":[{"name":"Peng Zou","ids":["49402130"]},{"name":"Fanzhang Li","ids":["2314391"]},{"name":"Li Zhang","ids":["48571392"]}],"doi":"10.1109/ICPR.2018.8546021"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206096","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Disaster response crawler robot OCTOPUS has four arms and four flippers for better adaptability to disaster environments. To further improve the robot mobility and terrain adaptability in unstructured terrain, we propose a new locomotion control method called compound motion pattern (CMP) for multi-limb robots like OCTOPUS. This hybrid locomotion by cooperating the arms and flippers would be effective to adapt to the unstructured terrain due to combining the advantages of crawling and walking. As a preliminary study on CMP, we proposed a fundamental and conceptual CMP while clarifying problems in constructing CMP, and developed a semi-autonomous control system for realizing the CMP. Electrically-driven OCTOPUS was used to verify the reliability and correctness of CMP. Results of experiments on climbing a step indicate that the proposed control system could obtain relatively accurate terrain information and the CMP enabled the robot to climb the step. We thus confirmed that the proposed CMP would be effective to increase terrain adaptability of robot in unstructured environment, and it would be a useful locomotion method for advanced disaster response robots.","inCitations":[],"pmid":"","title":"A semi-autonomous compound motion pattern using multi-flipper and multi-arm for unstructured terrain traversal","journalPages":"2704-2709","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206096"],"entities":["Autonomous robot","Coat of arms","Control system","Correctness (computer science)","Experiment","Requirement","Robot combat","Semiconductor industry","Sensor","Simultaneous localization and mapping","Web crawler"],"journalVolume":"","outCitations":["24fef57dec21d45112cd80c97f3e4f15c93d79df","c9725dfe046636132e394f7dcc67914d61ef1126","13b3338a6b57ac2efa9f709952ea03378355cdf5","2d81296e2894baaade499d9f1ed163f339943ddc","41b14d07dbe451cda0b4c93548c7d3a01218f956","d6d79bdd73cd21364cdfbcf273c649d8e4c75d12","cd07e68d252d2b6033720a9b4819ff5d61c52117","248c266a19172f34ae95b8dd8efb74224e8473b1","c01f04c808a512ef0086d8dcdebea177d401cca0","21f3e61b19bf5dedea9338eed7bd03aba50021b2"],"id":"53a72c41800e9c9491ad64e253134c7bb06af4c6","s2Url":"https://semanticscholar.org/paper/53a72c41800e9c9491ad64e253134c7bb06af4c6","authors":[{"name":"Kui Chen","ids":["48543168"]},{"name":"Mitsuhiro Kamezaki","ids":["1989408"]},{"name":"Takahiro Katano","ids":["9051179"]},{"name":"Taisei Kaneko","ids":["26939300"]},{"name":"Kohga Azuma","ids":["26930521"]},{"name":"Tatsuzo Ishida","ids":["1794600"]},{"name":"Masatoshi Seki","ids":["2109742"]},{"name":"Ken Ichiryu","ids":["2136774"]},{"name":"Shigeki Sugano","ids":["2023682"]}],"doi":"10.1109/IROS.2017.8206096"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545108","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose multi-spectral fusion and denoising (MFD) of RGB and NIR images using multi-scale wavelet analysis. We formulate MFD of RGB and NIR images as a maximum a posterior (MAP) estimation problem in the wavelet domain. The direct fusion of noisy RGB and NIR image often leads to contrast attenuation due to the discrepancy between RGB and NIR images. Thus, we generate the wavelet scale map for fusion and denoising based on correlation between NIR and RGB wavelet coefficients. To consider local contrast and visibility of NIR data on RGB components, we provide the contrast preservation term for scale map estimation based on the local contrast and visibility. We use the regularization term to select high visibility and contrast of NIR wavelet coefficients in the scale map. Since noise generally appears in the high frequency band, we use gradients of NIR wavelet coefficients as the weight for weighted least square (WLS) smoothing in the scale map. Based on the wavlet scale map, we perform fusion and denoising of RGB and NIR wavelet coefficients. Experimental results show that the proposed method successfully performs fusion of RGB and NIR images with noise reduction and detail preservation as well as outperforms state-of-the-arts in terms of discrete entropy (DE) and feature-based blind image quality evaluator (FBIQE).","inCitations":[],"pmid":"","title":"Multi-Spectral Fusion and Denoising of RGB and NIR Images Using Multi-Scale Wavelet Analysis","journalPages":"1779-1784","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545108"],"entities":[],"journalVolume":"","outCitations":["39a87147b635ac8ff3a57f77ca065c4905602980","9377ff055de047c1e348d4e44439dc57782627e3","f8fc6bf201638680f694b678d66fcb183408c221","5e3d529598bb88d778ca5f4afdfea45a7c1eca52","4de273617d28f5a908e3e2e80de54eb8c0fbd79c","7051d67e219b2452f7e0a4db0cb23251835eb555","4f180d91a3524605a07c76f8aac0976e8212ecfd","25950c74b2dc7f20de856bb69eb7b7b2f13496f0","4ecd83f61ba9a8f5e03046b391c48e764a4f675f","4664df401fdee58da377473def8fa7a630477e40","84eb47699528e4c239bafa7b89640fbc83a3fb0d","cd7c6705b08e32c0cf9892a22c3b9ecbbf817240","21896d2113e552d992328a7812d29dfbf7ae5893","b7198a91d8b4a119d21bbf0cfe255fa939326a37","e6407b5a3c79de302227d5b909ada3766200ed80","ece70c094e27072a6c5ceaa89de60bdb75b22254","ad4a871287f705e02ef9f6159d588bdee67bf628","59cde223fad0c99133163b22f80799eaddd764e3","d5688ba4ee6f31ff406b7d3706b78b609ca4ef2a"],"id":"31b2953f79400c881491026fd91808372563a382","s2Url":"https://semanticscholar.org/paper/31b2953f79400c881491026fd91808372563a382","authors":[{"name":"Haonan Su","ids":["3403892"]},{"name":"Cheolkon Jung","ids":["48063346"]}],"doi":"10.1109/ICPR.2018.8545108"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206267","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this work, we concern the problem of motion planning for a hexapod walking robot crawling in a semi-structured environment where a precise foot-tip positioning is necessary. We propose pipelined approach utilizing an RGB-D camera to perceive and map the forthcoming terrain in 2.5 D which is then processed for available foot-tip positions. The robot motion control is based on sampling-based planning to determine the most suitable leg supporting configurations for the individual body positions in the created terrain map. The individual body positions are connected into a roadmap with taking into account a feasibility of the robot transition between the individual configurations. The resulting trajectory is then planned in the created roadmap using a standard A∗ planner. The proposed method has been experimentally evaluated in the on-line and onboard setup with a real hexapod crawling robot. The herein reported results support feasibility of the proposed approach for a precise motion planning of small hexapod crawling robot in a semi-structured environment.","inCitations":[],"pmid":"","title":"Foothold placement planning with a hexapod crawling robot","journalPages":"4096-4101","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206267"],"entities":["Algorithm","Experiment","Mobile robot","Motion planning","Online and offline","Pipeline (computing)","Sampling (signal processing)","Semiconductor industry","Simultaneous localization and mapping","Stepping level"],"journalVolume":"","outCitations":["7bbbbd2073503720f304d031cb4641eb45a3edee","5a53e6ce20e4d591b9ab02c6645c9dafc2480c61","1a87557b29dd5bbe747e3af94f657727b42cd7fd","7e98801ab0b438115ea4d86fdd4babe7ae178561","6e70a0b274c153678890741c31f3d2ffcb48cd1d","5f7389e313ff9fd9c0ec3b6e08624afc920a3f92","5f1997dad87037804e75ac3992e4fc099a168f15","9bc108b3d85765de9e0c4f14165b100f72307e66","989c334edccf3d39b02a6399405209e748f13693","4d90b7c13813654232d32a8eacd80ee7b4a6b38b","2d0d3506fa64cdff646367d3c533838a02f6e3d0","8dce587303ed0b4c4f0449a8b5b3dc32a2eceec8","49165e9587045a4d963b9c6c376df7f8bf9249a6","dbcba7083516a3cfb880951486a9727ae8614d4d"],"id":"ea640b246b2cb29b4e22dfff729d8fbbe2bbc193","s2Url":"https://semanticscholar.org/paper/ea640b246b2cb29b4e22dfff729d8fbbe2bbc193","authors":[{"name":"Petr Cizek","ids":["32618117"]},{"name":"Diar Masri","ids":["2331316"]},{"name":"Jan Faigl","ids":["1766210"]}],"doi":"10.1109/IROS.2017.8206267"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593541","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Traditional Simultaneous Localization and Mapping (SLAM) approaches build maps based on points, lines or planes. These maps visually resemble the environment but without any semantic or information about the objects in the environment. Recent advancements in machine learning have made object detection highly accurate and reliable with large set of objects. Object detection can effectively help SLAM to incorporate semantics in the mapping process. One of the main obstacles is data association between detected objects over time. We demonstrate a nonparametric statistical approach to solve the data association between detected objects over consecutive frames. Then we use an unsupervised clustering method to identify the existence of objects in the map. The complete process can be run in parallel with SLAM. The performance of our algorithm is demonstrated on several public datasets, which shows promising results in locating objects in SLAM.","inCitations":[],"pmid":"","title":"Localization of Classified Objects in SLAM using Nonparametric Statistics and Clustering","journalPages":"161-168","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593541"],"entities":[],"journalVolume":"","outCitations":[],"id":"5a9061145235a24d693720beac56134cb212f3a2","s2Url":"https://semanticscholar.org/paper/5a9061145235a24d693720beac56134cb212f3a2","authors":[{"name":"Asif Iqbal","ids":[]},{"name":"Nicholas R. Gans","ids":[]}],"doi":"10.1109/IROS.2018.8593541"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205989","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"There have been increasing demands for developing robotic system combining camera and inertial measurement unit in navigation task, due to their low-cost, lightweight and complementary properties. In this paper, we present a Visual Inertial Odometry (VIO) system which can utilize sparse depth to estimate 6D pose in GPS-denied and unstructured environments. The system is based on Multi-State Constraint Kalman Filter (MSCKF), which benefits from low computation load when compared to optimization-based method, especially on resource-constrained platform. Features are enhanced with depth information forming 3D landmark position measurements in space, which reduces uncertainty of position estimate. And we derivate measurement model to access compatibility with both 2D and 3D measurements. In experiments, we evaluate the performance of the system in different in-flight scenarios, both cluttered room and industry environment. The results suggest that the estimator is consistent, substantially improves the accuracy compared with original monocular-based MSKCF and achieves competitive accuracy with other research.","inCitations":["23845719d3325763b886ffdba7ac2bdeb2790483","c9bbe64ae797b8d522eac5cc115ac31e8e5491bf"],"pmid":"","title":"Depth enhanced visual-inertial odometry based on Multi-State Constraint Kalman Filter","journalPages":"1761-1767","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205989"],"entities":["Computation","Experiment","Global Positioning System","Kalman filter","Mathematical optimization","Odometry","Open-source software","Robot","Sparse matrix"],"journalVolume":"","outCitations":["2c8e95bc331024105cbde6f6918cda8493f263c8","2d5678dd885b4d7fd8ba84573f685eecc663f956","362f9af442e7242ec2f0b19a83ede6fa07637d37","8309ef1a1bb9d07549f8b770a968541e6033b3e9","833561f676dc427b5b562d6906fc8605454e2feb","e9d3be8fb36a22549e2c305b0d751446bc2826da","0be0c13803cd08e81b7adaada537e91222eb1491","4329bfa13125f5e1809aea8b26d90cfa47d610a2","f8e68a5dd2e990edc878c71fadd7064896880f1e","badf73b0415e03bbd86ef1abcc3d7ef3da8f6ef9","3a23e02ef7855544bf8f38274d2b523fff03e6eb","6f7892e29b76ba107ca3e6a923ce56dd3a7cd580","a13dc9739e4637599359d792fd60d511ab8a016e","53226f929c8affbf49c1bdb4cbe5e8ff1b05e0d4","7200dce06a410959760aca287c53557830f43fcf","042915c918b4cae7149661096eca9c2495a42867","1365d179232048c2d80f6f7f10330edb069015f0","8af1af95b6dac2481081e3e84c68ee91fe9d5403","2e7975fb0351638bd2646a217e5885ae56ca5cff","3bbcc8b1f81b350fd0e4aa832d52362d16a530f2","19a5e5b31e741d0f9cacf9c93de782184d24c947"],"id":"e4f5ea444c790ed90ecaba667c7ed6db21549b61","s2Url":"https://semanticscholar.org/paper/e4f5ea444c790ed90ecaba667c7ed6db21549b61","authors":[{"name":"Fumin Pang","ids":["9571964"]},{"name":"Zichong Chen","ids":["2172527"]},{"name":"Li Pu","ids":["49225833"]},{"name":"Tianmiao Wang","ids":["8269676"]}],"doi":"10.1109/IROS.2017.8205989"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594317","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We propose a force-feedback brake pedal with series elastic actuation to preserve the conventional brake pedal feel during cooperative regenerative braking. The novelty of the proposed design is due to the deliberate introduction of a compliant element between the actuator and the brake pedal whose deflections are measured to estimate interaction forces and to perform closed-loop force control. Thanks to its series elasticity, the force-feedback brake pedal can utilize robust controllers to achieve high fidelity force control, possesses favorable output impedance characteristics over the entire frequency spectrum, and can be implemented in a compact package using low-cost components. The applicability and effectiveness of the proposed series elastic brake pedal have been tested through human subject experiments that evaluate simulated cooperative regenerative braking scenarios with and without pedal feel compensation. The experimental results and responses to the accompanying questionnaire indicate that pedal feel compensation through the series elastic brake pedal can significantly decrease hard braking instances, improving safety and driver experience.","inCitations":[],"pmid":"","title":"A Series Elastic Brake Pedal to Preserve Conventional Pedal Feel under Regenerative Braking","journalPages":"1367-1373","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594317"],"entities":[],"journalVolume":"","outCitations":[],"id":"b0275a91d3343bc9443720725b61db8046858cea","s2Url":"https://semanticscholar.org/paper/b0275a91d3343bc9443720725b61db8046858cea","authors":[{"name":"Umut Caliskan","ids":[]},{"name":"Ardan Apaydin","ids":[]},{"name":"Ata Otaran","ids":[]},{"name":"Volkan Patoglu","ids":[]}],"doi":"10.1109/IROS.2018.8594317"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593660","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we address the problem of extrinsic calibration of a camera and a 3D Light Detection and Ranging (LiDAR) sensor using a checkerboard. Unlike previous works which require at least three checkerboard poses, our algorithm reduces the minimal number of poses to one by combining 3D line and plane correspondences. Besides, we prove that parallel planar targets with parallel boundaries provide the same constraints in our algorithm. This allows us to place the checkerboard close to the LiDAR so that the laser points better approximate the target boundary without loss of generality. Moreover, we present an algorithm to estimate the similarity transformation between the LiDAR and the camera for the applications where only the correspondences between laser points and pixels are concerned. Using a similarity transformation can simplify the calibration process since the physical size of the checkerboard is not needed. Meanwhile, estimating the scale can yield a more accurate result due to the inevitable measurement errors of the checkerboard size and the LiDAR intrinsic scale factor that transforms the LiDAR measurement to the metric measurement. Our algorithm is validated through simulations and experiments. Compared to the plane-only algorithms, our algorithm can obtain more accurate result by fewer number of poses. This is beneficial to the large-scale commercial application.","inCitations":[],"pmid":"","title":"Automatic Extrinsic Calibration of a Camera and a 3D LiDAR Using Line and Plane Correspondences","journalPages":"5562-5569","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593660","http://frc.ri.cmu.edu/~kaess/pub/Zhou18iros.pdf"],"entities":[],"journalVolume":"","outCitations":["490020c0d4fa1eb85fe353add5713e49f08c628d","653d13eaafc7f1b930af776abbc6a3fc49add266","626740c77d95b425134d24d23ab1b649664b48f3","1b9ba6c71113dc2e67f719fe1db5246a59db6eda","691d0a287a2515ebe5019cda498dcb6d24dfd5a4","dc71d6c56aae3ec9f13ca0e7be745ea6c23a6d19","7fc62b438ca48203c7f48e216dae8633db74d2e8","32424bf8dc906e09997e79b8c858834c8edbbbed","e0ed4defcc3917cd01f55e7104fec8576405eb22","f224613708b4c011d8946890caee34c395d1ba91","12ca4a3444cdda6aca7891d79c6bc48b218dc80d","9c6466b0f53149a1456951cb3461b85cac9151b7","b0b243b8748a76d71fbcc826f57afc074e4654ea","ad1350d0e742fd1c574a87616063e21f40898787","9b6f9e72a888fdaaa2b92a9a534b68dacbb93e13","1ab209db7b464520602a5871d5ec879252064c81","1552697d4ace68cf2f410e86dc897a51fd215f0e","4b70507f187ecb13a90322fa9647c9da75a764a9","04b4247e11f78c8efa94bab7a706a104723c08ca","577cc24e9fd5d107cbce4548567044ba9e0607c8","89a4618d9af12b98af656f9d620256e7cfcd4a1c","32544e6ff0229788181e1c157f192e38083a9e5d","140a6bdfb0564eb18a1f51a39dff36f20272a461","c2628cbdd834f1b460a3d9e664f80473f7a810d3","f99505e342bb8f22f939ffdd0f71326cea4edee2","281b8b22d3a2a1415e223068ca70d839a11805a2","c9b49b341d25fa057106bc2b3d13a7023870a141","b75419a5530cf1ba417078142c05d9b043ad56db","684ec0385d82e141837d481bc17c32c183bb64c5","0742676f24dc29df783905fa8d5d48cbfed7a253","b0a4bdc758b8d4b7aeed776580803704655cdf7f","03c48927c29c8a739343db9cb2cdd8b46b97d46b","708355d319a88485fdbbea3524104982b8cf37c2","f79481821a0d2b644b8b3c92a3f5cc924e41f9a4","4b684499d6b0e6da57bf5a1bfb4a873c0e6469bb","736e2be188fffec66d46f17f54422f53fca3dcf9"],"id":"6162ae2cb64b5d328999f4858bc1b434ede20fe0","s2Url":"https://semanticscholar.org/paper/6162ae2cb64b5d328999f4858bc1b434ede20fe0","authors":[{"name":"Lipu Zhou","ids":["38512773"]},{"name":"Zimo Li","ids":["2825680"]},{"name":"Michael Kaess","ids":["47640216"]}],"doi":"10.1109/IROS.2018.8593660"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546100","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"String recognition is one of the most important tasks in computer vision applications. Recently the combinations of convolutional neural network (CNN) and recurrent neural network (RNN) have been widely applied to deal with the issue of string recognition. However RNNs are not only hard to train but also time-consuming. In this paper, we propose a new architecture which is based on CNN only, and apply it to handwritten digit string recognition (HDSR). This network is composed of three parts from bottom to top: feature extraction layers, feature dimension transposition layers and an output layer. Motivated by its super performance of DenseNet, we utilize dense blocks to conduct feature extraction. At the top of the network, a CTC (connectionist temporal classification) output layer is used to calculate the loss and decode the feature sequence, while some feature dimension transposition layers are applied to connect feature extraction and output layer. The experiments have demonstrated that, compared to other methods, the proposed method obtains significant improvements on ORAND-CAR-A and ORAND-CAR-B datasets with recognition rates 92.2% and 94.02%, respectively.","inCitations":[],"pmid":"","title":"Handwritten Digit String Recognition using Convolutional Neural Network","journalPages":"3729-3734","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546100"],"entities":[],"journalVolume":"","outCitations":["54c8d9fb1c3c113b70d389be91c028ed4f196b45","8e9149ab00236d04db23394774e716c4f1d89231","4a939b28e40b5fab0e17f0184f242626ffd1d67a","6074c1108997e0c1f97dc3c199323a162ffe978d","722fcc35def20cfcca3ada76c8dd7a585d6de386","261a056f8b21918e8616a429b2df6e1d5d33be41","2c03df8b48bf3fa39054345bafabfeff15bfd11d","1f9c83420fb342d357b5c0c783f672d1e980d854","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","83174a52f38c80427e237446ccda79e2a9170742","45524d7a40435d989579b88b70d25e4d65ac9e3c","dd6b8ab87d6467a79c72dc632648f106b95ba8bc","0af2a9b6629315ef862cf84c225f392656a14e6b","14318685b5959b51d0f1e3db34643eb2855dc6d9","b38ec88597bc19372e945e0de1c183a6eb61684f","2116b2eaaece4af9c28c32af2728f3d49b792cf9","1e5ba98b7d672bd676f4f46eec8890103ed145f3","501d99e392783e4acafb220136d32ea68a921282","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","061356704ec86334dbbc073985375fe13cd39088","01489d3a861f7dc1b2a68e8e9ec093d1c8f9373f"],"id":"873c9a16c848aa4053358b4e5f578cb600e70dad","s2Url":"https://semanticscholar.org/paper/873c9a16c848aa4053358b4e5f578cb600e70dad","authors":[{"name":"Hongjian Zhan","ids":["3492133"]},{"name":"Shujing Lyu","ids":["52179322"]},{"name":"Yue Lu","ids":["50028535"]}],"doi":"10.1109/ICPR.2018.8546100"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594506","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Typical inference approaches that work with high-dimensional visual measurements use hand-engineered image features (e.g, SIFT) that require combinatorial data association, or predict only hidden state mean without considering its uncertainty and multi-modality aspects. We develop a novel approach to infer system hidden state from visual observations via CNN features which are outputs of a CNN classifier. To that end, at pre-deployment stage we use neural networks to learn a generative viewpoint-dependent model of CNN features given the robot pose and approximate this model by a spatially-varying Gaussian distribution. Further, at deployment this model is utilized within a Bayesian framework for probabilistic inference, considering a robot localization problem. Our method does not involve data association and provides uncertainty covariance of the final estimation. Moreover, we show empirically that the CNN feature likelihood is unimodal which simplifies the inference task. We test our method in a simulated Unreal Engine environment, where we succeed to retrieve high-level state information from CNN features and produce trajectory estimation with high accuracy. Additionally, we analyze robustness of our approach to different light conditions.","inCitations":[],"pmid":"","title":"Bayesian Information Recovery from CNN for Probabilistic Inference","journalPages":"7795-7802","s2PdfUrl":"","pdfUrls":["https://indelman.github.io/ANPL-Website/Publications/Kopitkov18iros_slides.pdf","https://indelman.github.io/ANPL-Website/Publications/Kopitkov18iros.pdf","https://doi.org/10.1109/IROS.2018.8594506"],"entities":[],"journalVolume":"","outCitations":["8cab3bd872a631d2552754be340018e435bd358b","1ce512c9897e46c692c9fcd69b9664a68264a5e1","11f73583ba373487967225ae4797d723ff367c1c","5b3d8922953fede4435e7acd4032831c3a979c85","692e41680451ab9c13de39c1a9c70e151f5e7341","305c819a7c77d083e2c3b7224735dd74cd7a99e9","127316fbe268c78c519ceb23d41100e86639418a","0e87609a15af0f6f0b0449a6f9fef7a234093189","289546f1cc31323fa73b694a32cecebe18e0652b","1d827e24143e5fdfe709d33b7b13a9a24d402efd","26d3887193808875115f68c7fd8ef9e86659fd3b","8aa83912c15fd88dd13ced861e3c60f75a7c27e5","2153fb8ba5b10961da93ba5ece69114862e0fe56","1c7c46c220d29eef2201fc0b763b6d08ac745ef6","4454a51c0cf8f84cd28911b26a718ddba003133d","041a09a9db9a318596b72d041a832b05100b0ddf","54bfd42964f85355765a3de5ef6a464ca8aa00c3","0626908dd710b91aece1a81f4ca0635f23fc47f3"],"id":"4868dae76607bde36f6abe2a0e3e9c7fee9d43a1","s2Url":"https://semanticscholar.org/paper/4868dae76607bde36f6abe2a0e3e9c7fee9d43a1","authors":[{"name":"Dmitry Kopitkov","ids":["8139091"]},{"name":"Vadim Indelman","ids":["2784557"]}],"doi":"10.1109/IROS.2018.8594506"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206583","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"A popular class of lidar-based grid mapping algorithms computes for each map cell the probability that it reflects an incident laser beam. These algorithms typically determine the map as the set of reflection probabilities that maximizes the likelihood of the underlying laser data and do not compute the full posterior distribution over all possible maps. Thereby, they discard crucial information about the confidence of the estimate. The approach presented in this paper preserves this information by determining the full map posterior. In general, this problem is hard because distributions over real-valued quantities can possess infinitely many dimensions. However, for two state-of-the-art beam-based lidar models, our approach yields closed-form map posteriors that possess only two parameters per cell. Even better, these posteriors come for free, in the sense that they use the same parameters as the traditional approaches, without the need for additional computations. An important use case for grid maps is robot localization, which we formulate as Bayesian filtering based on the closed-form map posterior rather than based on a single map. The resulting measurement likelihoods can also be expressed in closed form. In simulations and extensive real-world experiments, we show that leveraging the full map posterior improves the localization accuracy compared to approaches that use the most likely map.","inCitations":["0890c600587e7b3a2232c413ffbcb5b587687cfa","3a451858e65f07c4b68287050b2a43a7817068a7"],"pmid":"","title":"Closed-form full map posteriors for robot localization with lidar sensors","journalPages":"6678-6684","s2PdfUrl":"","pdfUrls":["http://ais.informatik.uni-freiburg.de/publications/papers/luft17iros.pdf","https://doi.org/10.1109/IROS.2017.8206583"],"entities":["Algorithm","Computation","Diffuse reflection","Experiment","Map","Robot","Robotic mapping","Sensor","Simulation","Simultaneous localization and mapping","Whole Earth 'Lectronic Link"],"journalVolume":"","outCitations":["93f669f5dfbeefc85a62cc2b207a71e111898d9b","30d86789481968007d2255d2d887bd85f243a045","5da0f1e4f4a7907efd0b95c6730174a09ae1656c","7830529fa077c7063b82a69fc444974cffed9a10","0c4c4ddd51e9f023414fed769ec065e9bf6acbef","9f37a257382d442f905d3a7246a6fe694f3ac1a1","0bc7d8572a88123b60295932263bb43541363b0a","57c1321fdafd33fa223af6e36494a17bc03e5d0a","0df9a304101921ef633e2a278044c7cd3d5f1da9","0f36625ca99359b96d6736a0027c5244f4484f5c","8dfe13156288a877e2db0a79398d047e27946b99","0501ae2b5c3ffa0779337865b2dbb670e88fe907","b15a8415b64daf108f8041fa77edabe137e7494e","090b0e7478b8189e511b2fe34170c73a9b6a2838"],"id":"9cba7166d213334c84e6132ffd56347d3452bced","s2Url":"https://semanticscholar.org/paper/9cba7166d213334c84e6132ffd56347d3452bced","authors":[{"name":"Lukas Luft","ids":["40535272"]},{"name":"Alexander Schaefer","ids":["47968778"]},{"name":"Tobias Schubert","ids":["37576719"]},{"name":"Wolfram Burgard","ids":["1725973"]}],"doi":"10.1109/IROS.2017.8206583"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593587","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Limitations with the on-board computational resources installed on untethered robots such as humanoids and mobile robots in general affects significantly the performance and capabilities of these machines. An approach to address this issue is to make use of the cloud robotics concept and take advantage of the extensive computational resources of the cloud. XBotCloud is a recently developed component of the XBot framework. It tackles the above challenges by introducing the tools and mechanisms to enable users and robots to exploit the computational resources of the cloud allowing the execution of services with low, soft or hard Real-Time execution/communication performance. The latter is ensured thanks to the functionality provided by the XBotCore Real-Time cross-robot software component of the XBot framework. XBotCloud addresses also one of the main challenges related with cloud robotics: security. To avoid remote attacks it takes advantage of the Amazon Web Services (AWS)Cloud Security and it uses an internal VPN Network to handle the connectivity between the robot and the cloud server. The full implementation of the framework is presented and its functionality is demonstrated in realistic tasks involving pipelines that mix the execution of cloud services with moderate execution time constraints and Real-Time modules running on the robot local control unit. XBotCloud performances and cross-robot flexibility are experimentally validated on two different robotic platforms, the WALK-MAN humanoid and the CENTAURO upper body / full-body.","inCitations":[],"pmid":"","title":"XBotCloud: A Scalable Cloud Computing Infrastructure for XBot Powered Robots","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593587"],"entities":[],"journalVolume":"","outCitations":[],"id":"ad430e7b0be74534daa95afe6aa9dc78786223a3","s2Url":"https://semanticscholar.org/paper/ad430e7b0be74534daa95afe6aa9dc78786223a3","authors":[{"name":"Luca Muratore","ids":[]},{"name":"Barry Lennox","ids":[]},{"name":"Nikolaos G. Tsagarakis","ids":[]}],"doi":"10.1109/IROS.2018.8593587"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206196","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Learning complex robot manipulation policies for real-world objects is challenging, often requiring significant tuning within controlled environments. In this paper, we learn a manipulation model to execute tasks with multiple stages and variable structure, which typically are not suitable for most robot manipulation approaches. The model is learned from human demonstration using a tactile glove that measures both hand pose and contact forces. The tactile glove enables observation of visually latent changes in the scene, specifically the forces imposed to unlock the child-safety mechanisms of medicine bottles. From these observations, we learn an action planner through both a top-down stochastic grammar model (And-Or graph) to represent the compositional nature of the task sequence and a bottom-up discriminative model from the observed poses and forces. These two terms are combined during planning to select the next optimal action. We present a method for transferring this human-specific knowledge onto a robot platform and demonstrate that the robot can perform successful manipulations of unseen objects with similar task structure.","inCitations":["3f094bb07199169b901700f8bfc194b4823be707","bb9cca683f356985e5e42b0678269104f753068d","8f4e4eb91ce517f0df32ec239259b0e72a82107f","15010f6bb7e41e2ee0bf3571050ae6e17f5a9a13","998eadc85cb13876de65a5bb89de11fd9431027d","d77d2925eee76dbb41d7c2fbda138b2f7beeec62","e8b3a257a0a44d2859862cdec91c8841dc69144d","4bfe4cc6c91bfbc65bd0e084449a73f853844490","264a0bd8a44e1c1580ad828b1979aebbdba6989b"],"pmid":"","title":"Feeling the force: Integrating force and pose for fluent discovery through imitation learning to open medicine bottles","journalPages":"3530-3537","s2PdfUrl":"","pdfUrls":["http://web.cs.ucla.edu/~syqi/publications/iros2017openbottle/iros2017openbottle.pdf","http://www.stat.ucla.edu/~sczhu/papers/Conf_2017/IROS2017_OpenBottle.pdf","https://fen9.github.io/papers/IROS17_OpenBottle.pdf","http://www.mjedmonds.com/papers/IROS17_OpenBottle_final.pdf","https://doi.org/10.1109/IROS.2017.8206196"],"entities":["Baseline (configuration management)","Bottom-up parsing","Bottom-up proteomics","Causal filter","Causality","Cluster analysis","Correspondence problem","Counterfactual conditional","Dictionary","Discriminative model","Electrical engineering","Experiment","Replication (computing)","Robot","SIM lock","Stochastic grammar","Supervised learning","Top-down and bottom-up design","Turing completeness","Wired glove"],"journalVolume":"","outCitations":["04cbc0118dd26d30f9742e613a3f41845b99d193","e9b1c93223ea7f7c8a835bd588a39d4aae45a5fe","b4488bb6787bd2546ba25efe069f6a20db08c06a","106cd2e5f9dee97ae5bcd54ecaf038245990cff4","0643c960e0af066f95d8a634fad40c55fdf50a47","605a4b4ec65bd6bf3528cdcd09390a4cc5d334a5","220bdd265e6721e1d7ec1c4252aa41825147e61b","c34c98a4033ded0de348c2486df0c1787013891b","578e391656372fffa81e57504df3fa51652bc1bf","d94e456e8f29b6247a066e418f2d8cde23d3a9cb","1e32d052e3be45dff46fbc5011793a21d2398011","75322fdaff24e98748eaae16fa0d474ab0972565","131809b84364158b434f40f61be14feecd352d1d","016335ce7e0a073623e1deac7138b28913dbf594","b6073929c8c449332a6c5a6d9143b85fa9958d87","2a02d9727187757dde59cf60cc252d51e67f75ad","d45eaee8b2e047306329e5dbfc954e6dd318ca1e","64034d5c2b82e81dd357fd32e3b84667b5c5472f","cf776f343046fc63049b1f6814bfbcc0ea70e9d0","299af7d4fe6da8ac0b390e3ce45c48f7a8b5bb37","0ba7c35ddeacac74f06ece9321f35cf9235dc15a","857eb48ebc2d8e4a12b3a1745d9dfe9853c2a312","fa635c9140799c5026ccfd444275d4ec29dfac8e","5d9ee01ca729e3f838ab3e7c82c1e03d38c69085","6f8c84c58edecf69bf7e3bbdbb9dd324ca759622","6b7d3c9bfc636cf914447d3f0bb834d5a1ccc58e","2390379714e7b003c811f81b92bd5f0f765f3b7a","bcc81c8f0acea47f659f963f6716a3475e889b19","23f8e2e0134f9c3d3d1cee861f3f856d936cc917","0c196122410cacc1509830e0dbae6a53b6bdfe78","8de1eaefba91bd4c9ea0349eb2d642850ce097bf","2f2c9a545fbbd375772e6f1af836e8e00f6527af","6d00a63e36065af5cf0d39ce3a567881170aa2e5","86fe7c1df3be45950249184a717c554361491150","0d86ab901cc2082c57770dac545d8564474a4d0b","82673205bf76c6fc788790308bc14a9a2d8e41ad","17eddf33b513ae1134abadab728bdbf6abab2a05","18260bc6754361a2396b787d7ed7cfc769269c10","74ca2dfa0fb4a60a13ec14c3909cfda130c19ca5","11b6bdfe36c48b11367b27187da11d95892f0361","0674d626264fe191cf81cab3de33d59b67a86b71","7cad9c8c96d3fd652180b090039e5c401034429a","51c2b7b7c5235419dced9e324403d39b8afd738f","28beed9d247ca1b77720dea8b44039f0185dd69b","1c5abc4b6ae4757be3f8f61c0b5bd01a3d0e006a","264a0bd8a44e1c1580ad828b1979aebbdba6989b","a90deeb41b811461f8e0a62a82e8b0776e5b90dd","248040fa359a9f18527e28687822cf67d6adaf16","b7bb365a55b6a71f15c3ce28824cffe98506fb53","711a480059fc4ae9e8e2b09d59bc20b5e4aa24f3","346484b2c15cd0e49a866d426376e88ac27f0c51"],"id":"e8fe71704970bcdca5ae254670cf449a51a909f9","s2Url":"https://semanticscholar.org/paper/e8fe71704970bcdca5ae254670cf449a51a909f9","authors":[{"name":"Mark Edmonds","ids":["37156109"]},{"name":"Feng Gao","ids":["50433697"]},{"name":"Xu Xie","ids":["49418998"]},{"name":"Hangxin Liu","ids":["3440176"]},{"name":"Siyuan Qi","ids":["3390244"]},{"name":"Yixin Zhu","ids":["2672448"]},{"name":"Brandon Rothrock","ids":["1904850"]},{"name":"Song-Chun Zhu","ids":["3133970"]}],"doi":"10.1109/IROS.2017.8206196"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593992","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Interpreting the brain activity to identify user goals or to ground a robot's hypotheses about them is a promising direction for non-intrusive and intuitive communication. Such a capability can be of particular relevance in the context of human-robot cooperation scenarios. This paper proposes a novel approach to utilize the natural brain responses to highlighted objects in the scene for object selection. By this, it circumvents the need for additional interfaces or user training. Our approach uses methods from information geometry to classify the target/non-target response of these event-related potentials. Online experiments carried out with a real robot demonstrate an accurate detection of target objects solely based on the user's attention.","inCitations":[],"pmid":"","title":"Guess What I Attend: Interface-Free Object Selection Using Brain Signals","journalPages":"7111-7116","s2PdfUrl":"","pdfUrls":["http://ais.informatik.uni-freiburg.de/publications/papers/kolkhorst18iros.pdf","https://doi.org/10.1109/IROS.2018.8593992"],"entities":[],"journalVolume":"","outCitations":["8395db44a6673ffca825a2175f48a42acdd5c818","c3ffcdafb5886ef4d9877b9f7ff651ca62adf1d2","174515613a4d200dffb07a01db3cb2daadd8d3c6","262dfa5b7d7bf40de21a252eab69af9fd87337a7","30b25dcbe04cbc5c7ee88cd48fa2a3b24779e8af","264e0834468fe400c5745bddba74749ce75bca9c","24b90fd6534c825413a687374a3279a3f718372c","f953eef0dc0a8292513b57a238ee2c2b25e008ad","d6cb64abd546dd0b528c77613d1ec5a8eca87a78","0c059a3c6ee88d4f5850c3cc44382149cc7054fa","8ab1d2a6947e123f682a30bfd5aa8fdbd4dabda3","d730dbcf28271bc1c782e068d137942822c00423","ee3adda283061ea7fa8806dbc284e0507e2947e8","247b37f24a21aa842d7ed2158d2af0caaaaac32f","5f7d5ba8477a81986a91f51a21fbad3283061308","06a6b4a5b79aa54d9bffed2802809d9f71ba2afb","12fd36c7a9794cc82689ff30ed503b649dd57cb0","d6de0ba81008537f8b581ddf562a5d908283241f"],"id":"a96b30a64fc9311204ae08718f8db02349b3f98a","s2Url":"https://semanticscholar.org/paper/a96b30a64fc9311204ae08718f8db02349b3f98a","authors":[{"name":"Henrich Kolkhorst","ids":["2535124"]},{"name":"Michael Tangermann","ids":["2345823"]},{"name":"Wolfram Burgard","ids":["1725973"]}],"doi":"10.1109/IROS.2018.8593992"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206282","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In recent years, hydrofoils have become ubiquitous and critical components of high-performance surface vehicles. Twenty-meter-long hydrofoil sailing craft are capable of reaching speeds in excess of 45 knots. Hydrofoil dinghies routinely travel faster than the wind and reach speeds up to 30 knots. Besides, in the quest for super-maneuverability, actuated hydrofoils could enable the efficient generation of large forces on demand. However, the control of hydrofoil systems remains challenging, especially in rough seas. With the intent to ultimately enable the design of versatile, small-scale, high-speed, and super-maneuverable surface vehicles, we investigate the problem of controlling the lift force generated by a flexible, surface-piercing hydrofoil traveling at high speed through a random wave field. We present a test platform composed of a rudder-like vertical hydrofoil actuated in pitch. The system is instrumented with velocity, force, and immersion depth sensors. We carry out high-speed field experiments in the presence of naturally occurring waves. The 2 cm chord hydrofoil is successfully controlled with a LTV/feedback linearization controller at speeds ranging from 4 to 10+ m/s.","inCitations":["6aedf4bacbe4aee63b9390e2d72c50bcd70bc43a"],"pmid":"","title":"Control of a flexible, surface-piercing hydrofoil for high-speed, small-scale applications","journalPages":"4203-4208","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206282"],"entities":["Experiment","Immersion (virtual reality)","Morphing","Offset binary","Pitch (music)","Rudder","Sensor","Synthetic Environment for Analysis and Simulations","Velocity (software development)"],"journalVolume":"","outCitations":["b12b86a44557903de0f650faf7617165769b7b1e","a26058bae1280bc7a4d2bcdd6f4d1c6d3d5b2dc4","7ae9c497d27c9d1b78e485afc202b565769cc3b0","e4a76113c413febf2f6651c7413f73c566a5e810","91877b1912412f703769947899dc76ab9b050d60","9758288b22d9f55a5b0e6a77a7138ecc5e9bfb28"],"id":"ebdf053d9f03a120b551d5183b6c10295a760648","s2Url":"https://semanticscholar.org/paper/ebdf053d9f03a120b551d5183b6c10295a760648","authors":[{"name":"Gabriel D. Bousquet","ids":["31873570"]},{"name":"Michael S. Triantafyllou","ids":["2280688"]},{"name":"Jean-Jacques E. Slotine","ids":["1740591"]}],"doi":"10.1109/IROS.2017.8206282"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545751","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"To diagnose various autoimmune diseases, the accurate Human Epithelial-2 (HEp-2) cell image classification is a very important step. Automatic classification of HEp-2 cell using microscope image is a highly challenging task due to the strong illumination changes derived from the low contrast of the cells. To address this challenge, we propose a deep residual network (ResNet) based framework to recognize HEp-2 cell automatically. Specifically, a residual network of 50 layers (ResNet-50) with substantial deep layer is adopted to acquire the informative feature for accurate recognition. To further boost the recognition performance, we devise a novel ResNet-based network with deep supervision. The deeply supervised ResNet (DSRN) can address the optimization problem of gradient vanishing/exploding and accelerate the convergence speed. DSRN can directly guide the training of the lower and upper levels of the network to counteract the effects of unstable gradient variations by the adverse training process. As a result, DSRN can extract more discriminative features. Experimental results show that our proposed DSRN method can achieve an average classification accuracy of 93.46% and 95.88% on ICPR20l2 and ICPR20l6- Taskl datasets, respectively. Our proposed method outperforms the traditional methods as well.","inCitations":[],"pmid":"","title":"Deeply Supervised Residual Network for HEp-2 Cell Classification","journalPages":"699-703","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545751"],"entities":[],"journalVolume":"","outCitations":["a916578bf7d3f39f8a9ffdb73a83db383b15b822","25470f652218cf8e59aae89d4b95de8107bffa3c","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","eb96362636bd423a8cf4a18abe11765d4d749df4","820c28c9231be954620c706f5be321ff9f702c6d","10eb7bfa7687f498268bdf74b2f60020a151bdc6","14b5e8ba23860f440ea83ed4770e662b2a111119","06b4d8409837dce9d6eb919efd1debdaecc40d01","a8e8f3c8d4418c8d62e306538c9c1292635e9d27","fa77f1d60080c63a3785e43c87fede2fb99e4255","66d95081b0e23aa5fb34c6d3fcd47403f74dc939","7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7","a9a4f0668ea5ade518c5613da6ad5126c9c098fd","081651b38ff7533550a3adfc1c00da333a8fe86c","2c03df8b48bf3fa39054345bafabfeff15bfd11d","088bfd70085dc7112e45de113c6756cd68ccc254","5febe17309b8c77e39cc5d495c6ffd6ec3be2f3b","e69b1314cd65a115c98082a5863b92daa4dcf9f0","72b1108e7cb34668c82d5167dcd4a9488a7c339a","ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","1ad7730c91fb6664e33f5727e39579b107002155"],"id":"a4b83afedf35019b4d101e378d746093474c509d","s2Url":"https://semanticscholar.org/paper/a4b83afedf35019b4d101e378d746093474c509d","authors":[{"name":"Hai Xie","ids":["48327519"]},{"name":"Yejun He","ids":["34854855"]},{"name":"Haijun Lei","ids":["40493897"]},{"name":"Tao Han","ids":["2512667"]},{"name":"Zhen Yu","ids":["47055363"]},{"name":"Baiying Lei","ids":["40216538"]}],"doi":"10.1109/ICPR.2018.8545751"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545182","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"We propose ALFA - a novel late fusion algorithm for object detection. ALFA is based on agglomerative clustering of object detector predictions taking into consideration both the bounding box locations and the class scores. Each cluster represents a single object hypothesis whose location is a weighted combination of the clustered bounding boxes. ALFA was evaluated using combinations of a pair (SSD and DeNet) and a triplet (SSD, DeNet and Faster R-CNN) of recent object detectors that are close to the state-of-the-art. ALFA achieves state of the art results on PASCAL VOC 2007 and PASCAL VOC 2012, outperforming the individual detectors as well as baseline combination strategies, achieving up to 32% lower error than the best individual detectors and up to 6% lower error than the reference fusion algorithm DBF - Dynamic Belief Fusion.","inCitations":[],"pmid":"","title":"ALFA: Agglomerative Late Fusion Algorithm for Object Detection","journalPages":"2594-2599","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545182"],"entities":[],"journalVolume":"","outCitations":["009fba8df6bbca155d9e070a9bd8d0959bc693c2","7d39d69b23424446f0400ef603b2e3e22d0309d6","6c9f45c76b4f96fe66d8e1d7b31f89b7cc6caa44","95a213c530b605b28e1db4fcad6c3e8e1944f48b","cc3bd45efe9a6db3c4dc06c83ab2d63605696fc4","abe9f3b91fd26fa1b50cd685c0d20debfb372f73","1bf5d581ed03ca103b5fbab25afdd61fde92d798","061356704ec86334dbbc073985375fe13cd39088","21a1654b856cf0c64e60e58258669b374cb05539","2c03df8b48bf3fa39054345bafabfeff15bfd11d","0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a","20a78d3145279dcd799cd7a856ae2714f4863a16","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a","3eaa860f2735fce8b839237397455c13dfad1ed1","424561d8585ff8ebce7d5d07de8dbf7aae5e7270"],"id":"9e64582b644481a734b018304a22b536886ed024","s2Url":"https://semanticscholar.org/paper/9e64582b644481a734b018304a22b536886ed024","authors":[{"name":"Evgenii Razinkov","ids":["52215203"]},{"name":"Iuliia Saveleva","ids":["52208352"]},{"name":"Jiri Matas","ids":["1691679"]}],"doi":"10.1109/ICPR.2018.8545182"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206272","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Interactive agents are becoming increasingly common in many application domains, such as education, healthcare and personal assistance. The success of such embodied agents relies on their ability to have sustained engagement with their human users. Such engagement requires agents to be socially intelligent, equipped with the ability to understand and reciprocate both verbal and non-verbal cues. While there has been tremendous progress in verbal communication, mostly driven by the success of speech recognition and question-answering, teaching agents to appropriately react to facial expressions has received less attention. In this paper, we focus on non-verbal facial cues for face-to-face communication between a user and an embodied agent. We propose a method that automatically learns to update the agent's facial expressions based on the user's expressions. We adopt a learning scheme and train a deep neural network on hundreds of videos, containing pairs of people engaging in a conversation, and without external human supervision. Our experimental results show the efficacy of our model in sustained long-term prediction of the agent's facial landmarks. We present comparative results showing that our model significantly outperforms baseline approaches and provide insightful human studies to better understand our model's qualitative performance. We release our dataset to further encourage research in this field.","inCitations":[],"pmid":"","title":"Learn2Smile: Learning non-verbal interaction through observation","journalPages":"4131-4138","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206272"],"entities":["Artificial neural network","Baseline (configuration management)","Deep learning","Embodied agent","Intelligent agent","Norm (social)","Question answering","Speech recognition"],"journalVolume":"","outCitations":["595a58fcc8d10956bcb423bea5f9d91420813edf","863996000fab53f76e73c083b3c6d14eea5efd76","50832083df91f83333364544b24f8af4798eca32","24959d1a9c9faf29238163b6bcaf523e2b05a053","860287296e960dcc54508813b9bd55c89f5c23ea","1f0b26c1dd7fa80af8592e3836768402c68127d2","e5bed1cfa48711241548e5914535290da0fddc83","b33e8db8ccabdfc49211e46d78d09b14557d4cba","a76415c475a8e40ded6697982ebbe7b6141505ca","64fe14cb57889f95b770b088dc846671ea644769","e8222b4f2460bfeb3db8a8820be5c03441177838","0f88de2ae3dc2ec1371d1e9f675b9670902b289f","24bf94f8090daf9bda56d54e42009067839b20df","81a2908e4a138585ac3e0a15abce692b69c76817","083ac856e443bb0cb18420c31407c9fe6231e1f3","d0a21f94de312a0ff31657fd103d6b29db823caa","303e2a546cf0a32931a0810a6900d921f5325c6a","21dff32ff0f6b1244ce9c4aad2f468889748fbb2","681fa031a1076429653dd8f1204bb4fca1f4f273","33bdbe00c532ee1a4a51a84a6c259f1d15efbccc","188ece11e8152122ad5363622fae04477bcd03cd","43784c3cd584b3a51e5f22cc59c22bc2ab71b389","4641a8b7143582990cb3610864bc226a58ae2144","2bccd2ed38f10bec8a228548dbdcf9a2b317c62a","2fda461869f84a9298a0e93ef280f79b9fb76f94","e80635b9b48df5ad263c51ecec62d7d4bd7327fd","4b68d63bcbaa2c776f42c25718b6e2f3dedcf18a","5789b22b5bf17861e2b62e2ea75cb3223b5ea3f3","8386839cf792836efe9e3ff0e9e7891d8a479c00","56573e975f24b096830a8640829681e7a70da387","6074c1108997e0c1f97dc3c199323a162ffe978d","38192a0f9261d9727b119e294a65f2e25f72d7e6","7d2f4a909082fecdb0e94f64f670950d4e72b554","67fd4f209aa6e8359fc86bdc12c62bbdb0529077"],"id":"3a6088445a4ca04ab4e362b371d48d434012299e","s2Url":"https://semanticscholar.org/paper/3a6088445a4ca04ab4e362b371d48d434012299e","authors":[{"name":"Will Feng","ids":["11636339"]},{"name":"Anitha Kannan","ids":["39248118"]},{"name":"Georgia Gkioxari","ids":["2082991"]},{"name":"C. Lawrence Zitnick","ids":["1699161"]}],"doi":"10.1109/IROS.2017.8206272"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202196","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Caterpillars are the soft bodied larvae of lepidopteran insects. They have evolved to occupy an extremely diverse range of natural environments and to locomote in complex three-dimensional structures without articulated joint or hydrostatic control. These animals make excellent bio-inspiration for the field of soft robotics because of their diversity and adaptability. In this paper, we present SquMA Bot, a caterpillar-inspired soft robot. The robot's body is primarily composed of a soft viscoelastic foam, and it is actuated using a motor-tendon system. SquMA Bot is able to mimic the inching gait of a caterpillar and can use its flexible body to adapt to a range of environments. This bio-inspired prototype demonstrates the effectiveness of a soft robot as a potential tool for exploring environments too dangerous for humans.","inCitations":["1c7699a87e5f99d44b67fb956172cae9f670cb2d"],"pmid":"","title":"Soft foam robot with caterpillar-inspired gait regimes for terrestrial locomotion","journalPages":"476-481","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202196"],"entities":["British Informatics Olympiad","Prototype","Robot","Soft robotics","Terrestrial television","Video game bot"],"journalVolume":"","outCitations":["e55400c20677fe47832c11f6c89eae5f687da3b6","946d5aaa50cf1707db447cdbef02f35c751f177e","022c3497cc5b8ef445cb32c7bfcb2106450cb9f9","d9789c8efab2763c133777e1cec6fb948199dbda","7595e47011c1df1a929c37588c9185c742c8790c","e19c2178e5ebc3669c3b10531d9cf0fb791ce19a","c1d4d02c7ee76669c05e92562efe2025e3c728e3","4864fa3a5a6dc95ab8b205329b93f37ab7875a84","08bb6e47c25283b78c6d237212bc2fcbf06fb4cb","891dab797b8fead0ac1b49b8495d2f7f1217f234","933fd2eaf2f4cd2c6bd50680594886e4bd1547b3","854d559fd7073da5b82d6fdfae627f8f26985cdf","9c42648e1117ab1e40dbccd4d0d8aa302c613bff","cc92343c7349b08b3b19868791e6a8eb2adc2430","49a4cefeebf8bb771b0f1efda6ad5d68194eeb35","9fda00d89fc7b88be3d4ca8c418041d113b64dac","853a8fcb2296a9191faf55f601a53a8a2d403ca2"],"id":"ca109788882fda655b6aae22eef854522c08c0fa","s2Url":"https://semanticscholar.org/paper/ca109788882fda655b6aae22eef854522c08c0fa","authors":[{"name":"Cassandra M. Donatelli","ids":["35260805"]},{"name":"Zachary T. Serlin","ids":["31216448"]},{"name":"Piers Echols-Jones","ids":["31446688"]},{"name":"Anthony E. Scibelli","ids":["15603038"]},{"name":"Alexandra Cohen","ids":["31040823"]},{"name":"Jeanne-Marie Musca","ids":["3213745"]},{"name":"Shane Rozen-Levy","ids":["32539774"]},{"name":"David Buckingham","ids":["2418196"]},{"name":"Robert White","ids":["49091134"]},{"name":"Barry A. Trimmer","ids":["3207626"]}],"doi":"10.1109/IROS.2017.8202196"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206236","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Automated assembly operations have traditionally been carried out using \u201chard automation\u201d, where all parts to be handled during the operation are located at known positions with very high accuracy. Such automation solutions are often mechanically very expensive to implement and also the programming time can be substantial. Hence, hard automation is only suitable for large batch sizes. However, with the upcoming programming paradigms based on joining preprogrammed subtasks (skills) to a complete solution, the programming time can be shortened significantly. Skills are subtasks with well defined interfaces in terms of pre- and postconditions. However, skills still rely on high accuracy as there is no methodology for handling the propagation of inaccuracies when combining the skills. Therefore, skills are typically only useful for hard automation situations. In cases with smaller batch sizes, the high accuracy property will have to be loosened to lower the costs on the hardware side. In this paper, we extend the skill interfaces with a unified formalism for how the skill propagates inaccuracies. The formalism includes how the inaccuracy propagation for a skill can be learned and how the learned propagations can be used to join the skills while also predicting how inaccuracies will propagate through the entire skill chain. We also briefly discuss the perspectives using the formalism to offline search for the most robust combinations of skills for the entire task.","inCitations":[],"pmid":"","title":"A framework for handling and combining inaccuracy propagation in robot subtasks for industrial assembly","journalPages":"3848-3854","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206236"],"entities":["Assembly language","Benchmark (computing)","Experiment","Industrial robot","Mathematical optimization","Online and offline","Postcondition","Precondition","Programming paradigm","Semantics (computer science)","Simulation","Software propagation"],"journalVolume":"","outCitations":["91fbfdfde2ca341fb1f88aca7a0617fdd515d7c8","e76dada84396d5d12d5f3ec99c67fb9bff10b570","1153691476241aed35de896998fd129589537a45","ee92141465e08b84b7884df8770415c4f9a3e4ce","1541a6503018b255da57449c3c88f36130098406","a94ad2f054ecdd3768692e259284653cc92e0279","7e16fe4cbd7d7cc9989cee5c19117aad0e5d4f34","a8bb16da64b33657c5e45f32b4bf9a537153dbf1","ff174e927afd490abc814e72f9e2d942ccd29916","7fdeb6d3cfa6812ffb963eb1e38c46a95c405334","8894f9d30d513129a59e59acfc6309927be6b459"],"id":"029013cc27308f7de2854091a0f3eed8966baa2f","s2Url":"https://semanticscholar.org/paper/029013cc27308f7de2854091a0f3eed8966baa2f","authors":[{"name":"Jacob Pørksen Buch","ids":["40566262"]},{"name":"Henrik Gordon Petersen","ids":["1793390"]}],"doi":"10.1109/IROS.2017.8206236"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594094","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We introduce UNDERWORLDS, a novel lightweight framework for cascading spatio-temporal situation assessment in robotics. UNDERWORLDS allows programmers to represent the robot's environment as real-time distributed data structures, containing both scene graphs (for representation of 3D geometries) and timelines (for representation of temporal events). UNDERWORLDS supports cascading representations: the environment is viewed as a set of worlds that can each have different spatial and temporal granularities, and may inherit from each other. UNDERWORLDS also provides a set of high-level client libraries and tools to introspect and manipulate the environment models. This article presents the design and architecture of this open-source tool, and explores some applications, along with examples of use.","inCitations":[],"pmid":"","title":"UNDERWORLDS: Cascading Situation Assessment for Robots","journalPages":"7750-7757","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594094"],"entities":[],"journalVolume":"","outCitations":[],"id":"6ccf9d1cda34fa6cfda1fe5e385df329a267aca1","s2Url":"https://semanticscholar.org/paper/6ccf9d1cda34fa6cfda1fe5e385df329a267aca1","authors":[{"name":"Séverin Lemaignan","ids":[]},{"name":"Yoan Sallami","ids":[]},{"name":"Christopher Wallhridge","ids":[]},{"name":"Aurélic Clodic","ids":[]},{"name":"Tony Belpaeme","ids":[]},{"name":"Rachid Alami","ids":[]}],"doi":"10.1109/IROS.2018.8594094"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545535","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a new heuristic training procedure to help a deep neural network (DNN) repeatedly escape from a local minimum and move to a better local minimum. Our method repeats the following processes multiple times: randomly reinitializing the weights of the last layer of a converged DNN while preserving the weights of the remaining layers, and then conducting a new round of training. The motivation is to make the training in the new round learn better parameters based on the \u201cgood\u201d initial parameters learned in the previous round. With multiple randomly initialized DNNs trained based on our training procedure, we can obtain an ensemble of DNNs that are more accurate and diverse compared with the normal training procedure. We call this framework \u201cretraining\u201d. Experiments on eight DNN models show that our method generally outperforms the state-of-the-art ensemble learning methods. We also provide two variants of the retraining framework to tackle the tasks of ensemble learning in which 1) DNNs exhibit very high training accuracies (e.g., $> 95\\%$) and 2) DNNs are too computationally expensive to train.","inCitations":[],"pmid":"","title":"Retraining: A Simple Way to Improve the Ensemble Accuracy of Deep Neural Networks for Image Classification","journalPages":"860-867","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545535"],"entities":[],"journalVolume":"","outCitations":["73801d5bab1dd5cc2aaaf8855e4365a1a5d0d109","24da8a52d034be461b9c3aa076f941205a8e8c1f","02b28f3b71138a06e40dbd614abf8568420ae183","162d958ff885f1462aeda91cd72582323fd6a1f4","6b8870e6b69ff05b4d36b204bdd7f90620bd3c5f","b30e12c4d0874112a0bc7308a3e41e21932633f9","042f8effa0841dca3a41b58fab12f0fd8e3f9ccb","081651b38ff7533550a3adfc1c00da333a8fe86c","127316fbe268c78c519ceb23d41100e86639418a","280cd4a04cf7ecc36f84a7172483916a41403f5e","07925910d45761d96269fc3bdfdc21b1d20d84ad","9534a6ed62dad35285895face782b493130a7789","79f279a8800c5236cf38f1a0592cadc95378825a","d1ee87290fa827f1217b8fa2bccb3485da1a300e","55b81991fbb025038d98e8c71acf7dc2b78ee5e9","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","02227c94dd41fe0b439e050d377b0beb5d427cda","061356704ec86334dbbc073985375fe13cd39088","053912e76e50c9f923a1fc1c173f1365776060cc","15cf63f8d44179423b4100531db4bb84245aa6f1","b134d0911e2e13ac169ffa5f478a39e6ef77869a","03cb609fcfce6c60cbe3eb0dd8254069bf6d7573","5d90f06bb70a0a3dced62413346235c02b1aa086","331af962330bb2e60fe5547c869da3bf4ff4f516","c0c5776cdee17ec764c6c6708741d24f79b42101","8c1c21f18da180dedf14b48f5842473d9c9057e3","ccf415df5a83b343dae261286d29a40e8b80e6c6","722fcc35def20cfcca3ada76c8dd7a585d6de386","ad6faa71d03be4a95d5884d22a2da2f8b4de007b","33c68d6bc73e74ea042dc5b9fe966625565c475e","3b2697d76f035304bfeb57f6a682224c87645065"],"id":"4c02deeba2e3787f877b149a0c48ae9c35dca776","s2Url":"https://semanticscholar.org/paper/4c02deeba2e3787f877b149a0c48ae9c35dca776","authors":[{"name":"Kaikai Zhao","ids":["48814354"]},{"name":"Tetsu Matsukawa","ids":["2816822"]},{"name":"Einoshin Suzuki","ids":["1690503"]}],"doi":"10.1109/ICPR.2018.8545535"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546102","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"For IoT networks, signals are captured by edge sensors, system-wide decisions are made at a central host, and processing may be performed at either or both sides of the network. Decision on edge or cloud placement of methods depends in part on processing and bandwidth efficiencies. Video analytics offers a good application to investigate this because signals from edge cameras are large both spatially and temporally, and processing usually involves a sequence of methods. For public camera analytics where processing and bandwidth need only be expended for active periods, we find by experiment a data range for crowd, traffic, and building scenes. For a typical sequence of video analytics methods applied to surveillance, we find upper bounds for processing and bandwidth, and experimental measurements for real \u2013 much sparser \u2013 video. Explicit description of algorithmic requirements and knowledge of experimentally determined loads gives information to balance methods between edge and cloud.","inCitations":[],"pmid":"","title":"Balancing Video Analytics Processing and Bandwidth for Edge-Cloud Networks","journalPages":"2618-2623","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546102"],"entities":["Efficiency","Equilibrium","Experiment","Requirement","Sensor","Video content analysis"],"journalVolume":"","outCitations":["1790ae874429c5e033677c358cb43a77e4486f55","614f81b2b106b9d72cb09571ce990443701c7682","20b938e7655ea2f761f465c47862b60be7642e8a","496c370e0a5cd1e7de07f126cd01572dff5c50a5","1aefecbef713b8ff7d5b40814132c9464a6f3f09","51fea461cf3724123c888cb9184474e176c12e61","6dfc312ebd656faf7c01018f5cd53345818d0e1c","1c454ae4e1bbc600791f3a4796fdb6b1ee2ca016","59a6a2bdeb9055334d3749272c7171b06a6ca838","7b90c6b91386626809513c85c96207425acaf2b7","2a012fe3318dd32b96f0aad31e9b0ec7b43fced9","e7bf9803705f2eb608db1e59e5c7636a3f171916","922b5eaa5ca03b12d9842b7b84e0e420ccd2feee","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","e88eec15946dd19bdcf69db882f204386e05ff48","15cf63f8d44179423b4100531db4bb84245aa6f1","afca14b1989694e90cfc6989d6c81fd5abb40998","7f0bbe9dd4aa3bfb8a355a2444f81848b020b7a4","061356704ec86334dbbc073985375fe13cd39088","2d2f8a511caa9821102d2862b444d7026288b4d2"],"id":"e75fdf49b6229b62380f90d7167cc22319972e51","s2Url":"https://semanticscholar.org/paper/e75fdf49b6229b62380f90d7167cc22319972e51","authors":[{"name":"Lawrence O&#x0027;Gorman","ids":["52132138"]},{"name":"Xiaoyang Wang","ids":["48631781"]}],"doi":"10.1109/ICPR.2018.8546102"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206169","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, we propose a touch-based localization approach for a potentially large and complex object with multiple internal degrees of freedom. Should a task only require a partial localization of the object, our method selects the appropriate information gathering actions to register the desired features. We use probabilistic methods to reason over the distribution of the estimated object poses in the 6-DOF configuration space. We introduce the datum-based particle filter to handle intrinsic tolerances between each of the sections of the object. We describe two alternative methods for the particle filter system: one using the full joint belief and the other reasonably simplifying the belief to achieve a better ability to scale. We present simulation results for both proposed methods to show the advantages of our approaches.","inCitations":["095ec8431952d2f5036005b19eea38418b375ac9"],"pmid":"","title":"The datum particle filter: Localization for objects with coupled geometric datums","journalPages":"3325-3332","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206169"],"entities":["Approximation algorithm","Geodetic datum","Information gain in decision trees","Kullback\u2013Leibler divergence","Particle filter","Scalability","Simulation"],"journalVolume":"","outCitations":["f6f8aab4ad641dd3972dfe636ea923ebdae25e2f","496c6f999e8ddf706fed9b5b76f13e816582048b","07c97a162ac37c49aa3e654de0a172d099a609d3","3c680d692563534ad5c8744ecead453c42026d91","00897212624d5cfe6068d098f034b369d609e268","5c2f635fd11d2d001b7f9921007c6d3cf201eebf","4bee59c548199aa3931be56550a145028cb950c2","0353c95b7e7cac147f915ff11eccbf87252b3849","8e1cc8b093c6f6d7a87104fe7ffc9513d882adce","115535b29f574f5a81e0ca4998e1248f06080545","01e8a2af9cc70b4d487d742a08f052bb78003753","922b5eaa5ca03b12d9842b7b84e0e420ccd2feee","37a9f578f2cecb46e2a0827682a2d687334cc8b1","2a83c466c5d57b711258dccbc0ac12c14171e1f0","1233f38bddaebafe9f4ae676bb2f8671f6c4821a","29a7e15081c63f0e28359a8feea4dea4054f601e"],"id":"624bbbb191ac931646e06683a56f30255bac0013","s2Url":"https://semanticscholar.org/paper/624bbbb191ac931646e06683a56f30255bac0013","authors":[{"name":"Shiyuan Chen","ids":["2774762"]},{"name":"Brad Saund","ids":["20519533"]},{"name":"Reid Simmons","ids":["47172141"]}],"doi":"10.1109/IROS.2017.8206169"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593390","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Sliding window approaches have been widely used for object recognition tasks in recent years [19], [4], [5], [18]. They guarantee an investigation of the entire input image for the object to be detected and allow a localization of that object. Despite the current trend towards deep neural networks, sliding window methods are still used in combination with convolutional neural networks [22]. The risk of overlooking an object is clearly reduced compared to alternative detection approaches which detect objects based on shape, edges or color. Nevertheless, the sliding window technique strongly increases the computational effort as the classifier has to verify a large number of object candidates. This paper proposes a sliding window approach which also uses depth information from a stereo camera. This leads to a greatly decreased number of object candidates without significantly reducing the detection accuracy. A theoretical investigation of the conventional sliding window approach is presented first. Other publications to date only mentioned rough estimations of the computational cost. A mathematical derivation clarifies the number of object candidates with respect to parameters such as image and object size. Subsequently, the proposed disparity sliding window approach is presented in detail. The approach is evaluated on pedestrian detection with annotations and images from the KITTI [10] object detection benchmark. Furthermore, a comparison with two state-of-the-art methods is made. Code is available in C++ and Python https://github.com/julimueller/disparity-sliding-window.","inCitations":[],"pmid":"","title":"Disparity Sliding Window: Object Proposals from Disparity Images","journalPages":"5777-5784","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1805.06830","https://doi.org/10.1109/IROS.2018.8593390","https://arxiv.org/pdf/1805.06830v2.pdf"],"entities":[],"journalVolume":"","outCitations":["d3d37a44a7a0453445e6e433a527b0164ec99b88","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","72e08cf12730135c5ccd7234036e04536218b6c1","aa23d33983b1abd2d8a677040eb875e93c478a7f","b623b8d6282b78e7f894a6a0a390fc05500f9534","18fe8a42cb3b7ce14e29a11faa3d7cc45f1be22f","0674792f5edac72b77fb1297572c15b153576418","19b0b1c4fd28a832e5bc2b2cd72b50ac8aa86e7d","0638774eb0f0f0d07d6f377b9836136ac264b240","16973218fdd515665078043827ed277e4647f3b1","314fa3992c1328adae7a5eb895d497a98a3a5f68","25d7da85858a4d89b7de84fd94f0c0a51a9fc67a","55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1","1109b663453e78a59e4f66446d71720ac58cec25","03ed4064310cf3a1b93187e26eeaa4ecf4539532","1485b72151d53895a1e251d2e42d84f78c1a5009","0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a","6723f5b86de00b30497d43dc2a73fca4c41f437b","b183947ee15718b45546eda6b01e179b9a95421f","75b1e3d15ea1b8442edb8463e51aa6e3938f3f1d","a114d163aa331aaa417be65dc6ec5f898c9660f1","20a78d3145279dcd799cd7a856ae2714f4863a16","0b43f48c933c615e305ebd25521635cff8df4707","6770baa6751ca85942e8ed2f0008cb2dd068e812","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486","12a376e621d690f3e94bce14cd03c2798a626a38"],"id":"c9c54aa21756e714380f639dd492b3f2979ab2fc","s2Url":"https://semanticscholar.org/paper/c9c54aa21756e714380f639dd492b3f2979ab2fc","authors":[{"name":"Julian Müller","ids":["34011666"]},{"name":"Andreas Fregin","ids":["2537402"]},{"name":"Klaus C. J. Dietmayer","ids":["1684594"]}],"doi":"10.1109/IROS.2018.8593390"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206167","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Small 3D LIDAR and a multimodal-based localization are fundamentally important for autonomous robots. This paper describes presentation and demonstration of a sensor and a method for LIDAR-image based localization. Our small LIDAR, named SPAD LIDAR, uses a single-photon avalanche diode (SPAD). The SPAD LIDAR incorporates laser receiver and environmental light receiver in a single chip. Therefore, the sensor simultaneously outputs range data and monocular image data. By virtue of this structure, the sensor requires no external calibration between range data and monocular image data. Based on this sensor, we introduce a localization method using a deep convolutional neural network (SPAD DCNN), which fuses SPAD LIDAR outputs: range data, monocular image data, and peak intensity data. Our method regresses LIDAR's position in an environment. We also introduce improved SPAD DCNN, designated as Fast SPAD DCNN. To reduce the computational demands of SPAD DCNN, Fast SPAD DCNN integrates range data and peak intensity data. The integrated data reduces runtime without greatly increasing localization error compared to the conventional method. We evaluate our SPAD DCNN and Fast SPAD DCNN localization method in indoor environments and compare its performance. Results show that SPAD DCNN and Fast SPAD DCNN improve localization in terms of accuracy and runtime.","inCitations":["ebbfab5b98190da5d5ed0c25134aa4ffd98d6670"],"pmid":"","title":"SPAD DCNN: Localization with small imaging LIDAR and DCNN","journalPages":"3312-3317","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206167"],"entities":["Artificial neural network","Autonomous robot","Convolutional neural network","Diode","Internationalization and localization","Motion capture","Multimodal interaction","Supervised learning"],"journalVolume":"","outCitations":["307d322d6a296305c6a0896c5566217a0d448d21","548dcd32abd2f1cee76eb5955d8bbaabf89a7186","50b19c368c0f9e32bdc31642e4cd8663c7d5068e","b45d972d99b722fb2fbb077d9d1d698abcc70974","8beb24ebe38e5f7e94743e0a9896d1a0bae707ab","45231444ea99671f707298883a6398c4d204b8db","0d4b949dd77a6e71f244c78b5924e691d51e643b","0f348056a828acd578450b4ac5e0ac48be32b01e","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a"],"id":"fcf0876175bdc80cc2e3b9a0635703efdb2f1c29","s2Url":"https://semanticscholar.org/paper/fcf0876175bdc80cc2e3b9a0635703efdb2f1c29","authors":[{"name":"Seigo Ito","ids":["1794079"]},{"name":"Shigeyoshi Hiratsuka","ids":["2446601"]},{"name":"Mitsuhiko Ohta","ids":["49613972"]},{"name":"Hiroyuki Matsubara","ids":["38504013"]},{"name":"Masaru Ogawa","ids":["3035609"]}],"doi":"10.1109/IROS.2017.8206167"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206618","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Human foot injury due to being run over by a robot is a hazardous event caused by robot motion. This study aims to provide information for the safety design of robots in terms of the risk of foot fracture. A model for scaling the fracture loads of alternate specimens (bear metatarsals) to those of human metatarsals considering the effects of soft tissue is proposed and validated. Using the model, fracture tolerance of metatarsals of a human female foot of statistically typical size is predicted to be 1.24 kN at 10% fracture probability, and 0.53 kN at 1% fracture probability with 95% confidence.","inCitations":[],"pmid":"","title":"Static fracture tolerance of human metatarsal in being run over by robot","journalPages":"6935-6942","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206618"],"entities":["Image scaling","Industrial robot","Intrusion tolerance","Mobile robot","Robot"],"journalVolume":"","outCitations":["08a8dd75f4c4b8cb32740d5d0972ff7576639cf7","064e9127d37ab66349e6db6f3c6693934e9fe335","64785f8a541e00413045cc7c0d3941e5e8f7b940","434b3fc4014fddf5e156610587326090d319ccd9","34fa8adef52ce934483e4ce237df0a354f3349cf","dd61b34f94fa3bf0e1cffe32ea9be4ec05125154","53c7d0a83841eb183d2c7b18ed23d52b11aa13f6","256ba67b94a8476774dc1670061e853dc386a6a5","7fe0b49360e9a4b742d63072fb3ce41b02dcde42","c0179dee485009e64434040b90c848cd7e82ca24","12e811f98cdd1a79bcc1f284c2a8c95062fef6ab","b0beddfbe8b0c13e56d3dc1970f234f6e7b9d3ca"],"id":"f102148a58bad0da7b4513cae1138642f4c5c445","s2Url":"https://semanticscholar.org/paper/f102148a58bad0da7b4513cae1138642f4c5c445","authors":[{"name":"Tatsuo Fujikawa","ids":["2012891"]},{"name":"Yoichi Asano","ids":["2432649"]},{"name":"Tetsuya Nishimoto","ids":["50384368"]},{"name":"Rie Nishikata","ids":["11954615"]}],"doi":"10.1109/IROS.2017.8206618"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206368","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Various control strategies using trajectory planning, object recognition and learning are researched for avoiding obstacles when legged robot is walking. In this paper, designed trajectory by using Non Uniform Basis Spline (NUBS) curve and control strategy, by using proposed trajectory, to effectively overcome obstacles are presented. The trajectory designed by NUBS curve has several advantages: 1) local modification, 2) tracking velocity control for each domain, and 3) low degree trajectory with a large number of control points. The robot gets remarkable effectiveness when these advantages are used to generate the trajectory for walking and overcoming obstacles. By implementation of the proposed control strategy, quadruped robot can walk over obstacles while keeping its gait, speed and balance without collision although adjusting relatively preferable state which is more suitable position or posture of the walking robot shortly before encountering obstacles is not required. The proposed trajectory and control strategy are discussed, and performance is validated through experimental evaluations.","inCitations":[],"pmid":"","title":"Trajectory design and control of quadruped robot for trotting over obstacles","journalPages":"4897-4902","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206368"],"entities":["B-spline","Control theory","Mobile robot","Motion planning","Obstacle avoidance","Outline of object recognition","Poor posture","Velocity (software development)"],"journalVolume":"","outCitations":["20d5ef2f57e9432e943fccdf207dc47a5510dd75","b9e268038c891eb2a63b840b88ed3c69edb248c5","86cf98c8ad25b3cb185b7524bccf32f77a91b616","95a20b4c2a80c69b3d9681aeac2f960b4f5a7d94","2e56b2e93c0efe9c7fde1f073a69961cd33ca729","1a87557b29dd5bbe747e3af94f657727b42cd7fd","78abdf1e1033e2eb00c05be9a36675d2580b2b29","835dd2ebc4284d9b1414f4f322deb49e1ef803e1","3a5a4604a2ed01cf0278b6206a567bca1111ff20","4cff614041715a5f9daedd8cdfa915a3ce25a6a8","7c14a234a9cf9c36ad94c4fbcb8f485ef20d990d","52aee36c292072b115cb4c1d7c5e755ad62ddd06"],"id":"40311b59655533dcb39ab63c631c5c15a1a71b23","s2Url":"https://semanticscholar.org/paper/40311b59655533dcb39ab63c631c5c15a1a71b23","authors":[{"name":"Young Hun Lee","ids":["10257780"]},{"name":"Yoon Haeng Lee","ids":["2628293"]},{"name":"Hyunyong Lee","ids":["38420529"]},{"name":"Luong Tin Phan","ids":["3076732"]},{"name":"Hansol Kang","ids":["13893016"]},{"name":"Uikyum Kim","ids":["8130510"]},{"name":"Jeongmin Jeon","ids":["8046844"]},{"name":"Hyoukryeol Choi","ids":["1685707"]}],"doi":"10.1109/IROS.2017.8206368"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206098","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper introduces a new class of self-reconfigurable robot: the variable topology truss (VTT). Related to an existing class of robots, the variable geometry truss (VGT), variable topology trusses have the additional capability to change the topology of the truss through self-reconfiguration. The hardware necessary to achieve this is introduced, and the constraints and capabilities of this new type of robot are analyzed by introducing the concept of a topology neighbor graph. Lastly, the minimal reconfigurable VTTs, which require 18 members, are identified and their achievable topologies are enumerated.","inCitations":["84a1e70ed303041638a106cd0047b0e341d22af8","d2ce032742d076dd02e3077d4a0ffbd94e31d3ef"],"pmid":"","title":"Variable topology truss: Design and analysis","journalPages":"2717-2722","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206098"],"entities":["Linked data structure","Robotics","Self-reconfiguring modular robot","Truss (Unix)","Workspace"],"journalVolume":"","outCitations":["ff095070575632a23a7499324e097932a59c4dcc","b140b0705e86e776437101c7c64268ed9ce0e236","ded68bd8068b4698548bd9df04ed91d753583ad9","82d17028650be4f4f718ca0f4b5973e5dd8f40fe","ac97d86c8b4affaed8c38ddba88cf97fefc67087","56dc792729301e26048a0b277ee58ba52da18133","f8de0019e9dd947e156de096adffbfa7219738a9","22713ea0db602bc93401ea02b28458998f03f52e"],"id":"d92babbdf587f93d886b5c7fcc7df8a188ce19eb","s2Url":"https://semanticscholar.org/paper/d92babbdf587f93d886b5c7fcc7df8a188ce19eb","authors":[{"name":"Alexander Spinos","ids":["23194426"]},{"name":"Devin Carroll","ids":["48420360"]},{"name":"Terry Kientz","ids":["2999170"]},{"name":"Mark Yim","ids":["1715986"]}],"doi":"10.1109/IROS.2017.8206098"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202147","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Information gathering algorithms aim to intelligently select the robot actions required to efficiently obtain an accurate reconstruction of a physical process, such as an occupancy map, or a magnetic field. Many recent works have proposed algorithms for information gathering. However, these algorithms employ discretization of the state space, which makes them computationally intractable for robotic systems with complex dynamics. Moreover, most algorithms are not suited for online information gathering tasks. This paper presents a novel approach that tackles the two aforementioned issues. Specifically, our approach includes two intertwined steps: a Gaussian processes (GPs)-based prediction that allows a robot to identify highly unexplored locations, and an RRT∗-based informative path planning that guides the robot towards those locations. The combination of the two steps allows an online realization of the algorithm, while eliminates the need of discretization. We demonstrate the effectiveness of the proposed algorithm in simulations, as well as with an experiment in which a ground-based robot explores the magnetic field intensity within an indoor environment populated with obstacles.","inCitations":["85272bb0d76dffe234ab073381c37213e1640bf4"],"pmid":"","title":"Online information gathering using sampling-based planners and GPs: An information theoretic approach","journalPages":"123-130","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202147","https://elib.dlr.de/113515/1/efficient-information-gathering%20(23).pdf"],"entities":[],"journalVolume":"","outCitations":["1e8fc519c3041a5b5511f9c430374ce767b010f9","83479d07ae7702ed4e1dff8bc78e7e5a52058362","28d98857d1327b697bb9ee2cd330fa9ac9b11e00","168101bb952065f0a1904adb29e51a67f149e14a","0b2d197f90f3bce607a4465ea7312995f024a1a0","2b1b0d1c55a7adb59fc63e9486540ab76d9e4aa6","47598c6267a065ad0f9226c0a130728fedf18b81","96c6e46e0fbb348a7209f309209ad4d5a7bccda3","74da903a59921644a1b3ef469f2dbe08aa8006e3","088e21d6931729e005e6a622c6a92695e3c9dce8","18ecbf75ae41353130314419b2d357a9c2122eb8","58f3edefa50c6b82c8871bfc570192fde415bc53","da201cefb100c4259f4d3eafd340dd060c396613","0059c85ca0207cec01d2e2fbeceb19f0cf342e5c","3e704f516e40aea3940b71a3146ef2740145a0d8","232eb85939cbdcf45770998488e820520c82d5cd","30b81be8b0ef5b7e7853f15220032f240ab1bd93","67aaf4e84f11066d0db26a62723ade94625aac0a","abd77b1022dc279ca595442baa38ce55f936b95e","8dc4025cfa05e6aab60ce578f9c9b55e356aeb79","2cc85de6c80639d3204ae0d4f247296cf6d9999d","6eab24c520dd62e34ea63643cfa1497d8b8332ef"],"id":"52bef101749e42eac19224a39e5c46cee047523b","s2Url":"https://semanticscholar.org/paper/52bef101749e42eac19224a39e5c46cee047523b","authors":[{"name":"Alberto Viseras Ruiz","ids":["3215351"]},{"name":"Dmitriy Shutin","ids":["1733984"]},{"name":"Luis Merino","ids":["1738621"]}],"doi":"10.1109/IROS.2017.8202147"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206099","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Modular robots can be rapidly reconfigured into customized articulated legged morphologies capable of mobile manipulation and inspection. However, current gait generation methods do not keep pace with the speed of physical reconfiguration. This work focuses on quickly creating gaits for modular legged robots. We build on a recent method that uses trajectory optimization to design quasi-static gaits given only robot geometry and foot contact patterns. We develop methods to automatically generate contact patterns for new gaits and transitions between them. We show the utility of these methods applied to robots with many limbs, such that limbs can be fluidly reassigned to locomotion, manipulation, or inspection tasks, or to adapt gaits to hardware failures online. We demonstrate gait and transition generation with our modular hexapod and dodecapod robots. The robots switch between gaits that use all limbs for locomotion and those that leave some limbs free to pick up objects or position a camera.","inCitations":[],"pmid":"","title":"Generating gaits for simultaneous locomotion and manipulation","journalPages":"2723-2729","s2PdfUrl":"","pdfUrls":["http://crl.ethz.ch/papers/iros2017-gaits.pdf","https://doi.org/10.1109/IROS.2017.8206099"],"entities":["Mathematical optimization","Mobile manipulator","Robot","Simultaneous multithreading","Trajectory optimization"],"journalVolume":"","outCitations":["189362ddbdbf12c29685bfa4bacaf783eda81963","31813766617fa7c753f100dcf69d66cd462d6bfa","449619678716c6e0fb169b401f4dde6e8b1620c4","f0f753d1ee494899cbaafc7fb2bc61db1894776f","6eadff238e367f2b9b15c33650917151e1de2e83","b2e16b6da7c04f030a44565c87b3329b7dad93c5","46fb018e4035b7ee1ce5304a9010d2d37055dfae","41146983cdbd273e3d5dc7e734f53e2b847a5477","07e880c468301a9d5b85718eee029a3cba21e5e0","91c50f239890e64c246130e94e795568a2e38c98","65f5fe3f10393228e5d863798d0a643ebd481f8a","1912a367444a5c07183695b57ccf07b8fb7c8958","f71ab51b4c3e01a551ed2c9def4aaa0a935788c9","c2beb8d6a4c84b950f51eb4c9934da6af0a2ca73","7a962e4f7be22615059aa4faf6472567a32aa3a0","1a3c3b17a07f1d225d9b58094db834da4d5625e2","57126a6cfcdf916976b0250791a997cc0954f647","43a417355f89692cb26a412d0d9aab257c6d95c9","36858369ce62126774ab958a5bf70aa7eb63a36b"],"id":"c2d086a576b72c190dc90f7710c3ad6fdd6993dc","s2Url":"https://semanticscholar.org/paper/c2d086a576b72c190dc90f7710c3ad6fdd6993dc","authors":[{"name":"Julian Whitman","ids":["35384227"]},{"name":"Shuang Su","ids":["2755475"]},{"name":"Stelian Coros","ids":["1783776"]},{"name":"Alex Ansari","ids":["3212781"]},{"name":"Howie Choset","ids":["1742948"]}],"doi":"10.1109/IROS.2017.8206099"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202266","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a low weight (1.3 kg), human size dual arm system with compliant joints designed for aerial manipulation with a multirotor platform. Each arm provides four degrees of freedom (DOF) for end effector positioning in a kinematic configuration close to the human arm: shoulder pitch, roll and yaw, and elbow pitch. The aluminum frame structure of the arms has been designed with a double purpose: protecting the servo actuators against direct impacts and overloads, and allowing the integration of a compliant transmission mechanism with deflection measurement between the servo shaft and the output link Mechanical joint compliance increases safety in the physical interactions with the environment, removing also joint overloads typical in closed kinematic chain configurations. The dual arm system has been integrated in a hexarotor platform with a visual servoing system for object grasping, evaluating its performance first in a fixed base test bench and later in outdoor flight tests.","inCitations":["520d9b63ea5465f7ccf0aedb683ad9132a71589d","9a7a62d0cac7242ee22ff53c3ab15ee9ceae58e3","0a018e1eb19081d5a5b6c15a908f615bf87f3b5f","e969bccbe4781425e604572b619a65965a5a8d61"],"pmid":"","title":"Anthropomorphic, compliant and lightweight dual arm system for aerial manipulation","journalPages":"992-997","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202266"],"entities":["Aerial photography","Coat of arms","Floating-point unit","Fundamental interaction","Kinematic chain","Object detection","Prototype","Robot end effector","Servo","Test bench","Visual servoing","Yaws"],"journalVolume":"","outCitations":["5dea60549b709d4fd73a2b42e77d686e989c08f5","51b8e0436d249ff8dae4f3756053af284f5ddcab","0115584408b458c20f30618371c55f8339a38206","37bb42e3951770d968bce84a53e190d5cbc1d1f0","8c7e62df4b1e2c44734c901682508d9203662eb7","3099b43109c6f9097051373af95f5e7c6adecc9c","1bd916d5f312b6044b134cb38b41ed6f404b291e","26c9d899c990e3c82f1a9df81bdac1cac99ca10c","c17aa80e219dea405512ea024d9d1f1de07bd2d3","36efd2d60bedc0b251d3aaabaf30c2611c1f6f99","ee0b4c30ce7f5cb383d7a6798492e3a7627b6203","a90f7269f1d1d6e6ba274fae6eb063c7b10b6d3c","fff73b4a46ac8d071155ffe30d708cf5cf3f2b53","55d7551c54edacef55902209d7fc0fddcf7f0c5c","70b1f7f7304b968239de399f5e271d98e72594f4","17cf7644d6d4884e752964500616ee51f3c000c4","1f2ee7714c3018c1687f9b17f2bc889c5c017439","5a46d05ae1130d87697b179aa23f3936609fb646"],"id":"951450392f1e08b4dc96d814754a90f61d6151ae","s2Url":"https://semanticscholar.org/paper/951450392f1e08b4dc96d814754a90f61d6151ae","authors":[{"name":"Alejandro Suárez","ids":["2376447"]},{"name":"Pablo Ramon Soria","ids":["12301745"]},{"name":"Guillermo Heredia","ids":["1809416"]},{"name":"Begoña C. Arrue","ids":["1937592"]},{"name":"Aníbal Ollero","ids":["1748298"]}],"doi":"10.1109/IROS.2017.8202266"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206503","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Machine learning algorithms can be expensive to deploy, in particular, those used in robotics applications that perform many variations of the same task. Solutions to one variation of a task may be found via Reinforcement Learning algorithms, and are typically modeled as a vector of N parameters encoding the robot's behavior policy. When N is large or executing robot trials is time-consuming, searching in the space of solutions becomes prohibitively expensive. In this paper, we introduce a method that allows robots to generalize behaviors by analyzing solutions to a small number of previously-trained related tasks. This allows for approximate policies for novel tasks to be rapidly estimated. We present a method that achieves this type of generalization by performing nonlinear regression directly on the policy manifold \u2014 i.e., the solution space spanned as we change the parameters describing tasks. Because tasks are typically described by few parameters, the corresponding policy manifold has few degrees of freedom, which leads to low-dimensional surfaces. We exploit this property to construct a function that maps task parameters to policy parameters (a parameterized skill). Our method uses manifold clustering techniques to deal with discontinuous manifolds, a challenging situation arising from physical obstacles or robot constraints. We evaluate our method on a set of robot manipulation tasks and show that it can efficiently estimate policies for novel tasks from a small number of training examples.","inCitations":[],"pmid":"","title":"Task-based behavior generalization via manifold clustering","journalPages":"6047-6052","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206503","http://inf.ufrgs.br/~bsilva/manifoldClustering_iros2017.pdf"],"entities":["Approximation algorithm","Cluster analysis","Computation","Feasible region","Gaussian process","Machine learning","Map","Nonlinear system","Reinforcement learning","Robot","Robotics"],"journalVolume":"","outCitations":["322ba1ee9b42a2b43f9456b88f166f233167203a","0f7217e8bc4e3b9a586f67b0e79b30f5c15ef97e","47598c6267a065ad0f9226c0a130728fedf18b81","b40255df3f0876133d7787ba5c14615881911a72","27f9b43737e234cefb3c5cd72324a36cbe61ee3c","05d5d29ca199769396ec10d96ca6f3d263f79e11","8ea1d82cadadab86e7d2236a8864881cc6b65c5c","456a32b2393e654be719e5ac63025897d3d8cc9a","82673205bf76c6fc788790308bc14a9a2d8e41ad","8674ad8eaaed5d69ac4d771ec191da0667e5758b","0b3ecb428a7cfb318a0e316ec0389ed7102d7551"],"id":"e7ba7fbc0cd252a20cf771d6a39246a0a480ab66","s2Url":"https://semanticscholar.org/paper/e7ba7fbc0cd252a20cf771d6a39246a0a480ab66","authors":[{"name":"Rafael Garcia","ids":["40167662"]},{"name":"Bruno C. da Silva","ids":["38774714"]},{"name":"João Luiz Dihl Comba","ids":["32142900"]}],"doi":"10.1109/IROS.2017.8206503"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545058","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Automatic discovery of changes in a human's routine is one of the requirements for the future of smart home living, and its contribution to the E-health of the community. In this paper, a Bayesian modelling approach is used which models routine change discovery as a pairwise model selection problem. The method is evaluated on a collected office kitchen dataset that captures snapshots of the routine of the same person over multiple years (2014\u20132017). The results show that our method is able to detect not only the presence of routine changes, but also which activity patterns have been changed, fully automatically, and in a fully unsupervised manner. Moreover, changes within the same activity pattern can be discovered. Interestingly, discovered changes demonstrate subtle variations that are missed by the visual inspection of a human observer.","inCitations":[],"pmid":"","title":"Human Routine Change Detection using Bayesian Modelling","journalPages":"1833-1838","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545058"],"entities":[],"journalVolume":"","outCitations":["968db2502410813fce3ba6ea8ba4963446cb2ece","8cfcf23d36038bb8bc46b6938d77e8d4a8c3760e","1ddddd35fbf6422a67f1512d9540f584a3d2d9cc","fc50c9392fd23b6c88915177c6ae904a498aacea","a72ccbf4ccfa39388abd233344321679cc91d90a","71ca8c6d3b3887c919cb4969429f5ffa77612aa0","b75dd1a3ed50369ab218c3586e6ab63e996eb82d","15b35446c29c3d702f686f7aeb73e77da773416f","3687e6535ee90e2518dd55aa7ed8a0633a7590e6","2d2f8a511caa9821102d2862b444d7026288b4d2","1fbde67e87890e5d45864e66edb86136fbdbe20e","45a7c9deb52e0326618a6220716fe8f45d5dca7d","3ce9da2d2182a2fbc4b460bdb56d3c34110b3e39","49aec52ff74120061cb35c65dd899f3f7bcb2fab","77cdcfc64900820d7bb8a6bbec95d9ab52caa21b","4e949441a44373393f35a11845ff9a28040fddf9","820b1e45db5fea1a039a7830e3885d7bf1bbcb66","82e777011f8438e9084b342e502f5d1265552516"],"id":"7a4dbb2f892c900d7c8cd55b84c981bd94a61c03","s2Url":"https://semanticscholar.org/paper/7a4dbb2f892c900d7c8cd55b84c981bd94a61c03","authors":[{"name":"Yangdi Xu","ids":["3415113"]},{"name":"Dima Damen","ids":["1728459"]}],"doi":"10.1109/ICPR.2018.8545058"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545209","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The pedestrian detection task which aims to predict bounding-boxes of all the pedestrian instances in an image is of paramount importance for many real-world applications and has attracted much attention within the computer vision community. However, the researchers generally ignore the critical issue that due to the reasons of partial occlusion or being out of FOV, the definition for pedestrian is ill-posed in many cases and even humans will find it difficult to give accurate bounding-boxes. It is found that in many real applications, pedestrian detection can be substituted by upper-body detection, which is more robust and is much less affected by occlusion or being partially out of FOV. However, few studies have been conducted in this area. To fill this research gap to some extent, we make two contributions in this paper. Firstly, in order to facilitate the study of upper-body detection, a large-scale benchmark dataset is established. This dataset comprises 9585 images extracted from typical surveillance video clips and for each image, all the upper-body instances were carefully labeled. Secondly, the performances of four state-of-the-art object-detection frameworks were thoroughly evaluated in the context of upper-body detection, which can serve as a baseline for other researchers to develop even more sophisticated methods. To make the results fully reproducible, the collected dataset has been made publicly available at https://github.com/AmazingMei/upper-body-detection.","inCitations":[],"pmid":"","title":"A Comprehensive Study on Upper-Body Detection with Deep Neural Networks","journalPages":"171-176","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545209","http://sse.tongji.edu.cn/linzhang/files/ICPR2018.pdf"],"entities":[],"journalVolume":"","outCitations":["1a54a8b0c7b3fc5a21c6d33656690585c46ca08b","0addfc35fc8f4419f9e1adeccd19c07f26d35cac","13f07d51c073964d11f9af6463fe3ffe5475c393","009fba8df6bbca155d9e070a9bd8d0959bc693c2","21a1654b856cf0c64e60e58258669b374cb05539","9a972b5919264016faf248b6e14ac51194ff45b2","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","7d39d69b23424446f0400ef603b2e3e22d0309d6","148a5fa66480afa7744409cde659f79c7c9b3fdc","213a579af9e4f57f071b884aa872651372b661fd","20a78d3145279dcd799cd7a856ae2714f4863a16","0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a","12a376e621d690f3e94bce14cd03c2798a626a38","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","bbc76f0e50ab96e7318816e24c65fd3459d0497c","061356704ec86334dbbc073985375fe13cd39088","97ee35db6b389a7bcc4b7975d12dbcd165226aad","424561d8585ff8ebce7d5d07de8dbf7aae5e7270"],"id":"734be7f538d4cb9fc727794ac86ff2b04aa5d468","s2Url":"https://semanticscholar.org/paper/734be7f538d4cb9fc727794ac86ff2b04aa5d468","authors":[{"name":"Yamei Zhu","ids":["49780852"]}],"doi":"10.1109/ICPR.2018.8545209"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206367","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The identification of inertial parameters is crucial to achieve high-performance model-based control of legged robots. The inertial parameters of the legs are typically not altered during expeditions and therefore are best identified offline. On the other hand, the trunk parameters depend on the modules mounted on the robot, like a motor to provide the hydraulic power, or different sets of cameras for perception. This motivates the use of recursive approaches to identify online mass and the position of the Center of Mass (CoM) of the robot trunk, when a payload change occurs. We propose two such approaches and analyze their robustness in simulation. Furthermore, experimental trials on our 80-kg quadruped robot HyQ show the applicability of our strategies during locomotion to cope with large payload changes that would otherwise severely compromise the balance of the robot.","inCitations":["291f68313aed46698da268aa4d455753fa0a0d66"],"pmid":"","title":"Online payload identification for quadruped robots","journalPages":"4889-4896","s2PdfUrl":"","pdfUrls":["https://hal.archives-ouvertes.fr/hal-01575033/document","https://doi.org/10.1109/IROS.2017.8206367"],"entities":["Emoticon","Ground truth","Image scaling","Least squares","Online and offline","Recursion","Robot","Rough set","Simulation"],"journalVolume":"","outCitations":["5075fb85bd1830b61d2aa41f65ea8ad7b31006f0","cbc2f631ad9c6396dc689a2c497285440126b753","537f4b4464f255d3d7a2d31e2a416cba1a877bd6","fa677e824b989b36c6b227cf1192e462925cd962","c38922b5d60a49cf1fd0bca1b86071e9754b1ecd","8e3bd2d48226033aba2ad41bddc4b0ff1a2333a0","0e87b10ccba118c3edd27f4ab5e59ab9466fb4b2","1e722f07cfeb0d44ed9ac4255e69cecc5fda6e73","0176d366c47643dcb2adf307cba2effac10a98c0","33523bf54366a86b6fbc58263dd2e366aff86fa6","76a2c5f9ec54ce6626a0b42d0af6b66285d8d031","b06743e609163105447b5da19539857115426049","116d6e724d0d15fbb2e84d741477102f9d988ed3","2a8fb3c6c81d5213424b1486316c9ff942ce73b0","7870fff0c80ea73703f77f1e1d4eaf0a2b2d8022"],"id":"fcf6f3d580d28575c46e34fc52c91db797a22be4","s2Url":"https://semanticscholar.org/paper/fcf6f3d580d28575c46e34fc52c91db797a22be4","authors":[{"name":"Guido Tournois","ids":["31420648"]},{"name":"Michele Focchi","ids":["1900872"]},{"name":"Andrea Del Prete","ids":["3153720"]},{"name":"Romeo Orsolino","ids":["30507645"]},{"name":"Darwin G. Caldwell","ids":["1745158"]},{"name":"Claudio Semini","ids":["1805218"]}],"doi":"10.1109/IROS.2017.8206367"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593942","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Research in human teamwork shows that a key element of fluid and fluent interactions is the interpretation of implicit verbal and non-verbal cues in context. This poses an issue to robotic platforms, however, as they have historically worked best when controlled through explicit commands that have employed structured, unequivocal representations of the external world and their human partners. In this work, we present a framework for effectively grounding situated and naturalistic speech to action selection during human-robot collaborative activities. This is accomplished by maintaining and incrementally updating separate \u201cspeech\u201d and \u201ccontext\u201d models that jointly classify a collaborator's utterance. We evaluate the efficacy of the system on a collaborative construction task with an autonomous robot and human participants. We first demonstrate that our system is capable of acquiring and deploying new task representations from limited and naturalistic data sets, and without any prior domain knowledge of language or the task itself. Finally, we show that our system is capable of significantly improving performance on an unfamiliar task after a one-shot exposure.","inCitations":[],"pmid":"","title":"Situated Human\u2013Robot Collaboration: predicting intent from grounded natural language","journalPages":"827-833","s2PdfUrl":"","pdfUrls":["https://alecive.github.io/papers/2018_Brawer_IROS_situated_hrc.pdf","https://doi.org/10.1109/IROS.2018.8593942"],"entities":[],"journalVolume":"","outCitations":["9b644794de7f1a546dd490ef14f6302be549d8cb","c14d3eb270a2de4feb5fb5965a85e994840a1dad","86428fc6eb917dd914a6837f49dbbde5fc37c6d4","37edbe223b24426b3c3d0a8b4037a1cb22fca193","e8563bb2c74a5b012a5fb6b8ff7abbaf573e30be","8d3afd89c442ee1a926a171ab930834e0f1212f1","38a62849b31996ff3d3595e505227973e9e3a562","f41d56b349762a53eb60723f8e28c029604f4daa","1be16d8c557b15cdf2db9e7eb4453f2274fd60af","077e69f27e246f7d8625c44bd39ffae247fd8a28","0654b8a315ae00e9838ad797838145ae57a76ef6","b10fb601766270673cfea230a9748bfe329cd0e6","1009662b82e13eea6eeec18a707c440f62676b73","014782be8ccd40118e88520b97e492105a547254","5855ccdf69da752bef292f415a295739c7253e4d","14c50b8a9f9fe708c537e055e2cb8a1339ba09f3","7674fa2aae8d4148922fad00aacfe7ed84cebdde","bcfd0b1566a54ebfeb44932601147fdd52b74b21","23f9c9e1188f605ddcd7d5d1266b7effaded54dc","0676c5f30f688039e43ffb25ccc8edc590917464","0c4e274363a9f3c24087bf870579893edfa05b51","7e5d0107dbeee796e1e1abd60363024e922b429b","5aac41a786e59586b60eb7fa1410c588f496b4c7","e050b06b2e4937b516d630043cb68b8bc77078df","11b96a0b8b4a0a4400170df440e003aef0a5fb7e","2d90537b709ae92e07856b93aab70614ac5af561","ad3cd8ab307718f7a2ec0adb7839a89f46b4015b","67fc4f820888a81da37146e96ae5303d385bb847","8b0ea115f0f48a6a5e91a40dba9ab131a58929e6","637ba488425e3e293eefb30508b9c0ad98835934","4ef78554b9e0a5a990abb536cc2cd0c146552c77","41c0fc6f291b227159d672f7dd7ca6fa8ddbb071"],"id":"1eca4e34f720e51ae434bc555d56dcbfc9c120ce","s2Url":"https://semanticscholar.org/paper/1eca4e34f720e51ae434bc555d56dcbfc9c120ce","authors":[{"name":"Jake Brawer","ids":["40521066"]},{"name":"Olivier Mangin","ids":["2566334"]},{"name":"Alessandro Roncone","ids":["3077150"]},{"name":"Sarah Widder","ids":["21088768"]},{"name":"Brian Scassellati","ids":["1792053"]}],"doi":"10.1109/IROS.2018.8593942"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546082","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The task of Person re-identification (re-ID) is to recognize an individual observed by non-overlapping cameras. Robust feature representation is a crucial problem in re-ID. With the rise of deep learning, most current approaches adopt convolutional neural networks (CNN) to extract features. However, the feature representation learned by CNN is often global and lacks detailed local information. To address this issue, this paper proposes a simple CNN architecture consisting of a re-ID subnetwork and an attribute sub-network. In re-ID sub-network, global feature and semantic feature are extracted and fused in a weighted manner, and triplet loss is adopted to further improve the discriminative ability of the learned fusion feature. On the other hand, attribute sub-network focuses on local aspects of a person and offers local structural information that is helpful for re-ID. The two sub-networks are combined on the loss level and their complementary aspects are leveraged to improve the re-ID accuracy. Comparative evaluations demonstrate that our method outperforms several state-of-the-art ones. On the challenging Market1501 and DukeMTMC datasets, 86.3% rank-1 accuracy and 69.4% mAP, and 72.1% rank-1 accuracy and 53.4% mAP are achieved respectively.","inCitations":[],"pmid":"","title":"Person Re-identification Based on Feature Fusion and Triplet Loss Function","journalPages":"3477-3482","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546082"],"entities":["Artificial neural network","Convolutional neural network","Deep learning","Entity Name Part Qualifier - adopted","Evaluation","Extraction","Loss function","Neural Network Simulation","Subnetwork","Triplet state","Weight-Loss Agents","hearing impairment"],"journalVolume":"","outCitations":["2dd2c7602d7f4a0b78494ac23ee1e28ff489be88","d745cf8c51032996b5fee6b19e1b5321c14797eb","193089d56758ab88391d846edd08d359b1f9a863","6123e52c1a560c88817d8720e05fbff8565271fb","e5b4700a615cde23b91be3eadf1c99642cd33e42","4308bd8c28e37e2ed9a3fcfe74d5436cce34b410","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","3f45d73a7b8d10a59a68688c11950e003f4852fc","1d0dcb458aa4d30b51f7c74b159be687f39120a0","2b50f8e4568ecd84e2f9d6357254272d8db4bbd4","27a2fad58dd8727e280f97036e0d2bc55ef5424c","9812542cae5a470ea601e7c3a871331694105093","6d5e12ee5d75d5f8c04a196dd94173f96dc8603f","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","46a01565e6afe7c074affb752e7069ee3bf2e4ef","2cd03c6e78d09bb98872bb34bb70e08c32dc5f7e","12fa3c73a7764cb65bb76fed0601fc5d79893bcd","51144f331aa9310e4fd9b4dab00e4baf6fba7ef3","2788f382e4396290acfc8b21df45cc811586e66e","7f23a4bb0c777dd72cca7665a5f370ac7980217e","02e43d9ca736802d72824892c864e8cfde13718e","6bd36e9fd0ef20a3074e1430a6cc601e6d407fc3","061356704ec86334dbbc073985375fe13cd39088","359c2483195a3217095dcf8738abe47181a0fc94","19d583bf8c5533d1261ccdc068fdc3ef53b9ffb9","2c03df8b48bf3fa39054345bafabfeff15bfd11d","9fede3a43d6b63a8d5988cb7f52d74bba8d164cd","6c690af9701f35cd3c2f6c8d160b8891ad85822a","34aa3dca30dc5cbf86c92d5035e35d264540a829","754ee07789f6ff28fc121bb9f771895e971ac28c","b73cdb60b2fe9fb317fca4fb9f5e1106e13c2345","de02043fd479a2bcf23d30ab4496cc4e0d84f699","207e0ac5301a3c79af862951b70632ed650f74f7","110919f803740912e02bb7e1424373d325f558a9"],"id":"cbf5b3469c7216c37733efca6c2cdb94357b14a7","s2Url":"https://semanticscholar.org/paper/cbf5b3469c7216c37733efca6c2cdb94357b14a7","authors":[{"name":"Jun Xiang","ids":["49236326"]},{"name":"R. Lin","ids":["41211947"]},{"name":"Jianhua Hou","ids":["36811194"]},{"name":"Wenjun Huang","ids":["47504513"]}],"doi":"10.1109/ICPR.2018.8546082"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593499","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Wire detection, depth estimation and avoidance is one of the hardest challenges towards the ubiquitous presence of robust autonomous aerial vehicles. We present an approach and a system which tackles these three challenges along with generic obstacle avoidance as well. First, we perform monocular wire detection using a convolutional neural network under the semantic segmentation paradigm, and obtain a confidence map of wire pixels. Along with this, we also use a binocular stereo pair to detect other generic obstacles. We represent wires and generic obstacles using a disparity space representation and do a C-space expansion by using a non-linear sensor model we develop. Occupancy inference for collision checking is performed by maintaining a pose graph over multiple disparity images. For avoidance of wire and generic obstacles, we use a precomputed trajectory library, which is evaluated in an online fashion in accordance to a cost function over proximity to the goal. We follow this trajectory with a path tracking controller. Finally, we demonstrate the effectiveness of our proposed method in simulation for wire mapping, and on hardware by multiple runs for both wire and generic obstacle avoidance.","inCitations":["9057a9536a761e194691a23a983d6d5f0aab681f"],"pmid":"","title":"DROAN - Disparity-Space Representation for Obstacle Avoidance: Enabling Wire Mapping & Avoidance","journalPages":"6311-6318","s2PdfUrl":"","pdfUrls":["https://www.ri.cmu.edu/wp-content/uploads/2018/08/IROS18_1856_FI.pdf","https://doi.org/10.1109/IROS.2018.8593499"],"entities":[],"journalVolume":"","outCitations":["1057dfcc4e92bf28766d7ad19d7263a8215b0337","9201bf6f8222c2335913002e13fbac640fc0f4ec","39fbdc822534651e16f926897af62c8cf1a09a91","4e693afcc45c8df9d97cab15c429e1619ca7f25f","709df9cb0574f1ff3cb3302c904db38365040fa8","79ba9b1e1b16f166da84e206b71bbb4e6b9be9c5","26360210e8d7adefeac70e3581cc878efc1573dd","12fa95102cbb42bdb2f04ca09d7a97e592b2f702","ec5574816a447bbfbd5ed89c6d74d5031b7e1e61","65caf9290f4ea6ace241c335cfad9601b4394eaa","420c46d7cafcb841309f02ad04cf51cb1f190a48","4afb7b4eb9c4f0b0beaefe242a085040ddf5699b","78b4777451f8c01e604237f8c800ac0fd6a5216b","64557702b19b9aadb494e781fb52ce80d87291ba","92f66d26dc52d4b9bde8684520a3dddfb3bd8d36","a696a817cbde92e4d5c4a216875831e74ee6c30c","bee6bc8da99540117b49d333546dc7932fa0952f","40d679dc40e5737251734d329c72649e459338b5","7a564f5ca6dc361000b358ca2dc85fb93a871825","0fa4aa09930eb6718777fa186030a3effeca832e","0d830b1d2522b9cf9fabd414ff5caf239c57ee86","3cdb1364c3e66443e1c2182474d44b2fb01cd584","4041cd4993409d851b0c3db4bc799324efbe62e6","365c33f522ce12a3eade1837b00184f8f4374c9f","450de1664f674bce33c31f32985553d4217344ba"],"id":"2bfdc2f72ba9f543d4a06eb511f49293591c9d6a","s2Url":"https://semanticscholar.org/paper/2bfdc2f72ba9f543d4a06eb511f49293591c9d6a","authors":[{"name":"Geetesh Dubey","ids":["2032994"]},{"name":"Ratnesh Madaan","ids":["31549065"]},{"name":"Sebastian Scherer","ids":["32634992"]}],"doi":"10.1109/IROS.2018.8593499"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545434","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"It is a well-known fact that the variations in illumination could seriously affect the performance of 2D face analysis algorithms, such as face landmarking and face recognition. Unfortunately, the illumination condition is usually uncontrolled and unpredictable in most practical applications. Numerous methods have been developed to tackle this problem but the results is poor, especially for images with extreme lighting condition. Furthermore, most traditional illumination processing methods only demonstrate on grayscale images and require strict alignment of face images, resulting in limited applications in real world. In this paper, we proposed to reformulate the face image illumination processing problem as a style translation task with a Generative Adversarial Network (GAN). The key insight is to use the powerful mapping ability of GAN between two domains without knowing their true distributions. In this new sight, we developed a new multi-scale dual discriminate nets and employed multi-scale adversarial learning for visually realistic illumination processing. Advocating the use of the insights from traditional method, we also use reconstruction learning and add two new loss items of image quality assessment to enforce the preservation of all other illumination excluding details on the generated image. Experiments on CMU Multi-PIE and FRGC datasets show that our method can obtain promising illumination normalization results and preserve a superior visual quality.","inCitations":[],"pmid":"","title":"Face Image Illumination Processing Based on Generative Adversarial Nets","journalPages":"2558-2563","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545434"],"entities":[],"journalVolume":"","outCitations":["4a18adc7f5a090a041528a88166671248703f6e0","1e213b03e1b8a6067bf37503904491e98b9e42df","4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5","cfab5b500506078117125146c0d283d2392ff2e3","08517a94cbcd1c2cb39a74db13570c05ec191c58","a4a1bf9f4b9d897673f6d5c00c1aeb33a5637ddc","6b4da897dce4d6636670a83b64612f16b7487637","4273c24df71ec59c0c1cad95342d01cf53bb7d8d","c1a70d63d1667abfb1f6267f3564110d55c79c0d","3bfb9ba4b74b2b952868f590ff2f164de0c7d402","55c22f9c8f76b40793a8473248873f726abd8ce9","acd87843a451d18b4dc6474ddce1ae946429eaf1","6140cbe70713d92f53c7e48dc31633fae961d9c7","d0e51833b3db3af1c762ab723efd08f117a497c8","102a2096ba2e2947dc252445f764e7583b557680","59e9ef8b61182acace9e37f41f9c2a03db69c15b","529e2ce6fb362bfce02d6d9a9e5de635bde81191","072fd0b8d471f183da0ca9880379b3bb29031b6a","1d58d83ee4f57351b6f3624ac7e727c944c0eb8d"],"id":"5ad1372de52c6eefe3bdc0133481045075beb579","s2Url":"https://semanticscholar.org/paper/5ad1372de52c6eefe3bdc0133481045075beb579","authors":[{"name":"Wei Ma","ids":["47121184"]},{"name":"Xiaohua Xie","ids":["2002129"]},{"name":"Chong Yin","ids":["49168832"]},{"name":"Jianhuang Lai","ids":["1750721"]}],"doi":"10.1109/ICPR.2018.8545434"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202223","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"While humans are highly capable of recovering from external disturbances and uncertainties that result in large tracking errors, humanoid robots have yet to reliably mimic this level of robustness. Essential to this is the ability to combine traditional \u201cankle strategy\u201d balancing with step timing and location adjustment techniques. In doing so, the robot is able to step quickly to the necessary location to continue walking. In this work, we present both a new swing speed up algorithm to adjust the step timing, allowing the robot to set the foot down more quickly to recover from errors in the direction of the current capture point dynamics, and a new algorithm to adjust the desired footstep, expanding the base of support to utilize the center of pressure (CoP)-based ankle strategy for balance. We then utilize the desired centroidal moment pivot (CMP) to calculate the momentum rate of change for our inverse-dynamics based whole-body controller. We present simulation and experimental results using this work, and discuss performance limitations and potential improvements.","inCitations":["9cf991fa8bcfc2792da120bde924facb174b8697","20d6e23d6f7013387e89fcaf9e532cc94620b4ef","d01dba6093d03f12dd643f1849103354f07af8f2","3e91cfb10342d436c57e4cd3d42c4df9a6851457","bd08cab2bff2b62494d3ef0e7015ddbb3a1d58d5","1b6d95417647985107f41932fc7983de5e4b2d7d","325ae834e29c55d2ab3b9e72ac04d863fc9fe295"],"pmid":"","title":"Walking stabilization using step timing and location adjustment on the humanoid robot, Atlas","journalPages":"667-673","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1703.00477","https://arxiv.org/pdf/1703.00477v2.pdf","https://doi.org/10.1109/IROS.2017.8202223","https://arxiv.org/pdf/1703.00477v1.pdf"],"entities":["Algorithm","AngularJS","Chaos theory","Gradient descent","Humanoid robot","Inverse dynamics","Mathematical optimization","Real-time transcription","Simulation"],"journalVolume":"","outCitations":["23dd632c54e53a98b0e675a874e9b49491e2c07c","0067b34855b97a894bb55ade9681a24c40f7eeef","c2a347ae729f805bc428b5b3710495a0b913bf8b","4b9ac9e1493b11f0af0c65c7a291eb61f33678b6","ae087149c2745cf6bb32edba338cb7ffe646d3d7","eb35523e6847c515dd7e8c250e1cdc67cb18f90b","f03b45562497776814dd7a2e4ecaa2c922c987c7","1499b095dac672c38998f48f1420b5caa4418397","9a2b94af5c91271da6c69cbcbf258f3eca43e681","29a33bc93bc030508811e019c492b9ce73c5d178","4f534e3244e01976ebe03e890fe4739f272c27e7","6a316e0d44e35a55c41a442b3f0d0eb1f9d4d0ca","80fecd0cf4c1a454e975999b0f3cb2a144bee738","3e8b69ee65dc20e82f257374b35e924dd12dc8b3","680bc1fbbff746aa6be0bb4528c0c6e2bf171b01","1e722f07cfeb0d44ed9ac4255e69cecc5fda6e73","10e46d76ac5f9ec361030886afd4222648fd96c1","5f12826e560141add9470a36349a93c6b31b2640","80ecc36669a11dc209dbe056b08f42ba0e801c7c","28be5bc73866ca06be55056405937b328c82e05d","6facbe854c57343710d7a4de32cb8ab8d7b1c951","8f4f6b43d666794718da35e3bdf1fc7f91c30445","6c267aafaceb9de1f1af22f6f566ba1d897054e4","3b979cca666dcb0d7f2cfb49223227772458c38b"],"id":"500407acf3421b462def5575840314bdc7ea7881","s2Url":"https://semanticscholar.org/paper/500407acf3421b462def5575840314bdc7ea7881","authors":[{"name":"Robert J. Griffin","ids":["2131457"]},{"name":"Georg Wiedebach","ids":["3437780"]},{"name":"Sylvain Bertrand","ids":["2779421"]},{"name":"Alexander Leonessa","ids":["2149093"]},{"name":"Jerry E. Pratt","ids":["2885347"]}],"doi":"10.1109/IROS.2017.8202223"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593738","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"A combination of several autonomous UAVs can be used to perform collaborative tasks. Such a combination is referred to as a swarm of drones. The use of multiple platforms can extend the system global capacities thanks to the resulting variety of embedded sensors and to information sharing. In this case, path planning and thus obstacles avoidance is still a major task. To deal with this issue, mobility models have to be implemented. Our contribution presented in this paper is a mobility model for swarms of UAVs based on the Artificial Potential Fields (APF) principle. In our model, the involved UAVs collaborate by sharing data about the obstacles that they detected. By doing so, a UAV which is not close enough to an obstacle to detect it thanks to its own sensors will still have the proper data to take this obstacle into account in its path planning. To validate our mobility strategies with realistic constraints we simulate the performances of existing sensors and transmitters, and consider real-world environment.","inCitations":[],"pmid":"","title":"A Mobility Model Based on Improved Artificial Potential Fields for Swarms of UAVs","journalPages":"8499-8504","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593738"],"entities":[],"journalVolume":"","outCitations":[],"id":"847f359dac3bdf256a05bab6fcba00a4f789881e","s2Url":"https://semanticscholar.org/paper/847f359dac3bdf256a05bab6fcba00a4f789881e","authors":[{"name":"Ema Falomir","ids":[]},{"name":"Serge Chaumette","ids":[]},{"name":"Gilles Guerrini","ids":[]}],"doi":"10.1109/IROS.2018.8593738"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593984","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we present a new characterization of mobility for formations of unicycle robots defined by distance-bearing constraints. In fact, by introducing a simple reduction procedure which associates a prescribed formation with a \u201cmacro-robot\u201d, we extend the classification by type proposed by Campion et al., to multi-agent systems. To simplify the classification task, which only leverages the nonslip condition for a conventional centered wheel, we assume that the robots are disposed at the vertices of a regular convex polygon. We demonstrate the practical utility of the notion of macro-robot in a trajectory-tracking control problem for a formation of unicycles.","inCitations":[],"pmid":"","title":"A New Characterization of Mobility for Distance-Bearing Formations of Unicycle Robots","journalPages":"4833-4839","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593984"],"entities":[],"journalVolume":"","outCitations":[],"id":"8569e1ce47608a717cc71f0c73a2feefdb7b2782","s2Url":"https://semanticscholar.org/paper/8569e1ce47608a717cc71f0c73a2feefdb7b2782","authors":[{"name":"Fabio Morbidi","ids":[]},{"name":"Estelle Bretagne","ids":[]}],"doi":"10.1109/IROS.2018.8593984"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594012","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The use of model predictive control for quadro-tor applications requires balancing trajectory tracking performance and constraint satisfaction with fast computation. This paper proposes a Flatness-based Model Predictive Control (FMPC) approach that can be applied to quadrotors, and more generally, differentially flat nonlinear systems. Our proposed FMPC couples feedback model predictive control with feedforward linearization. The proposed approach has the computational advantage that, similar to linear model predictive control, it only requires solving a convex quadratic program instead of a nonlinear program. However, unlike linear model predictive control, we still account for the nonlinearity in the model through the use of an inverse term. In simulation, we demonstrate improved robustness over approaches that couple model predictive control with feedback linearization. In experiments using quadrotor vehicles, we also demonstrate improved trajectory tracking compared to classical linear and nonlinear model predictive control approaches.","inCitations":[],"pmid":"","title":"Flatness-Based Model Predictive Control for Quadrotor Trajectory Tracking","journalPages":"6740-6745","s2PdfUrl":"","pdfUrls":["http://www.dynsyslab.org/wp-content/papercite-data/pdf/greeff-iros18.pdf","https://doi.org/10.1109/IROS.2018.8594012"],"entities":[],"journalVolume":"","outCitations":["5c5672d897dd62aa4a3a7bccb577122b49160b23","bc852183092e33adb49d09108cca8cdcc86855dd","71a6286ebb116c35d7bbe313064e04b00e7166f7","1a0190a9a5bd95604cf0664befed7635c38d6e9d","6b34566c92d7317fde06f63bdd1b6963513721e5","6abf9d33fc37651238d5144413bb3ae280f4a29f","1d1beab14865cd3691ee873e3fc773760cce9fbc","15fb59613a97fdea9746a57ca5b2d6eca6d8463c","62f19192a03227dce590fff92b4425ce9569d130","8303e96e0d373ddcd40d9128a75673d7dc373cf8","abccf9516f5201d49d170ccca3595adb825493d4","20b799d179f953737f85bd4a342bf25ff3d4a9f1","f1c5d598d29851d924f1fa1c2b9a811751ead7d8","23a0a9b16462b96bf7b5aa6bb4709e919d7d7626","f1c94e775e9b2d2d8e8df26d96a6f545994af2a2","fe0651beafdd8eb75f69337e514c175a5b2b6450","0796433dc34b6ac21c6c2e8a390c72323a3b5d3c","d5accd5598282a7713dbed8581be28e4770f4edd","6facbe854c57343710d7a4de32cb8ab8d7b1c951","75d2f0286c6b69fee31630cbd11865e5e7838e99"],"id":"cf2a1ea5c7ea292b10817b64d1cacaa21aa4f99e","s2Url":"https://semanticscholar.org/paper/cf2a1ea5c7ea292b10817b64d1cacaa21aa4f99e","authors":[{"name":"Melissa Greeff","ids":["47297149"]},{"name":"Angela P. Schoellig","ids":["3008301"]}],"doi":"10.1109/IROS.2018.8594012"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545845","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"This paper presents random dot markers (RDM) printed on transparent sheets as transparent fiducial markers. They are extremely unobstructive, and useful for developing novel user interfaces. However, the marker identification is required to be robust to observable back sides of the transparent sheets. To realize such markers, we propose a graph based framework for geometric feature based robust point matching for RDM. Instead of building one-to-one correspondences, we first build one-to-many correspondences using a 2D affinity matrix, and then globally optimize the matching assignment from the matrix. Especially, we incorporate pairwise relationship between neighboring points using local geometric descriptors into the matrix, and finally solve it with spectral matching. In the evaluation, we investigate the effectiveness of the global assignment from one-to-many correspondences, and finally show that our proposed method is enough robust to identifying overlapped markers.","inCitations":[],"pmid":"","title":"Transparent Random Dot Markers","journalPages":"254-259","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545845"],"entities":[],"journalVolume":"","outCitations":["6b9ef88d0ad6320e39a6999b051fdf29b24586e2","0674c1e2fd78925a1baa6a28216ee05ed7b48ba0","f3ab61d9faace6cfffcfc89503c0b804206d9641","19b3a2a52baef7311b02478ea8c31cb069004a82","239a470bf02fbed511b8e83de901280878cb8902","9f7e083541a0b78ee84678740c5a73d94f2e46ed","31864e13a9b3473ebb07b4f991f0ae3363517244","2b5f76af285747ced457540292a6a4a86d80bd91","3dc937b9c635d08d3428703f11cb0fc6270930b2","ce5271110e0b1ef852fd3bd3ed57b1932e08642e","56ca893694e7a36f913d508105ce6391f7f6f7f0","4cae1464141754b0e04c6d08e427e6bb0c9a25da","10c57079dbdf23b960583b50334e11466fc1e202"],"id":"c1f9a6e31a2ffefa9cdd209d7f029d2df11652fe","s2Url":"https://semanticscholar.org/paper/c1f9a6e31a2ffefa9cdd209d7f029d2df11652fe","authors":[{"name":"Hideaki Uchiyama","ids":["47849822"]},{"name":"Yuji Oyamada","ids":["48253153"]}],"doi":"10.1109/ICPR.2018.8545845"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593618","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a probabilistic approach for online dense reconstruction using a single monocular camera moving through the environment. Compared to spatial stereo, depth estimation from motion stereo is challenging due to insufficient parallaxes, visual scale changes, pose errors, etc. We utilize both the spatial and temporal correlations of consecutive depth estimates to increase the robustness and accuracy of monocular depth estimation. An online, recursive, probabilistic scheme to compute depth estimates, with corresponding covariances and inlier probability expectations, is proposed in this work. We integrate the obtained depth hypotheses into dense 3D models in an uncertainty-aware way. We show the effectiveness and efficiency of our proposed approach by comparing it with state-of-the-art methods in the TUM RGB-D SLAM & ICL-NUIM dataset. Online indoor and outdoor experiments are also presented for performance demonstration.","inCitations":[],"pmid":"","title":"Probabilistic Dense Reconstruction from a Moving Camera","journalPages":"6364-6371","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593618"],"entities":[],"journalVolume":"","outCitations":[],"id":"e0144ef0d2209576d83e5b5c52684c31ee84ddca","s2Url":"https://semanticscholar.org/paper/e0144ef0d2209576d83e5b5c52684c31ee84ddca","authors":[{"name":"Yonggen Ling","ids":[]},{"name":"Kaixuan Wang","ids":[]},{"name":"Shaojie Shen","ids":[]}],"doi":"10.1109/IROS.2018.8593618"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593648","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Modular self-folding robots are versatile systems that can change their own shape from two-dimensional patterns at instant commands. This reconfigurability is commonly restrained by power limitation in autonomous environments, The robotic systems with insufficient torque may lead to inaccurate movements and even transformation failures. This paper presents methodology for optimized reconfiguration planning with torque limitation in modular self-folding robots. We determine reconfiguration schemes with optimal initial pattern and robotic base that result in minimal peak torque by minimizing robotic inertia of the modular architecture. We present minimal bounding box and capacitated spanning tree heuristic algorithms to generate optimal initial patterns and propose 3 heuristic rules for robotic base selection. Our approach is demonstrated in simulation by applying the algorithms to the robotic concept of Mori, a modular origami robot. The simulation results show that the proposed algorithms yield reconfiguration schemes with low peak torque, thereby appropriate for real-time applications in modular robotic systems.","inCitations":[],"pmid":"","title":"Towards Peak Torque Minimization for Modular Self-Folding Robots","journalPages":"7975-7982","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593648"],"entities":[],"journalVolume":"","outCitations":[],"id":"6fa5d6283bae9dbee553868eb4fc02954b48a7d5","s2Url":"https://semanticscholar.org/paper/6fa5d6283bae9dbee553868eb4fc02954b48a7d5","authors":[{"name":"Meibao Yao","ids":[]},{"name":"Hutao Cui","ids":[]},{"name":"Xueming Xiao","ids":[]},{"name":"Christoph H. Belke","ids":[]},{"name":"Jamie Kyujin Paik","ids":[]}],"doi":"10.1109/IROS.2018.8593648"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545288","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"This paper is a contribution towards interpretability of the deep learning models in different applications of time-series. We propose a temporal attention layer that is capable of selecting the relevant information to perform various tasks, including data completion, key-frame detection and classification. The method uses the whole input sequence to calculate an attention value for each time step. This results in more focused attention values and more plausible visualisation than previous methods. We apply the proposed method to three different tasks. Experimental results show that the proposed network produces comparable results to a state of the art. In addition, the network provides better interpretability of the decision, that is, it generates more significant attention weight to related frames compared to similar techniques attempted in the past.","inCitations":["96babdc014fdd5758e54c00988f673254eb48e3e"],"pmid":"","title":"Focusing on What is Relevant: Time-Series Learning and Understanding using Attention","journalPages":"2624-2629","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545288","http://arxiv.org/abs/1806.08523","https://arxiv.org/pdf/1806.08523v1.pdf"],"entities":["Artificial neural network","Deep learning","Interpreted language","Key frame","Time series","Volume rendering"],"journalVolume":"","outCitations":["41ef53e9506bd48be6feb59e3dedcf90d499ec1c","05c025af60aeab10a3069256674325802c844212","04556d5f283d7f90e24d43371d3f51faff8c0423","38502c84f76aaebc436317fb1ec086c66b158d40","1feebbe8af1e60048cb82bbcbadc07b4d3f210d6","86fc985c08eb65a965571db448223fac81aecbd1","092b64ce89a7ec652da935758f5c6d59499cde6e","9ed34a5c27d5c0289acc254327e27ee97881fbc1","0b2cbe47a9bdea2898bce630165ec04a304aed53","14b5e8ba23860f440ea83ed4770e662b2a111119","040190c0df2bb2295430c1756616a2608d023c54","2382814a374fbfaa253e7830415482bf4166bd61","0611c3041ee855b68b6333b4c521decacb95d189","0abc0c90685ff0fd5f4b8e85834c96d79a2bb980","146f6f6ed688c905fb6e346ad02332efd5464616","8027f50bbcee3938196c6d5519464df16c275f8d","0f016b36f9336de30e0b8f90652e4bc9a7420f2a","071b16f25117fb6133480c6259227d54fc2a5ea0","27ed7f342ab5e040fd658db488cf63021ca68999","9575147eff3abfe8ef003c0fa1a834c4880d68b6","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","3b2bf65ebee91249d1045709200a51d157b0176e","96abcdded2985bd44b9514e28f5b8da4fa1e4371"],"id":"d2495378391e9862eb985b27f984797468a423c0","s2Url":"https://semanticscholar.org/paper/d2495378391e9862eb985b27f984797468a423c0","authors":[{"name":"Phongtharin Vinayavekhin","ids":["2688941"]},{"name":"Subhajit Chaudhury","ids":["34597365"]},{"name":"Asim Munawar","ids":["1688057"]},{"name":"Don Joven Agravante","ids":["3352943"]},{"name":"Giovanni De Magistris","ids":["2285702"]},{"name":"Daiki Kimura","ids":["40433860"]},{"name":"Ryuki Tachibana","ids":["34769239"]}],"doi":"10.1109/ICPR.2018.8545288"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546026","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Face anti-spoofing has encountered increasing demand as one of the key technologies for reliable and safe authentication with faces. Current face anti-spoofing methods generally take a single crop of face region as input for classification, i.e. exploiting information at only one scale. This single-scale scheme mainly focuses on facial characteristics but not utilize the surrounding information, causing poor generalization for different scenarios with varied means of attacks. Besides, it is tedious or highly empirical to determine an optimal scale of face crops. To overcome the limitations of single-scale methods, in this work we propose to integrate Multi-Scale information for better Face ANti-Spoofing (MS-FANS). Specifically, the proposed MS-FANS method takes multiple face crops at different scales as input followed by a convolutional neural network (CNN) for feature extraction. Then the features from different scales form as a sequence, which are fed into a Long Short-Term Memory (LSTM) network for adaptive fusion of multi-scale information, constructing the final representation for classification. Benefited from this multi-scale design, MS-FANS can adaptively utilize context information from multiple scales, leading to promising performance on two challenging face anti-spoofing datasets, Idiap REPLAY-ATTACK and CASIA-FASD, with significant improvement compared with the existing methods.","inCitations":[],"pmid":"","title":"Face Anti-Spoofing with Multi-Scale Information","journalPages":"3402-3407","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546026"],"entities":["Artificial neural network","Authentication","Biological Neural Networks","Convolutional neural network","Face","Feature extraction","Fetal Alcohol Spectrum Disorders","Generalization (Psychology)","Long short-term memory","Numerous","Replay attack"],"journalVolume":"","outCitations":["7bbd9c7fc4191022f48bd1bbd354fa957835943f","0d3e8b3a4bb5ffc8bc0527c443a99eb1438956a0","bec757b63093832ea47b14976f47e17abbadd8b5","e20db4085f776a7354c6542b9c6560490d43fbef","63ada149373f27d1ded6df24acaa5871d0c3e379","2742509e8ebc68fd150418080e651583d04af165","8868f2ed9079d203abadb3257e11b7efd8a4460c","c3a3f7758bccbead7c9713cb8517889ea6d04687","f81769db0c7a742efc463983c97f88aa390bb16a","ceaed903eaeaa1a541831e2c70b4b3251de67719","8624ce77324215936d153b3819dfc276da95fc10","3c0b66ddf9f669bca73fe3c54f1287c514c42ce4","3ffbf41c399354156c877aeabc424da8f62fc026","da2578011996190f87ca35ae637d60b177c0fdac","cdac436dcebe8b2c90a8de5479bd3bbd8d9a087f","7d2154d95380864f30ccd5842c5f46bbdf0e374a","c088ab4f49325ac577519e90ed3a4e7abf68da5d","99a5f5eb492d3a609611268e9d107ce4408fa474","7b7570f3f40c41c1921432ae1a48f258a231411c","d87d1dc31d874fcf55618560c148af202caa8f8a","22e2066acfb795ac4db3f97d2ac176d6ca41836c","1332ddd16e4bcdeac05cda3bcae681f481fc2805","2556c03e70c269cce19144f00827508989a839ff"],"id":"ca6de347e9dd75097178e018bbee5f3c93dcf72e","s2Url":"https://semanticscholar.org/paper/ca6de347e9dd75097178e018bbee5f3c93dcf72e","authors":[{"name":"Shiying Luo","ids":["50823154"]},{"name":"Meina Kan","ids":["1693589"]},{"name":"Shuzhe Wu","ids":["3126238"]},{"name":"Xilin Chen","ids":["1710220"]},{"name":"Shiguang Shan","ids":["1685914"]}],"doi":"10.1109/ICPR.2018.8546026"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206333","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a novel robotic hand design that aims at producing a realistic human-robot handshake. A standard characteristic model of the human-palm compliance is developed based on human hand anatomy and an empirical study. Based on this model a realistic palm-compliance rendering is implemented. The backdrivability of the system allows a position-controlled feedback loop which renders human-like agility. An objective and a subjective experimental session, also depicted in the accompanying video, are then presented in order to validate the haptic feedback of the robotic hand in comparison to a real human handshake.","inCitations":[],"pmid":"","title":"Design, control and experimental validation of a haptic robotic hand performing human-robot handshake with human-like agility","journalPages":"4626-4633","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206333"],"entities":["Control theory","Experiment","Feedback","Haptic technology","Holography","Rendering (computer graphics)","Responsiveness","Robot","Robotic arm","eric"],"journalVolume":"","outCitations":["df3bbddbbb435000a95d9dc9f64b8af901768284","9844869c0d4d996084b5aa8c50e35c0466653a2f","076c0b029d3483baeecf320f3a43520b5703d2cb","d06b398eed4ca7c3f91bd1fd249df2eff61cd922","3017ebb4634393f929c97bfa9d7f149b1c7761c6","c387e0e8052cc24d6d473402dd63df7c556899da","c78ca8a3f3001e618c1d50f0e78f25802f6a769e","b6a7b8f396136d21010584ef40b773bcd068a936"],"id":"c5bc66829f192d3e206235770fada661283123e1","s2Url":"https://semanticscholar.org/paper/c5bc66829f192d3e206235770fada661283123e1","authors":[{"name":"Moritz Arns","ids":["48194290"]},{"name":"Thierry Laliberté","ids":["2051391"]},{"name":"Clément Gosselin","ids":["1785645"]}],"doi":"10.1109/IROS.2017.8206333"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593988","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a (patent-pending) small, quasi-static, minimal-complexity Stair Climbing Robot (SCR). The vehicle design is given simply by adding a third motor to a (Segway-like) Mobile Inverted Pendulum (MIP), enabling it to maneuver up stairs, leveraging feedback control, by planting it's \u201cfoot\u201d onto the ground in front of the next step, lifting the chassis/wheel assembly up it's own \u201cleg\u201d, leaning over onto the top of the next step, self uprighting, and repeating for the following step(s). Fore/aft stabilization during leg balancing is given by using the MIP drive wheels as reaction wheels, while left/right stability is given by the width of the foot itself. The design is small and simple enough to potentially be ruggedized as a stair-climbing throwbot, akin to the Recon Scout (but able to climb up stairs) for reconnaissance in military and homeland security applications.","inCitations":[],"pmid":"","title":"A minimalist Stair Climbing Robot (SCR) formed as a leg balancing & climbing Mobile Inverted Pendulum (MIP)","journalPages":"2464-2469","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593988","https://docs.wixstatic.com/ugd/910533_d74ad28ee9264640b0b180540feb3de1.pdf"],"entities":[],"journalVolume":"","outCitations":["f3babc31af2a826ee99888fe5891d8d812df17c3","3f62ec64ad4f2708e0b0d6cb207c3aaf9c466bed","306af00b12a6c44e883bcfb73ceccf991a3338a8","0177cb47bbc24ee6656b33b5f9c7d0f169579d23","c2c92dbcfed604e0ac772b3ab98434aed24827d8","12c84faedf23ba44c813026d0eaf5c42f106850f"],"id":"67b8229615209e8ead53f7d74379656416a40f7c","s2Url":"https://semanticscholar.org/paper/67b8229615209e8ead53f7d74379656416a40f7c","authors":[{"name":"Daniel Yang","ids":["2930803"]},{"name":"Thomas Bewley","ids":["39740496"]}],"doi":"10.1109/IROS.2018.8593988"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202148","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Minimally invasive robotic surgery holds a fundamental role in modern surgery. However, one of its major limitations compared to classic laparoscopy is that the surgeon can only rely on visual perception, for the lack of haptic force feedback. A new solution for a force sensor placed at the end-tip of the trocar is presented here. This solution allows measuring the interaction forces between the surgical instrument and the environment without any changes to the instrument structure and with full adaptability to different robot platforms and surgical tools. A prototype of the sensor has been realized with 3D printed technology for a proof of concept. The static and dynamic characterization of the sensor is provided together with experimental validation.","inCitations":["55f43d2f0929d31b26ea113c2a02a94ba9902ce5","1c8d436444369da2bff7c601196456c83a52e2cd","0f13e38c8367ba8a94ee4e7caba648b6adeee08c","1a38f1d6efd27c4a9c83a91ae8348c99478b02a0","e9dd0069606c08e4d3f1ad5f623576b9997f396f","481add0c55ad9dcf348398f7ead578775b1ef61d","ed6d088267cdff4d95a54bfe6d321dac26e9984d"],"pmid":"","title":"A novel force sensing integrated into the trocar for minimally invasive robotic surgery","journalPages":"131-136","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202148"],"entities":["3D printing","Algorithm","Apache Axis","Haptic technology","Prototype","Robot","Sensor"],"journalVolume":"","outCitations":["dcbc1a28c950e53c3c439b62688dde2b530a988b","3475f64618797b9d1708a4ee228b3024093a9199","0853f8561b3693d263d94d35301bde937d11acf6","2b2e9d5293ed98ad887185e9cd9b8c5d91308bee","a787aee59b43eaedf1c7a78f392c9d07db6f4e8a","7095678006bdfc9aa85d7373fed62e0b8759d31c","1230c17063420ce294ff3dfc44d62885f7977daf","481add0c55ad9dcf348398f7ead578775b1ef61d","59c56b588bdab31abf98387abed8f4d98611c49b","fd97d1757d88f8c3576836615e47c8e021d97369"],"id":"29bcba8d0db57b1ba8d88c2188d8cafb11cd2ce6","s2Url":"https://semanticscholar.org/paper/29bcba8d0db57b1ba8d88c2188d8cafb11cd2ce6","authors":[{"name":"Giuseppe Andrea Fontanelli","ids":["39686267"]},{"name":"Luca Rosario Buonocore","ids":["9370730"]},{"name":"Fanny Ficuciello","ids":["1773159"]},{"name":"Luigi Villani","ids":["38073952"]},{"name":"Bruno Siciliano","ids":["1783131"]}],"doi":"10.1109/IROS.2017.8202148"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206069","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Accurate biological agent localization offers the opportunity for both researchers and institutions to gain new knowledge about individual and group behaviors of biosystems. This paper presents a sensor-fusion approach for tracking biological agents, combining the data from automated video logging with magnetic, angular rate, and gravity (MARG) and inertial measurement unit (IMU) data, with professionally managed dolphins as the representative example. Our method of video logging allows for accurate and automated dolphin location detection using a combination of Laplacian of Gaussian (LoG) and multi-orientation elliptical blob detection. These data are combined with MARG/IMU measurements to generate a localization estimate through a series of drift-correcting Kalman and gradient-descent filters, finalized with Incremental Smoothing and Mapping (iSAM2) pose-graph localization.","inCitations":[],"pmid":"","title":"A framework for enhanced localization of marine mammals using auto-detected video and wearable sensor data fusion","journalPages":"2505-2510","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206069"],"entities":["AngularJS","Blob detection","Dolphin","Gradient descent","Smoothing","Video logging","Wearable computer"],"journalVolume":"","outCitations":["a517000e22213551202aa8e3f1d60624d13ce869","692e41680451ab9c13de39c1a9c70e151f5e7341","46e11d1cbb959e98f45d31b589b7eab95c49beee","665df4e9ae850967ea8436d3fde0934ad48f3157","848c717ba51e48afef714dfef4bd6ab1cc050dab","4d7bfd4eddb0d238c7e00b511f936d98c7dd6b99","24d8e306668167e2d4495f508092e95cb0540cfd","d419deb64dc8d3226853024298817dcb2838d0a3","843c947e3b67084521d05ab2e43c4454663e9d9e","37be06697cb90d8392546b1365699e447984ab2d"],"id":"ea9c50bbfc6369786480d339245207ea14cb1504","s2Url":"https://semanticscholar.org/paper/ea9c50bbfc6369786480d339245207ea14cb1504","authors":[{"name":"Joaquin Gabaldon","ids":["32725741"]},{"name":"Ding Zhang","ids":["1775777"]},{"name":"Kira Barton","ids":["32629583"]},{"name":"Matthew Johnson-Roberson","ids":["2803647"]},{"name":"K. Alex Shorter","ids":["10285450"]}],"doi":"10.1109/IROS.2017.8206069"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206386","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a method for navigating 3D dynamically walking bipedal robots amidst obstacles. Our framework relies on composing gait primitives corresponding to limit-cycle locomotion behaviors and it produces nominal motion plans that are compatible with the system's dynamics and can be tracked with high fidelity. The low-level controllers of the biped are designed within the Hybrid Zero Dynamics (HZD) framework. Exploiting the dimensional reduction afforded by HZD and properties of invariant sets of switching systems among multiple equilibria, we obtain polynomial approximations of a reduced order Poincaré map and of the net change of the center of mass location over a stride. These polynomials are then incorporated in a high-level Rapidly Exploring Random Tree (RRT) planner to generate nominal plans which are tracked by the biped with drastically low drifting errors, without adversely affecting the time for computation.","inCitations":["876166a5b2926ab708287ce822271fe6f294e2c2","c223ef9f915941f8c4ad4af8ea6776131e666d60","0888183253e72e00936d18b539a7151eee0ac085","4d1f274b25085e692720d7c5ad2aed319283a86b","35bcad67f0a7fa4c8314c5be2aa4d955d057e885","660fb1b250589f2686a373b179380f79124d5ce8"],"pmid":"","title":"Almost driftless navigation of 3D limit-cycle walking bipeds","journalPages":"5025-5030","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206386"],"entities":["Approximation algorithm","Computation","Electronic switching system","Experiment","High- and low-level","Limit cycle","Motion planning","Polynomial","Random tree","Robot"],"journalVolume":"","outCitations":["36256596180fe57c345210057bc360a7024bfc9d","c2fe4c41b04f67fe1ef662baf669f7ff3e265a6f","214ebf5df64bbda2945c7d85ffd6875dba6f4f3b","e52c92965d65058045652ce64477ebec07d63d95","77e3a1aee5f4dc4000321f37f9a9e5cca2485967","8dc904a75fe59a04b7b146a984abeddcafb398da","c6beb7f9e149acc5eec186de823005d448777afc","971e9fbbbb9c16254fa96bc5a8bb69e8c98036f7","674d0c61553af234d163f35f286662f84cf7cb72","333a83381657661849f4f00a40b55b835ee73d6b","602a2ffaf4ff247e6d0bc4e05b223b10b203d79e","2de147fe320e5cc7325b6a41bced12ecfbd25051","7d0bfb58d6f8efccc128889c2ffbfffa3f217a78","444241b17653e20a784a2e11891edb8535f0befa","b344bb74e0844c2d47272f7d69616df4870bc5ca","9ddc7b1753f39b480eccc1f0d6dbc9b8eb6ee9d7","789d8ee60df2a7f1fc4e83b22bba37669aba9416"],"id":"444e16890006f4e9e8be46b1aacf4ecb55c4ed48","s2Url":"https://semanticscholar.org/paper/444e16890006f4e9e8be46b1aacf4ecb55c4ed48","authors":[{"name":"Sushant Veer","ids":["2083048"]},{"name":"Mohamad Shafiee Motahar","ids":["2457902"]},{"name":"Ioannis Poulakakis","ids":["1698307"]}],"doi":"10.1109/IROS.2017.8206386"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594470","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents the development and hovering control of a tailsitter unmanned aerial vehicle (UAV) that merges long endurance and vertical takeoff and landing (VTOL) abilities. The designed tailsitter contains one flying-wing with two motors and two elevons. Vehicle aerodynamics and a six-degrees-of-freedom (6-DOF) model are especially developed for the tailsitter. To achieve a good performance in outdoor stationary hovering and accurate vertical flying, the active disturbance rejection control (ADRC) for attitude controller is proposed. With signals from extended state observer (ESO) and tracking differentiator (TD), ADRC decouples the system model into a controllable chain of integrators. Based on the decoupled system dynamics, the motion of tailsitter can be easily handled by developed position controller. Experimental results are presented to corroborate the effectiveness of the controller in disturbance rejection.","inCitations":[],"pmid":"","title":"Active Disturbance Rejection Control of a Flying-Wing Tailsitter in Hover Flight","journalPages":"6390-6396","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594470"],"entities":[],"journalVolume":"","outCitations":[],"id":"9ba23b210ae91fbbfe2300b6c7a9cb8ebf09fdc8","s2Url":"https://semanticscholar.org/paper/9ba23b210ae91fbbfe2300b6c7a9cb8ebf09fdc8","authors":[{"name":"Yunjie Yang","ids":[]},{"name":"Jihong Zhu","ids":[]},{"name":"Xiaojun Zhang","ids":[]},{"name":"Xianyang Wang","ids":[]}],"doi":"10.1109/IROS.2018.8594470"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594042","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this work we introduce Natural Segmentation and Matching (NSM), an algorithm for reliable localization, using laser, in both urban and natural environments. Current state-of-the-art global approaches do not generalize well to structure-poor vegetated areas such as forests or orchards. In these environments clutter and perceptual aliasing prevents repeatable extraction of distinctive landmarks between different test runs. In natural forests, tree trunks are not distinctive, foliage intertwines and there is a complete lack of planar structure. In this paper we propose a method for place recognition which uses a more involved feature extraction process which is better suited to this type of environment. First, a feature extraction module segments stable and reliable object-sized segments from a point cloud despite the presence of heavy clutter or tree foliage. Second, repeatable oriented key poses are extracted and matched with a reliable shape descriptor using a Random Forest to estimate the current sensor's position within the target map. We present qualitative and quantitative evaluation on three datasets from different environments - the KITTI benchmark, a parkland scene and a foliage-heavy forest. The experiments show how our approach can achieve place recognition in woodlands while also outperforming current state-of-the-art approaches in urban scenarios without specific tuning.","inCitations":[],"pmid":"","title":"Seeing the Wood for the Trees: Reliable Localization in Urban and Natural Environments","journalPages":"8239-8246","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594042","https://arxiv.org/pdf/1809.02846v2.pdf","http://arxiv.org/abs/1809.02846","http://www.robots.ox.ac.uk/~mobile/Papers/2018IROS_tinchev.pdf","https://export.arxiv.org/pdf/1809.02846","http://www.robots.ox.ac.uk/~mobile/drs/Papers/2018IROS_tinchev.pdf","http://arxiv-export-lb.library.cornell.edu/pdf/1809.02846"],"entities":[],"journalVolume":"","outCitations":["d088f9710ff8c9e896fe17859dd4120b0d676c9a","5a3e2899deed746f1513708f1f0f24a25f4a0750","d3fd7128c9aff00d22c0f424f5f4be5e9617b0ac","026e3363b7f76b51cc711886597a44d5f1fd1de2","226e9f36bc0da559921e9270576fddc12cfb1c7d","9b682ac9123a0902e1eedb663a43032af5a61a6a","6873a4db9703c9bf38ddabf9abed17ac5b673b59","4cc610b6372f1db0c572948fb0c69a2c685c1fa7","48735bb8dd02613f481e79ac7703d96c4831313f","7842c2f0dedf9f891685a0abf6958435149eeea6","364be5719529e6b5f05254de9a2dc9a81869d01f","1457f7dbd7294cea1e7745820c5ed634420f87a8","367b02df89a7f0ad9f61356acf165068fcd5614a","07121fa5e8e881aa1217d7f590c3e2cbf16f6855","b8dd8fd303fe3a3202e5111da32da0d9c4311bba","5f0d86c9c5b7d37b4408843aa95119bf7771533a"],"id":"b56bd323601e1213c2e11448457dc6f4956dc577","s2Url":"https://semanticscholar.org/paper/b56bd323601e1213c2e11448457dc6f4956dc577","authors":[{"name":"Georgi Tinchev","ids":["51258452"]},{"name":"Simona Nobili","ids":["21301376"]},{"name":"Maurice Fallon","ids":["5665874"]}],"doi":"10.1109/IROS.2018.8594042"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594516","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"A topological shape analysis is proposed and utilized to learn concepts that reflect shape commonalities. Our approach is two-fold: i) a spatial topology analysis of point cloud segment constellations within objects. Therein constellations are decomposed and described in an hierarchical manner - from single segments to segment groups until a single group reflects an entire object. ii) a topology analysis of the description space in which segment decompositions are exposed in. Inspired by Persistent Homology, hidden groups of shape commonalities are revealed from object segment decompositions. Experiments show that extracted persistent groups of commonalities can represent semantically meaningful shape concepts. We also show the generalization capability of the proposed approach considering samples of external datasets.","inCitations":["646ced35185d6f88e822ccf5f509ffe78f51d24d","ed2c96071e89c93557bbb648232e8ca687a5d9cf"],"pmid":"","title":"Conceptualization of Object Compositions Using Persistent Homology","journalPages":"1095-1102","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594516","http://arxiv.org/abs/1803.02140","https://arxiv.org/pdf/1803.02140v3.pdf"],"entities":[],"journalVolume":"","outCitations":["227dd1ce2b65101694745e94ee16d1d532fe02de","182b048fdc9456979ea72c8b7b4760c8a69a735d","10eb7bfa7687f498268bdf74b2f60020a151bdc6","47c3c0273c010115cd1d5ee90210937f47658d4e","507a5f137deb46622ebfb2de996b04990db7eff8","2ce8e635741b91a9386df660923989a8973aa1cd","cb9dd4b09ac583fcc22f90f6f19557b164987c0f","68ee0ca68e558eee13c208bb0c10f96a4730a248","3ddc768508fac418205689075546d78518a4b10d","04fd5a259ccbdb6f5fd383dca2607f6ae8510348","055fec9810fe1e98944a229303b0000afbb47b31","43b8ee3235c1f6ef7cdc44ed1e93172e0d11d022","9e4d74d16cf57cf0a54aa7b21c63f41a1ad8cc7a","6a29c3473547e3a1667c0b90973fd9b93df63a7f","22d9933f76d2ca24bca07e00f1b7696e826c843e","3f42607589377d7d3a629a42e1058f4e0b4ab3d0","d7fd575c7fae05e055e47d898a5d9d2766f742b9","77ed1356a1f5055b89a86f2dff894ca9a389fbc5","8bf3d13e09fea2fd682b7e3d1f65f4272749bd86"],"id":"acb681fb3f5a53be6bb48419adb552cf5528b0c0","s2Url":"https://semanticscholar.org/paper/acb681fb3f5a53be6bb48419adb552cf5528b0c0","authors":[{"name":"Christian A. Mueller","ids":["48117831"]},{"name":"Andreas Birk","ids":["1708805"]}],"doi":"10.1109/IROS.2018.8594516"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546299","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"For transfer learning, many research works have demonstrated that effective use of information from multi-source domains will improve classification performance. In this paper, we propose a method of Targetize Multi-source Domain Bridged by Common Subspace (TMSD) for face recognition, which transfers rich supervision knowledge from more than one labeled source domains to the unlabeled target domain. Specifically, a common subspace is learnt for several domains by keeping the maximum total correlation. In this way, the discrepancy of each domain is reduced, and the structures of both the source and target domains are well preserved for classification. In the common subspace, each sample projected from the source domains is sparsely represented as a linear combination of several samples projected from the target domain, such that the samples projected from different domains can be well interlaced. Then, in the original image space, each source domain image can be represented as a linear combination of neighbors in the target domain. Finally, the discriminant subspace can be obtained by targetized multi-source domain images using supervised learning algorithm. The experimental results illustrate the superiority of TMSD over those competitive ones.","inCitations":[],"pmid":"","title":"Multi-source Domain Adaptation for Face Recognition","journalPages":"1349-1354","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546299"],"entities":[],"journalVolume":"","outCitations":["2cd0da17f7d20f01a58a22880f24d34cfddbca7c","a918f86d4052de3431fc939334eb4229a6a47f38","4cd547ae45556e7cf232eee261219872a8073649","cb6b9cc908a2862a17a1b4560ead1a49989275bc","5805a7ebfab3daf215df2c6f8c0eeeca2be642a1","94fd74be648857ae6355b99528b48b6085505df5","5f69b8a5922b0dbd46a295e5757ccd0977c0c35b","00859bb61a4ff955daa41373b2f6acaf6050bc38","075bc988728788aa033b04dee1753ded711180ee","1ccffaca596c42f42b573924e48b94a5661ff3a0","f38d09b11a8f6853ef27822df317646b70af5949","c99c7ece54e6f313d71a1ace54f61576b9ed7b72","3a84f3971fa1dff802a3d864d580085d8ce0c7cc","a8e8f3c8d4418c8d62e306538c9c1292635e9d27","05bd81538cbf475f47e1a2a8f416b271ae494bbb","227b18fab568472bf14f9665cedfb95ed33e5fce","e7f4e40dd896472cfebd22f8ba6ddee90c8b8eea","a4fde6c11d64dc8dd7e29142932acba67bd86297","764f5d33638780056f4619bcee35ee3efff79900","ff82825a04a654ca70e6d460c8d88080ee4a7fcc","a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","90713384f57fe8d3457158ee56b545699726c0e9","41b255bef542523af2e21b173ab49330f90c5ceb","2485c98aa44131d1a2f7d1355b1e372f2bb148ad","04ec16fac893ecb44b2ee59f3b34c6bd62c1dee5","4823e52161ec339d4d3526099a5477321f6a9a0f"],"id":"52d4cdf75cdb593fa449dd6a9afd26c5c9761d66","s2Url":"https://semanticscholar.org/paper/52d4cdf75cdb593fa449dd6a9afd26c5c9761d66","authors":[{"name":"Haiyang Yi","ids":["52143858"]},{"name":"Zhi Xu","ids":["32235450"]},{"name":"Yimin Wen","ids":["1879393"]},{"name":"Zhigang Fan","ids":["38266049"]}],"doi":"10.1109/ICPR.2018.8546299"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593529","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents the design and implementation of an assisted control technology for a small multirotor platform for aerial inspection of fixed energy infrastructure. Sensor placement is supported by a theoretical analysis of expected sensor performance and constrained platform behaviour to speed up implementation. The optical sensors provide relative position information between the platform and the asset, which enables human operator inputs to be autonomously adjusted to ensure safe separation. The assisted control approach is designed to reduced operator workload during close proximity inspection tasks, with collision avoidance and safe separation managed autonomously. The energy infrastructure includes single vertical wooden poles and crossarm with attached overhead wires. Simulated and real experimental results are provided.","inCitations":[],"pmid":"","title":"Assisted Control for Semi-Autonomous Power Infrastructure Inspection Using Aerial Vehicles","journalPages":"5719-5726","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1804.02154","https://doi.org/10.1109/IROS.2018.8593529","http://export.arxiv.org/pdf/1804.02154","https://arxiv.org/pdf/1804.02154v2.pdf"],"entities":[],"journalVolume":"","outCitations":["3ed3948827e0949770e8583b51bd0fedf4fd73fe","28d1edda2069495b08a07105e9e4069a395dc533","0dc31ff4731da71c0e38c1849e93e89a85020419","820f089e513335bc5bffa149770c3f741eade982","5c1e1ecf47d274ca5f8e8531e9cc1dcf03c05ee8","70b89abf5f13d2a77e09a3b249a753f280accdf4","ba1560638bb7e3417937cb5d2aac722af6705577","21bac421eafa59240ecf30f0d601841bfa608ec9","33600e7d5afd21bc32ecc3fa2e0b96ac7c8a7067"],"id":"da2c098e9e66f53a8872b487dd22199cd03fa63a","s2Url":"https://semanticscholar.org/paper/da2c098e9e66f53a8872b487dd22199cd03fa63a","authors":[{"name":"Aaron McFadyen","ids":["35646366"]},{"name":"Feras Dayoub","ids":["1757942"]},{"name":"Steve Martin","ids":["47361842"]},{"name":"Jason Ford","ids":["40563421"]},{"name":"Peter I. Corke","ids":["1714296"]}],"doi":"10.1109/IROS.2018.8593529"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202238","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Reliable lane detection is a fundamental necessity for driver assistance, driver safety functions and fully automated vehicles. Based on other detection and classification tasks, deep learning based methods are likely to yield the most accurate outputs for detecting lane markers, but require vast amounts of labeled data. We propose to train a deep neural network for detecting lane markers in camera images without manually labeling any images. To achieve this, we project high definition maps for automated driving into our image and correct for misalignments due to inaccuracies in localization and coordinate frame transformations. The corrections are performed by calculating the offset between features within our map and detected ones in the images. By using detections in the actual image for refining the projections, our labels become close to pixel perfect. After a fast, visual quality check, our projected lane markers can be used for training a fully convolutional network to segment lane markers in images. A single worker can easily generate 20,000 of those labels within a single day. Our fully convolutional network is trained only on automatically generated labels. All of our detections are based solely on gray-scale mono camera inputs without any additional information. The resulting network regularly detects clean lane markers at distances of around 150 meters on a 1 Megapixel camera.","inCitations":["1b0ffab263fa997150a5b967768485795153eb7b","2083f96cc5269e6b48ff3d009704ff37f0b4eb64"],"pmid":"","title":"Deep learning lane marker segmentation from automatically generated labels","journalPages":"777-782","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202238"],"entities":["Artificial neural network","Autonomous car","Convolutional neural network","Deep learning","Display resolution","Embedded system","Freeway","Grayscale","Internationalization and localization","Map","Map projection","Pixel Perfect","STRIPS","Sensor","Software deployment"],"journalVolume":"","outCitations":["784fbe1a809139e452ae24ca5c44972af603efff","6917799fc9bffed1edfc50adddaa270ae5a1daee","9201bf6f8222c2335913002e13fbac640fc0f4ec","69b9a4b31934ff69821dc0a082e35466e80b1097","32cde90437ab5a70cf003ea36f66f2de0e24b3ab","1c56a787cbf99f55b407bb1992d60fdfaf4a69b7","06289b677c6f5e81813ef3cba777a376041376bd","a538b05ebb01a40323997629e171c91aa28b8e2f","6c8b30f63f265c32e26d999aa1fef5286b8308ad","20f5b475effb8fd0bf26bc72b4490b033ac25129","c1fcc5eb6443033a2fe2a805ce29e2fc038f837f","6b570069f14c7588e066f7138e1f21af59d62e61","272216c1f097706721096669d85b2843c23fa77d","3e7f5dae276a3da8d3221964900732d41f9ecf05","12806c298e01083a79db77927530367d85939907","7aacf55ffc9caa31d6ba45e86a4c8bf97a0b06dd"],"id":"c72e13581cc69ab9cf59fb3d41015a67de68306e","s2Url":"https://semanticscholar.org/paper/c72e13581cc69ab9cf59fb3d41015a67de68306e","authors":[{"name":"Karsten Behrendt","ids":["8349249"]},{"name":"Jonas Witt","ids":["35108627"]}],"doi":"10.1109/IROS.2017.8202238"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202307","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"A model of hierarchical Slow Feature Analysis (SFA) enables a mobile robot to learn a spatial representation of its environment directly from images captured during a random walk. After the unsupervised learning phase a subset of the resulting representations are orientation invariant and code for the position of the robot. Hence, they change monotonically over space even though the variation of the sensory signals received from the environment might change drastically e.g. during rotation on the spot. Furthermore, the property of spatial smoothness allows us to infer a navigation direction by taking the difference between the measurement at the current location and a measurement at a target location. In our work we investigated the use of slow feature representations, learned for a specific environment, for the purpose of navigation. We present a straightforward method for navigation using gradient descent on the difference between two points specified in slow feature space. Due to its slowness objective, the resulting slow feature representations implicitly encode information about static obstacles, allowing a mobile robot to efficiently circumnavigate them by simply following the steepest gradient in slow feature space.","inCitations":["e72357e7efe88fa43f255dc1cd37ed8ba9a3b82a"],"pmid":"","title":"Efficient navigation using slow feature gradients","journalPages":"1311-1316","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202307"],"entities":["Algorithm","ENCODE","Experiment","Feature vector","Gradient descent","Mobile robot","Offline learning","Online and offline","Randomness","Simple Features","Simulation","Unsupervised learning"],"journalVolume":"","outCitations":["d46a3efd1b85c11cc9376fc8b48388eae5221d2b","d7022b8ff7c7327434e6cd1b9b70bba0655f3ed7","28e64e2b2269447b24b4e91ebfe9aabcba307b0a","14aa3690512c352cae9bc009b3e1d5a846346136","c8a04d0cbb9f70e86800b11b594c9a05d7b6bac0","4dee88275f83df7f34be3f45d6c723bcc12d9054","157a4495f2c53d67ddb87aa7f336a364d8aa4dd9","8fabd24ae76bfd19e2b59d545c494d5936933421","b05d91344f5510a1b549876ebf404b0329967419","42ed6f9da6049e3140269813ba60d9752fa2d756","49c9a64c47b20426356e560f992e7afa9171dd34","fc2ffb9daf9f0dd07e755d7ad5633907efb0a4b7","822160590349053d529d4404d06e77c07d8a584f","a710126263b22a6e05c5329a3ca6016afbaa8a09","43d0439492123cb82ce8b74aab7bbf6a8a32649a","d21da539b6f1e71f7500c4f94cc59e98dd946e15"],"id":"5c9fb8796423166a2871ae44378d0e63f1f19058","s2Url":"https://semanticscholar.org/paper/5c9fb8796423166a2871ae44378d0e63f1f19058","authors":[{"name":"Benjamin Metka","ids":["2743658"]},{"name":"Mathias Franzius","ids":["3149655"]},{"name":"Ute Bauer-Wersing","ids":["2150954"]}],"doi":"10.1109/IROS.2017.8202307"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594425","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper aims to identify in a practical manner unknown physical parameters, such as mechanical models of actuated robot links, which are critical in dynamical robotic tasks. Key features include the use of an off-the-shelf physics engine and the Bayesian optimization framework. The task being considered is locomotion with a high-dimensional, compliant Tensegrity robot. A key insight, in this case, is the need to project the space of models into an appropriate lower dimensional space for time efficiency. Comparisons with alternatives indicate that the proposed method can identify the parameters more accurately within the given time budget, which also results in more precise locomotion control.","inCitations":[],"pmid":"","title":"Efficient Model Identification for Tensegrity Locomotion","journalPages":"2985-2990","s2PdfUrl":"","pdfUrls":["http://arxiv-export-lb.library.cornell.edu/pdf/1804.04696","http://arxiv.org/abs/1804.04696","https://arxiv.org/pdf/1804.04696v1.pdf","https://doi.org/10.1109/IROS.2018.8594425"],"entities":[],"journalVolume":"","outCitations":["0c3afc552e9de1ae44e4d8cdc2848279c3de99ff","011d4b766a34bb1399ee50f8cceaeb1d25181335","195ecfe11433d39baf461386cfe616f385c9025a","5ba6dcdbf846abb56bf9c8a060d98875ae70dbc8","681a6e7f8c168b75eeb55d61cc8d594165cf3625","47598c6267a065ad0f9226c0a130728fedf18b81","dddf7781de8832dfc2262c40a52a873aa18ad90f","9d9df2110ce50e484ce1574d5048890b05216e2a","0f88de2ae3dc2ec1371d1e9f675b9670902b289f","d5b3b2a5d668ea53dac4881b012567fc5a4f7f12","7bde20c50a360e1d15e9e897a0dccd52b3c21e98","b28031f93033f147328f8aa2907d759ce6ed368c","846f19fcb631516f50064466fd7f0fee56f69987","ae96f13570bdcc55d3030c409e1f24a253b5b85a","0266f574a53de25cc475678e9f6f26b4ef8a894e","8ccd751a4d5797a1b3ad06c0d4eba3e651b2c95a","dd90dee12840f4e700d8146fb111dbc863a938ad","20b81502123839790405efe763564783a339acd9","59311339a482d828cc84171b933583342aba80f5","3d54efedc99c3c8eb7e073761f8b210408c8cfee","114b852ee1ece8d94f868467472ff011d9801f8d","c51152e3f80d50ac6df4f57dabc338d217a3f6e3","340f48901f72278f6bf78a04ee5b01df208cc508","d6d5e93e76d8d3373b12494e1e670dc20636c0dd","24b5eb0ce72bb62624b94c83b4047b2d4ded798c","01ad16f243d2fc77c279b7d73d74bc2522103d1a","48d893f022e3167fd6ec42d1475a82623e9d6456","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","17e2f0afba2421c7b2edc631fc2eca2191087555","0859657b9303b57a55d32174fb95ddf64f8be84e","1c78d3c154fabf1bc42b0f8a2b563de238dd49a4","1d7842d053865f13122cc80b25cfbf365cd11279","15fed4bbc4f2eb86f5ed39bef47e185f4df9a0b7"],"id":"f69cc11873ac37aba602700b3bf8e48ab8a50872","s2Url":"https://semanticscholar.org/paper/f69cc11873ac37aba602700b3bf8e48ab8a50872","authors":[{"name":"Shaojun Zhu","ids":["40534924"]},{"name":"David Allen Surovik","ids":["3094982"]},{"name":"Kostas E. Bekris","ids":["1739036"]},{"name":"Abdeslam Boularias","ids":["2209847"]}],"doi":"10.1109/IROS.2018.8594425"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545392","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Hashing based approximate nearest neighbor search has received considerable attention due to the demand of fast query for big multimedia data. Cross-modal hashing focuses on retrieval tasks across different modalities, which is more useful in practical applications. In this paper, we propose a novel two-stage cross-modal hashing method, referred to as Multi-Kernel Supervised Hashing with Graph Regularization (MKSRH). To better capture the essential attribute of original data, MKSRH first maps original data to a kernel space constructed by a linear combination of multiple kernel functions. Then the preliminary hash functions are learned using the Adaboost framework in the kernel space. To produce more accurate hash codes, the obtained hash function are then refined using a graph regularization based strategy. Experimental results on two canonical datasets show that MKSRH significantly outperforms than some typical cross-modal hashing methods, demonstrating the effectiveness and the superiority of the proposed approach.","inCitations":[],"pmid":"","title":"Multi-Kernel Supervised Hashing with Graph Regularization for Cross-Modal Retrieval","journalPages":"2717-2722","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545392"],"entities":[],"journalVolume":"","outCitations":["62b8efeb79e08ee4304d70226eefee5573b4d4b6","8adc24a49cc4407e6fea1b1ebd2f73dd5c0533d6","5fc8dd97ee6dcf60b474f2f169331a05b0242805","570689c91d8f663458319a53e4a48759e10263e9","5bd864d0a85a2b9306d9490ba618f525e87db8af","69ec8f0d28eb0cd894af7a5bff81a711c726caa1","1d1ed3ee382434cbad4323d539a47dc46f1ee913","50d6de8dd8f30d34a2c13cc9d3d203d4a978e543","478815622d22d85b0ade98c59b6ac78c3fb1ac21","7bfedccd58f59a143a455ca33e7241b72b79422c","0c8bc99df4f852d894b813ab88ca98eca539edc7","e18c394a10b6dc0914c60dd43eaa74a2dcd8f358","5b6c603fba0a66fb3c037632079bdca82ec3bf91","6eebd8762996501b28d3d94a7c166c79d37e7a57","381099409b460a7b32c09277597d454f87199edc","9c6a16a241c82ae695896572edd2f31febcc822f","385b401bf75771b02b5721641ae04ace43d2bbcd","56c05bd2779fa3738c71d82e272d4ebd1d8bfa42","9617b1076385d93ac85e183a4b67c46ca85f1f41","1c799eca7983c62f7815ac5f41787b3e552567b6","ad0226d57f4fa6a99b947caa1df47defe095685c","1379ad7fe27fa07419b7f6956af754bdb6d49558","6900231bca122a00f1cd989e16104a763d374dc1","6184ddbe780cb934f036b04dd1d28226b6bcbcce","9ca7db8c6ca82fe6436a806fc9b35e5aaa6df02f","868be5e9e7f965977140f24bcdfbe563521373eb","48e681ee58b2615500cad3c69046bfa0f0400a33","01c923d9e8a6215a0e9d63f949426fd4c6fb9575","c24495b0c14bf6b563e4b8c9656a4072d6f83995","7b3f39094a99d037c989ec0a9702f0dd05898d2a"],"id":"42047384a6c6d14516323f4e4ee773aa4a297fc3","s2Url":"https://semanticscholar.org/paper/42047384a6c6d14516323f4e4ee773aa4a297fc3","authors":[{"name":"Ming Zhu","ids":["46694855"]},{"name":"Huanghui Miao","ids":["32871884"]},{"name":"Jun Tang","ids":["49763172"]}],"doi":"10.1109/ICPR.2018.8545392"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545305","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"This paper develops a hierarchical feature representation that is based on a Bayesian non-parametric method. Feature learning is an important issue in classification and data analysis. It can improve the classification performance and increase the convenience of data processing and analysis. Popular methods of representation learning include methods that are based on mixture models or dictionary learning methods. However, current methods have some disadvantages. The use of a traditional mixture model, such as the Gaussian mixture model (GMM), involves the model selection problem and suffers a lack of hierarchy between components. Inspired by h-LDA, distance-based Gaussian hierarchical Dirichlet allocation (distance-based GhLDA) is proposed herein. This method can automatically determine the number of components and construct a hierarchical representation. The distance function between data is used in the prior distribution. The learnt representation in the proposed model has the advantage of hLDA, which can handle shared components and distinct components. The quantization loss problem, which commonly arises when a topic model is used to deal with continuous data, can be solved by assuming that the distribution of words follows a Gaussian rather than a Dirichlet distribution. The performance of the proposed model in solving audio and image classification problems is evaluated. Experimental results indicate that the distance-based GhLDA outperforms baseline methods.","inCitations":[],"pmid":"","title":"Learning a Hierarchical Latent Semantic Model for Multimedia Data","journalPages":"2995-3000","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545305"],"entities":[],"journalVolume":"","outCitations":["5a1d8aa620fc3f36a79c2bd43ecee2c1cc8ce1b3","27be8d825224432d7e019ca6ae3f187d657fd010","9ae252d3b0821303f8d63ba9daf10030c9c97d37","0523e14247d74c4505cd5e32e1f0495f291ec432","4a8b8746def96caa3efd65548040c5c597c4312a","55ef10672b69aaa4db60faeb3776d85f7af1a5f9","f4278dac2abafb66a0a824f1d59e72b67e7540a1","0602a179593a238a5cc4ef6f8d6df5d37296e391","8c0031cd1df734ac224c8c1daf3ce858140c99d5","fa6cbc948677d29ecce76f1a49cea01a75686619","2a3e107dad356099205fb86a44ede8dfd4d9956e","7fdf31d5ebdd293b3027e6555e256a936ff5515a","46c411af36a769de3b3ee6256d619fb874f3020c","05c8ab1345c62d56ea77d5fb0a17230765c6209b","1218e212d4b576ddfd4784320a0db3bb6b3a09cc","2edaa3d588ec84b93ef20b5242edd24d97a66b58","220586567c8ae0ced3b4eed7ea39cca121ea3d86","a4a1c80def21e4de6810aea897ba2bfc1772151f","12439a6ff384e95ee2262ee982bc055534e30487","1e56ed3d2c855f848ffd91baa90f661772a279e1","38dc55fcb7c187919eb596981dd6f7bb703bee86"],"id":"6e73d015e171ab1519c9d6f724e661d488eb9143","s2Url":"https://semanticscholar.org/paper/6e73d015e171ab1519c9d6f724e661d488eb9143","authors":[{"name":"Shao Hui Wu","ids":["35690349"]},{"name":"Yuan-Shan Lee","ids":["2033188"]},{"name":"Sih-Huei Chen","ids":["1940358"]},{"name":"Jia-Ching Wang","ids":["3205648"]}],"doi":"10.1109/ICPR.2018.8545305"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205974","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Tactile surface sensors (TSSs) are often utilized for contact management in human robot interaction scenarios. To provide added value in these applications, online robot programming approaches may exploit TSSs as gesture input devices. To this end, we introduce an invariant, compact gesture representation which facilitates robust and efficient online gesture recognition even for very small training data sets. The proposed two-stage recognition approach permits reliable gesture classification as well as convenient parameter extraction in the presence of typical disturbances affecting TSSs attached to manipulator links. Our experimental results demonstrate the remarkable recognition performance of the proposed approach using a set of 16 gestures and data of up to 31 subjects.","inCitations":[],"pmid":"","title":"Robust recognition of tactile gestures for intuitive robot programming and control","journalPages":"1643-1650","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205974"],"entities":["Data acquisition","Experiment","Gesture recognition","Human\u2013robot interaction","Input device","Multi-touch","Programmer","Programming paradigm","Real-time locating system","Robot end effector","Robotics","Sensor","Test set","Usability testing"],"journalVolume":"","outCitations":["0e0ceccfa5b89551a3d5485a50ac009d23993b03","bbe96fd0fa4194c5445bb80f0eb5f8ada6be41d3","ca61c529723121de6159b5f87639c87bac2cff7e","c5a198627d755b3f9145ec7d1f578d7bce844100","57144e8d13acd381f0679f26b79a681642c5f7d8","11246b9f6692272a35e65ce11419251efef86c1e","cb56ee5fdc9da10145d2f775347aca4483f9f6ea","4774462e321b3e92f17b29fdc086645dea276f05","f554d458600eefceccf0298b362d1263beb8dab6","8379c2d88365ce1a3f74f542d06986abab01a113","37b014e76b25b167cf764e5513ad5756e8de4e1a","20d7a27ef6aa66a1dfa87d5bb67cd20e8ea00141","c6af88bdf57a78395caa936813ad87c146145a55","0e8afcc5ce6f3af0c65c8629064bf8f0969d7fed","77f6e019783cc34aa586da8283a77ac1d48ca03e","2f1c9596a09ecf13fc137201e34beab1c38dd6f0","6434a1632f981ce7e54aaeb4438f3c088bf2d423","e1013b54dcf26763c6c132049999985960968ea9","7d66a6f9553fee482239f79830c3e03818346681"],"id":"921b432fbf86258713c9ff6d3e0051820e5a9b23","s2Url":"https://semanticscholar.org/paper/921b432fbf86258713c9ff6d3e0051820e5a9b23","authors":[{"name":"Daniel Kubus","ids":["2905740"]},{"name":"Arne Muxfeldt","ids":["2379460"]},{"name":"Konrad Kissener","ids":["31819466"]},{"name":"Jan Niklas Haus","ids":["34873392"]},{"name":"Jochen Steil","ids":["27551792"]}],"doi":"10.1109/IROS.2017.8205974"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.51","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Traditionally, practical authentication systems have considered only a single enrolled subject for verification. However, with the advent of mobile devices this paradigm has changed since a mobile device may be accessed by more than a single enrolled user. In this context, verification of multiple enrolled users has a practical importance. We address the issue of perfonnance degradation associated with multiple user authentication as compared to single user authentication. We interpret this problem in an open-set framework and introduce the notion of probability of negativity to alleviate the effect of multiple users in authentication. We further introduce a simple fusion scheme with the existing authentication methods to increase the intruder detection accuracy. Effectiveness of the proposed method is demonstrated using three publicly available face and touch gesture-based mobile active authentication datasets.","inCitations":["c7890690e6db6915b5d876b316f978d2e7dcddc6"],"pmid":"","title":"Towards Multiple User Active Authentication in Mobile Devices","journalPages":"354-361","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.51","http://www.rci.rutgers.edu/~vmp93/Conference_pub/MULTIUSER_AA_06.pdf"],"entities":["Authentication","Elegant degradation","Intruder detection","Mobile device","Multi-user","Negativity (quantum mechanics)","Programming paradigm","Rejection sampling"],"journalVolume":"","outCitations":["35a940734cc7f8085cb7b7694fd56c0aaa2fc181","f2d4ff0b45cab690f11576fc548c7df7281709dd","06cc7f3f9bb685b6ad9801751f653f014fa0a097","2bcd59835528c583bb5b310522a5ba6e99c58b15","16b5369a6b62f8ffe9f0b06b2aceded5411ff4dc","8765f22fbcdcf610a08b01db01edc4b8cc67d082","cf3f691fdf25449eea8136196a570a735b0efc38","759d9a6c9206c366a8d94a06f4eb05659c2bb7f2","21d5c838d19fcb4d624b69fe9d98e84d88f18e79","08442d3caf292c3af12b6fee7a3dfb6029ce97ec","0b370962301354d6f8572eb32610128ce69f1444","68a4af75887976e3c3cf43452e10ea8d5f7f8aa1","d40cb3e3ede3cff90792995af3b7c732f8658032","0d56fd8f6388683c4327e4af0a3e45c368742c5d","b511643e507a1789dccadc9da6683f3a00e103a4","52bc0f02e34ed1e2ce1f77d8f07aea2b87813e89","5367d509d76c2efb144a681efd442ddbf3b25f4a","8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486","ea21683b071eb7ccad6387e52be1694efaef8b61","05055c55f7a27dba649a11665ba4ebaa81432baa"],"id":"a3bcbca1f865849d2a619f609b3667e1d507d5d0","s2Url":"https://semanticscholar.org/paper/a3bcbca1f865849d2a619f609b3667e1d507d5d0","authors":[{"name":"Pramuditha Perera","ids":["15206897"]},{"name":"Vishal M. Patel","ids":["1741177"]}],"doi":"10.1109/FG.2017.51"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202269","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Simultaneous localization and mapping (SLAM) is a core element of every autonomous mobile robot. The underlying engine of a SLAM system is its back-end, which aims at optimally estimating the trajectory and map of the environment based on sensor data abstractions. Over the past decade, SLAM solutions based on graph optimization approaches prevailed over the filtering based solutions, since they dominated in performance over a wider range of applications. In this paper we propose a novel filtering based SLAM back-end based on the exactly sparse delayed state filter (ESDSF) derived on Lie groups (LG-ESDSF). The proposed filter retains all the good characteristics of the classic ESDSF, but also respects the state space geometry by employing filtering equations directly on Lie groups. We have compared our SLAM system with two current state-of-the-art SLAM solutions, namely ORB-SLAM and LSD-SLAM, on the KITTI vision benchmark suite. Test results show that the proposed SLAM based on the LG-ESDSF back-end can achieve same level of accuracy as the methods based on the graph optimization techniques, while maintaining lower computation times.","inCitations":["125ea938cb392b311473e29a9354a58dd1996f3b","9b70d7b9f169c5701857aa0841b2c7bfed7e474c"],"pmid":"","title":"Revival of filtering based SLAM? Exactly sparse delayed state filter on Lie groups","journalPages":"1012-1018","s2PdfUrl":"","pdfUrls":["http://lamor.fer.hr/images/50020776/Lenac2017b.pdf","https://doi.org/10.1109/IROS.2017.8202269"],"entities":["Algorithm","Autonomous robot","Benchmark (computing)","Computation","Extended Kalman filter","Mathematical optimization","Mobile robot","Simultaneous localization and mapping","Sparse matrix","State space","Time complexity"],"journalVolume":"","outCitations":["80ba7c7a8447557e606bd5665b688bdafec895af","36632fe163ee6942eba7693275d36820a11df8bc","91b174158b0cf20f18d752a3ac017464ccbff649","00e05b249c40ce6f3bdee7832956cba7780cfd39","3303b29b10ce7cd76c799ad0c796521751347f9f","55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1","41460a9c4a36f0c21e8ab5041ec000064981b4b3","84c6c752e9295a39544ab2ecb2d360ae820e9ab1","2aa362740ac9a2b304a74122da820e3829689842","0348cc294883adf24b48c00f50df9870983317d2","45aece5453774ee8520a3ca03da3f61a6c9106b2","3bec4f88a9c268fecadf22640de7b6dc7ea7ef71","03bd09f62445ee68095f20000342c1c76b57d7c9","24d1afc81644877f6fc34a5a15d7a41e03a4e522","230ad73e8bd1d3268d56c66a83442d24176b864d","95e35ac9f097290e8637a8439ebb40bb9ba4eeed","ec4601c4bf6a8c4072bc16de38766c6d596e0e67","4e2ea0414c9bdbe46fd5e666516bf36ff56d19fd","c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2","2fcf20fe96194f84f9774fd592a654fa9fb5ebf3","362f9af442e7242ec2f0b19a83ede6fa07637d37","4613727ef686c6186cab69e6b8be8cb1fa3ba800","3134a1e43715dc1554757103401832f1147d28e3","ce6d34010f04afa4cf3018f51bad8f480ebc759c","0be0d64304c2cc354dcad009e63c314a4d252bb1","4d5908a27f21d97fbe526d247fceae4c5bbceba0","99081a318ea8b552954e41ca7203dcf91fe2b018","ec322e123644a16cc44d6f1374fbdaea9cdd18ce","86918e800f10951ac56f376a84e74014fc1226de","27bb3145c6973e52fbb5f08472c5dc8e11edd026"],"id":"bb3998995d2f4eb1207703cd172f114df9ec4ff0","s2Url":"https://semanticscholar.org/paper/bb3998995d2f4eb1207703cd172f114df9ec4ff0","authors":[{"name":"Kruno Lenac","ids":["1886941"]},{"name":"Josip Cesic","ids":["2988291"]},{"name":"Ivan Markovic","ids":["2346045"]},{"name":"Igor Cvisic","ids":["2720665"]},{"name":"Ivan Petrovic","ids":["1747179"]}],"doi":"10.1109/IROS.2017.8202269"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594146","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Localization robustness against environment dynamics is significant for robots to achieve autonomous navigation in unmodified environments. A basic method of improving the robustness of a robot is considering the sensor observations obtained from mapped obstacles and using them for localizing the robot's pose. This study proposes an observation model that considers the class of sensor observations, where \u201cclass\u201d categorizes the sensor observations as those obtained from mapped and unmapped obstacles. In the proposed approach, the robot's pose and the class are estimated simultaneously. As a result, the robot's pose can be localized using the sensor observations obtained only from mapped obstacles. First, we evaluated the performance of the proposed approach using simulations. Further, we tested the proposed approach in a real-world mobile robot navigation competition, called \u201cTsukuba Challenge,\u201d held in Japan. The robustness and effectiveness of the proposed approach against environment dynamics were verified from the experimental results.","inCitations":["6775e5071228e7a437b983b6c5807a4237779d69"],"pmid":"","title":"Mobile Robot Localization Considering Class of Sensor Observations","journalPages":"3159-3166","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594146"],"entities":[],"journalVolume":"","outCitations":[],"id":"a4630583d90ea00e1d19e42167b87f0b2fe4ad0e","s2Url":"https://semanticscholar.org/paper/a4630583d90ea00e1d19e42167b87f0b2fe4ad0e","authors":[{"name":"Naoki Akai","ids":[]},{"name":"Luis Yoichi Morales","ids":[]},{"name":"Hiroshi Murase","ids":[]}],"doi":"10.1109/IROS.2018.8594146"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594058","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The public perception of android robots is a field of growing applied relevance. Currently, most androids are confined within controlled environments rendering interactions between potential end-users, and robots challenging. Even more challenging is for researchers to investigate end-users' perception of androids. We exploit pre-existing YouTube comments as artifacts for quantitative content analysis to gain an indication of social perception on androids. We perform a content analysis of 10301 YouTube comments from four different videos, and reflect on the textual reactions to video stimuli of four extremely human-like android robots. We use text mining and machine learning techniques to process and analyze our corpus. Our findings reveal three equally important topics that should be considered for paving the way towards a robotic society: human-robot relationships, technical specifications, and the science fiction valley. Considering people's attitudes, fears and wishes towards androids, researchers can increase citizen awareness, and engagement.","inCitations":[],"pmid":"","title":"Public perception of android robots: Indications from an analysis of YouTube comments","journalPages":"1255-1260","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594058"],"entities":[],"journalVolume":"","outCitations":[],"id":"ca9794b885d10e9c71b96c30645308ee5cbb51b7","s2Url":"https://semanticscholar.org/paper/ca9794b885d10e9c71b96c30645308ee5cbb51b7","authors":[{"name":"Evgenios Vlachos","ids":[]},{"name":"Zheng-Hua Tan","ids":[]}],"doi":"10.1109/IROS.2018.8594058"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202226","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, we described a new developed life-size humanoid robot. A purpose of the developed robot is to realize continuous operation for a long time and to improve an action autonomously. we considered three aspects of robustness, mechanical robustness, functional robustness and robustness of an action. Mechanical robustness was confirmed by the experiment that the robot fell down without mechanical failures and continued to work after falling down by using hard points. Functional robustness was designed to use power cable and to wear a suit which can be changed by required functionality. Robustness of an action was achieved as a standing up action using \u201cStateNet\u201d, which realized autonomous error recovery. Finally, we present a methodology to develop a humanoid robot platform which can continue to work in the real world.","inCitations":["16ba0385e87f311cdc8b9cd0f69ba18c57e91b15","5caf523f2817d2d05c4bea458e889ad199f93d92"],"pmid":"","title":"Development of life-sized humanoid robot platform with robustness for falling down, long time working and error occurrence","journalPages":"689-696","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202226"],"entities":["Autonomous robot","Continuous operation","Humanoid robot","Robustness (computer science)"],"journalVolume":"","outCitations":["1bb0382594b0ac5c29914fdd5d8ed2fcf9e49a47","a48e151f5cb2cc367a145f2910d82343349aef7b","7d7b3e74a40a87603798c8d675484713aef9ecbd","b023779a84ff18c466f5b65a2b28a438a1faad6d","d4d1266909b926ddc5840b4d69f70932b7382f47","1e5d86d0d88d315f46ad18167136dc31260866db","89c09c2b21e2e901aef8b631f7a58dbd00c57a94","6c267aafaceb9de1f1af22f6f566ba1d897054e4","2516398053e3fac614fa27e1073687ec39e48dca","92098bcab7f60bcb89ceb03f7d66b452d4486105","031dac310ada216a403bfd5c1cea2fa711dd928b","388c5e783f8d8cda66b324b456cbb5a5111a6153","48bc4d16f6da06832a4e294b1924198fefd110fd"],"id":"b7ab22caf67fc065dd9aab4111454d2435522bdb","s2Url":"https://semanticscholar.org/paper/b7ab22caf67fc065dd9aab4111454d2435522bdb","authors":[{"name":"Youhei Kakiuchi","ids":["2271736"]},{"name":"Masayuki Kamon","ids":["2775958"]},{"name":"Nobuyasu Shimomura","ids":["47327994"]},{"name":"Sou Yukizaki","ids":["3001275"]},{"name":"Noriaki Takasugi","ids":["40011901"]},{"name":"Shunichi Nozawa","ids":["2385539"]},{"name":"Kei Okada","ids":["1683608"]},{"name":"Masayuki Inaba","ids":["1749935"]}],"doi":"10.1109/IROS.2017.8202226"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206104","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents an investigation of human comfort with a small Unmanned Aerial Vehicle (sUAV) through a study offering a comparison of comfort with a sUAV versus a ground vehicle. Current research on human comfort with sUAVs has been limited to a single previous study, which did not include free flight, and while ground vehicle distancing has been studied, it has never been directly compared to a sUAV. The novelty in the approach is the use of a motion capture room to achieve smooth trajectories and precise measurements, while conducting the first free flight study to compare human comfort after interaction with aerial versus ground vehicles (within subjects, N=16). These results will contribute to understanding of social, collaborative, and assistive robots, with implications for general human-robot interactions as they evolve to include aerial vehicles. Based on the reduced stress and distance (36.5cm or 1.2ft) for ground vehicles and increased stress and distance (65.5cm or 2.15ft) for sUAVs, it is recommended that studies be conducted to understand the implications of design features on comfort in interactions with sUAVs and how they differ from those with ground robots.","inCitations":["cf560bac3b442161f8d5d6d814cb60771f2cd762"],"pmid":"","title":"Investigation of human-robot comfort with a small Unmanned Aerial Vehicle compared to a ground robot","journalPages":"2758-2765","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206104"],"entities":["Aerial photography","Baseline (configuration management)","HTTP 404","Interaction","Motion capture","Robot","Unmanned aerial vehicle"],"journalVolume":"","outCitations":["cf7d113a14be46b289a275cffec22f2c132127ad","4c908ae6af536c06e51858abc64a303bff91d938","450dbf3a0cc7de6809bebca153c36f546fdd4556","f7ce089462e62f872623ef330cc43b040c36fca2","dbf813f5a38c36155cd22485d62cbfcdcc01164a","9731665d6b9f926e6773eb5979b840d6b3b1fbe9","b92470be6c1bfbd59a541bdc6f271100ae683c95","213fedb60ed583fafaea3c5a8987b325927b59b9","1c84f0ccefdf9aa5ff482d4b126be871132c5c59","9c2db1ca1ac51ecf1281faa35f6b9c42f7aae865","4ff77b5d5a523772acd7e2dfcbe770cc7a7514da","0ccd9682878c639735e5b9351ba37a1d3b8ba55c","00590cce95f0077e1cf8b60755d0af887d070211","079afa32b43894dbf4e045a45153d0d60aece160","ca6b08930a5ce23a3173aeaf40a3db6488447b76","86e2a76cdda9661766a98e534a95c7e880afa8a1","c46cc46c85abc942a3cab4f1a3186ae4456b2f62","b013cf3d3d76ab8fe73915a4f17fb95173ac6dea","9b7256483998f84461536968d4d156c569cc6597","17e9debd875c8178c6f381949b1649c25c08b7c5","1678c7bd6580176574cda6dce3284b975797a613","114cd4177c3c345ef360cea3f472d040d449e20c","75c001fa4286a179064b9aa763b810ba4516a362","0bf381332ebd7012877aea1d3122879b5be0048b","4f801d41fb626fc2682f5ffd6c2b16130d1cf0c1"],"id":"d19ba6b19af415ab664e5b65146951b1ddaf6136","s2Url":"https://semanticscholar.org/paper/d19ba6b19af415ab664e5b65146951b1ddaf6136","authors":[{"name":"Urja Acharya","ids":["40868014"]},{"name":"Alisha Bevins","ids":["31912736"]},{"name":"Brittany A. Duncan","ids":["2666206"]}],"doi":"10.1109/IROS.2017.8206104"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00125","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"This demonstration is of an automated method for creating image collage in a mobile phone. The app creates semantically meaningful collages from images based on faces,,,,, image saliency and hybrid blending. The algorithm is designed for computational efficiency inorder for it run on a mobile device. Due to the increase in the use of social networks, users,,,, are uploading images and videos from social events. Collage presents a useful crisper summary of the event. It is popular among users as evident from Layout from Instagram: Collage app, which has over 100 million downloads. A limitation of these apps is that they are dependent on the user for selecting the grid layout and optimal cropping of the images. Our smart collage over comes these limitations and to the best of our knowledge this is one of the first mobile based solutions, which considers the faces and the region importance and later merges the salient images automatically in a non-rigid grid layout. Other interesting works either have a semi-rigid [1] or a fully rigid grid based layout [2] or are desktop based [3].","inCitations":[],"pmid":"","title":"Fast Face and Saliency Aware Collage Creation for Mobile Phones","journalPages":"788-788","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00125"],"entities":["Algorithm","Alpha compositing","Desktop computer","Instagram","Mobile device","Mobile phone","Semiconductor industry","Social network","Tree traversal","Upload"],"journalVolume":"","outCitations":["5557d2ebf06068414a7adbeb745e5c995322b0bd","196e8a4693f1f4677f538c01f6b7db00ffa6d2fa"],"id":"8d41318b89425712bd03d510715ad9985fd76d26","s2Url":"https://semanticscholar.org/paper/8d41318b89425712bd03d510715ad9985fd76d26","authors":[{"name":"Love Mehta","ids":["50515755"]},{"name":"Abhinav Dhall","ids":["1735697"]}],"doi":"10.1109/FG.2018.00125"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594271","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Semantic navigation is the navigation paradigm in which environmental semantic concepts and their relationships are taken into account to plan the route of a mobile robot. This paradigm facilitates the interaction with humans and the understanding of human environments in terms of navigation goals and tasks. At the high level, a semantic navigation system requires two main components: a semantic representation of the environment, and a reasoning system. This paper is focused on develop a model of the environment using semantic concepts. This paper presents two solutions for the semantic navigation paradigm. Both systems implement an ontological model. Whilst the first one uses a relational database, the second one is based on KnowRob. Both systems have been integrated in a semantic navigator. We compare both systems at the qualitative and quantitative levels, and present an implementation on a mobile robot as a proof of concept.","inCitations":[],"pmid":"","title":"Reasoning Systems for Semantic Navigation in Mobile Robots","journalPages":"5654-5659","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594271"],"entities":[],"journalVolume":"","outCitations":[],"id":"aff9dc11d03cd674de6f6e878edfd2a88c9fd09b","s2Url":"https://semanticscholar.org/paper/aff9dc11d03cd674de6f6e878edfd2a88c9fd09b","authors":[{"name":"Jonathan Crespo","ids":[]},{"name":"Ramón Barber","ids":[]},{"name":"Óscar Martínez Mozos","ids":[]},{"name":"Daniel Beßler","ids":[]},{"name":"Michael Beetz","ids":[]}],"doi":"10.1109/IROS.2018.8594271"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546023","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Image classifiers based on deep neural networks (DNNs) are vulnerable to tiny, imperceptible perturbations. Maliciously generated adversarial examples can exploit the instability of DNNs and mislead it into outputting a wrong classification result. Prior works showed the transferability of adversarial perturbations between models and between images. In this work, we shed light on the combination of source/target misclassification, black-box attack, and universal perturbation by employing improved evolutionary algorithms. We additionally find that the use of adversarial initialization enhances the efficiency of evolutionary algorithms finding universal perturbations. Experiments demonstrate impressive misclassification rates and surprising transferability for the proposed attack method using different models trained on CIFAR-10 and CIFAR-100 datasets. Our attach method also shows robustness against defensive measures like adversarial training.","inCitations":[],"pmid":"","title":"Universal Perturbation Generation for Black-box Attack Using Evolutionary Algorithms","journalPages":"1277-1282","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546023"],"entities":[],"journalVolume":"","outCitations":["b7fc6be6451c0b99e92add9060d8579e013fec3f","94187ef33e34af2cdb42502083c6f9b4c3f5ba6b","83bfdd6a2b28106b9fb66e52832c45f08b828541","909cbc349da89def005e9935b0addbe523299e9f","abf38db4775bec89c950013030d8eda56a89d32a","28550d85622f3f93134a54b203a37213a29f5e54","35734e8724559fb0d494e5cba6a28ad7a3d5dd4d","2483949663c6ba1632924e5a891edf7c8e2d3674","9866d0833277f767f1ac4192f9fb15a0dd94c9bf","5d90f06bb70a0a3dced62413346235c02b1aa086","34d6940c147e7679ec6d9e1729df161fdf73a026","307e3d7d5857942d7d2c9d97f7437777535487e0","68e4e5b7c1b862ba5d0cfe046f2e84c6a8c3c394","49e77b981a0813460e2da2760ff72c522ae49871","061356704ec86334dbbc073985375fe13cd39088"],"id":"786a762bbcb661d2265d1417ab6dd8490f57bddc","s2Url":"https://semanticscholar.org/paper/786a762bbcb661d2265d1417ab6dd8490f57bddc","authors":[{"name":"Sivy Wang","ids":["35412352"]},{"name":"Yucheng Shi","ids":["46571755"]},{"name":"Yahong Han","ids":["2302512"]}],"doi":"10.1109/ICPR.2018.8546023"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206016","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper proposes a new algorithm for human motion estimation using inertial measurement unit (IMU) measurements. We model the joints by matrix Lie groups, namely the special orthogonal groups SO(2) and SO(3), representing rotations in 2D and 3D space, respectively. The state space is defined by the Cartesian product of the rotation groups and their velocities and accelerations, given a kinematic model of the articulated body. In order to estimate the state, we propose the Lie Group Extended Kalman Filter (LG-EKF), thus explicitly accounting for the non-Euclidean geometry of the state space, and we derive the LG-EKF recursion for articulated motion estimation based on IMU measurements. The performance of the proposed algorithm is compared to the EKF based on Euler angle parametrization in both simulation and real-world experiments. The results show that for motion near gimbal lock regions, which is common for shoulder movement, the proposed filter is a significant improvement over the Euler angles EKF.","inCitations":[],"pmid":"","title":"Human motion estimation on Lie groups using IMU measurements","journalPages":"1965-1972","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206016","http://lamor.fer.hr/images/50020776/Joukov2017.pdf"],"entities":["Algorithm","Cartesian closed category","Euler","Experiment","Extended Kalman filter","Gimbal lock","Jacobian matrix and determinant","Kinesiology","Motion estimation","Recursion","Robot end effector","Sensor","Simulation","State space"],"journalVolume":"","outCitations":["0be0d64304c2cc354dcad009e63c314a4d252bb1","0bd7992f4fd04823a80773e1db368b0637a0381a","17554c40ee551b1fc905ed17f3ab803d4f04f633","00e05b249c40ce6f3bdee7832956cba7780cfd39","f22aa286399c1d3274fa9f6be592c26c12a87c6c","a46907b1c2ea5d00fb231c0f405fabe97f3b74af","91b174158b0cf20f18d752a3ac017464ccbff649","b707f74ee0b7d6e3e3d0646cf9ca6c2515f1480b","856e40be7b4a37ceac69e15db9205b5f78445c61","5433f452dcc9be2b862728e2c3dce9417f0c0b97","5ce805d6c3ac1e9965e20845bad9134679591673","b80d369868419c115c38939c096bd9b022795c54","56178de1001efe54792ad93f6980de5d5e91906b","53c290348f67aeb674c7f483696f3b452899755c","4b6cd441c3b28fda57ca76c41cbc77a404e48ac3","685fd4ada7eac705ee2d57bfd78a8b5334634424","41460a9c4a36f0c21e8ab5041ec000064981b4b3","84c6c752e9295a39544ab2ecb2d360ae820e9ab1","b8e70e21db2918c9932f0ca27c98ee7da168223f","25e26d4db49e280669e26cd05305d27f35a0791a","aaab5a9dac151bb7760cc7ba477b92e9c3034fc9","21a67ab1c2d8eae7daa6a3d4dbb6201fd892ac73","c2862a43be0335a5f9f4c927ef5e9a44f0ec2089","3134a1e43715dc1554757103401832f1147d28e3","66f9be62649a977c79d5eb56eb45d1028be76666","271c0dbbf14dd64e0eea8c709ecfa608b704079d","857a8b26061ef777aff7a7e0c08ce4e15f7cfa1d","f606dd9673bd955e3ee80acfe1fddc835f1c3e61","ec4601c4bf6a8c4072bc16de38766c6d596e0e67","44d2e5d3891ccdd7dd01f8dd09f0bc38a1fd7bb4","1593bdf1f169c0b115ebdfa7dd7c4228c015bd43"],"id":"eed3928a5510be78d46d0de40c37b3287d853d99","s2Url":"https://semanticscholar.org/paper/eed3928a5510be78d46d0de40c37b3287d853d99","authors":[{"name":"Vladimir Joukov","ids":["30761067"]},{"name":"Josip Cesic","ids":["2988291"]},{"name":"Kevin Westermann","ids":["49761399"]},{"name":"Ivan Markovic","ids":["2346045"]},{"name":"Dana Kulic","ids":["1768765"]},{"name":"Ivan Petrovic","ids":["1747179"]}],"doi":"10.1109/IROS.2017.8206016"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206189","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, we propose a GPU parallelized SLAM system capable of using photometric and inertial data together with depth data from an active RGB-D sensor to build accurate dense 3D maps of indoor environments. We describe several extensions to existing dense SLAM techniques that allow us to operate in real-time onboard memory constrained robotic platforms. Our primary contribution is a memory management algorithm that scales to large scenes without being limited by GPU memory resources. Moreover, by integrating a visual-inertial odometry system, we robustly track the camera pose even on an agile platform such as a quadrotor UAV. Our robust camera tracking framework can deal with fast camera motions and varying environments by relying on depth, color and inertial motion cues. Global consistency is achieved via regular checking for loop closures in conjunction with a pose graph, as a basis for corrective deformation of the 3D map. Our efficient SLAM system is capable of producing highly dense meshes up to 5mm resolution at rates close to 60Hz fully onboard a UAV. Experimental validations both in simulation and on a real-world platform, show that our approach is fast, more robust and more memory efficient than state-of-the-art techniques, while obtaining better or comparable accuracy.","inCitations":["55ef6ba6c2f6222471407c1536edd520b0d908a2","96279aaa0716507d0a1c338ae66712a6cee12f15"],"pmid":"","title":"Onboard real-time dense reconstruction of large-scale environments for UAV","journalPages":"3479-3486","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206189"],"entities":["Agile software development","Algorithm","File spanning","For loop","Graphics processing unit","Map","Match moving","Maxima and minima","Memory management","Odometry","Parallel computing","Real-time clock","Robot","Sensor","Simulation","Simultaneous localization and mapping","Unmanned aerial vehicle"],"journalVolume":"","outCitations":["c2adb138674a7f75ade139d0e56d7a8e83f0803e","4f82917d630d25a9b32f4a79497cd6819687498e","3157cf251c4e78405b2e5524da6109b29cc9e65e","9d14d1ccda55806e0a093851b52a17b8e5383cfe","0389da0dc8aacb7b1d8ccfb5f13fa01a7afae862","a30fbdcf8cf49552c0a358a61a4906a9cee781dc","cd35f65b8170c88d05286d93c332eaf318f14881","655b14a3a3033182a27aa01d5cb16b99f0419a70","5ae1d937c987936f98464b7f052b22754dfc2af7","1485b72151d53895a1e251d2e42d84f78c1a5009","7633c7470819061477433fdae15c64c8b49a758b","4466dcf725e5383b10a6c5172c860f397e006b48","41984604bf962ad33c8c07dec557945671246b74","282cf28ac9508bd66a6ddf0709c9db9dc0fdf162","0d2edc46f81f9a0b0b62937507ad977b46729f64","978e7f8557774fa1ebd228e181c4c154449423c9","19a5e5b31e741d0f9cacf9c93de782184d24c947","a13dc9739e4637599359d792fd60d511ab8a016e"],"id":"aa3f2501d74eb783d8fce70bf04d0f830363ea16","s2Url":"https://semanticscholar.org/paper/aa3f2501d74eb783d8fce70bf04d0f830363ea16","authors":[{"name":"Anurag Sai Vempati","ids":["1903638"]},{"name":"Igor Gilitschenski","ids":["2072248"]},{"name":"Juan I. Nieto","ids":["40830045"]},{"name":"Paul A. Beardsley","ids":["1777539"]},{"name":"Roland Siegwart","ids":["1720483"]}],"doi":"10.1109/IROS.2017.8206189"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594119","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Event cameras are an emerging technology in computer vision, offering extremely low latency and bandwidth, as well as a high temporal resolution and dynamic range. Inherent data compression is achieved as pixel data is only produced by contrast changes at the edges of moving objects. However, current trends in state-of-the-art visual algorithms rely on deep-learning with networks designed to process colour and intensity information contained in dense arrays, but are notoriously computationally heavy. While the combination of these visual technologies could lead to fast, efficient, and accurate detection and recognition algorithms, it is uncertain whether the compressed event-camera data actually contain the required information for these techniques to discriminate between objects and a cluttered background. This paper presents a pilot study in which off-the-shelf deep-learning is applied to visual events for object detection on the iCub robotic platform, and analyses the impact of temporal integration of the event data. We also present a novel pipeline that bootstraps event-based dataset annotation from mature frame-based algorithms, in order to more quickly generate the required datasets.","inCitations":[],"pmid":"","title":"Towards Event-Driven Object Detection with Off-the-Shelf Deep Learning","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594119"],"entities":[],"journalVolume":"","outCitations":[],"id":"c35b13ed0b385ad46334fdf344d26ba8b8d2308e","s2Url":"https://semanticscholar.org/paper/c35b13ed0b385ad46334fdf344d26ba8b8d2308e","authors":[{"name":"Massimiliano Iacono","ids":[]},{"name":"Stefan Weber","ids":[]},{"name":"Arren Glover","ids":[]},{"name":"Chiara Bartolozzi","ids":[]}],"doi":"10.1109/IROS.2018.8594119"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545720","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Modeling appearance and short-term dynamic information is the mainstream strategy for action recognition based on deep learning. We consider it important to model the multi-scale temporal information, including both short-term information and long-term information, for action representation. In this paper, a novel temporal inception architecture (TIA) is proposed to solve this problem, which is a general structure that can be combined with multi-segment-based frameworks for action recognition. The TIA is composed of multiple spatial-temporal convolutional branches, in which the temporal information of different scales is extracted. Then feature maps of all branches are concatenated as the output of TIA. In our experiments, the TIA is embedded into temporal segment networks (TSN) to construct our temporal segment inception networks (TSIN) for action recognition tasks. Extensive experiments demonstrate that TSIN outperforms TSN and achieves the state-of-the-art performance on HMDB51 and UCF101.","inCitations":[],"pmid":"","title":"Temporal Inception Architecture for Action Recognition with Convolutional Neural Networks","journalPages":"3216-3221","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545720"],"entities":["Concatenation","Convolutional neural network","Deep learning","Embedded system","Embedding","Epilepsy, Temporal Lobe","Experiment","Extraction","Neural Tube Defects","Numerous","Television Interface Adaptor","Time-Sensitive Networking"],"journalVolume":"","outCitations":["3afbb0e64fcb70496b44b30b76fac9456cc51e34","2d83ba2d43306e3c0587ef16f327d59bf4888dc3","b5f2846a506fc417e7da43f6a7679146d99c5e96","83174a52f38c80427e237446ccda79e2a9170742","51cb09ee04831b95ae02e1bee9b451f8ac4526e3","0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","39dc218eb5582e861e53c8451586786cfaca08ec","4c822785c29ceaf67a0de9c699716c94fefbd37d","16fc1065c296840cb0f8ca62601aa17b7f0a02bf","44f23600671473c3ddb65a308ca97657bc92e527","3c86dfdbdf37060d5adcff6c4d7d453ea5a8b08f","1c30bb689a40a895bd089e55e0cad746e343d1e2","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","10af69f11301679b6fbb23855bf10f6af1f3d2e6","070874b011f8eb2b18c8aa521ad0a7a932b4d9ad","025720574ef67672c44ba9e7065a83a5d6075c36","6d4e3616d0b27957c4107ae877dc0dd4504b69ab","4fcd19b0cc386215b8bd0c466e42934e5baaa4b7","2e5b160892b70a1e846aa9dcdf132b8011937ec6","36c473fc0bf3cee5fdd49a13cf122de8be736977","53698b91709112e5bb71eeeae94607db2aefc57c","24115d209e0733e319e39badc5411bbfd82c5133","b61a3f8b80bbd44f24544dc915f52fd30bbdf485","c5935b92bd23fd25cae20222c7c2abc9f4caa770","c6241e6fc94192df2380d178c4c96cf071e7a3ac","970b4d2ed1249af97cdf2fffdc7b4beae458db89","39b452453bea9ce398613d8dd627984fd3a0d53c","14b5e8ba23860f440ea83ed4770e662b2a111119","2f1485994ef2c09a7bb2874eb8252be8fe710db1","4f298d6d0c8870acdbf94fe473ebf6814681bd1f","722fcc35def20cfcca3ada76c8dd7a585d6de386","886431a362bfdbcc6dd518f844eb374950b9de86","4e6c9be0b646d60390fe3f72ce5aeb0136222a10"],"id":"e39db6a72f533b5adb2ecd9feaadbbab8204e024","s2Url":"https://semanticscholar.org/paper/e39db6a72f533b5adb2ecd9feaadbbab8204e024","authors":[{"name":"Wei Zhang","ids":["40321510"]},{"name":"Jiepeng Cen","ids":["3432527"]},{"name":"Huicheng Zheng","ids":["39458374"]}],"doi":"10.1109/ICPR.2018.8545720"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205998","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present a novel framework for integrity analysis of swarm robotic systems using the symmetric Kullback-Leibler Divergence. The objective is to understand a robot swarm's vulnerability to malicious intrusion and to develop the necessary computational tools that would detect the presence of malicious agents within the swarm. Using ensemble approaches for modeling and analyzing stochastic task allocation, we analyze the performance of the proposed strategy subject to different system parameters, and show how different design choices can facilitate early intrusion detection. We further evaluate the performance of our method in realistic scenarios through stochastic simulations for different team sizes. The main contribution is an analysis framework whose output can be used to avoid system-inherent design flaws and to decrease the damage that can be inflicted by an undetected attacker.","inCitations":["98ec665d9e3dd8bf8adfc9416acbf988362bd2ca"],"pmid":"","title":"Intrusion detection for stochastic task allocation in robot swarms","journalPages":"1830-1837","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205998"],"entities":["Autonomous robot","Centralized computing","Intrusion detection system","Kullback\u2013Leibler divergence","Malware","Robot","Sensor","Simulation","Stochastic process","Swarm robotics"],"journalVolume":"","outCitations":["5cfdc9b97e0877ebd9b38e2daf47c7092974409e","6b17c7f97202fca139dca97311c4573cb69d0b39","e6e217867c54a3a51e9b4d289371d4c92ba61d47","677b80e91168d446a8f1261820aedb74fe2f6832","1cdc01a40efee7ff5b4c546e2e078ed011ad77ae","667a08a882fab779b9fa281a8d8dbfacab99f4b4","37395ca45062663f864684b5afb690194dbdc992","e52b41646fabd5a1a5b0d19bcb9293291313a40c","a1849c8475e5cae104c33931733ec6f41e9469d4","08c0c3b42c337809e233371c730c23d08da442bf","4f8deabc58014eae708c3e6ee27114535325067b","31093f5a40a3992a4dbef8df93cf26f0e4db7a5b","c8e52479d3228059d0f44b2cdf1496504828d396","6550c029c8f16aeff322e6fb9f1558edb27068d5","06d3d2385c23399c9fe19343f52d6a1f94f12fa8"],"id":"9ffa02bd624886d7f21a85b123968167b517f621","s2Url":"https://semanticscholar.org/paper/9ffa02bd624886d7f21a85b123968167b517f621","authors":[{"name":"Florian Maushart","ids":["30870985"]},{"name":"Amanda Prorok","ids":["2081613"]},{"name":"M. Ani Hsieh","ids":["1711771"]},{"name":"Vijay Kumar","ids":["39806219"]}],"doi":"10.1109/IROS.2017.8205998"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593422","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose an end-effector for realizing various locomotion modes and simple work of a legged robot. The locomotion modes include climbing a vertical ladder, crawling, and walking. The simple work includes grasping and switching motions required at a disaster site. The developed end-effector has a two-pronged hook shape and two fingers for grasping and working and can be used to perform the locomotion and manipulation tasks described above. The experimental results confirmed that the four-limb robot WAREC-1 (WAseda REsCuer-No. 1) equipped with our proposed end-effector was able to climb a vertical ladder and perform the crawling motion. We also confirmed that the end-effector could grasp and switch five types of objects: a cylinder, cylinder with trigger, T-shaped, disk, and thin plate.","inCitations":[],"pmid":"","title":"End-effector with a Hook and Two Fingers for the Locomotion and Simple Work of a Four-limbed Robot","journalPages":"2727-2732","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593422"],"entities":[],"journalVolume":"","outCitations":[],"id":"b5cee2d89c7d60177fec7b8db55de82ef1d74955","s2Url":"https://semanticscholar.org/paper/b5cee2d89c7d60177fec7b8db55de82ef1d74955","authors":[{"name":"Takashi Matsuzawa","ids":[]},{"name":"Asaki Imai","ids":[]},{"name":"Kenji Hashimoto","ids":[]},{"name":"Tomotaka Teramachi","ids":[]},{"name":"Xiao Sun","ids":[]},{"name":"Shunsuke Kimura","ids":[]},{"name":"Nobuaki Sakai","ids":[]},{"name":"Yuki Yoshida","ids":[]},{"name":"Kengo Kumagai","ids":[]},{"name":"Takanobu Matsubara","ids":[]},{"name":"Koki Yamaguchi","ids":[]},{"name":"Atsuo Takanishi","ids":[]}],"doi":"10.1109/IROS.2018.8593422"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594010","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Proprioceptive terrain sensing is essential for rough terrain traversal because it helps legged robots to negotiate individual steps by reacting to terrain irregularities. In this work, we propose to utilize inertial data in the detection of the contact between the leg and the terrain during the stride phase of the leg. We show that relatively cheap accelerometers can be utilized to reliably detect a foot-strike, and thus allow the robot to crawl irregular terrains. The continuous data processing is compared with the interrupt mode in which data are provided only around the foot-strike event. The interrupt mode exhibits significantly better performance, and it also supports generalization of the foot-strike event detector learned from data collected in slow locomotion to faster locomotion where the signals slightly change. The proposed solution is experimentally validated using a real hexapod walking robot for which the walking speed has been improved in comparison to the previous adaptive motion gait based on a force threshold-based position controller for the foot-strike detection.","inCitations":[],"pmid":"","title":"Online Foot-Strike Detection Using Inertial Measurements for Multi-Legged Walking Robots","journalPages":"7622-7627","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594010"],"entities":[],"journalVolume":"","outCitations":[],"id":"0f5c64a6e338d321a20a1802009d577002cbaf7a","s2Url":"https://semanticscholar.org/paper/0f5c64a6e338d321a20a1802009d577002cbaf7a","authors":[{"name":"Petr Cizek","ids":[]},{"name":"Jirí Kubik","ids":[]},{"name":"Jan Faigl","ids":[]}],"doi":"10.1109/IROS.2018.8594010"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00034","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a novel convolutional neural architecture for facial action unit intensity estimation. While Convolutional Neural Networks (CNNs) have shown great promise in a wide range of computer vision tasks, these achievements have not translated as well to facial expression analysis, with hand crafted features (e.g. the Histogram of Orientated Gradient) still being very competitive. We introduce a novel Edge Convolutional Network (ECN) that is able to capture subtle changes in facial appearance. Our model is able to learn edge-like detectors that can capture subtle wrinkles and facial muscle contours at multiple orientations and frequencies. The core novelty of our ECN model is in its first layer which integrates three main components: an edge filter generator, a receptive gate and a filter rotator. All the components are differentiable and our ECN model is end-to-end trainable and learns the important edge detectors for facial expression analysis. Experiments on two facial action unit datasets show that the proposed ECN outperforms state-of-the-art methods for both AU intensity estimation tasks.","inCitations":[],"pmid":"","title":"Edge Convolutional Network for Facial Action Intensity Estimation","journalPages":"171-178","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00034"],"entities":["Backpropagation","Computer vision","Convolutional neural network","Edge detection","End-to-end principle","Experiment","Explicit Congestion Notification","Gradient","Network model","Sensor","Software propagation","Vii"],"journalVolume":"","outCitations":["0b2cbe47a9bdea2898bce630165ec04a304aed53","16fc1065c296840cb0f8ca62601aa17b7f0a02bf","b91f54e1581fbbf60392364323d00a0cd43e493c","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","24bf94f8090daf9bda56d54e42009067839b20df","c0c0b8558b17aa20debc4611275a4c69edd1e2a7","04c07ecaf5e962ac847059ece3ae7b6962b4e5c4","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","7ada60106605bebb66812f85eed16d64d1acb972","833b6e61468fe655b5067ca91608fc37246c767b","0d8b85953c25c23c512d4522d5597c8e3e0bb8c7","4aeebd1c9b4b936ed2e4d988d8d28e27f129e6f1","d0a21f94de312a0ff31657fd103d6b29db823caa","f9c431f58565f874f76a024add2aa80717ec5cf5","0021f46bda27ea105d722d19690f5564f2b8869e","21dff32ff0f6b1244ce9c4aad2f468889748fbb2","8729441d734782c3ed532a7d2d9611b438c0a09a","ea80bfaf8525317bde0dacf57ace2b8610c9e719","3ec77809aaa7bd30858a4274e3c28a2a0259b30c","f472cb8380a41c540cfea32ebb4575da241c0288","ce3304119ba6391cb6bb25c4b3dff79164df9ac6","029b53f32079063047097fa59cfc788b2b550c4b","dbb1871efda531c5340a49e147173c374d069ad1","b82f89d6ef94d26bf4fec4d49437346b727c3bd4","5a5f0287484f0d480fed1ce585dbf729586f0edc","2fda461869f84a9298a0e93ef280f79b9fb76f94","853f01a6d9b1a73b765c0b1a9748024d3e59c9ce"],"id":"f575acc7e71aa7c02b5d747b8ee8293fb4bdaf98","s2Url":"https://semanticscholar.org/paper/f575acc7e71aa7c02b5d747b8ee8293fb4bdaf98","authors":[{"name":"Liandong Li","ids":["2089565"]},{"name":"Tadas Baltrusaitis","ids":["1756344"]},{"name":"Bo Sun","ids":["38600799"]},{"name":"Louis-Philippe Morency","ids":["1767184"]}],"doi":"10.1109/FG.2018.00034"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206207","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Admittance control is a widely used approach for guaranteeing a compliant behavior of the robot in physical human-robot interaction. When an admittance-controlled robot is coupled with a human, the dynamics of the human can cause deviations from the desired behavior of the robot, mainly due to a stiffening of the human arm, and thus generate high-frequency unsafe oscillations of the robot. In this paper we present a novel methodology for detecting the rising oscillations in the human-robot interaction. Furthermore, we propose a passivity-preserving strategy to adapt the parameter of the admittance control in order to get rid of the high-frequency oscillations and, when possible, to restore the desired interaction model. A thorough experimental validation of the proposed strategy is performed on a group of 26 users performing a cooperative task.","inCitations":["24414db0148b25163f7fa125b347ff45e2485c49"],"pmid":"","title":"Variable admittance control preventing undesired oscillating behaviors in physical human-robot interaction","journalPages":"3611-3616","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206207"],"entities":["Admittance parameters","Computer multitasking","Human\u2013robot interaction","Robot","Sensor","Velocity (software development)"],"journalVolume":"","outCitations":["dfb5fc4bbae01a11427eddc7eadb2905ecd50e0e","3af849c37419835ba6086e465fa20863d4a9abef","5e5fbadc070bb0263f5abde578154e0896da8c80","6ae90e482217c0bbf6f014170ff9b27d189af7c9","5c09f1906be185e870d48b746fb395d4774e41b1","1469af61b228670b4de8d66f146acf0f29724bb2","dc837996cada49f47114d31119acc9098da2b055","d63968bb65c72cfb4d372564514b9d45ba08c1b9","9467b21886aec268470ac2e7dbe1834e9af8c76a","296f153eeb02a3a6a3f7c146ef03185f4bae4061","4f443f51f90a6a4f3553edb544428edfdb36dc16","2b5af70a4f618d3923c49f0b6d2728a3bb3df227"],"id":"ebe8183fc41c6513a3326dda2f90d4b803784574","s2Url":"https://semanticscholar.org/paper/ebe8183fc41c6513a3326dda2f90d4b803784574","authors":[{"name":"Chiara Talignani Landi","ids":["2921961"]},{"name":"Federica Ferraguti","ids":["31991456"]},{"name":"Lorenzo Sabattini","ids":["3120453"]},{"name":"Cristian Secchi","ids":["1690166"]},{"name":"Marcello Bonfé","ids":["3087074"]},{"name":"Cesare Fantuzzi","ids":["1737618"]}],"doi":"10.1109/IROS.2017.8206207"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.103","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Micro emotion recognition is a very challenging problem because of the subtle appearance variants among different facial expression classes. To deal with the mentioned problem, we proposed a multi-modality convolutional neural networks (CNNs) based on visual and geometrical information in this paper. The visual face image and structured geometry are embedded into a unified network and the recognition accuracy can be benefic from the fused information. The proposed network includes two branches. The first branch is used to extract visual feature from color face images, and another branch is used to extract the geometry feature from 68 facial landmarks. Then, both visual and geometry features are concatenated into a long vector. Finally, the concatenated vector is fed to the hinge loss layer. Compared with the CNN architecture only used face images, our method is more effective and has got better performance. In the final testing phase of Micro Emotion Challenge1, our method has got the first place with the misclassification of 80.212137.","inCitations":["6ac1dc59e823d924e797afaf5c4a960ed7106f2a","67ad22cd25095c2602d46ce4cb110b46bb5d1e81","e1ccb2aa8207366de390350695cfe4ddff081087","fa14aac0bbae7cfcd19cb49cb93aff382f637f74","39ade96e6e680d58a8aaa8e2a72616b706707dce"],"pmid":"","title":"Multi-modality Network with Visual and Geometrical Information for Micro Emotion Recognition","journalPages":"814-819","s2PdfUrl":"","pdfUrls":["http://shuaizhou.me/papers/FG17_microEmotion.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2017.103"],"entities":["Artificial neural network","Concatenation","Convolutional neural network","Displacement mapping","Embedded system","Emotion recognition","Facial recognition system","Hinge loss","Modality (human\u2013computer interaction)","Unified Framework","Vii"],"journalVolume":"","outCitations":["6dd440eec6718f29a35b464345df2c77a4f0e085","6c8b30f63f265c32e26d999aa1fef5286b8308ad","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","a3688ebcce02bbafa87e3f644841d7d78172fc08","2911e7f0fb6803851b0eddf8067a6fc06e8eadd6","2c285dadfa6c07d392ee411d0213648a8a1cf68f","177bc509dd0c7b8d388bb47403f28d6228c14b5c","1606b1475e125bba1b2d87bcf1e33b06f42c5f0d","9952d4d5717afd4a27157ed8b98b0ee3dcb70d6c","d8a8b7c08f042d891d32cf4991f676e791734246","c0c0b8558b17aa20debc4611275a4c69edd1e2a7","722fcc35def20cfcca3ada76c8dd7a585d6de386","1824b1ccace464ba275ccc86619feaa89018c0ad","2c03df8b48bf3fa39054345bafabfeff15bfd11d","33c68d6bc73e74ea042dc5b9fe966625565c475e","d58b16cd32d3056af45541504d5f4c7edd267241"],"id":"06214c4bdf2354b5a1a13958998d79bf46ac53ee","s2Url":"https://semanticscholar.org/paper/06214c4bdf2354b5a1a13958998d79bf46ac53ee","authors":[{"name":"Jianzhu Guo","ids":["46220439"]},{"name":"Shuai Zhou","ids":["2950852"]},{"name":"Jinlin Wu","ids":["49387921"]},{"name":"Jun Wan","ids":["1756538"]},{"name":"Xiangyu Zhu","ids":["8362374"]},{"name":"Zhen Lei","ids":["1718623"]},{"name":"Stan Z. Li","ids":["34679741"]}],"doi":"10.1109/FG.2017.103"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206452","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The increasing energy consumption within the industrial sector causes great concerns among numerous countries. Accordingly, an extensive reorientation towards an energy-efficient facility and processes design is essential for innovative production systems. In terms of automated object manipulation, energy efficiency significantly is affected by the physical design as well as by the dynamic characteristics of the manipulator. Since the influence on physical design parameters often is limited, a great impact on energy-efficiency of innovative manufacturing systems may result from an intelligent task management and suitable motion strategies. This contribution identifies energy-efficiency potentials for industrial manipulators in terms of motion design and redundant actuator configurations. In this context, an efficient trajectory planning algorithm is proposed, estimating the energy demand of object manipulation tasks in consideration of dynamic motion parameters as well as redundant actuator configurations. For this purpose, the geometric path as well as the motion law of given trajectories are optimized by a spatial displacement of predefined nodes and by an adjustment of corresponding time intervals. In order to verify its generality, the presented method is applied to the spatial n-PRPaR manipulator exhibiting actuation redundancy. According results show a significant energy reduction, establishing high potentials for an increased efficiency of industrial robots.","inCitations":[],"pmid":"","title":"A study on efficient motion design for redundantly actuated parallel kinematic manipulators","journalPages":"5638-5645","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206452"],"entities":["Algorithm","Automated planning and scheduling","Displacement mapping","Industrial robot","Iteration","Mathematical optimization","Microsoft Outlook for Mac","Physical design (electronics)","Triple modular redundancy","Velocity (software development)"],"journalVolume":"","outCitations":["dd351faf9874eae31f8d0043de8680606988bff8","bce7c2c5e7ac9eb0863f78e297683b04b83fb707","15b991b966a9b446705f4484e3380702121e470e","1bf8460f0922d48c7ef1767ef8e52a0fba1321e7","120d368f1f00014febb7eaac23d13179d04ab703","8c26faed3a7bb950110932a8a8ad9950739592c5","805fd9078f1d24564b45130cd527d4799af6b612","6429dca0b686431fa24ea4742555706ea70e7a64","615526383d6b05960aed65e44425b4e749d5c5fc","0f047c4bf459f4feb51b93522e7c6908747eb88b","270d24da13010c775f3b09b9fc53b2e612fc97aa","7add06b4f458363eba62e7941307162eb0839d1a"],"id":"16fc1aebafed8d04f274899614e6789402621a98","s2Url":"https://semanticscholar.org/paper/16fc1aebafed8d04f274899614e6789402621a98","authors":[{"name":"Michael Lorenz","ids":["46476731"]},{"name":"Jascha Paris","ids":["39421966"]},{"name":"Tobias Haschke","ids":["12671003"]},{"name":"Frederic Scholer","ids":["39561197"]},{"name":"Mathias Hüsing","ids":["2586116"]},{"name":"Burkhard Corves","ids":["2754908"]}],"doi":"10.1109/IROS.2017.8206452"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.100","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Zero shot learning (ZSL) is about being able to recognize gesture classes that were never seen before. This type of recognition involves the understanding that the presented gesture is a new form of expression from those observed so far, and yet carries embedded information universal to all the other gestures (also referred as context). As part of the same problem, it is required to determine what action/command this new gesture conveys, in order to react to the command autonomously. Research in this area may shed light to areas where ZSL occurs, such as spontaneous gestures. People perform gestures that may be new to the observer. This occurs when the gesturer is learning, solving a problem or acquiring a new language. The ability of having a machine recognizing spontaneous gesturing, in the same manner as humans do, would enable more fluent human-machine interaction. In this paper, we describe a new paradigm for ZSL based on adaptive learning, where it is possible to determine the amount of transfer learning carried out by the algorithm and how much knowledge is acquired from a new gesture observation. Another contribution is a procedure to determine what are the best semantic descriptors for a given command and how to use those as part of the ZSL approach proposed.","inCitations":["d11c6beaf4d08fdecf89ca405a7e5e9117c085c7","cd7b4fba7e1ab0c6671553881a1f5790ab26fd6c","a94ae434e45de46aae3c81f7d384b845ffa42c42","9831061dd9f3e53057ce98aa1208731119cb7f12"],"pmid":"","title":"A Semantical & Analytical Approach for Zero Shot Gesture Learning","journalPages":"796-801","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.100"],"entities":["Algorithm","Embedded system","Gesture recognition","Human\u2013computer interaction","Programming paradigm","Semantic data model","Spontaneous order"],"journalVolume":"","outCitations":["8968707716e0743c248f8da18c5873f83649c3a1","2830fb5282de23d7784b4b4bc37065d27839a412","06554235c2c9361a14c0569206b58a355a63f01b","0566bf06a0368b518b8b474166f7b1dfef3f9283","22b5bcd590f6d4c04b8de28217b001da9667ec33","0d77e82e45bea1ab04703373c37c9aead7481303","22b1b9a0d3df813ba5912d457d8f366e7a0a273c","0f96eee0407b9ce9ea01629ed01bcf6802f97272","8b9df16f88db4a0aa50fc51962766764ac316ff4","14761b89152aa1fc280a33ea4d77b723df4e3864","759a3b3821d9f0e08e0b0a62c8b693230afc3f8d","094a8048e2df9c0c4bfd2cb96723a1d08b940ae2","9c2c1b499c4e6fe0e3bf821d6754bde75660206b","0e650b5e54a3624792952899a0f79b91a1d68e79","0f6911bc1e6abee8bbf9dd3f8d54d40466429da7","a1dd806b8f4f418d01960e22fb950fe7a56c18f1","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","75dcf4f024c048668cd859b2974b69a433be9623","2e384f057211426ac5922f1b33d2aa8df5d51f57"],"id":"31697737707d7f661cbc6785b76cf9a79fee3ccd","s2Url":"https://semanticscholar.org/paper/31697737707d7f661cbc6785b76cf9a79fee3ccd","authors":[{"name":"Naveen Madapana","ids":["1785202"]},{"name":"Juan Pablo Wachs","ids":["1768610"]}],"doi":"10.1109/FG.2017.100"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.81","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"In this paper, we explore global and local features obtained from Convolutional Neural Networks (CNN) for learning to estimate head pose and localize landmarks jointly. Because there is a high correlation between head pose and landmark locations, the head pose distributions from a reference database and learned local deep patch features are used to reduce the error in the head pose estimation and face alignment tasks. First, we train GNet on the detected face region to obtain a rough estimate of the pose and to localize the seven primary landmarks. The most similar shape is selected for initialization from a reference shape pool constructed from the training samples according to the estimated head pose. Starting from the initial pose and shape, LNet is used to learn local CNN features and predict the shape and pose residuals. We demonstrate that our algorithm, named JFA, improves both the head pose estimation and face alignment. To the best of our knowledge, this is the first system that explores the use of the global and local CNN features to solve head pose estimation and landmark detection tasks jointly.","inCitations":["8fe2ab8b7f3418cbfcda418a90befbaa084dfd6d","3cb2841302af1fb9656f144abc79d4f3d0b27380","c907fcd38f76b889fb92d68020035938927715cb","eb2b9c34323d3a8d097a2564c8864a742b9f73be","06c5e2be3ca252d778f70d8565de2cbcdceff3d7","927933478b8a560749d2475005c91982f2e92e2a","587dad3d93ed1925868b8851ae3eab2d2d31f717","38d8ff137ff753f04689e6b76119a44588e143f3","faead8f2eb54c7bc33bc7d0569adc7a4c2ec4c3b","5b2cfee6e81ef36507ebf3c305e84e9e0473575a","4e4a4359c7dd25af7e2ef0910928cd9faa5d0cfb","1fc249ec69b3e23856b42a4e591c59ac60d77118","3933e323653ff27e68c3458d245b47e3e37f52fd","9ba4a5e0b7bf6e26563d294f1f3de44d95b7f682","f611f46455ed6ad9af85eeb22e294082dced9bed","f4421adced24d729d5ed22559308c2b4719b44c2"],"pmid":"","title":"Joint Head Pose Estimation and Face Alignment Framework Using Global and Local CNN Features","journalPages":"642-649","s2PdfUrl":"","pdfUrls":["http://cbl.uh.edu/pub_files/07961802.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2017.81"],"entities":["3D pose estimation","Algorithm","Bibliographic database","Convolutional neural network","Experiment","GNet"],"journalVolume":"","outCitations":["162dfd0d2c9f3621d600e8a3790745395ab25ebc","57ebeff9273dea933e2a75c306849baf43081a8c","75fd9acf5e5b7ed17c658cc84090c4659e5de01d","82e66c4832386cafcec16b92ac88088ffd1a1bc9","2c17d36bab56083293456fe14ceff5497cc97d75","f731b6745d829241941307c3ebf163e90e200318","2ea6a93199c9227fa0c1c7de13725f918c9be3a4","639937b3a1b8bded3f7e9a40e85bd3770016cf3c","2a4153655ad1169d482e22c468d67f3bc2c49f12","023be757b1769ecb0db810c95c010310d7daf00b","8d4f12ed7b5a0eb3aa55c10154d9f1197a0d84f3","0e3fcfe63b7b6620e3c47e9751fe3456e85cc52f","084bd02d171e36458f108f07265386f22b34a1ae","1ca815327e62c70f4ee619a836e05183ef629567","50832083df91f83333364544b24f8af4798eca32","1922ad4978ab92ce0d23acc4c7441a8812f157e5","b2cd92d930ed9b8d3f9dfcfff733f8384aa93de8","9901f473aeea177a55e58bac8fd4f1b086e575a4","624496296af19243d5f05e7505fd927db02fd0ce","085ceda1c65caf11762b3452f87660703f914782","607aebe7568407421e8ffc7b23a5fda52650ad93","e4754afaa15b1b53e70743880484b8d0736990ff","193debca0be1c38dabc42dc772513e6653fd91d8","72e10a2a7a65db7ecdc7d9bd3b95a4160fab4114","88a323dc70de8788a9cdd415f7c65fc3d3a8c7cb","f921e6f5085f1ebbd8289081e499240a89bf6c43","0e986f51fe45b00633de9fd0c94d082d2be51406","1824b1ccace464ba275ccc86619feaa89018c0ad","286812ade95e6f1543193918e14ba84e5f8e852e","5c124b57699be19cd4eb4e1da285b4a8c84fc80d","995b2868326837cde96e01390f87b2dee6239bdb","22e2066acfb795ac4db3f97d2ac176d6ca41836c","03f98c175b4230960ac347b1100fbfc10c100d0c","2724ba85ec4a66de18da33925e537f3902f21249","500b92578e4deff98ce20e6017124e6d2053b451"],"id":"466f80b066215e85da63e6f30e276f1a9d7c843b","s2Url":"https://semanticscholar.org/paper/466f80b066215e85da63e6f30e276f1a9d7c843b","authors":[{"name":"Xiang Jun Xu","ids":["47158418"]},{"name":"Ioannis A. Kakadiaris","ids":["1706204"]}],"doi":"10.1109/FG.2017.81"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206556","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"As robotic systems become more popular in household environments, the complexity of required tasks also increases. In this work we focus on a domestic chore deemed dull by a majority of the population, the task of ironing. The presented algorithm improves on the limited number of previous works by joining 3D perception with force/torque sensing, with emphasis on finding a practical solution with a feasible implementation in a domestic setting. Our algorithm obtains a point cloud representation of the working environment. From this point cloud, the garment is segmented and a custom Wrinkleness Local Descriptor (WiLD) is computed to determine the location of the present wrinkles. Using this descriptor, the most suitable ironing path is computed and, based on it, the manipulation algorithm performs the force-controlled ironing operation. Experiments have been performed with a humanoid robot platform, proving that our algorithm is able to detect successfully wrinkles present in garments and iteratively reduce the wrinkleness using an unmodified iron.","inCitations":[],"pmid":"","title":"Robotic ironing with 3D perception and force/torque feedback in household environments","journalPages":"6484-6489","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1706.05340","https://doi.org/10.1109/IROS.2017.8206556","https://arxiv.org/pdf/1706.05340v1.pdf"],"entities":["Algorithm","Domestic robot","Experiment","Humanoid robot","Point cloud"],"journalVolume":"","outCitations":["425534de2e78e729ced6c964e81f82c2956a881c","1222798bdf49999ee0a08fd7148536b0ab41157d","41984604bf962ad33c8c07dec557945671246b74","e3e734d23cc35c8fdc39d0ab51adbf59ef11679f","35fc11f937bd98992957b25c4ccf3d93a75d7c45","732d795b507474166cbd9bd678b947d2967923fa","4353017376291844c70a8b5ba4a1a475c2a73cd9","aab1cb0a3ad2ef3818afce62c232cd16753ddf3c","64034d5c2b82e81dd357fd32e3b84667b5c5472f"],"id":"29288243c794425a17051209e08563f53778d2d5","s2Url":"https://semanticscholar.org/paper/29288243c794425a17051209e08563f53778d2d5","authors":[{"name":"David Estevez","ids":["31709564"]},{"name":"Juan G. Victores","ids":["1679565"]},{"name":"Raul Fernandez-Fernandez","ids":["19251163"]},{"name":"Carlos Balaguer","ids":["1752580"]}],"doi":"10.1109/IROS.2017.8206556"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594130","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"For a deeper understanding of ecological functions and semantics of wild bird vocalizations (i.e., songs and calls), it is important to clarify the fine-scaled and detailed relationships among their characteristics of vocalizations and their behavioral contexts. However, it takes a lot of time and effort to obtain such data using conventional recordings or by human observation. Bringing out a robot to a field is our approach to solve this problem. We are developing a portable observation system called HARKBird using a robot audition HARK and microphone arrays to understand temporal patterns of vocalizations characteristics and their behavioral contexts. In this paper, we introduce a prototype system to 2D localize vocalizations of wild birds in real-time, and to classify their song types after recording. We show that the system can estimate the position of songs of a target individual and classify their songs with a reasonable quality to discuss their song - behavior relationships.","inCitations":[],"pmid":"","title":"Extracting the Relationship between the Spatial Distribution and Types of Bird Vocalizations Using Robot Audition System HARK","journalPages":"2485-2490","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594130"],"entities":[],"journalVolume":"","outCitations":[],"id":"f20ccdbf030af7fea196c0b0026ad6d514d9a513","s2Url":"https://semanticscholar.org/paper/f20ccdbf030af7fea196c0b0026ad6d514d9a513","authors":[{"name":"Shinji Sumitani","ids":[]},{"name":"Reiji Suzuki","ids":[]},{"name":"Shiho Matsubayashi","ids":[]},{"name":"Takaya Arita","ids":[]},{"name":"Kazuhiro Nakadai","ids":[]},{"name":"Hiroshi G. Okuno","ids":[]}],"doi":"10.1109/IROS.2018.8594130"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206372","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Robots performing everyday tasks such as cooking in a kitchen need to be able to deal with variations in the household tools that may be available. Given a particular task and a set of tools available, the robot needs to be able to assess which would be the best tool for the task, and also where to grasp that tool and how to orient it. This requires an understanding of what is important in a tool for a given task, and how the grasping and orientation relate to performance in the task. A robot can learn this by trying out many examples. This learning can be faster if these trials are done in simulation using tool models acquired from the Web. We provide a semi-automatic pipeline to process 3D models from the Web, allowing us to train from many different tools and their uses in simulation. We represent a tool object and its grasp and orientation using 21 parameters which capture the shapes and sizes of principal parts and the relationships among them. We then learn a \u2018task function' that maps this 21 parameter vector to a value describing how effective it is for a particular task. Our trained system can then process the unsegmented point cloud of a new tool and output a score and a way of using the tool for a particular task. We compare our approach with the closest one in the literature and show that we achieve significantly better results.","inCitations":["c2a04802e6de1820956652d3467f6d80dc367bdd","f79121c5bfca1cb5e9c8ace68234b603bb7bce98","a464e86b4116b84098f34317ee5c9a9984084d64","81d327ec41c67728b15438bca86d10b72de1d88f","910daa4cef89f120102ed417f9bda29f8f814717"],"pmid":"","title":"Learning how a tool affords by simulating 3D models from the web","journalPages":"4923-4929","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206372"],"entities":["3D modeling","Interpolation","Map","Nicolai Petkov","Norton Zone","Point cloud","Robot","Semiconductor industry","Simulation","Vii","World Wide Web"],"journalVolume":"","outCitations":["508cd67582d471e6e1374e4ccc7401f237b8d2df","235a4917b5e11db9867301f4d0115c605e3d08d5","440e5349f175d0071c18b00561750dfcc1c57ef0","7d4092d44d8401be2d1a1b1101db7be2d36d3537","0458cec30079a53a2b7726a14f5dd826b9b39bfd","486fe99b49523dafcb5dce3a2342c387191f8d3e","bb8123b5f19c05e63be57e8439ebab483174eac4","49e85869fa2cbb31e2fd761951d0cdfa741d95f3","5dc919fa03ec0071efd6079344bfdf1cfd866722","6a29c3473547e3a1667c0b90973fd9b93df63a7f","1632e29caa038f0957d9c30edf0988870a775786","5a871828fdfc6818cb54f90bc6addb12712f2571","13ab059e6b592ca7bcb14337316ec1ac14aa5c5a","bfb530a66a706b417056916516df8b6432ffe4ef","a1092b43720138db89d2bf46495c276a91ccb22f","da738311f71e5512e57225cc5da47a75a1a68861","ac77c6cd753e4a2ccba1dd4f6e3551ad65719879"],"id":"8123dca5279e4f15cbec6071241fff87596f6851","s2Url":"https://semanticscholar.org/paper/8123dca5279e4f15cbec6071241fff87596f6851","authors":[{"name":"Paulo Abelha","ids":["2605521"]},{"name":"Frank Guerin","ids":["8139616"]}],"doi":"10.1109/IROS.2017.8206372"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594360","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper we describe a control strategy for a user-adaptive human-robot system for an intelligent robotic Mobility Assistive Device (MAD)using raw data from a single laser-range-finder (LRF)mounted on the MAD and scanning the walking area. The proposed control architecture consists of three modules. In the first module, a previously proposed methodology (termed IMM-PDA-PF)delivers the augmented human state estimation of the user by providing robust leg tracking and on-line estimation of the human gait phases. This information is processed at the next module for providing the pathological gait parametrization and characterization, by computing specific gait parameters for each gait cycle. These gait parameters form the feature vector that classifies the user in a certain class related to risk of fall. Those are of particular significance to the system, since the gait parameters and the respective class are used in the third module, i.e. the human-robot formation controller, in order to adapt the desired formation of the human-robot system, by selecting the appropriate control variables. The experimental evaluation comprises gait data from real patients, and demonstrates the stability of the human-robot formation control, indicating the importance of incorporating an on-line gait characterization of the user, using non-wearable and non-invasive methods, in the context of a robotic MAD.","inCitations":["6cb9c41a30ac5127d87a4dc4fcd0809b4303b819"],"pmid":"","title":"User-Adaptive Human-Robot Formation Control for an Intelligent Robotic Walker Using Augmented Human State Estimation and Pathological Gait Characterization","journalPages":"6016-6022","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594360"],"entities":[],"journalVolume":"","outCitations":[],"id":"211562b3528a769ae91b2eb999d417e8b7c5d696","s2Url":"https://semanticscholar.org/paper/211562b3528a769ae91b2eb999d417e8b7c5d696","authors":[{"name":"Georgia Chalvatzaki","ids":[]},{"name":"Xanthi S. Papageorgiou","ids":[]},{"name":"Petros Maragos","ids":[]},{"name":"Costas S. Tzafestas","ids":[]}],"doi":"10.1109/IROS.2018.8594360"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593891","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper we present the design and control strategies of a novel quadruped climbing robot (named Magneto) with three degrees of freedom (3-DOF) actuated limbs and a 3-DOF compliant magnetic foot. By exploiting its high degrees of freedom, Magneto is able to deform its body shape to squeeze through gaps of 23cm, which is smaller than standard human entry portholes of industrial confined spaces. Its compact foot design of footprint 4cm allows Magneto to walk on narrow beams of thickness less than 5cm, even at varying separation. The inherent high dimensional system design enables the body to be positioned in a wide range of orientations and seamlessly switch a limb function from locomotion to manipulation mode mid-climb. This capability enables access to confined space openings and occluded pockets and navigation through complex 3-D structures previously not demonstrated on legged climbing robots.","inCitations":[],"pmid":"","title":"Magneto: A Versatile Multi-Limbed Inspection Robot","journalPages":"2253-2260","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593891"],"entities":[],"journalVolume":"","outCitations":[],"id":"bb75e0cc52db905af687ec271df3a305272dfce0","s2Url":"https://semanticscholar.org/paper/bb75e0cc52db905af687ec271df3a305272dfce0","authors":[{"name":"Tirthankar Bandyopadhyay","ids":[]},{"name":"Ryan Steindl","ids":[]},{"name":"Fletcher Talbot","ids":[]},{"name":"Navinda Kottege","ids":[]},{"name":"Ross Dungavell","ids":[]},{"name":"Brett Wood","ids":[]},{"name":"James Barker","ids":[]},{"name":"Karsten Hoehn","ids":[]},{"name":"Alberto Elfes","ids":[]}],"doi":"10.1109/IROS.2018.8593891"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.111","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"This paper presents an unified convolutional neural network (CNN), named AUMPNet, to perform both Action Units (AUs) detection and intensity estimation on facial images with multiple poses. Although there are a variety of methods in the literature designed for facial expression analysis, only few of them can handle head pose variations. Therefore, it is essential to develop new models to work on non-frontal face images, for instance, those obtained from unconstrained environments. In order to cope with problems raised by pose variations, an unique CNN, based on region and multitask learning, is proposed for both AU detection and intensity estimation tasks. Also, the available head pose information was added to the multitask loss as a constraint to the network optimization, pushing the network towards learning better representations. As opposed to current approaches that require ad hoc models for every single AU in each task, the proposed network simultaneously learns AU occurrence and intensity levels for all AUs. The AUMPNet was evaluated on an extended version of the BP4D-Spontaneous database, which was synthesized into nine different head poses and made available to FG 2017 Facial Expression Recognition and Analysis Challenge (FERA 2017) participants. The achieved results surpass the FERA 2017 baseline, using the challenge metrics, for AU detection by 0.054 in F1-score and 0.182 in ICC(3, 1) for intensity estimation.","inCitations":["0552051e3ee0c96895d8a717f2964da6f887a866","26ee3aa53b1063c08caecfaba474c12328ded796","94f725918586d4414437322af5b33dc1b45c0c64","d9c0310203179d5328c4f1475fa4d68c5f0c7324","a2550ec2abda71e6c323babb196f21e4766c891e","b1a3b19700b8738b4510eecf78a35ff38406df22","aeb09c11a94a1589b0404b5d14a68cc40fcc0705","88ad82e6f2264f75f7783232ba9185a2f931a5d1"],"pmid":"","title":"AUMPNet: Simultaneous Action Units Detection and Intensity Estimation on Multipose Facial Images Using a Single Convolutional Neural Network","journalPages":"866-871","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.111"],"entities":["Artificial neural network","Baseline (configuration management)","Computer multitasking","Convolutional neural network","F1 score","Hoc (programming language)","Job control (Unix)","Mathematical optimization","Spontaneous order"],"journalVolume":"","outCitations":["2ea6a93199c9227fa0c1c7de13725f918c9be3a4","b91f54e1581fbbf60392364323d00a0cd43e493c","863996000fab53f76e73c083b3c6d14eea5efd76","281d1a5ad7a1e7f6168f48678d6980a0349d6f74","cca230fceeab20f2b165f4c7beba9b2aa778579d","1061140c5177193585900e3a8a271366c0e48a43","1c42b5543c315556c8a961b1a4ee8bc027f70b22","7880138c9ec1f0f78b7c896a93179e9b38f44a47","ccfcbf0eda6df876f0170bdb4d7b4ab4e7676f18","f472cb8380a41c540cfea32ebb4575da241c0288","214eb90d0386379972cded05e9f57b884edb1675","2fda461869f84a9298a0e93ef280f79b9fb76f94","da23d90bacf246b75ef752a2cbb138c4fcd789b7","21dff32ff0f6b1244ce9c4aad2f468889748fbb2","b36b1485cc07df374cf2b01e4797a98da887d641","1d5fe82303712a70c1d231ead2ee03f042d8ad70","aed124c053b9c510487d68e0faf32aff2a84c3b5","dbb1871efda531c5340a49e147173c374d069ad1","52d7eb0fbc3522434c13cc247549f74bb9609c5d","272216c1f097706721096669d85b2843c23fa77d","fed90d84dc199d7bd996caddd1cab26ded89de98","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","53ae38a6bb2b21b42bac4f0c4c8ed1f9fa02f9d4","0021f46bda27ea105d722d19690f5564f2b8869e","3ba179bceb9692d4d21109d0b87b120195761148","ce3304119ba6391cb6bb25c4b3dff79164df9ac6","8359f65fd0e0ada2a3de8aead37a6680b53de2a6","c900e0ad4c95948baaf0acd8449fde26f9b4952a","3ec77809aaa7bd30858a4274e3c28a2a0259b30c","7484a73e0687f3e0dcace97004d1d17858ae4b35","45deb9d034e5c99de0e36e4c2a9a4f4277d1497f","061356704ec86334dbbc073985375fe13cd39088","160259f98a6ec4ec3e3557de5e6ac5fa7f2e7f2b"],"id":"3a1c40eced07d59a3ea7acda94fa833c493909c1","s2Url":"https://semanticscholar.org/paper/3a1c40eced07d59a3ea7acda94fa833c493909c1","authors":[{"name":"Julio Cesar Batista","ids":["19280357"]},{"name":"Vitor Albiero","ids":["19182134"]},{"name":"Olga R. P. Bellon","ids":["1800955"]},{"name":"Luciano Silva","ids":["1769311"]}],"doi":"10.1109/FG.2017.111"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545089","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Image-to-image translation between different domains has been an important research direction, with the aim of arbitrarily manipulating the source image content to become similar to a target image. Recently, cycle-consistent generative network (CycleGAN) has become a fundamental approach for general-purpose image-to-image translation, while almost no work has examined what factors may influence its performance. To provide more insights, we propose two new models roughly based on CycleGAN, namely Long CycleGAN and Nest CycleGAN. First, Long CycleGAN cascades several generators to perform the domain translation in a long cycle. It shows the benefit of stacking more generators on the generation quality. In addition to the long cycle, Nest CycleGAN develops new inner cycles to bridge intermediate generators directly, which can help constrain the unsupervised mappings. In the experiments, we conduct qualitative and quantitative comparisons for tasks including photo<-> label, photo<->sketch, and photo colorization. The quantitative and qualitative results demonstrate the effectiveness of our two proposed models.","inCitations":[],"pmid":"","title":"An Extensive Study of Cycle-Consistent Generative Networks for Image-to-Image Translation","journalPages":"219-224","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545089"],"entities":[],"journalVolume":"","outCitations":["915c4bb289b3642489e904c65a47fa56efb60658","0823b293d13a5efaf9c3f37109a4a6018d05d074","55c22f9c8f76b40793a8473248873f726abd8ce9","372bc106c61e7eb004835e85bbfee997409f176a","0bbc35bdbd643fb520ce349bdd486ef2c490f1fc","5287d8fef49b80b8d500583c07e935c7f9798933","b83aa227228175d616551e9be8dae029ec7ccc2f","9201bf6f8222c2335913002e13fbac640fc0f4ec","1450296fb936d666f2f11454cc8f0108e2306741","272216c1f097706721096669d85b2843c23fa77d","b69badabc3fddc9710faa44c530473397303b0b9","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","fd4537b92ab9fa7c653e9e5b9c4f815914a498c0","70f9e70a070bf3dc0c0251e3a44f02494aae0182","ba753286b9e2f32c5d5a7df08571262e257d2e53","1e21b925b65303ef0299af65e018ec1e1b9b8d60","0881bc06bef13f1cd645c99dfa66d1e65b295b2e","32cde90437ab5a70cf003ea36f66f2de0e24b3ab","2d0363a3ebda56d91d704d5ff5458a527775b609","ffadc1dcb90dd677d02f2759319a5118a9238745","072fd0b8d471f183da0ca9880379b3bb29031b6a","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","01c824989d24a8cae214c3156edd9d4492faa579","102a2096ba2e2947dc252445f764e7583b557680","333be4858994e6d9364341aeb520f7800a0f6a07","ef3c1f6c177e37f1d0d2a61702b60c766971700b","29aa3dc15450e6eb46c34f30f0e224e5ea16615e"],"id":"a7f5c133e097981567b14850db7087cf5a87bf3f","s2Url":"https://semanticscholar.org/paper/a7f5c133e097981567b14850db7087cf5a87bf3f","authors":[{"name":"Yu Liu","ids":["40457380"]},{"name":"Yanming Guo","ids":["1687503"]},{"name":"Wei Chen","ids":["31888526"]},{"name":"Michael S. Lew","ids":["1731570"]}],"doi":"10.1109/ICPR.2018.8545089"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546012","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Action recognition with 3D skeleton sequences is becoming popular due to its speed and robustness. The recently proposed Convolutional Neural Networks (CNN) based methods have shown good performance in learning spatio-temporal representations for skeleton sequences. Despite the good recognition accuracy achieved by previous CNN based methods, there exist two problems that potentially limit the performance. First, previous skeleton representations are generated by chaining joints with a fixed order. The corresponding semantic meaning is unclear and the structural information among the joints is lost. Second, previous models do not have an ability to focus on informative joints. The attention mechanism is important for skeleton based action recognition because there exist spatio-temporal key stages and the joint predictions can be inaccurate. To solve the two problems, we propose a novel CNN based method for skeleton based action recognition. We first redesign the skeleton representations with a depth-first tree traversal order, which enhances the semantic meaning of skeleton images and better preserves the structural information. We then propose the idea of a two-branch attention architecture that focuses on spatio-temporal key stages and filters out unreliable joint predictions. A base attention model with the simplest structure is first introduced to illustrate the two-branch attention architecture. By improving the structures in both branches, we further propose a Global Long-sequence Attention Network (GLAN). Experiment results on the NTU RGB+D dataset and the SBU Kinetic Interaction dataset show that our proposed approach outperforms the state-of-the-art, as well as the effectiveness of each component.","inCitations":["162403e189d1b8463952fa4f18a291241275c354"],"pmid":"","title":"Action Recognition with Visual Attention on Skeleton Images","journalPages":"3309-3314","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1801.10304","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546012"],"entities":[],"journalVolume":"","outCitations":["1a39da08bb881626ef90c2854da9c760791556af","0c56f414251d6c9f43623ee683dc6cae3be1045a","def773093c721c5d0dcac3909255ec39efeca97b","024d037d46ae933c7e12fd16af61953c7161773a","8387f38770e3c661c4ef466a8d9733aab58c068a","1839e17555160bd897b978c48b8ebd13dd21445f","10d85459ab6a9350931fcb4709bba171cd31bbde","11cb870f9d1f7626104747a116463a38fa0edf41","ad28f0ca038d5bf9b54235ac169b05c8803e4aaf","a470a81f989d5354239f1044c90e07b78c6beed7","a360526696a2698ad31dfca4c529e098d2dbdbd1","b04a503487bc6505aa8972fd690da573f771badb","7d7ee56f28688283ab9958bcc94fe88413fd89b6","1c30bb689a40a895bd089e55e0cad746e343d1e2","1feebbe8af1e60048cb82bbcbadc07b4d3f210d6","53698b91709112e5bb71eeeae94607db2aefc57c","24115d209e0733e319e39badc5411bbfd82c5133","b61a3f8b80bbd44f24544dc915f52fd30bbdf485","2a0e2e787d16f32a14c6f5a6b0b9bf116c3103f1","44f23600671473c3ddb65a308ca97657bc92e527","47194bdd36c8342184fb1b5ec6c9f78ebae151ac","146f6f6ed688c905fb6e346ad02332efd5464616","e2d969752015f90d50a2e85c19d5b391011bc9a3","341323d36b932b099b787ec5d825526a0509a054","911d6fc85399df59b574f42199b000404e407744","e43045a061421bd79713020bc36d2cf4653c044d","0f2f4edb7599de34c97f680cf356943e57088345","295441fcc1c5d170e6b3be69c024831df063b1c2","3c3633d08316fe527b7836b4038359538088fa8e","c39ffc56a41d436748b9b57bdabd8248b2d28a32","aeee940f46a28b2c4db6e84eb58880b93e21c8f8","2031b062f4c41f43a32835430b1d55a422baa564","a8879cf5cb423e5217e77dda35e2f3cfbe189cea","2c03df8b48bf3fa39054345bafabfeff15bfd11d","49ee231ee8d072f95a702717f8a823032f2679ff"],"id":"5ec838d80d484dd929524ccc82236fba96785843","s2Url":"https://semanticscholar.org/paper/5ec838d80d484dd929524ccc82236fba96785843","authors":[{"name":"Zhengyuan Yang","ids":["21518096"]},{"name":"Yuncheng Li","ids":["3092578"]},{"name":"Jianchao Yang","ids":["1706007"]},{"name":"Jiebo Luo","ids":["33642939"]}],"doi":"10.1109/ICPR.2018.8546012"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206260","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The main contribution of this paper is a planar scan matching algorithm that makes use of the incident angle of a scan point as a feature to enhance the robustness to large relative transformations, particularly in orientation. A new definition of the incident angle is introduced and its consistency with respect to relative transformation between scans is demonstrated. A method that uses the Radial distance and Incident Angle (RIA) for scan alignment, that can efficiently estimate the relative transformation between two scans, is proposed. This approach is designed as a preprocessor for fine scan matching and it is able to pre-align scans that have large angular but limited translational displacement. Scans collected with aggressive robot motion in two typical indoor environments as well as benchmark datasets, are used to demonstrate the effectiveness of RIA when compared with a range of scan matching algorithms reported in the literature.","inCitations":[],"pmid":"","title":"Planar scan matching using incident angle","journalPages":"4049-4056","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206260"],"entities":["Algorithm","Align (company)","AngularJS","Benchmark (computing)","CT scan","Displacement mapping","Preprocessor","Radial (radio)","Rich Internet application","Robot"],"journalVolume":"","outCitations":["7b487fc77f30e99712c53741bde9371645d0e150","bf10415339a86652c3f62a642ab1b7e292cf2da0","e2a113f91edda8100aed873affd95dd4f9ea5e2e","0a0997a5ec18e30eda0d59310dacaa1739d9cd80","5f0d86c9c5b7d37b4408843aa95119bf7771533a","bb0dc7f89a8e64aa537e2e2d26e8c44e30bead86","15e9c5c597924873acb77b6d54c36e106968271e","1f16244ce2e78881c7c96b33796a930ce73f7972","18336c93fd1cf624a4e843925a648020e359c0ac","05d3b4cfc83c1ccfeb75872f82aac75235c25c2d","15bcd7cea08e3fdb36535c4423ebca0099847bfe","9bcb5be21b698c2c6900de6cf9d6c64b69c91e2c","6756e80588b39072c1363f8b4edfed811836f194","2742a33946e20dd33140b8d6e80d5fd04fced1b2","46402562492f08274c62405f08f0c12f1fa246e8","7190eee1bb23a60e459b1060261d7d3e3fdb9c97"],"id":"1596cc4c9ceb835b4ce77739af0fad3d835430ad","s2Url":"https://semanticscholar.org/paper/1596cc4c9ceb835b4ce77739af0fad3d835430ad","authors":[{"name":"Jixin Lv","ids":["2331264"]},{"name":"Yue Wang","ids":["40349073"]},{"name":"Kanzhi Wu","ids":["3226292"]},{"name":"Gamini Dissanayake","ids":["1679214"]},{"name":"Yukinori Kobayashi","ids":["1864380"]},{"name":"Rong Xiong","ids":["5738033"]}],"doi":"10.1109/IROS.2017.8206260"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593515","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Till date, various seven-degrees-of-freedom (7-DOF)robot arms have been developed globally. In general, the robot arms utilized in factories are required to possess speed, power, and accuracy. They are isolated from humans using a fence to ensure human safety. However, a home robot should work near humans at an appropriate speed without a fence. One approach to meeting this requirement involves the installation of various sensors on the robot to control and stop the robot safely even if a collision has occurred. However, a robot system is complicated and expensive. In order to give inherent safety to the robot, we should lighten the robot arm. In this study, a 7-DOF lightweight arm with a wide wrist-motion range is introduced. The weight of movable parts is approximately 2.87 kg. To achieve such properties, we propose the use of three mechanisms: a shoulder mechanism with hollow cylinders, a wrist mechanism with a wide workspace, and an integrated wrist and elbow mechanisms of high power. To verify its feasibility, a prototype of the robot is designed and developed. The experimental results demonstrate its powerful performance and wide workspace of the wrist.","inCitations":[],"pmid":"","title":"A 7-Dof Wire-Driven Lightweight Arm with Wide Wrist Motion Range","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593515"],"entities":[],"journalVolume":"","outCitations":[],"id":"b24cb12a6ac658c7364f2ea06b6953cd12ab5099","s2Url":"https://semanticscholar.org/paper/b24cb12a6ac658c7364f2ea06b6953cd12ab5099","authors":[{"name":"Yuichi Tsumaki","ids":[]},{"name":"Yuya Suzuki","ids":[]},{"name":"Narumi Sasaki","ids":[]},{"name":"Eiki Obara","ids":[]},{"name":"Shuta Kanazawa","ids":[]}],"doi":"10.1109/IROS.2018.8593515"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545489","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Facial Expression Recognition (FER) has attracted considerable attention due to its potential applications in computer vision. Recently, convolutional neural network (CNN) has shown excellent performance on FER. However, most established deeper, wider and more complex network structures trained by small facial expression training dataset have a risk of overfitting. Moreover, most existing CNN models utilize the softmax loss as a supervision signal to penalize the deviation of classification, which enhances inter-class separation, yet intra-class compactness is not taken into consideration. In this paper, we propose a novel multi-scale CNN integrated with an attention-based learning layer (AMSCNN) for robust facial expression recognition. The attention-based learning layer is designed to automatically learn the importance of different receptive fields in the face during training. Moreover, the multi-scale CNN is optimized by the proposed Regularized Center Loss (RCL). Regularized center loss learns a center for deep features of each class and penalizes the distance between deep features and corresponding center, aiming to strengthen the discriminability of different facial expression. Extensive experiments conducted on two popular human FER benchmarks (CK+ and Oulu-CASIA dataset) demonstrated the effective of our proposed AMSCNN, and it obtained competitive results compared to the state-of-the-art.","inCitations":[],"pmid":"","title":"Facial Expression Recognition by Multi-Scale CNN with Regularized Center Loss","journalPages":"3384-3389","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545489"],"entities":[],"journalVolume":"","outCitations":["dc1510110c23f7b509035a1eda22879ef2506e61","2911e7f0fb6803851b0eddf8067a6fc06e8eadd6","c5366f412f2e8e78280afcccc544156f63b516e3","58fa85ed57e661df93ca4cdb27d210afe5d2cdcd","3e34632cd69be865fc4639cdd1f43a8b5f2a3ea6","f472cb8380a41c540cfea32ebb4575da241c0288","762dbb47137545c8378eca41c73427bd5366759b","c5b838de3589d3fd4b6b5d92f4af3393801a8643","48de3ca194c3830daa7495603712496fe908375c","4cfd770ccecae1c0b4248bc800d7fd35c817bbbd","4d9a02d080636e9666c4d1cc438b9893391ec6c7","2679e4f84c5e773cae31cef158eb358af475e22f","5773f6e0b0346e742aaaca42b0c7844f263f801e","1824b1ccace464ba275ccc86619feaa89018c0ad","2e8a0cc071017845ee6f67bd0633b8167a47abed","0568fc777081cbe6de95b653644fec7b766537b2","7be60f8c34a16f30735518d240a01972f3530e00","160259f98a6ec4ec3e3557de5e6ac5fa7f2e7f2b"],"id":"394ea2610db6bcaeb85c81c3d438a9813cae3fc6","s2Url":"https://semanticscholar.org/paper/394ea2610db6bcaeb85c81c3d438a9813cae3fc6","authors":[{"name":"Zhenghao Li","ids":["2166021"]},{"name":"Song Wu","ids":["41157779"]},{"name":"Guoqiang Xiao","ids":["1767696"]}],"doi":"10.1109/ICPR.2018.8545489"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594403","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper investigates the design of a soft elbow exosuit capable of providing supplemental lifting assistance by reducing muscle activity of the bicep muscle. The aim is to improve the efficiency and endurance of workers who are tasked with repetitive lifting. The design consists of an array of pneumatically pressurized soft actuators, which are encased in nylon fabric that allows for a high force-to-weight ratio of 211.SN/g. An analytical model governing the bending behavior of two consecutive actuators and torque generated by the exosuit is developed, with test results showing less than 10% error from the theoretical model. An elbow joint torque value of 27.6N.m is achieved at 300kPa, which is comparable to the 30N.m maximum set by OSHA requirements in the USA. Further testing with a healthy participant is performed using surface electromyography (sEMG) sensors and a motion capture system to assess the capabilities of the exosuit to provide active assistance to the bicep during isometric and concentric contractions. Measurable assistance to lifting is observed with minimal obstruction to the user's range of motion for all experiments.","inCitations":[],"pmid":"","title":"A Novel Soft Elbow Exosuit to Supplement Bicep Lifting Capacity","journalPages":"6965-6971","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594403"],"entities":[],"journalVolume":"","outCitations":[],"id":"e1e04e0ab2707ef02d1bc8e12af5926f5c17f2bb","s2Url":"https://semanticscholar.org/paper/e1e04e0ab2707ef02d1bc8e12af5926f5c17f2bb","authors":[{"name":"Carly M. Thalman","ids":[]},{"name":"Quoc P. Lam","ids":[]},{"name":"Pham Huy Nguyen","ids":[]},{"name":"Saivimal Sridar","ids":[]},{"name":"Panagiotis Polygerinos","ids":[]}],"doi":"10.1109/IROS.2018.8594403"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593901","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The Label Propagation (LP) algorithm, first introduced by Zhu and Ghahramani [1], is a semi-supervised method used in transductive learning scenarios, where all data are available already in the beginning. In this work, we present a novel extension of the LP algorithm for applications where data samples are observed sequentially - as is the case in autonomous driving. Specifically, our \u201cIncremental Label Propagation\u201d algorithm efficiently approximates the so called harmonic solution on a nearest-neighbor graph that is regularly updated by new labeled and unlabeled nodes. We achieve this by reformulating the original algorithm based on an active set of nodes and by introducing a threshold to decide whether the label of a given node should be updated or not. Our method can also deal with graphs that are not fully connected, and we give a formal convergence proof for this general case. In experiments on the challenging KITTI benchmark data stream, we show superior performance in terms of both test accuracy and number of required training labels compared to state-of-the-art online learning methods.","inCitations":[],"pmid":"","title":"Incremental Semi-Supervised Learning from Streams for Object Classification","journalPages":"5743-5749","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593901"],"entities":[],"journalVolume":"","outCitations":[],"id":"b95b1231f89c74e69c7bf22a2f0689250f3d74be","s2Url":"https://semanticscholar.org/paper/b95b1231f89c74e69c7bf22a2f0689250f3d74be","authors":[{"name":"Ioannis Chiotellis","ids":[]},{"name":"Franziska Zimmermann","ids":[]},{"name":"Daniel Cremers","ids":[]},{"name":"Rudolph Triebel","ids":[]}],"doi":"10.1109/IROS.2018.8593901"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593429","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This work presents a strategy for coordinated multi-agent weeding under conditions of partial environmental information. The goal of this work is to demonstrate the feasibility of coordination strategies for improving the weeding performance of autonomous agricultural robots. We show that, given a sufficient number of agents, the algorithm can successfully weed fields with various initial seed bank densities, even when multiple days are allowed to elapse before weeding commences. Furthermore, the use of coordination between agents is demonstrated to strongly improve system performance as the number of agents increases, enabling the system to eliminate all the weeds in the field, as in the case of full environmental information, when the planner without coordination failed to do so. As a domain to test our algorithms, we have developed an open source simulation environment, Weed World, which allows real-time visualization of coordinated weeding policies, and includes realistic weed generation. In this work, experiments are conducted to determine the required number of agents and their required transit speed, for given initial seed bank densities and varying allowed days before the start of the weeding process.","inCitations":[],"pmid":"","title":"Multi-Agent Planning for Coordinated Robotic Weed Killing","journalPages":"7955-7960","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593429"],"entities":[],"journalVolume":"","outCitations":[],"id":"16a3f4320dabec8a2502c090fd02b9028a67e37c","s2Url":"https://semanticscholar.org/paper/16a3f4320dabec8a2502c090fd02b9028a67e37c","authors":[{"name":"Wyatt McAllister","ids":[]},{"name":"Denis Osipychev","ids":[]},{"name":"Girish Chowdhary","ids":[]},{"name":"Adam Davis","ids":[]}],"doi":"10.1109/IROS.2018.8593429"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.96","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Suicide is the deliberate self-inflicted act with the intent to end one\u2019s own life. It reflects both profound personal suffering and societal failure. While certain suicide risk factors are well understood, predicting suicide attempts remains a very challenging problem. In this paper, we investigate non-verbal facial behaviors to discriminate among control, mentally ill, and suicidal patients. For this task, we used a balanced corpus containing interviews of male and female patients with and without suicide ideation and/or mental health disorders from 3 different hospitals. In our experiments, we explored smiling, frowning, eyebrow raising, and head motion behaviors. We investigated both the occurrence of these behaviors and also how they were conducted. We found that facial behavior descriptors such as the percentage of smiles involving the contraction of the orbicularis oculi muscles (Duchenne smiles) had statistically significant differences between the suicidal and nonsuicidal groups. Our experiments also demonstrated that the stage of the interview in which these facial behaviors occur impacts their discriminative power.","inCitations":["0b0e1da3ded64550954018dd10715404bba844d5","56fd4c05869e11e4935d48aa1d7abb96072ac242","49f0c027a0fb87e65f9e0a403531db2bbfae994e"],"pmid":"","title":"Investigating Facial Behavior Indicators of Suicidal Ideation","journalPages":"770-777","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.96","https://www.extremetech.com/wp-content/uploads/2017/06/fg2017-submitted.pdf","http://multicomp.cs.cmu.edu/wp-content/uploads/2017/10/2017_FG_Laksana_Investigating.pdf","https://www.cl.cam.ac.uk/~tb346/pub/papers/fg2017_suicid.pdf"],"entities":["Binary classification","Data descriptor","Discriminative model","Experiment","Feature model","Simplified molecular-input line-entry system","Velocity (software development)"],"journalVolume":"","outCitations":["f83a27e3f80b25bf1437ed6f41003ac0f56b8dba","12262be319fc4c2d439f439491a1a15fa4b26b3c","2c0fc87bef123f5556a0a63959e0f004f7730b1e","a1c5db821d42616f15d2394dfa6a8ebead916314","3387a93b2138bc6347189c248ef2a435f31981ce","2c5c9e45ce886c62746879ffe519aa195b355467","61a100040b496def51f01701e8dc79f0fa4e80e3","2fda461869f84a9298a0e93ef280f79b9fb76f94","74fdc429bfc3ae3a146a3b0049c86c3b4e9a318c","a16f538a57e44c23cda462efa27a5a920b45c234","6a657995b02bc9dee130701138ea45183c18f4ae","dcedec883a2d1335de2932b73bb44e551d5609c9","2d3fa735944906bf59e0406ff3988062d13d2db6","c714d579307bb6018d2d3d1f8ebe5b3cf88283aa","863996000fab53f76e73c083b3c6d14eea5efd76","62bffa01261eb1813f3233ad1ae615b6dc4d8cf0","660600761c60f08357ae4c6b5d5e977d08655060","da0cde1c4d3ba1383673c5b28332cbd0bff9da42","0c81586c45656082b41b8c061ced0d0755573ea1","d6bb19be12f8acccec4454b297ac8c3792140056","7d9f6beb6ed00358124c7784d42e94feaae2339b","88fbf478fd5d0baa2c1d5cc6d0b7daea38893a89","57fb1b2d52e5f29a455312967a6f0e6380db8d55","12417f4f32a3dbb6245a4c8dd345aee4d5a2f7b0","5c05340dc3ccdd0446cdc7c2281f03f3d0404f9a","c4b16f56b00b78c175e260e6de6eb74ab6f88221","18031898340d9e118ebd813730d8f33318343822","8eba133fc39c2e09bfc279ce2b1d0a4e428fce7c","c2dea047d5d71cd6500d92d4cdf2c6e36129d8cf","ef0080bdc426056217495834b0961411080ae656","6c7a3e2061055048190e1986b7b07cf332310a08","5f21bb1471baacaf3f195650a29efc8fe5013a29","c69f66671f758f913e9c6015d3471760d0505219","1499d4c60ebc643173c124406603a8d25ec7d811","13492dbdb23b97e5d3feca3139c024e694b2ec40"],"id":"04cb1bf4c021a40d0d51d0015d935efa10a204f8","s2Url":"https://semanticscholar.org/paper/04cb1bf4c021a40d0d51d0015d935efa10a204f8","authors":[{"name":"Eugene Laksana","ids":["2795834"]},{"name":"Tadas Baltrusaitis","ids":["1756344"]},{"name":"Louis-Philippe Morency","ids":["1767184"]},{"name":"John P. Pestian","ids":["32011466"]}],"doi":"10.1109/FG.2017.96"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545598","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Scene text detection is challenging as the input may have different orientations, sizes, font styles, lighting conditions, perspective distortions and languages. This paper addresses the problem by designing a Rotational Region CNN (R2CNN). R2CNN includes a Text Region Proposal Network (Text-RPN) to estimate approximate text regions and a multitask refinement network to get the precise inclined box. Our work has the following features. First, we use a novel multi-task regression method to support arbitrarily-oriented scene text detection. Second, we introduce multiple ROIPoolings to address the scene text detection problem for the first time. Third, we use an inclined Non-Maximum Suppression (NMS) to post-process the detection candidates. Experiments show that our method outperforms the state-of-the-art on standard benchmarks: ICDAR 2013, ICDAR 2015, COCO-Text and MSRA-TD500.","inCitations":[],"pmid":"","title":"R2 CNN: Rotational Region CNN for Arbitrarily-Oriented Scene Text Detection","journalPages":"3610-3615","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545598"],"entities":[],"journalVolume":"","outCitations":["009fba8df6bbca155d9e070a9bd8d0959bc693c2","7e703d57e9eea387f4d01e47f4990fbe403e4a8e","daa7e0cafee740ba91fdba581dea4d273153802c","a4a88145718ec8eff1228267bf3fe9f380b9495f","be77efa64ea9fcc0d724aadd387249cc6d2fd37a","e0977df9116c302c73e13ff7b52188f0924cc8be","11a08ced22775a217ba78c566528ed44ea98e3e3","ff8e46522ef1a0c5dffd72a4f6faf4cdf57b8061","75064b7675553c22112b76b5687e0aed4089b0ea","0f5b946ea4016f1c4813d926e917d3b3cb2c22de","5372150dabf77a75e88d23f13252e92764430b33","1d40b767d8c5ef7a93ca5cc5b0dbb850b8a0cd2e","2d5dba33c706d907733f15e7b57fde9909894e29","21a1654b856cf0c64e60e58258669b374cb05539","fb2d620e5499f65d2e10a7a74562afe91d918323","d318f3ca49f7f2159b9fc0face08eb284d5442dc","18f51e9bdc1abdb6f1601c5c0692d6c150421a48","c0d23b7b46c731477a6d30257b4f95a5f6f082a0","5653849219ea5b8e5f5c2edee87696a92e189036","1b2293cb53ffb2d4b4a0799bd3c3b3718a2f5af2","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","903f56b3206ec5ee6e6416a0e18a171349451332","e2743fa65d8509d2b6c1ee0c1417a0dd7cf4bf38","20a78d3145279dcd799cd7a856ae2714f4863a16","59c2f5e22f4bbd5c9ea95ca3cac4f651bc90f8ae","4682e5d4654b6040f213475c8b75ca039661b5f6","21df8df7f2c830e491ad0660fbb0f6bd09f40910","64ad80fe2e375ae4a36f943f0056a4bea526e116","a8f24fcc1eb0354ffd91f0e3031f5c4dc3e02dd6","32dd8a7eb658cc1d2d372c83f57fdfd065b0a379","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a","60b80ce60cfacd2b8e0376b326a1819084f014b8"],"id":"28dbeb8fc37ef9b5e39f6c9e3e85da35991e2b7f","s2Url":"https://semanticscholar.org/paper/28dbeb8fc37ef9b5e39f6c9e3e85da35991e2b7f","authors":[{"name":"Yingying Jiang","ids":["50262107"]},{"name":"Xiangyu Zhu","ids":["8362374"]},{"name":"Xiaobing Wang","ids":["47118957"]},{"name":"Shuli Yang","ids":["2438153"]},{"name":"Wei Li","ids":["39093312"]},{"name":"Hua Wang","ids":["49528231"]},{"name":"Pei Fu","ids":["5476284"]},{"name":"Zhenbo Luo","ids":["2637107"]}],"doi":"10.1109/ICPR.2018.8545598"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545356","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Human Embryonic Stem Cells (hESC's) are promising for the treatment of many diseases such as cancer, Parkinsons, Huntingtons, diabetes mellitus etc. and for toxicological testing. Automated detection and classification of human embryonic stem cell (hESC) videos is of great interest among biologists for quantified analysis of various states of hESC in experimental work. To date, the biologists who study hESC's have to analyze stem cell videos manually. In this paper we introduce a hierarchical classification system consisting of Convolutional Neural Networks (CNN) and Triplet CNN's to classify hESC images into six different classes. We also design an ensemble of Generative Adversarial Networks (GAN) for generating synthetic images of hESC's. We validate the quality of the generated hESC images by training all of our CNN's exclusively on the synthetic images generated by the GAN's and evaluating them on the original hESC images. Experimental results shows that we classify the original hESC images, with an accuracy of 85.67% using the CNN alone, 91.38% accuracy using the CNN and Triplet CNN and 94.11% accuracy by fusing the outputs of the CNN and Triplet CNN's, out performing existing state-of-the-art approaches.","inCitations":[],"pmid":"","title":"DeephESC: An Automated System for Generating and Classification of Human Embryonic Stem Cells","journalPages":"3826-3831","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545356","http://vislab.ucr.edu/PUBLICATIONS/pubs/Journal%20and%20Conference%20Papers/after10-1-1997/Conference/2018/ICPR18_0618_FI.PDF"],"entities":[],"journalVolume":"","outCitations":["7fddac954ad0a6d9c85b468f87b789abf7ee5f6f","130653c8f632bf8571a61f4433ef28e6d9e1108a","a876761314ea3c888287a3c560bd819d5f914b7c","045d9c062f820b76c4edaade5e9e67565112c784","6b22ad701663685c7e07cbe9fb0a04110ee8b9d3","6928cb62583e097bb50b1461f995052e00c03b69","7a3e42192a371e697b86e25b07b548f06a5ad4b2","2c7879b5f4adcc3c2e0a32abfa159d55849bd665","ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","35756f711a97166df11202ebe46820a36704ae77","4b73805f0cac941ec2e5275fe70d7b232468ebae","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","d842958e93f479d6146850c01dded7881611c47c","061356704ec86334dbbc073985375fe13cd39088","dfb23034abf79d76d43c94ed495755c151588323","a2213ce09e527b737d964f0339928c68655ddf6c","2c03df8b48bf3fa39054345bafabfeff15bfd11d","44760ff739ebc7feb8b62da7d621ea5814638f21"],"id":"589b75b347b67ac343c1af629a0e6c1d03bc9058","s2Url":"https://semanticscholar.org/paper/589b75b347b67ac343c1af629a0e6c1d03bc9058","authors":[{"name":"Rajkumar Theagarajan","ids":["7157211"]},{"name":"Benjamin Xueqi Guan","ids":["32602965"]},{"name":"Bir Bhanu","ids":["1707159"]}],"doi":"10.1109/ICPR.2018.8545356"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545685","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Heart rate (HR) monitoring based on Photoplethys-mography (PPG) has drawn increasing attention in modern wearable devices due to its simple hardware implementation and low cost. In this work, we propose a variational mode decomposition(VMD)-based HR estimation method using wrist-type PPG signals during physical exercise. To remove motion artifacts (MA), VMD was first used and then a post-processing method after VMD was proposed to guarantee the robustness of MA removal. The performance of our proposed method was evaluated on two PPG datasets used in 2015 IEEE Signal Processing Cup. The method achieved the average absolute error of 1.45 beat per minute (BPM) on the 12 training sets and 3.19 BPM on the 10 testing sets, confirmed by the experimental results.","inCitations":[],"pmid":"","title":"Variational Mode Decomposition-Based Heart Rate Estimation Using Wrist-Type Photoplethysmography During Physical Exercise","journalPages":"3766-3771","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545685"],"entities":[],"journalVolume":"","outCitations":["d09b14e12e1130c3f06c15bc6ba90f3d1e24b4e1","1e9bd8788b93b6b3569cc8fbec31fd0f16cbf7f5","ab62c8415b6d1d5a19e472d5bf7aa868c122b320","9bf8520d92a0f43c47e0ed3d08e87ed86c4391ef","39ebc80ea235ffe550c22e6dec07cb38e52e0edf","4030062ef0a530f5a98c7609d83f5a515eb5fc1e","a4286f778a851526e4ab3c410896b3054686aae3","ce88a1ed226ef1378db8fe9cc1833c5f6dbf4137","99a18d9c428fa578e9065b0296561dd7052739ff","2fc950ce3b8f532a287dfa34f9f54dd742dd71a7","ac07f8f77106932b9dbf17facba54350fe812283","d7798ea5db9932c84434f52464fc1de772bf94eb","39fa4177f614c814ce4d482479ef016bd046c5cf","d2a7a454d1b2e963296b2b7a3f4bbf5595c79429","443fa5446adc6833f07ca834213072f5fabd4b4a","3629d14ecbe9d4015dec304f72dd171257d3d037","af16404e67fd506509c577c6818019946209acbe","9cecd9eaeff409ccaa63aa6d25ad7ba4658a994a","7c184013d4a2f005be39424ef7e762e3cbc7c457","424e42a7a037a8e90e896bd173f3a1e5bf30fa12","c1f35fd4a128b14c61ed659e08facd666d27f43e"],"id":"c3d8e72867b420046a647cc7953b5ffb047f4ec2","s2Url":"https://semanticscholar.org/paper/c3d8e72867b420046a647cc7953b5ffb047f4ec2","authors":[{"name":"Wenwen He","ids":["47079536"]},{"name":"Yalan Ye","ids":["3103259"]},{"name":"Yunxia Li","ids":["48514467"]},{"name":"Haijin Xu","ids":["47995312"]},{"name":"Li Lu","ids":["47684778"]},{"name":"Wenxia Huang","ids":["9584593"]},{"name":"Ming Sun","ids":["40472561"]}],"doi":"10.1109/ICPR.2018.8545685"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206553","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Continuum Reconfigurable Incisionless Surgical Parallel (CRISP) robots consist of multiple needle-diameter flexible instruments that are assembled into a parallel structure inside the human body. With a camera placed at the tip of one of the instruments, the CRISP robot can be used to inspect anatomical sites in constrained body cavities in a minimally invasive manner. We introduce a motion planner for CRISP robots that computes manipulations of the flexible instruments outside the body such that the camera can visually inspect a user-specified site of clinical interest inside the body. Our sampling-based motion planner ensures avoidance of collisions with anatomical obstacles inside the body, enforces remote-center-of-motion constraints on the instrument's entry points into the body, and efficiently handles the expensive computation of CRISP robot kinematics. We also extend the motion planner to estimate the set of points inside a body cavity that can be visually inspected by the camera of a CRISP robot for a given setup. We demonstrate our method in a simulated endoscopic medical procedure in the pleural space around a lung.","inCitations":["cd1a0ac0f844f7af37042c215876233f82c2b35a","553a6c0dbb8638d0212d519cde4bce9839170b33"],"pmid":"","title":"Motion planning for continuum reconfigurable incisionless surgical parallel robots","journalPages":"6463-6469","s2PdfUrl":"","pdfUrls":["http://adkuntz.web.unc.edu/files/2017/10/Kuntz2017_IROS.pdf","http://research.vuse.vanderbilt.edu/MEDLab/sites/default/files/papers/Kuntz2017_IROS_optimized.pdf","https://doi.org/10.1109/IROS.2017.8206553","http://robotics.cs.unc.edu/publications/Kuntz2017_IROS.pdf"],"entities":["Apache Continuum","Computation","Cross Industry Standard Process for Data Mining","Heuristic (computer science)","Inverse kinematics","Motion planning","Parallel computing","Parallel manipulator","Precomputation","Prototype","Robot","Sampling (signal processing)","Triune continuum paradigm"],"journalVolume":"","outCitations":["2b02f066b33a2ecb8764e132c075cfb1d16c9d10","5092443a6396b57260c9eaf98cb6fd4f5536d070","03613bbadde699e9e47f0a9cd2da6c8ef8dd44cf","d227b37a2739b28405d1c00d2e7b8c1486009a3b","622c149a627183dc0057758489faf0ee412895e7","292966732e996e395e19f8c61254f76f7d7fc20d","d2f90b882bb2a205cf30777d06c6260e53924ddc","595145e2647e8079a0d3613b8e4bd91ba55b8011","504628f419ad823e560d887c372fd1495d360484","02185a136321650baa5161e36dcb2a378d905e49","b8975ce91fa2f88bbe18b01a693f1a095fda8c4d","096453f865d9b1acd153ece3d8bb2ac9e692a03c","8ee96fe2ab10b59c81fcf20d18d5aa9f52ba28da","4212dd793b0bbd1ab42979ff0d4e0cc2e2248d3c","5f4e2250978a9408df98cf82afd7f709958600ad","07f07f6a56fce60ac88e3354705647ba1fddceb0","2518d6b39387cc6c2ac64b3f921d5e60d544072c","57d11fee24e646a2f4c79db91d7f87bd9672d209","3c7f7cea0b4cd8853124ce620fbde77246cc7a6f","862ecf3d209874aa634fd925f9b3f2942bf7c30f","2b5ea763c466bb5016ce7d6a4a69638890cd5597"],"id":"a7113514b36d78637748cbd3f4a3710c1849e5b1","s2Url":"https://semanticscholar.org/paper/a7113514b36d78637748cbd3f4a3710c1849e5b1","authors":[{"name":"Alan Kuntz","ids":["39135743"]},{"name":"Arthur W. Mahoney","ids":["38213717"]},{"name":"Nicolas E. Peckman","ids":["46278825"]},{"name":"Patrick L. Anderson","ids":["9970286"]},{"name":"Fabien Maldonado","ids":["1975961"]},{"name":"Robert J. Webster","ids":["1730209"]},{"name":"Ron Alterovitz","ids":["1682091"]}],"doi":"10.1109/IROS.2017.8206553"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594473","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Odometry forms an important component of many manned and autonomous systems. In the rail industry in particular, having precise and robust odometry is crucial for the correct operation of the Automatic Train Protection systems that ensure the safety of high-speed trains in operation around the world. Two problems commonly encountered in such odometry systems are miscalibration of the wheel encoders and slippage of the wheels under acceleration and braking, resulting in incorrect velocity estimates. This paper introduces an odometry system that addresses these problems. It comprises of an Extended Kalman Filter that tracks the calibration of the wheel encoders as state variables, and a measurement pre-processing stage called Sensor Consensus Analysis (SCA) that scales the uncertainty of a measurement based on how consistent it is with the measurements from the other sensors. SCA uses the statistical z-test to determine when an individual measurement is inconsistent with the other measurements, and scales the uncertainty until the z-test passes. This system is demonstrated on data from German Intercity-Express highspeed trains and it is shown to successfully deal with errors due to miscalibration and wheel slip.","inCitations":[],"pmid":"","title":"Robust Odometry using Sensor Consensus Analysis","journalPages":"3167-3173","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1803.02237v1.pdf","http://arxiv.org/abs/1803.02237","https://doi.org/10.1109/IROS.2018.8594473"],"entities":[],"journalVolume":"","outCitations":["da03e75b014231769a423977d042dac2239ae0fc","47ea9a71f177640ed9538a7bc7be9283d8f0c769","7fcbc0cd28a6a45418169b37c8d4f908454c7cdb","9a002891d4a18a4e70eed95873ddbbf1712450b0","42f212ecb3340bd3fce166c7e0d0a08e95610e57","919417ee4d842310f83fa40c3ecaf3c227504560","98dccdaa7a26bb0ace5e105ae385f009db078ade","ac794ac1e6567a8c16a2d30fe1472e2c703d8d33","b1ed5db4446859547504c1bce51728b9041fce57","6c04fe696ac95b41532650f3eb521ba6a7411ef4","40715cc1dec475097b661e46db036511eac8aa10","c7f3857918fa4f5176e2883af790f725e2fbe31c","95cd61347d7bd3d7a3040e60c36f74d3a2b75b08","55746f2e74e8a3d41b9d87f98aae4b3ca173fa67","c3dd44e19ab7d7788d9e12e40bb24690e97642f4","0952d9122f062b61f164aa9375f37c4188bf8a9b","65dcd707a15c8a5d8647ebcbf5b947f79705f568","c8965cc5c62a245593dbc679aebdf3338bb945fc","5e8986af7df725f88d07e345cf3caabbab35e9b4"],"id":"9614298ffe6cedbb588000f78619cb54b952cd73","s2Url":"https://semanticscholar.org/paper/9614298ffe6cedbb588000f78619cb54b952cd73","authors":[{"name":"Andrew W. Palmer","ids":["30421216"]},{"name":"Navid Nourani-Vatani","ids":["3058164"]}],"doi":"10.1109/IROS.2018.8594473"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594021","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a versatile robotic system for sewing a 3D structured object. Leveraging on using a customized robotic sewing device and closed-loop visual servoing control, an all-in-one solution for sewing personalized stent graft is demonstrated. Stitch size planning and automated knot tying are proposed as two key functions of the system. By using effective stitch size planning, sub-millimetre sewing accuracy is achieved for stitch sizes ranging from 2mm to 5mm. In addition, a thread manipulator for thread management and tension control is also proposed to perform successive knot tying to secure each stitch. Detailed laboratory experiments have been performed to evaluate the proposed instruments and allied algorithms. The proposed framework can be generalised to a wide range of applications including 3D industrial sewing, as well as transferred to other clinical areas such as surgical suturing.","inCitations":[],"pmid":"","title":"Robotic Sewing and Knot Tying for Personalized Stent Graft Manufacturing","journalPages":"754-760","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594021","https://arxiv.org/pdf/1803.08478v1.pdf","http://arxiv.org/abs/1803.08478"],"entities":[],"journalVolume":"","outCitations":["2cc358d072b1b2b6ab37c0721183d5ce9be23e64","b6d98c4ad24b27fb03f734dd37cbf616dabba6b7","9dbf5112ca9aebf0dab79d353f0d81253a73b4d6","f80cfd0e19121bc99abb7e41bc002ef40ee54e67","31684c92ea0d43a8074474c2ea189ca7f3313d91","22e7ad62d6edec4a6384ea29c5a73049016946a4","854c8e61fc6b4e9ea78c556ea06cf57807b31cb9","d4d3fae3fa810d13fab7db28fc45bdc6b84ca15c","49ceb48c4375c9d3ca2d2ef091cd85eb4c25bf5b","2224a796bf819a51ab6d4ae1691e7a5bf8af372a","5b10f5901b981d2a5c3d0ef789d7f5677e649a88","ae5fb15977a7d37e58be79733893618a6c0edf41","4d7946d97f9e494f5058fb2ece81c51ce57209bc","d385adc8ec0e7ef1056a2fd301e93c8503c96f51","bd2bc978479f274369a60288921fd02528880720","358c709654551e37ebff1425b6d41c054bfb11f0","19b389a797a55c8b63dca8b6d1889df4cff8bfaa","189b06d02fd64442eb440531f2d98082b87fc588","dc59086a06843dc72c7141cba3eb280bd12fa68c","5c41c8dafe614e9b5b9c322e4e1886618adc6f48"],"id":"d0737c4a6343fd104e6087c6271c3980f441e0f8","s2Url":"https://semanticscholar.org/paper/d0737c4a6343fd104e6087c6271c3980f441e0f8","authors":[{"name":"Yang Hu","ids":["49995121"]},{"name":"Lin Zhang","ids":["39415079"]},{"name":"Wei Li","ids":["47113208"]},{"name":"Guang-Zhong Yang","ids":["1743111"]}],"doi":"10.1109/IROS.2018.8594021"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206074","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Existing robots for multi-scale motion (MSM) are unsuitable for micro-surgery in deep surgical sites where miniaturization and traversal of often tortuous anatomical passageways is required. Also, new emerging surgical paradigms for natural orifice surgery and image-based diagnosis and guidance at the micro-scale level promise to provide accurate verification of tumor resection margins. To overcome the limitations of current robot architectures, and to enable image-based biopsy and micro-surgery in confined spaces, we present a new concept of continuum robots with equilibrium modulation (CREM). CREM robots are a modification of multi-backbone continuum robots that achieve micro-motion by using indirect actuation through modulation of their static equilibrium by change of the distribution of their cross-sectional flexural rigidity. As a first step towards modeling the micro-scale kinematics of these robots, solutions for micro-motion tracking are presented and verified to achieve tracking accuracies of better than 2μm. Preliminary evaluation of the micro-motion capabilities of a first prototype demonstrates motion resolutions at 1μm level and hysteresis of less than 10μm \u2014 despite the use of inexpensive actuators with significant backlash. Finally, a demonstration of a first effort at integrating such a robot with a custom-made optical coherence tomography (OCT) probe is presented.","inCitations":[],"pmid":"","title":"Continuum robots for multi-scale motion: Micro-scale motion through equilibrium modulation","journalPages":"2537-2542","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206074"],"entities":["Algorithm","Apache Continuum","Cross-sectional data","Hysteresis","Image processing","Internet backbone","Markov switching multifractal","Modulation","Nash equilibrium","Prototype","Repeatability","Robot","Tomography","Triune continuum paradigm","Video-in video-out"],"journalVolume":"","outCitations":["12cc22d70f04cd89aaf58f0fe30c08a421c22b0a","168664743532ecbf1b605fa2cdcf28208171f7d4","bb3eedab9ac1fb9d6a09a7bf8f2b323045ae2d9c","1c6d400fef981c8db137318c7c582804c17b98f9","0b2c246221c072d0cb27b19f71562a4091b6279b","767984ef713d1adbb536c92e2aaeeaf154dd97be","16623514973e1a7f9541ca53bc62b49f4696ccc6","ec77d3c2b6af15827def94a3acb0ae395b5e6875","5901edbd7268d7c00c77f8773c8e9731733ab21c","3457eb2162249e2854ec27cf4d1f9651fade779b","0dc882388fbae84402bda16ba328eea7f5483cbe","1f1b41392a01252f4b19bc24f2555f25f61f6881","eaee0ff79f6d977c2a36de25f3eb35c16a231dd3","a1567f88cb11690b695ad8aa27d609132c12e8fe","843a82e23f9fb58264ba0e722109f831ebe97177","f47134448efcc8e8214858ecd1957724ff89d0b7","429367644663fb9bc01ea573c31aba465ea6ca23","d09c346026f5b5c40627346bc792b05e50484793","e11eda6ee5d422a39b5125f58c05bc08569b99bc","3e7cb76bb4b0e75fe36b7be8d47ecc983253e8d8","28b186ee0fc5a43f9921b942882b7f3fd5e5f31a","fcd2d1e10400bba627458b9aad64f5ffccacf09f","4c6d993e00824bead7136433e80dd71e9b4d5aa3"],"id":"c23251ab82a023ec6903dfe7db173b7fe84ad68e","s2Url":"https://semanticscholar.org/paper/c23251ab82a023ec6903dfe7db173b7fe84ad68e","authors":[{"name":"Giuseppe Del Giudice","ids":["48916963"]},{"name":"Long Wang","ids":["38510126"]},{"name":"Jin-Hui Shen","ids":["46904778"]},{"name":"Karen M. Joos","ids":["2210387"]},{"name":"Nabil Simaan","ids":["3307788"]}],"doi":"10.1109/IROS.2017.8206074"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546222","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"We propose a novel compressive sensing framework for color images. Recently, compressive sensing (CS) has gain its popularity with the development of deep learning. To our best knowledge, existing methods all deal with RGB images channel by channel. This brings redundancy of measurements. In this paper, we do a breakthrough work. Instead of recovering RGB images channel by channel uniformly, we adopt non-uniform sampling in different channels in YCbCr color space. The luminance component takes up more measurements while the other channels take up less in the proposed framework. It greatly enhances the performance on CS for color images. Moreover, perceptual loss gives a powerful ability to better capture the structure information. We give the measurement rate at 2% as an example in the experiments, and the results show the proposed method outperforms all the existing methods with better structure of images.","inCitations":[],"pmid":"","title":"Color Image Reconstruction with Perceptual Compressive Sensing","journalPages":"1512-1517","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546222"],"entities":[],"journalVolume":"","outCitations":["00498a081478924c86597dc5d252562b545c7df7","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","9dcfa2baa00342c6f81cac1c9dc2b83a6a1f2065","2d4254edb1479b698f7730df7fd7d5bf0981f33c","a0b2e31e077ec9740ad09c2d52aa7342eff1f35d","70d8f9d7b87235ac4177d2901286571d8ad3c427","41b5d0652a844ea444f4f43f4ea979a565a7f669","19f364b58160ea7b3c8565a0a1ae23257dd354c8","915c4bb289b3642489e904c65a47fa56efb60658","1687422d3969111bfcfe6642dfefb4e3d9867c0b","254feccf4b2dc73ab9b8243712949568dcc995ef","397490d0b59e983aadbee73fd09fab98194ecfe7","c20f4f5268e9ab7197af5319a3608e49ef4e34c1","0df60433201af2e407850bdcc947f6d37d845827","a03be746a22060252095bcf87646969da74af720","c58a7dc8f84df5089797d43dd218c40093d1f3c8","f226ec13e016943102eb7ebedab7cf3e9bef69b2","28167c64461cd27c04e104f79ed051af80dc1fdf","4774e0c8dcbf6e7cefffb0622bd329d52aea638e","34760b63a2ae964a0b04d1850dc57002f561ddcb","34fe87515826dce400f0b3090c4cdd638bc3277b","03f4141389da40a98517efc02b0fe910db17b8f0","a45885b29e4d2efaf75759131fb8bb524cffbd94","a29f2bd2305e11d8fe139444e733e9b50ea210d6","0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a","d1c3e59bc699c22eaa9edc8cdb4464998a0cf1a8","4b75f6ccc782d8ae09b0793a6b5058ffded5dd6d","6cac3f5009b5728bda2457fef6180d28357b3ae6","2c03df8b48bf3fa39054345bafabfeff15bfd11d"],"id":"1b3be14d267b3302fd0a0ac0be630cdf2a8ba00b","s2Url":"https://semanticscholar.org/paper/1b3be14d267b3302fd0a0ac0be630cdf2a8ba00b","authors":[{"name":"Jiang Du","ids":["3102302"]},{"name":"Xuemei Xie","ids":["2033687"]},{"name":"Chenye Wang","ids":["8206618"]},{"name":"Guangming Shi","ids":["3878949"]}],"doi":"10.1109/ICPR.2018.8546222"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545525","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Volumetric integration method is widely used to fuse depth maps in dense 3D reconstruction systems. High memory footprint is one of its main disadvantages. We introduce a method to de-noise depth maps and save memory usage during volumetric integration of depth maps with the use of plane priors. We develop a new planar region detection method with the use of depth gradients and then de-noise the planar region of depth maps. During volumetric integration we allocate the voxels and integrate depth maps with the use of plane priors as well. Extensive experiments show that our method saves approximately 30% memory footprint and has higher reconstruction quality compared with some of the current state-of-the-art systems. These characteristics enable our method to be used for 3D scanning on mobile devices which have limited memory resources.","inCitations":[],"pmid":"","title":"High-Quality and Memory-Efficient Volumetric Integration of Depth Maps Using Plane Priors","journalPages":"1749-1754","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545525"],"entities":[],"journalVolume":"","outCitations":["c2adb138674a7f75ade139d0e56d7a8e83f0803e","4f82917d630d25a9b32f4a79497cd6819687498e","2d8e268ae3d99e519c01a5338dd8974606e2c142","5e70bb805cb99904504f12b3fa32ddaae066323c","a8a618363b8dee8037df9133668ec8dcd532ee4e","8c91acdcc81eb900752ca95356cad6205a6a756e","07ae527fa374a9a32b20f1464dd982d989ce1872","849250eacfc2ab643db83091f4a1d6cb5169f794","41984604bf962ad33c8c07dec557945671246b74","cacbb2c91a52c1f0a84449f91cd0fcce126debbd","44e6fdffac4e670bd2a96203628fcfb6c6816ad4","42d81f5c002586a2c6d7be2f0944ae5585b80b89","539a06ab025005ff2ad5d8435515faa058c73b07","5eb4c55740165defacf08329beaae5314d7fbfe6","2692039d0fea0b75acf38055568501a81b870ebd","6c4fc0ca14a74dd75ba78d7d783b67c4c7c2b15b","4744c5a76c9f9a50f7833ad660fd97fc46eb47d2","233aaaba32065f8e16bada051e6d4dba73961762","06bf7920a31f1b932bc231a169d186735e3b1c15","4bb0ea475f7686a56f1f0f34f13143de352188d9","bd20118a55adc2625e66e4a9c5d2444bb470bbdf","282cf28ac9508bd66a6ddf0709c9db9dc0fdf162","ef6f418c57b32cf0e0acdf93fe346b5634f2ee97"],"id":"783694c19ee8af92465c9a472c11dd37de22d846","s2Url":"https://semanticscholar.org/paper/783694c19ee8af92465c9a472c11dd37de22d846","authors":[{"name":"Yangdong Liu","ids":["2055116"]},{"name":"Wei Gao","ids":["46207764"]},{"name":"Zhanyi Hu","ids":["33953379"]}],"doi":"10.1109/ICPR.2018.8545525"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00113","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Learning regression-based machine learning models for computer vision problems is a challenging task due to noisy features, variation in pose and illumination, occlusion, etc. Typically the problem is compounded by the non-uniform distribution of labels in the training data, resulting in parts of the label space that suffer from data sparsity and a problem of label imbalance in general. Deep Convolutional Neural Networks (CNN) have shown remarkable success on a number of computer vision tasks such as object classification and face recognition. However, they too suffer from sparse and imbalanced training datasets for regression problems, even when those datasets are very large. Cumulative Attributes have previously been proposed to address the issue of label imbalance, but to date this concept has not been integrated with Deep Learning. In this work, we propose a CNN-based framework for learning regression models by using Cumulative Attributes as intermediate features. We evaluate our method on a number of tasks which includes pain intensity estimation, Facial Action Unit intensity estimation and age estimation. Our results show that the proposed method is robust to imbalance and sparsity present in the training datasets, and performs significantly better than the current methods where CNNs are learnt directly for regression.","inCitations":[],"pmid":"","title":"Deep Learned Cumulative Attribute Regression","journalPages":"715-722","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00113"],"entities":["Computer vision","Convolutional neural network","Deep learning","Facial recognition system","Machine learning","Neural Networks","Sparse matrix"],"journalVolume":"","outCitations":["f38904f42caa7dccd71381dacb28e19734b1e23d","214eb90d0386379972cded05e9f57b884edb1675","b38c7a58b4c5298705b8f63dcb6a1c21ee297af8","dbb1871efda531c5340a49e147173c374d069ad1","b82f89d6ef94d26bf4fec4d49437346b727c3bd4","ab29f97e7180b284be7003fbc6725566ea0cd459","5a5f0287484f0d480fed1ce585dbf729586f0edc","4b4f8f463a02cbc8790eee86458ddde4e111c1f1","dc128d77a641312f358071edfd12f05988306465","c570f70459af243af9ca73709646239d82d07655","7c18845fad026c616edaef48295d47a687a5d1bf","833b6e61468fe655b5067ca91608fc37246c767b","22b1b9a0d3df813ba5912d457d8f366e7a0a273c","c1a4096d677e2b136731b05cf5f751b7b1804617","b47a3c909ee9b099854619054fd00e200b944aa9","b84b7b035c574727e4c30889e973423fe15560d7","176a507ebbfdc0fad141da14d30d89caa35bfaf9","d62d0505b37f3df983d97e39452137d31d44c94e","085f86db9dd5e8c62889bee829443e9208cdb889","59420fd595ae745ad62c26ae55a754b97170b01f","6618cff7f2ed440a0d2fa9e74ad5469df5cdbe4c","3c8b0f6a51bdcd0ddadaf7c0373bf5c84e006322","1a6a5a33aa27e44b80b51d0bbc26734a7fc2cccf","56ffa7d906b08d02d6d5a12c7377a57e24ef3391","29fc5339e299b47c3d4f871974069a2971b4b8b6","a0fd85b3400c7b3e11122f44dc5870ae2de9009a","614079f1a0d0938f9c30a1585f617fa278816d53","a8ef957793793c22547aaab3677870c7336c25c1","98ff7597197bfc7b4fe42575cf17d0fc42f501bd","f25af91c87eb2b855ef996db328a20d1260010a5","6c8b30f63f265c32e26d999aa1fef5286b8308ad","2e7e1ee7e3ee1445939480efd615e8828b9838f8","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","4a733a0862bd5f7be73fb4040c1375a6d17c9276","061356704ec86334dbbc073985375fe13cd39088","9abc9e3cadbec9139b39dfddb0de6c08b7aaf2d0","5ce2cb4c76b0cdffe135cf24b9cda7ae841c8d49","eb86c6642040944abc997848a32e631d1f25a2f5","6ab33fa51467595f18a7a22f1d356323876f8262","89d12f74c4195ec04c8bd6e16919e339b2fdcc63","ab4e3ddf21ff23ab0b8354c286ca8e0a22196a96","b1b4b030b405b35689f6405e400697a2701e8119","67db50715cf08e06d6fe7588c205c85d5656714f","162ea969d1929ed180cc6de9f0bf116993ff6e06","16f940b4b5da79072d64a77692a876627092d39c","3b2df7d70ecbe3d0d65d27801d159ddaa150bf42","7ed27f10ff2961611bb8604096a64adfa38c9022","49209adc42ab37b69d3a9553151a9edfa2d0fe72","f472cb8380a41c540cfea32ebb4575da241c0288","09111da0aedb231c8484601444296c50ca0b5388","debe5f241a678d40b0457d448710d9a3483787ac","93221e14e6ce639908c9983e02e960bc2e5939ea","3ec77809aaa7bd30858a4274e3c28a2a0259b30c","7336ddab4a6ae0f8a3db4023e859fea6870e828d","ad1679295a5e5ebe7ad05ea1502bce961ec68057","3edb0fa2d6b0f1984e8e2c523c558cb026b2a983","0daaa56d724c11e64338996e99a257fa69900236","336d4d4f1038ddd1bf62edd0b86c422ed4aef7c8","326b9c8391e89f5bd032aebd1b65e925083c269b","a4543226f6592786e9c38752440d9659993d3cb3","fed90d84dc199d7bd996caddd1cab26ded89de98"],"id":"0b07e4341303272b26be11262feb66c0a4412c43","s2Url":"https://semanticscholar.org/paper/0b07e4341303272b26be11262feb66c0a4412c43","authors":[{"name":"Shashank Jaiswal","ids":["2736086"]},{"name":"Joy Egede","ids":["8692741"]},{"name":"Michel F. Valstar","ids":["1795528"]}],"doi":"10.1109/FG.2018.00113"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594128","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"From archaeology to the inspection of subsea structures, underwater mapping has become critical to many applications. Because of the balanced trade-off between range and resolution, multibeam sonars are often used as the primary sensor in underwater mapping platforms. These sonars output an image representing the intensity of the received acoustic echos over space, which must be classified into free and occupied regions before range measurements are determined and spatially registered. Most classifiers found in the underwater mapping literature use local thresholding techniques, which are highly sensitive to noise, outliers, and sonar artifacts typically found in these images. In this paper we present an overview of some of the techniques developed in the scope of our work on sonar-based underwater mapping, with the aim of improving map accuracy through better segmentation performance. We also provide experimental results using data collected with a DIDSON imaging sonar that show that these techniques improve both segmentation accuracy and robustness to outliers.","inCitations":[],"pmid":"","title":"Multibeam Data Processing for Underwater Mapping","journalPages":"1877-1884","s2PdfUrl":"","pdfUrls":["http://frc.ri.cmu.edu/~kaess/pub/Teixeira18iros.pdf","https://doi.org/10.1109/IROS.2018.8594128"],"entities":[],"journalVolume":"","outCitations":["3abed4183735e2ca3effecd092bfec6feec08a54","480dfd0b953ebbd336c703f79e8bf5e552d4da10","25bb28be7eb140a43c0c63806ca24e8855563647","2067555e697184b7adcdf1b1ba290df099aa8d21","8dfe13156288a877e2db0a79398d047e27946b99","4fd191fd7b8e25090c9e31f1746833fd39b78295","330ec6eb1bcedea792a708850d2ebdcb00e898fe"],"id":"4dd67d12efba3e0518de35ed7afa858abf8eea6d","s2Url":"https://semanticscholar.org/paper/4dd67d12efba3e0518de35ed7afa858abf8eea6d","authors":[{"name":"Pedro V. Teixeira","ids":["23619821"]},{"name":"Michael Kaess","ids":["47640216"]},{"name":"Franz S. Hover","ids":["2177763"]},{"name":"John J. Leonard","ids":["7136913"]}],"doi":"10.1109/IROS.2018.8594128"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594096","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The inherent scale ambiguity in monocular vision is a well known issue that forces the integration of other sensory sources to obtain metric references. However, 2D or 3D LiDARs and RGB-D sensors, while guaranteeing metrological accuracy, impose a non negligible burden both in terms of computational load and power requirements limiting the feasibility of being implemented on small exploration vehicles. This paper presents a scale aware monocular Visual Odometry framework that fuses range data from a laser altimeter in order to recover and maintain a correct metric scale. The proposed Visual Odometry method consists of a keyframe based tracking and mapping algorithm using optical flow where range data serves as a scale constraint on a keyframe to keyframe basis. An optimization backend based on iSAM2 is employed in order to refine the trajectory and map estimates eliminating the scale drift without the need of performing loop closures. We demonstrate that our algorithm can obtain very similar performances to state of the art stereo visual SLAM and RGB-D methods.","inCitations":[],"pmid":"","title":"Scale Correct Monocular Visual Odometry Using a LiDAR Altimeter","journalPages":"3694-3700","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594096"],"entities":[],"journalVolume":"","outCitations":[],"id":"bd230bb5c96b7f40a5cbea59de17234431c4fa34","s2Url":"https://semanticscholar.org/paper/bd230bb5c96b7f40a5cbea59de17234431c4fa34","authors":[{"name":"Riccardo Giubilato","ids":[]},{"name":"Sebastiano Chiodini","ids":[]},{"name":"Marco Pertile","ids":[]},{"name":"Stefano Debei","ids":[]}],"doi":"10.1109/IROS.2018.8594096"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206336","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In human-robot collaborative tasks, incorporating qualitative information provided by humans can greatly enhance the robustness and efficacy of robot state estimation. We introduce an algorithmic framework to model qualitative information as quantitative constraints on and between states. Our approach, named Sequentially Constrained Hamiltonian Monte Carlo, integrates Hamiltonian dynamics into Sequentially Constrained Monte Carlo sampling. We are able to generate samples that satisfy arbitrarily complex, non-smooth and discontinuous constraints, which in turn allows us to support a wide range of qualitative information. We evaluate our approach for constrained sampling qualitatively and quantitatively with several classes of constraints. SCHMC significantly outperforms the Metropolis-Hastings algorithm (a standard Markov Chain Monte Carlo (MCMC) method) and the Hamiltonian Monte Carlo (HMC) method, in terms of both the accuracy of the sampling (for satisfying constraints) and the quality of approximation. Compared to Sequentially Constrained Monte Carlo (SCMC), which supports similar kinds of constraints, our SCHMC approach has faster convergence rates and lower parameter sensitivity.","inCitations":["fa5c7406d09af3f06a3a7ead49975e3ee90ed584"],"pmid":"","title":"Incorporating qualitative information into quantitative estimation via Sequentially Constrained Hamiltonian Monte Carlo sampling","journalPages":"4648-4655","s2PdfUrl":"","pdfUrls":["https://personalrobotics.cs.washington.edu/publications/yi2017schmc.pdf","https://doi.org/10.1109/IROS.2017.8206336"],"entities":["Approximation","Bridging (networking)","Burn-in","Constraint (mathematics)","Experiment","Hybrid Memory Cube","Hybrid Monte Carlo","Markov chain Monte Carlo","Metropolis","Metropolis\u2013Hastings algorithm","Mind","Monte Carlo method","Primary direction","Rate of convergence","Sampling (signal processing)","Series acceleration","Simulation"],"journalVolume":"","outCitations":["47b3bf325edb1ce07e8a37a22124dce06e844598","4587add84f38a30a7f63175a7b21d7767cd59d29","d2de0a0ec2cdb90a6148f5c22fe33d0cb1a333b7","10bb3a5228db6fd76a86e9e9dd21580f0f5c18f5","a7245dd660b25930dce0bf14612d5861ce406c72","37b798bfe195f8c1bf77697327ff526f6844c232","0cea5599f4e81e79642c4e6d8c2e405c1007d1d1","3c680d692563534ad5c8744ecead453c42026d91","e9d5e16206c8277f4e00329c2740bd0d94edae03","e681d9d561e5062aa25309cadc272af36274525b","06529efb02693595d1d455e7c6ea243c4f0c5816","71a0b1ee09b2926003e984f9aa19c893a3465a29","8c4b6527459a69b17714e5dd74acff6f9d6d565c","3d9628361796e5d3c86b3d4d10b260440cec19ea","20e90298081016907869886f1fca0652131d7fbe","45559dac0da222e0c6f555751a308b30fa098243","10bb8b65d0bbec2fbebe03665bc89b05b4bd3b4e","301d01e74a2d8b5644e73578b04e99fdd76c78d5","d2592b4dcec0a22ce39a3bd8c13917a9d31a46eb","3f7b24acf2c12f3607e42ecc5cd44393d486a086","31e3ca7625dd350abf523f257c807d52edcc82d0","507f3041a25b0a0ba93b8d6ecf94e47a4498b970"],"id":"120ea53e70cdfae8f1dac6f81ed428ef7393a1d9","s2Url":"https://semanticscholar.org/paper/120ea53e70cdfae8f1dac6f81ed428ef7393a1d9","authors":[{"name":"Daqing Yi","ids":["39927134"]},{"name":"Shushman Choudhury","ids":["2836907"]},{"name":"Siddhartha S. Srinivasa","ids":["1752197"]}],"doi":"10.1109/IROS.2017.8206336"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593576","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"As roles for unmanned aerial vehicles (UAVs)continue to diversify, the ability to sense and interact closely with the environment becomes increasingly important. Within this paper we report on the initial flight tests of a novel adaptively compliant actuator which will allow a UAV to carry out such tasks as the \u201cpick and placement\u201d of remote sensors, structural testing and contact-based inspection. Three key results are discussed and presented; the ability to physically apply forces with the UAV through the use of an active compliant manipulator; the ability to tailor these forces through tuning of the manipulator controller gains; and the ability to apply a rapid series of physical pulses in order to excite remotely placed sensors, e.g. vibration sensors. A series of over sixty flight tests have been used to generate initial results which clearly demonstrate the potential of this new type of compliant aerial actuator.","inCitations":[],"pmid":"","title":"Towards an Adaptive-Compliance Aerial Manipulator for Contact- Based Interaction","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1709.08536v2.pdf","http://arxiv.org/abs/1709.08536","https://doi.org/10.1109/IROS.2018.8593576"],"entities":[],"journalVolume":"","outCitations":["80718c81c08e7a6fc70df98313c65f8359192113","472e606595ee8af317a22a553ad09929def1a3df","381d192093a3f5d65981125fd6e1a5512b6c5476","ea90930fa85029ab97ea00bbf1041f0e4e541226","9984620aabe6065337f38c21a91ab6615cfe117b","8c7e62df4b1e2c44734c901682508d9203662eb7","205da9b20f5f61f773f8760d6fe8c41fae61019b","70b1f7f7304b968239de399f5e271d98e72594f4","55d7551c54edacef55902209d7fc0fddcf7f0c5c","acdf1433ef956e748eda4af7e2d2c04547da105f","10659fb27307b62425a8f59fe39b6c786f39401b","0052c9768f873bebf4b1788cc94bb8b0b3526be7","20d3308db7ec3871cffd58db5e9e77d032f10e3a","573794b821ae3fba86ea7e1dbc2982f40ed0f43d","db5fad57f5b1ebedd519d4618ecf351ccd001eb4"],"id":"ad8f2e2d20565b66765ef0e1ac2f37ffc02dc70c","s2Url":"https://semanticscholar.org/paper/ad8f2e2d20565b66765ef0e1ac2f37ffc02dc70c","authors":[{"name":"Salua Hamaza","ids":["9647280"]},{"name":"Ioannis Georgilas","ids":["2532066"]},{"name":"Thomas Richardson","ids":["1932728"]}],"doi":"10.1109/IROS.2018.8593576"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594510","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The paper presents design, fabrication and characteristics of two kinds of conductive Knit-covered Pneumatic Artificial Muscle (it is called as k-PAM in the paper) actuators, in which two different knits are made by braiding silver-coated (conductive) yarn and spandex (non-conductive) yarn with different stitch methods. The k-PAM is able to measure the change in length of the actuator body according to the applied air pressures as well as the strain due to external force. A complete fabrication method is presented to make the actuator work for higher pressure (≥ 300[kPa]). Since the force generated by the actuator is decoupled from the external force, ultimately, it can be directly used to measure not only the length but also the force. Experimental validations are performed describing the characteristics of two different types of k-PAMs. It is expected that the k-PAM can be used directly for robotic applications in higher pressure condition, while the semi-permanent conductive knit provides the actuator with durability in high repetitive operation environment.","inCitations":[],"pmid":"","title":"Conductive Knit-covered Pneumatic Artificial Muscle (k-PAM) Actuator","journalPages":"1476-1481","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594510"],"entities":[],"journalVolume":"","outCitations":[],"id":"3a284d57cca7de6e8d877b15867cb29b6847c44a","s2Url":"https://semanticscholar.org/paper/3a284d57cca7de6e8d877b15867cb29b6847c44a","authors":[{"name":"Babar Jamil","ids":[]},{"name":"Seul Ah Lee","ids":[]},{"name":"Youngjin Choi","ids":[]}],"doi":"10.1109/IROS.2018.8594510"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206457","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Differential Dynamic Programming (DDP) has become a popular approach to performing trajectory optimization for complex, underactuated robots. However, DDP presents two practical challenges. First, the evaluation of dynamics derivatives during optimization creates a computational bottleneck, particularly in implementations that capture second-order dynamic effects. Second, constraints on the states (e.g., boundary conditions, collision constraints, etc.) require additional care since the state trajectory is implicitly defined from the inputs and dynamics. This paper addresses both of these problems by building on recent work on Unscented Dynamic Programming (UDP) \u2014 which eliminates dynamics derivative computations in DDP\u2014to support general nonlinear state and input constraints using an augmented Lagrangian. The resulting algorithm has the same computational cost as first-order penalty-based DDP variants, but can achieve constraint satisfaction to high precision without the numerical ill-conditioning associated with penalty methods. We present results demonstrating its favorable performance on several simulated robot systems including a quadrotor and 7-DoF robot arm.","inCitations":["07897974f84b88f3303e1316fd084fa5e3705c5f","b42af5c9763031ac837da4c2e1272e5543237126"],"pmid":"","title":"Constrained unscented dynamic programming","journalPages":"5674-5680","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206457","https://agile.seas.harvard.edu/files/agile/files/constrained-udp.pdf"],"entities":["Algorithm","Algorithmic efficiency","Augmented Lagrangian method","Boundary case","Computation","Condition number","Constraint satisfaction","Data-directed programming","Differential dynamic programming","Experiment","First-order predicate","Image scaling","Iteration","Mathematical optimization","Nonlinear system","Numerical analysis","Penalty method","Robot","Robotic arm","Schedule (computer science)","Time complexity","Trajectory optimization"],"journalVolume":"","outCitations":["a5b9067cf07b1091beddc56e3b2d2cf3eae4cd3a","7aa94e48145eb4970222d8072de275c3dd979f7e","876abf2ffe53a381c2f7841487e981c86bd55505","eb2c8ad4666264f0b027bb11b459881fa181034b","2386abd05c0f1de4a3f55584e697932dea3af654","75c6ac5f11dcf54df0bd8f5a19eace584f7ace1a","cf121e07bc234cdd69d2d9e5be03b45b698d283f","5562da2356af800a5777386d5fa277325a51102a","3e677b1fa07d66f539f8086046779190694327eb","48230ed0c3fa53ef1d43d79e1f6b113f13e83b9b","19174cec984043b7c8a86b781388ae633ad11811","387e0d200604c3f05306b28dcd59b4d9b46ec645","204da6a56eb9746e174d6e2108d9c11e6d4f673d","1a5060521e0cf6ac44bb914b61ae04e33f20c657","45fd8d3745b20349ef763c80bf01ced802eaf75a","74414cc12aa57b36c01340327b5a3c2e3d1fe8b3","2449bd5b171a3881ab2623f520e9c4af6a177096","8b896fe4cba2b772902cb894a3050c3589152418"],"id":"be731d56bfd1da32107fccfe7a1da35d006eda24","s2Url":"https://semanticscholar.org/paper/be731d56bfd1da32107fccfe7a1da35d006eda24","authors":[{"name":"Brian Plancher","ids":["10803865"]},{"name":"Zachary Manchester","ids":["7987394"]},{"name":"Scott Kuindersma","ids":["1794340"]}],"doi":"10.1109/IROS.2017.8206457"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206284","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, we propose a robust on-the-fly estimator initialization algorithm to provide high-quality initial states for monocular visual-inertial systems (VINS). Due to the non-linearity of VINS, a poor initialization can severely impact the performance of either filtering-based or graph-based methods. Our approach starts with a vision-only structure from motion (SfM) to build the up-to-scale structure of camera poses and feature positions. By loosely aligning this structure with pre-integrated IMU measurements, our approach recovers the metric scale, velocity, gravity vector, and gyroscope bias, which are treated as initial values to bootstrap the nonlinear tightly-coupled optimization framework. We highlight that our approach can perform on-the-fly initialization in various scenarios without using any prior information about system states and movement. The performance of the proposed approach is verified through the public UAV dataset and real-time onboard experiment. We make our implementation open source, which is the initialization part integrated in the VINS-Mono1.","inCitations":["a4493f4ea3dc91d9f3a470ac2bc1ae7b01000307","fc66cb625c85b90e2eead91ded803323f1d93542","8f226c275ddc38ff69910b921223e1f9b1def8c3","cabaf4d57ef31c6789015d94389664eb9296bc20","bd0d4580b9de7e2407fb909ddf4aa6ea18791ed4","79d53382135180579fe13ed2044fb13ad5e19542","d61556e6ec5c8911eb60fd5188dc7dc4f94a3b9f","012df8381ab41b269252956e722e4b364e3aca7a","5ce891f56d6266f9f9db3104ffe10e644f714512","9da965529ee3da77178ed99cf13d97be0fa85f6e","b937844485d497147fa33a76e62067717415ad46","d5a643610653c820d8bb242221142794f42e2c2e"],"pmid":"","title":"Robust initialization of monocular visual-inertial estimation on aerial robots","journalPages":"4225-4232","s2PdfUrl":"","pdfUrls":["http://www.ece.ust.hk/~eeshaojie/iros2017tong.pdf","https://doi.org/10.1109/IROS.2017.8206284"],"entities":["Aerial photography","Algorithm","Autonomous robot","Bias\u2013variance tradeoff","Control theory","Exponent bias","Filter (signal processing)","Graph (discrete mathematics)","Gyroscope","Mathematical optimization","Motion planning","Nonlinear system","Obstacle avoidance","Open-source software","Real-time transcription","Robot","Robustness (computer science)","Structure from motion","Unmanned aerial vehicle","Vehicle identification number","Velocity (software development)"],"journalVolume":"","outCitations":["d2b0742c91747fa82b3e2070a6f8edbc52c7ec4d","2cbc2797efe322f8ac286d90cbc2c4852f0f6bde","d2be2b103aa6e19a362a614693081eb07f6b5cdc","1457f7dbd7294cea1e7745820c5ed634420f87a8","cef24dd603f1d5292625c79072f5c4cc255b950e","6946e3a78a7ef890b1becb121d0d6c3fcf1174e5","5bea5d98d2ccf44cf54ba1b3e2da4d20f244e053","21735966b951d3745c71f209c70493dc9ac03b7d","1dcae5e5e9fc1b4c0315b6e68f19b9bb2d9dba05","2295fda13d5118808557d04310b0e176a1341063","91339cdd93d9250e0a5cf33eb13901a61d23e996","3f803ccf2d038cb9930006ed598dab327806528c","a56ed0bc2d69094e351a044ae8bc64ca0da691f8","71759b5ec708493103898b3bcf5785d08402eb53","d45b7715d849d1e643afc4c761dff400412c7100","5ee84ae7d9a2822709da1554f712736263c9db63","27eaf1c2047fba8a71540496d59f3fec9b7f46bb","53ead98595bc6d2e2c3570595bafd2e7eeee0841","10cfa5bfab3da9c8026d3a358695ea2a5eba0f33","0e6ecd419e6728b97871fb3113a895694e9952c4","c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2","6f7892e29b76ba107ca3e6a923ce56dd3a7cd580","25e26d4db49e280669e26cd05305d27f35a0791a","dc89af07733ac65b9f61ffcac846f9851065de35","7200dce06a410959760aca287c53557830f43fcf","0a202f1dfc6991a6a204eaa5e6b46d6223a4d98a","a13dc9739e4637599359d792fd60d511ab8a016e","0be0c13803cd08e81b7adaada537e91222eb1491","1f2ee7714c3018c1687f9b17f2bc889c5c017439","4a0fd215a2aff831a278cc4af1212c2898f1738d","230ad73e8bd1d3268d56c66a83442d24176b864d","19a5e5b31e741d0f9cacf9c93de782184d24c947","6cfffb91f23b6744b003ec629f87199dc42f6383","2e7975fb0351638bd2646a217e5885ae56ca5cff"],"id":"d2aeff586ee389df3572ea3f26c82c02443d19b9","s2Url":"https://semanticscholar.org/paper/d2aeff586ee389df3572ea3f26c82c02443d19b9","authors":[{"name":"Tong Qin","ids":["49725918"]},{"name":"Shaojie Shen","ids":["3225993"]}],"doi":"10.1109/IROS.2017.8206284"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545762","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Data augmentation techniques have been employed to overcome the problem of model over-fitting in deep convolutional neural networks and have consistently shown improvement in classification. Most data augmentation techniques perform affine transformations on the image domain. However these techniques cannot be used when object position is significant in the image. In this work we propose a data augmentation technique based on sampling an representation built by inequality constraints propagated from local binary patterns. We sample nine distinct variations for an image in a manner meant to preserve local structure and differ only in the amount of contrast between pixels. These contrast invariants are then used to augmented the original images. We present evaluations on CIFAR-10 and validate our gains in performance across four criteria, accuracy, precision, recall and Fl-Score; using a 2-layer convolutional neural network with different configuration of filters, and report improvement by about 13%, 9%, 12%, and 22% respectively over the baseline.","inCitations":[],"pmid":"","title":"Appearance-based data augmentation for image datasets using contrast preserving sampling","journalPages":"1235-1240","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545762"],"entities":[],"journalVolume":"","outCitations":["2d83ba2d43306e3c0587ef16f327d59bf4888dc3","398c296d0cc7f9d180f84969f8937e6d3a413796","6c8b30f63f265c32e26d999aa1fef5286b8308ad","82af450121c9119ee75cc8258ab95b640fb5ae9e","7b1cc19dec9289c66e7ab45e80e8c42273509ab6","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","4e5c01240c080ccd2d0b637440d2cd803bc1a1c7","0abb49fe138e8fb7332c26b148a48d0db39724fc","20aef74fb523e9e67074cea89c727cbb44bcf217","fb4948e7df2fcd0e2a1f3f4794fba5654f55498c","5d90f06bb70a0a3dced62413346235c02b1aa086","2dc9b005e936c9c303386caacc8d41cabdb1a0a1","1bcf142e3b2f5a7ade185f220ee031657203c293","8641f82c7f6c1bb724af693e2a80b491b8da7dcf","10b496ad48513f8585aa56f2c682159357858960"],"id":"b4ceb8160ae35e23bd4a8f33733e81b3290e216d","s2Url":"https://semanticscholar.org/paper/b4ceb8160ae35e23bd4a8f33733e81b3290e216d","authors":[{"name":"Alishan K. Merchant","ids":["20815967"]},{"name":"Tahir Q. Syed","ids":["2393456"]},{"name":"Behraj Khan","ids":["10849970"]},{"name":"Rumaisah Munir","ids":["51138301"]}],"doi":"10.1109/ICPR.2018.8545762"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546268","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Due to the continuous development of GAN, vivid faces can be generated, and the use of GAN for face aging becomes a novel trend. However, many existing works for face aging require tedious pre-processing of datasets. This brings a lot of computational burden and limits the application of face aging. In order to solve these problems, a face aging network is constructed using IcGAN without any data pre-processing which map a face image into personality and age vector spaces through encoders Z and Y. Different from the previous work, we make an emphasis on the preservation of both personalized and aging features. Thus, the minimize absolute reconstructing loss is proposed to optimize vector z, which can remain the personality characteristics, meanwhile preserving the pose, hairstyle and background of the input face. Additionally, we introduce a novel age vector optimization approach by classifying reconstruction loss and introduce the parameter λ which is well-balanced between large age features and subtle texture features. The experimental results demonstrate our proposed AIGAN provides better aging faces over other state-of-the-art age progression methods.","inCitations":[],"pmid":"","title":"Face Aging with Improved Invertible Conditional GANs","journalPages":"1396-1401","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546268"],"entities":["Classification","Color gradient","Data pre-processing","Encoder","Face","GIANT AXONAL NEUROPATHY 1","Mathematical optimization","Personalization","Population Parameter","Pose (computer vision)","Preprocessor","Vector optimization"],"journalVolume":"","outCitations":["9b91989f5f9b05ac6612ac75d77bc37fb91bb766","989332c5f1b22604d6bb1f78e606cb6b1f694e1a","5c4d1c6ea78d0fe13e9fab182649131bb0cfe062","6a5d7d20a8c4993d56bcf702c772aa3f95f99450","7fab17ef7e25626643f1d55257a3e13348e435bd","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","385750bcf95036c808d63db0e0b14768463ff4c6","2836d68c86f29bb87537ea6066d508fde838ad71","c44c84540db1c38ace232ef34b03bda1c81ba039","ba753286b9e2f32c5d5a7df08571262e257d2e53","9695630f74f8601c1ca1d7eda12f89bb1d9500d3","42917e8bf8dc220ee6145cd13aed688f556f64ff","f8f090600b6a6b745a224db05bab7e8d59ed21f8","0f112e49240f67a2bd5aaf46f74a924129f03912","0e5557a0cc58194ad53fab5dd6f4d4195d19ce4e","3379521f1cb36df1501d0797ea95eb4c0341ab94","40a63746a710baf4a694fd5a4dd8b5a3d9fc2846","9a700c7a7e7468e436f00c34551fbe3e0f70e42f","8355d095d3534ef511a9af68a3b2893339e3f96b","b234d429c9ea682e54fca52f4b889b3170f65ffc","35756f711a97166df11202ebe46820a36704ae77"],"id":"d2be07499aea3ef593fae5b2f6cd89fe627f4d98","s2Url":"https://semanticscholar.org/paper/d2be07499aea3ef593fae5b2f6cd89fe627f4d98","authors":[{"name":"Li Jia","ids":["50493022"]},{"name":"Yonghong Song","ids":["1682580"]},{"name":"Yuanlin Zhang","ids":["1767044"]}],"doi":"10.1109/ICPR.2018.8546268"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206195","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We introduce a variant of the multi-vehicle routing problem which accounts for nonholonomic constraints and dense, dynamic obstacles, called MVRP-DDO. The problem is strongly motivated by an industrial mining application. This paper illustrates how MVRP-DDO relates to other extensions of the vehicle routing problem. We provide an application-independent formulation of MVRP-DDO, as well as a concrete instantiation in a surface mining application. We propose a multi-abstraction search approach to compute an executable plan for the drilling operations of several machines in a very constrained environment. The approach is evaluated in terms of makespan and computation time, both of which are hard industrial requirements.","inCitations":[],"pmid":"","title":"Multi vehicle routing with nonholonomic constraints and dense dynamic obstacles","journalPages":"3522-3529","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206195","http://oru.diva-portal.org/smash/get/diva2:1168586/FULLTEXT01.pdf"],"entities":["Algorithm","Application domain","Computation","Dynamic drive overlay","Executable","High- and low-level","Iterative method","Local search (optimization)","Makespan","Principle of abstraction","Refinement (computing)","Requirement","Robot","Scalability","Time complexity","Universal instantiation","Vehicle routing problem"],"journalVolume":"","outCitations":["67920298f58c2faf89f89c728066fdb2d887cf6b","dc17a99e067ad2d8830fe4771948778a4bcebd16","56d4d356f2b598dbf8d92b799e26761ed8282f4f","03f6b7cad157f3d874e0f76f6aa83dbd4450a06a","2aca37bb5e84e35af16e553bbe22ad9b2252319e","99e269b87d4a0f52dff5c5f95e083e08c7fae312","d00585cb74c86052043a38cb30f2100f6a8512c8","777d358be6df431c275dbdf25e714a7a6ae1df4f","1e7845ebdcf9b63cc20d7b39cce8aa7791ff3a9b","003e3f9eab07473b66c03b69adf9c389c0204b64","3755559483473b256b4afcca22185b6fe56bb1f0","defeba946cffb4ef6d3dfb82ccd71f541875286a","344927c0a38e7ba9167d5324fa3f0d435ac2fb2c","5409224fc19b84bc9c9b879edb855f4c784f37cb","19299588f12a4f6261252d33ba7437537f2ed51b","b233a839a01dabafa11cc7434f910c5338a616e0"],"id":"9b32d303efb2ce1786ebdbcacb1095460550160b","s2Url":"https://semanticscholar.org/paper/9b32d303efb2ce1786ebdbcacb1095460550160b","authors":[{"name":"Masoumeh Mansouri","ids":["3320153"]},{"name":"Fabien Lagriffoul","ids":["3270529"]},{"name":"Federico Pecora","ids":["2766656"]}],"doi":"10.1109/IROS.2017.8206195"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593713","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Intelligent service robots are used at a significant level to uplift the living standards of domestic users. These robots are expected to possess human-friendly interactive features. Service robots should be able to provide a variety of tasks to support independent living of users in domestic environments. Therefore, a service robot often needs to approach users to execute these services and the approach toward the users should be human friendly. In order to achieve this, proxemics planner of a service robot should be cable of deciding the approaching proxemics based on user behavior. This paper proposes a method to decide the approaching proxemics based on the behavior of the user. A fuzzy interference system has been designed to decide the proxemics based on the user behavior identified through body parameters. This leads to an effective interaction mechanism initiated by a robot in such a way that the approaching scenario looks more humanlike. The proposed concept has been implemented on MIRob platform and experiments were conducted in an artificially created domestic environment. The experimental results of the proposed system have been compared with results of a human study to evaluate the performance of the system.","inCitations":[],"pmid":"","title":"Proxemics and Approach Evaluation by Service Robot Based on User Behavior in Domestic Environment","journalPages":"8192-8199","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593713"],"entities":[],"journalVolume":"","outCitations":[],"id":"8efeb41ea739f9c07b8b994097f9a11fd9764a69","s2Url":"https://semanticscholar.org/paper/8efeb41ea739f9c07b8b994097f9a11fd9764a69","authors":[{"name":"S. M. Bhagya P. Samarakoon","ids":[]},{"name":"H. P. Chapa Sirithunge","ids":[]},{"name":"M. A. Viraj J. Muthugala","ids":[]},{"name":"A. G. Buddhika P. Jayasekara","ids":[]}],"doi":"10.1109/IROS.2018.8593713"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594519","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Hand-eye coordination is a requirement for many manipulation tasks including grasping and reaching. However, accurate hand-eye coordination has shown to be especially difficult to achieve in complex robots like the iCub humanoid. In this work, we solve the hand-eye coordination task using a visuomotor deep neural network predictor that estimates the arm's joint configuration given a stereo image pair of the arm and the underlying head configuration. As there are various unavoidable sources of sensing error on the physical robot, we train the predictor on images obtained from simulation. The images from simulation were modified to look realistic using an image-to-image translation approach. In various experiments, we first show that the visuomotor predictor provides accurate joint estimates of the iCub's hand in simulation. We then show that the predictor can be used to obtain the systematic error of the robot's joint measurements on the physical iCub robot. We demonstrate that a calibrator can be designed to automatically compensate this error. Finally, we validate that this enables accurate reaching of objects while circumventing manual fine-calibration of the robot.","inCitations":[],"pmid":"","title":"Transferring Visuomotor Learning from Simulation to the Real World for Robotics Manipulation Tasks","journalPages":"6667-6674","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594519"],"entities":[],"journalVolume":"","outCitations":[],"id":"fdd9658c2fcbb6ef2caa84eac9920b9a047ac3b4","s2Url":"https://semanticscholar.org/paper/fdd9658c2fcbb6ef2caa84eac9920b9a047ac3b4","authors":[{"name":"Phuong D. H. Nguyen","ids":[]},{"name":"Tobias Fischer","ids":[]},{"name":"Hyung Jin Chang","ids":[]},{"name":"Ugo Pattacini","ids":[]},{"name":"Giorgio Metta","ids":[]},{"name":"Yiannis Demiris","ids":[]}],"doi":"10.1109/IROS.2018.8594519"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593603","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Accurate state estimation is a fundamental module for various intelligent applications, such as robot navigation, autonomous driving, virtual and augmented reality. Visual and inertial fusion is a popular technology for 6-DOF state estimation in recent years. Time instants at which different sensors' measurements are recorded are of crucial importance to the system's robustness and accuracy. In practice, timestamps of each sensor typically suffer from triggering and transmission delays, leading to temporal misalignment (time offsets) among different sensors. Such temporal offset dramatically influences the performance of sensor fusion. To this end, we propose an online approach for calibrating temporal offset between visual and inertial measurements. Our approach achieves temporal offset calibration by jointly optimizing time offset, camera and IMU states, as well as feature locations in a SLAM system. Furthermore, the approach is a general model, which can be easily employed in several feature-based optimization frameworks. Simulation and experimental results demonstrate the high accuracy of our calibration approach even compared with other state-of-art offline tools. The VIO comparison against other methods proves that the online temporal calibration significantly benefits visual-inertial systems. The source code of temporal calibration is integrated into our public project, VINS-Mono11https://github.com/HKUST-Aerial-Robotics/VINS-Mono.","inCitations":[],"pmid":"","title":"Online Temporal Calibration for Monocular Visual-Inertial Systems","journalPages":"3662-3669","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1808.00692v1.pdf","https://doi.org/10.1109/IROS.2018.8593603","http://arxiv.org/abs/1808.00692"],"entities":[],"journalVolume":"","outCitations":["215fe6ebcf44a364edca5e9a19ff79d3360d0e86","1dcae5e5e9fc1b4c0315b6e68f19b9bb2d9dba05","362f9af442e7242ec2f0b19a83ede6fa07637d37","a56ed0bc2d69094e351a044ae8bc64ca0da691f8","c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2","4a0fd215a2aff831a278cc4af1212c2898f1738d","4329bfa13125f5e1809aea8b26d90cfa47d610a2","692e41680451ab9c13de39c1a9c70e151f5e7341","4466dcf725e5383b10a6c5172c860f397e006b48","51fea461cf3724123c888cb9184474e176c12e61","10cfa5bfab3da9c8026d3a358695ea2a5eba0f33","ac5b6fb2da8240acac5227fc9d754f22198e00ad","55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1","5eb4c55740165defacf08329beaae5314d7fbfe6","3f803ccf2d038cb9930006ed598dab327806528c","2d486e2e0ddee30ac33f86edc6474e8bd7bbd2a4","0be0c13803cd08e81b7adaada537e91222eb1491","91cd699bff346db2e25b5efa5a194f38ac54a5c4","2e7975fb0351638bd2646a217e5885ae56ca5cff","a13dc9739e4637599359d792fd60d511ab8a016e","230ad73e8bd1d3268d56c66a83442d24176b864d","7200dce06a410959760aca287c53557830f43fcf","4257552ef497680b7d3508f68cd2b39378f5b70f","25e26d4db49e280669e26cd05305d27f35a0791a","0a202f1dfc6991a6a204eaa5e6b46d6223a4d98a","9da965529ee3da77178ed99cf13d97be0fa85f6e","19a5e5b31e741d0f9cacf9c93de782184d24c947","53ead98595bc6d2e2c3570595bafd2e7eeee0841"],"id":"5dd6effe125d6fb341b1b9d23f1044b903c06104","s2Url":"https://semanticscholar.org/paper/5dd6effe125d6fb341b1b9d23f1044b903c06104","authors":[{"name":"Tong Qin","ids":["49725918"]},{"name":"Shaojie Shen","ids":["3225993"]}],"doi":"10.1109/IROS.2018.8593603"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593906","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Belt conveyors are transport systems composed by a plurality of belt lines. When those systems are used to transport fragile materials or liquid containers, it is necessary to minimize the oscillations of the transported objects in order to avoid damage, spillage and particle degradation. Those oscillations are regularly caused by steps (i.e. differences in level) at the joints of the belt lines, and for that reason, it is necessary to inspect those steps during installation and maintenance. Regular inspections involve the visual verification of the steps, which stops production, takes significant time, occasionally requires system disassembly and is subjected to human errors. In this paper, a novel belt conveyor inspection system which is able to detect and measure the steps at the joints of the belt lines is presented. This system consists in the acquirement of data of the belt conveyor with an inertial measurement unit (IMU), and the processing of this data with original algorithms for zero offset filtering, sensor progressing direction detection, step event detection and step height calculation. The presented system had successfully detected and measured the steps of a complex belt conveyor with an accuracy of $\\pm 0.3\\ \\text{mm}$. It is demonstrated that this cost-effective and ready to use system enables an automatic and prompt inspection of the whole belt conveyor system at once, thus reducing the workload, time and errors of the belt conveyor inspection.","inCitations":[],"pmid":"","title":"Inspection System for Automatic Measurement of Level Differences in Belt Conveyors Using Inertial Measurement Unit","journalPages":"6155-6161","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593906"],"entities":[],"journalVolume":"","outCitations":[],"id":"22927fe5d4e5ad0c1e8777c28d4b78db3d102de3","s2Url":"https://semanticscholar.org/paper/22927fe5d4e5ad0c1e8777c28d4b78db3d102de3","authors":[{"name":"Andre Yuji Yasutomi","ids":[]},{"name":"Hideo Enoki","ids":[]},{"name":"Shigeki Yamaguchi","ids":[]},{"name":"Kazuma Tamura","ids":[]}],"doi":"10.1109/IROS.2018.8593906"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593720","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We study the problem of learning a navigation policy for a robot to actively search for an object of interest in an indoor environment solely from its visual inputs. While scene-driven visual navigation has been widely studied, prior efforts on learning navigation policies for robots to find objects are limited. The problem is often more challenging than target scene finding as the target objects can be very small in the view and can be in an arbitrary pose. We approach the problem from an active perceiver perspective, and propose a novel framework that integrates a deep neural network based object recognition module and a deep reinforcement learning based action prediction mechanism. To validate our method, we conduct experiments on both a simulation dataset (AI2-THOR)and a real-world environment with a physical robot. We further propose a new decaying reward function to learn the control policy specific to the object searching task. Experimental results validate the efficacy of our method, which outperforms competing methods in both average trajectory length and success rate.","inCitations":["551fedfeaf55e3f7a7cf19d2b21f1a56f8cbe9f6","b8bb323941860a8d37e0d27dd8e7959a39b4d18a"],"pmid":"","title":"Active Object Perceiver: Recognition-Guided Policy Learning for Object Searching on Mobile Robots","journalPages":"6857-6863","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1807.11174","https://arxiv.org/pdf/1807.11174v1.pdf","https://doi.org/10.1109/IROS.2018.8593720"],"entities":[],"journalVolume":"","outCitations":["3057b47d756a541fcc769a17ce1eb9a66de982e6","7d39d69b23424446f0400ef603b2e3e22d0309d6","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","f3805df1f1c15e31d561054964f14124380a4a7f","f1e9f30d360f4493e17e72637c34ea2d6508f7b1","82e80e187fbaff0c6b0a05542e26f92f56ada09e","10bf5a7fdf7039dbe12ea293cd50391a5adda08b","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","1aa664cadef71e10dbf26af2e67c67e19da3d61e","e65d5ca2a2efe6f5756f579029def41fac4b374e","2c03df8b48bf3fa39054345bafabfeff15bfd11d","21a1654b856cf0c64e60e58258669b374cb05539","340f48901f72278f6bf78a04ee5b01df208cc508","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a"],"id":"af1e2a556e8a3a21a9d190ae53be737c627c1cab","s2Url":"https://semanticscholar.org/paper/af1e2a556e8a3a21a9d190ae53be737c627c1cab","authors":[{"name":"Xin Ye","ids":["50183910"]},{"name":"Zhe Lin","ids":["40340165"]},{"name":"Haoxiang Li","ids":["3131569"]},{"name":"Shibin Zheng","ids":["51150053"]},{"name":"Yezhou Yang","ids":["7607499"]}],"doi":"10.1109/IROS.2018.8593720"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593883","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Robots hold promise in many scenarios involving outdoor use, such as search-and-rescue, wildlife management, and collecting data to improve environment, climate, and weather forecasting. However, autonomous navigation of outdoor trails remains a challenging problem. Recent work has sought to address this issue using deep learning. Although this approach has achieved state-of-the-art results, the deep learning paradigm may be limited due to a reliance on large amounts of annotated training data. Collecting and curating training datasets may not be feasible or practical in many situations, especially as trail conditions may change due to seasonal weather variations, storms, and natural erosion. In this paper, we explore an approach to address this issue through virtual-to-real-world transfer learning using a variety of deep learning models trained to classify the direction of a trail in an image. Our approach utilizes synthetic data gathered from virtual environments for model training, bypassing the need to collect a large amount of real images of the outdoors. We validate our approach in three main ways. First, we demonstrate that our models achieve classification accuracies upwards of 95% on our synthetic data set. Next, we utilize our classification models in the control system of a simulated robot to demonstrate feasibility. Finally, we evaluate our models on real-world trail data and demonstrate the potential of virtual-to-real-world transfer learning.","inCitations":[],"pmid":"","title":"Virtual-to-Real-World Transfer Learning for Robots on Wilderness Trails","journalPages":"576-582","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1901.05599v1.pdf","https://doi.org/10.1109/IROS.2018.8593883"],"entities":[],"journalVolume":"","outCitations":[],"id":"b29ea39c4faa9cc45d4a6745847db81c5150db24","s2Url":"https://semanticscholar.org/paper/b29ea39c4faa9cc45d4a6745847db81c5150db24","authors":[{"name":"Michael L. Iuzzolino","ids":[]},{"name":"Michael E. Walker","ids":[]},{"name":"Daniel Szafir","ids":[]}],"doi":"10.1109/IROS.2018.8593883"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545661","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The research and analysis on multi-source data is one of important tasks in information science. Compared with traditional single-source data learning algorithms, multi-source data learning ones can describe objects more real and complete. Meanwhile, the learning process of multi-source data is more in line with the cognitive mechanism of human brain. So far, the research on multi-source data learning algorithms includes three classes, multi-source data transfer learning, multi-source data collaborative learning and multi-source multi-view learning. The traditional multi-source multi-view learning algorithms lack the ability of handling with the data missing issue, which means that these algorithms require the multi-source data to be complete. This paper proposes a multi-source clustering algorithm. Based on the spectral properties of Laplace operator, we first obtain the complete representation of multi-source data. Then, we utilize the multi-view spectral embedding (MVSE) to construct the fusion model. Experimental results show that our proposed method can improve the ability of clustering efficiently in the case of data missing.","inCitations":[],"pmid":"","title":"Multi-Source Clustering based on spectral recovery","journalPages":"231-236","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545661"],"entities":[],"journalVolume":"","outCitations":["05ffc37ed1289c9dbd01f1cd96d5a5ae908b12cb","0bdb21672685878d39ac67c6d81c1ef48ed5efda","5dacc108011a7d0d683df4b4455239b7d6734356","f3e646977ab78178fe05eb64421b421171c1c6fa","de85a52f4cb4f7c3e4897b4ba839d4da6be4fbbf","64f1b9a5eba8c25dee4e74e9d9d5e8c71de58523","486c40ab602a9325ec132360a1a66a10e4f32e38","ef155e39361598e0ac065101521678e8db69b4c3","917e01d4be4a3417b2941ecd548186a6bf868358","d72317e5188f738dbdf257ae51fc3cefd53f1699","d4d629f9111ea7b2adf9c6f88abccec4ccb2eb34","19924ad30338ea6192a3ce2fba0a0788c6172cde","3d49a0840e7d2660df586dc99c8dd7289aede6ca","ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2","1c7e4f61cdd401356b30cbb1e97be33ba305d707","9383f08c697b8aa43782e16c9a57e089911584d8","fb25566db4ff4d676dc4aec99a0122966e3e90b0","f8e379fd1a7f46d46a3919dc08f6a39705c6e6d8","6367d3d4e6ff7882b5f9b0f179b8ac32180036fb","3d8c9e6af31a0f4cd3cd47706a8735167ca95b0b","e0d2861a9022667a93a8a0573d44f238f7c3a027","f88370e1207c9ee5257a895f5b587db30148454b","2eacb75728c0c81f51bf10da9e7e7ea4aa4303eb","ba9233dd7c81765b7afb2cc1a6e5e9a075518d8c","6dd8d8be00376ac760dc92f9c5f20520872c5355"],"id":"b11a4b4ac51561ac93e58e734e95eb6c06b39b25","s2Url":"https://semanticscholar.org/paper/b11a4b4ac51561ac93e58e734e95eb6c06b39b25","authors":[{"name":"Hongwei Yin","ids":["1692442"]},{"name":"Fanzhang Li","ids":["2314391"]},{"name":"Li Zhang","ids":["48571392"]}],"doi":"10.1109/ICPR.2018.8545661"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593419","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Visual odometry and SLAM methods have a large variety of applications in domains such as augmented reality or robotics. Complementing vision sensors with inertial measurements tremendously improves tracking accuracy and robustness, and thus has spawned large interest in the development of visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI benchmark, a novel dataset with a diverse set of sequences in different scenes for evaluating VI odometry. It provides camera images with 1024×1024 resolution at 20 Hz, high dynamic range and photometric calibration. An IMU measures accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and IMU sensors are time-synchronized in hardware. For trajectory evaluation, we also provide accurate pose ground truth from a motion capture system at high frequency (120 Hz) at the start and end of the sequences which we accurately aligned with the camera and IMU measurements. The full dataset with raw and calibrated data is publicly available. We also evaluate state-of-the-art VI odometry approaches on our dataset.","inCitations":["ae0322119c8d70ba98f32aeb205393dad8dd287b","99addb1a85b1cfd8c22eca439bc1954e72254753","0cf0dafb9815bf8d1da8c9d4bd9cb55a71d9a62a","29bd9cdcb6f3c8c7475df5918c0d87283ffa254f","ce058303668eb87595f311533b6cdb8a80ecd5f6"],"pmid":"","title":"The TUM VI Benchmark for Evaluating Visual-Inertial Odometry","journalPages":"1680-1687","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1804.06120v2.pdf","http://arxiv.org/abs/1804.06120","https://doi.org/10.1109/IROS.2018.8593419","http://arxiv-export-lb.library.cornell.edu/pdf/1804.06120"],"entities":[],"journalVolume":"","outCitations":["5cd11d6b6cb7a2b8c00fcb535879edbd6b008a01","1dcae5e5e9fc1b4c0315b6e68f19b9bb2d9dba05","026e3363b7f76b51cc711886597a44d5f1fd1de2","2cbc2797efe322f8ac286d90cbc2c4852f0f6bde","c5e63829fe90314e0303713d3df78381d06728d7","c3809b2dbfa5c69e81ba5a31755fc2ad88917795","9d14d1ccda55806e0a093851b52a17b8e5383cfe","068e31286f8cfa5aa1bf0e325006da5e660fc53c","182df40034d937c74df0edb0acb295088e63f1ed","4466dcf725e5383b10a6c5172c860f397e006b48","35aeb647e872091ef70f08da5c18ab60630093cf","5eb4c55740165defacf08329beaae5314d7fbfe6","3751e0f44d9c87bb8f178c86ffffe3489224f5be","58a32115f751f8548c5132491ecad21d76887e7e","7200dce06a410959760aca287c53557830f43fcf","9da965529ee3da77178ed99cf13d97be0fa85f6e","667ff68d1a91da9cb57a0ab79d11a732310e480c","19a5e5b31e741d0f9cacf9c93de782184d24c947"],"id":"fbbc2978758fe459f05d749a0fbd1ef3a3ba3c6c","s2Url":"https://semanticscholar.org/paper/fbbc2978758fe459f05d749a0fbd1ef3a3ba3c6c","authors":[{"name":"David Schubert","ids":["4705741"]},{"name":"Thore Goll","ids":["41017333"]},{"name":"Nikolaus Demmel","ids":["1979699"]},{"name":"Vladyslav C. Usenko","ids":["2025181"]},{"name":"Jörg Stückler","ids":["1683956"]},{"name":"Daniel Cremers","ids":["1695302"]}],"doi":"10.1109/IROS.2018.8593419"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593653","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Here we propose and investigate a novel vibrissal tactile sensor - the Tac Whisker array - based on modifying a 3D-printed optical cutaneous (fingertip) tactile sensor - the TacTip. Two versions are considered: a static Tac Whisker array analogous to immotile tactile vibrissae (e.g. rodent microvib-rissae) and a dynamic Tac Whisker array analogous to motile tactile vibrissae (e.g. rodent macrovibrissae). Performance is assessed on an active object localization task. The whisking motion of the dynamic Tac Whisker leads to millimetre-scale location perception, whereas perception with the static Tac Whisker array is relatively poor when making dabbing contacts. The dynamic sensor output is dominated by a self-generated motion signal, which can be compensated by comparing to a reference signal. Overall, the Tac Whisker arrays give a new class of tactile whiskered robots that benefit from being relatively inexpensive and customizable. Furthermore, the biomimetic basis for the Tac Whiskers fits well with building an embodied model of the rodent sensory system for investigating animal perception.","inCitations":[],"pmid":"","title":"TacWhiskers: Biomimetic Optical Tactile Whiskered Robots","journalPages":"7628-7634","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1810.00460v2.pdf","https://doi.org/10.1109/IROS.2018.8593653","http://arxiv.org/abs/1810.00460"],"entities":[],"journalVolume":"","outCitations":["6ece019066e248de776fb5ff931f7d3e6e0a863e","775e7d850e81e7de76e50577e38a1346003628ec","1fe9ab812ca1195f3821bda50c7cc320e3abf575","31be30d263acf72a7505ebb115cb704905660bd6","a972dc2ff54d9da62f04c37956c4254d5a127955","a9dd79c264674c68815009d2f15937b97f8d273f","b25e9d944cb02a53b1ca0c64a60fabafcc845855","29b6a35b98e97e0c41b2446e0d895f25cc3bb88e","98ac08811d338e9c15c8914ba73c648cd5a707e3","11442e571570779657b096e8081467dc346ec237","333d4fcb869a0097f5b7127bbfc4c4ba959344bc","0ac344425cdcc4a253a1b9b13cfe8a2cdf652def","d98142a2a6785f87880e6a2592622fff6f8f8b52","3e3403ba2d8e7a05c79fbe83fa4b8875ca4ed7be","0cc7f56c2a9bc156c2c3188013a5e755c4adf1f5","02cca7181841f4cdd231648a16d035ccdae58560","ae11bc6ae526756374959383b047764d14a7b3b8","97d7d980d037511240265edd937372baf3360172","4d98355d98119b38fc33cdbc1c713a801ba30623","193a89e51dee31548fe0e242c895829535d4af94"],"id":"b6ee4f95d99b3a6e737edb4d5a1d2b33a065c347","s2Url":"https://semanticscholar.org/paper/b6ee4f95d99b3a6e737edb4d5a1d2b33a065c347","authors":[{"name":"Nathan F. Lepora","ids":["2467565"]},{"name":"Martin J. Pearson","ids":["38088491"]},{"name":"Luke Cramphorn","ids":["3414671"]}],"doi":"10.1109/IROS.2018.8593653"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206371","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We propose a new method for the propagation of semantic labels in RGB-D video of indoor scenes given a set of ground truth keyframes. Manual labeling of all pixels in every frame of a video sequence is labor intensive and costly, yet required for training and testing of semantic segmentation methods. The availability of video enables propagation of labels between the frames for obtaining a large amounts of annotated pixels. While previous methods commonly used optical flow motion cues for label propagation, we present a novel approach using the camera poses and 3D point clouds for propagating the labels in superpixels computed on the unannotated frames of the sequence. The propagation task is formulated as an energy minimization problem in a Conditional Random Field (CRF). We performed experiments on 8 video sequences from SUN3D dataset [1] and showed superior performance to an optical flow based label propagation approach. Furthermore, we demonstrated that the propagated labels can be used to learn better models using data hungry deep convolutional neural network (DCNN) based approaches for the task of semantic segmentation. The approach demonstrates an increase in performance when the ground truth keyframes are combined with the propagated labels during training.","inCitations":["5c6bb6bf7f3f267cc96a0a00752f914331f10a89"],"pmid":"","title":"Label propagation in RGB-D video","journalPages":"4917-4922","s2PdfUrl":"","pdfUrls":["http://cs.gmu.edu/~ggeorgak/IROS17_0458_FI.pdf","https://doi.org/10.1109/IROS.2017.8206371","https://cs.gmu.edu/~mreza/publications/lp_rgbd_video_IROS17.pdf"],"entities":["Artificial neural network","Conditional random field","Convolutional neural network","Energy minimization","Experiment","Ground truth","Key frame","Optical flow","Pixel","Point cloud","Software propagation"],"journalVolume":"","outCitations":["f9f2044f2075a222f6a6779643f363348dc62293","9201bf6f8222c2335913002e13fbac640fc0f4ec","7fe7f66531ee900e57e5711cd7f9866662049e2e","2873ba51690c683218b011fc2e5a6b07f972133c","0690ba31424310a90028533218d0afd25a829c8d","21218e0592a182890143d2b82dda9579668176e9","9b16ff0e8ac87c5c4994e9025216df6e399375e5","56b249a0b998be6c0f3eb939969284a1a5884a1a","014ad0ec0fac206d5a9f02afeda047e177bf6743","069c40a8ca5305c9a0734c1f6134eb19a678f4ab","4d9d25e67ebabbfc0acd63798f1a260cb2c8a9bd","148686aebefff0a7fb3f80024f765ef4b06d2efc","058abdc6d1087d09a7c22fd2e2b6af0800c814de","24c6240c511f4daa7cf51e28b0a9fb15e365d4cc","07f77ad9c58b21588a9c6fa79ca7917dd58cca98","6c4de7cb27be2d44e23d2973c1b25e8f982b4120","2402322ba48ab839017eef2e2bc6f7c1fa381011","7e011eee579f3065edf99d780e18ac9f4f2c5f4a","3cdb1364c3e66443e1c2182474d44b2fb01cd584","03fb94fd63b13b4a591243fb0598b49b6f6d022f"],"id":"26c58afbf3ae9cdff289b18fca2ef3ea3b7d5f5e","s2Url":"https://semanticscholar.org/paper/26c58afbf3ae9cdff289b18fca2ef3ea3b7d5f5e","authors":[{"name":"Md. Alimoor Reza","ids":["3174164"]},{"name":"Hui Zheng","ids":["1689251"]},{"name":"Georgios Georgakis","ids":["40379773"]},{"name":"Jana Kosecka","ids":["1743020"]}],"doi":"10.1109/IROS.2017.8206371"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593777","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The paper proposes a spatial sound perception system for an autonomous mobile robot. The system performs three-dimensional position localization and recognition as online processing from a robot in motion. For online processing, the sound positions are estimated as probabilistic regions in three dimensional space, because the robot could observe only arrival direction of the sound at a moment. The detected sound signals are recognized using Convolutional Neural Network (CNN), for the adjustment to short input signals. The experimental results show our mobile robot could observe surrounding sound sources online and continuously update its position and sound label.","inCitations":[],"pmid":"","title":"Online Spatial Sound Perception Using Microphone Array on Mobile Robot*","journalPages":"2478-2484","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593777"],"entities":[],"journalVolume":"","outCitations":[],"id":"273720e8e2ee63befcef9bf43f492fe127234b83","s2Url":"https://semanticscholar.org/paper/273720e8e2ee63befcef9bf43f492fe127234b83","authors":[{"name":"Yoko Sasaki","ids":[]},{"name":"Ryo Tanabe","ids":[]},{"name":"Hiroshi Takernura","ids":[]}],"doi":"10.1109/IROS.2018.8593777"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00060","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"This paper presents a multi-task cascaded deep neural network which jointly predicts people's affective levels (valence and arousal) and personal factors using EEG signals recorded in response to presentation of affective multimedia content. Studied personal factors are the Big-five personality traits, mood (Positive Affect and Negative Affect Schedules) and social context (individual vs group). The cascaded network consists of two levels of prediction. The first level consists of a hybrid network designed to predict affective levels for individual video segments by combining the capabilities of CNN and RNN units. The second level consists of an RNN unit designed to predict personal factors based on the sequence of affective levels predictions of consecutive video segments. The first level reduces the dimensionality of the input while keeping affective information. The fusion of decisions of RNN and CNN networks produces an increase in performance of 3% and 4% f1-score respectively for valence and arousal recognition. And, our results for personal factors recognition, on average, outperform baseline studies by at least 2:7% mean f1-score.","inCitations":[],"pmid":"","title":"A Multi-Task Cascaded Network for Prediction of Affect, Personality, Mood and Social Context Using EEG Signals","journalPages":"373-380","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00060"],"entities":["Artificial neural network","Baseline (configuration management)","Computer multitasking","Deep learning","Dimensionality reduction","Electroencephalography","F1 score","Random neural network"],"journalVolume":"","outCitations":["0fc334e7f5a4738fdd1e150db0c729851b0399e7","00a403a0fe1e44510cc5096631ca9c585e69fc37","cd8596ab2c55edcc12ca4aa07c29fba826935e7a","d6aa842517e41f382612a875dd282b130a747823","c354510bdf38d6f51bb13528a7057bb431a15904","e70c7a9a8568203921a9a582626cbaf97cc6c2d8","3a60678ad2b862fa7c27b11f04c93c010cc6c430","d660abfbe5f84c1c49f1e7174eb166b8b23e53c4","7c6de0a8dd1409a1743c3f4e6e6b15a682efda0f","bbe9b410d0feaac933f51b1fe8ee99f49a9f021a","f11c76efdc9651db329c8c862652820d61933308","132e3d3b5cfc2f59db6ed69ac1eac4a1ee6dca71","9e1583dd26e0741e2bf74112405e97d948170128","6c8b30f63f265c32e26d999aa1fef5286b8308ad","a4a8c6018fc5ee957896b2b56131057fa3d553e4","966019f0bade0fe6ef96fac913f2b27a6504755b","98fad0d5c3ebdab54732b49cad3d8b3a3a55d9e6","69f1d4da9384876efdd2c54f3bc20328ba078f51","aa474ae1be631696b790ffb19468ce9615899631","e1cef5dbc358e57ea7d68dc49ca6cfe3a5225f6e","7d4edaadef2527667455630676744ab8fc452362","c8523bba348d00b65c516518281a35837509d617","f6874829c43dd5257ad1e8c70b0d407c9d9c1cf9","c5b4ccccaa7f59ea4b6089e7be19dc1ae395275a","ab5bf94e2974b4321c91c6ce0fb5f52dd8808fa8"],"id":"15d761ed20ca0a9c8daa32f5b04153296584c398","s2Url":"https://semanticscholar.org/paper/15d761ed20ca0a9c8daa32f5b04153296584c398","authors":[{"name":"Juan Abdon Miranda Correa","ids":["31414693"]},{"name":"Ioannis Patras","ids":["1744405"]}],"doi":"10.1109/FG.2018.00060"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206133","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Batch-cell manipulation is a key technology in biological applications. Robotic manipulation has important significance to improve the operation success rate and reduce the technical threshold, but the problem of inefficiency still exists in batch-cell experiments. In this paper, an automated cell transportation system is designed for batch-cell manipulation. It has some technical aspects such as a cell groove to contain the cells, the micromanipulator and motor stage control methods, and computer vision algorithms. Since the cells are arranged in a line in the groove, the transportation system improves the efficiency of finding the cells in the petri dish. Furthermore, the minimum pressure to drag and release the cell are analyzed theoretically, so that the other cells will not be affected when manipulating one cell. The visual algorithms to detect the cell position and cell holding state are evaluated by porcine oocyte. Experimental results show both algorithm has high success rates: 96% and 100%. Finally, cell rotating experiments are introduced to verify the effectiveness of the transportation system. The average transfer efficiency has been improved by 20% compared to manual operation. The results show that this system can be used in many manipulations.","inCitations":[],"pmid":"","title":"Automated cell transportation for batch-cell manipulation","journalPages":"2974-2979","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206133"],"entities":["Algorithm","Cell (microprocessor)","Computer vision","Experiment","Line level","PID","Petri net","Robot"],"journalVolume":"","outCitations":["82225509d9e702ec362986eadf262889866f6f38","c2e50fc61870fd2a3b50c8f68b16701581e90de2","156d986b7a83450fd079080c2ac877695dc756eb","f1d883846c9cb2e05f93ddb11571feaca793db69","23b3ab8d8435a9478ce1c3ac927910258ae18d0f","e122409278918f6de2af53910ce7b76d8416ddfb","03370c832851462f322ebf4413ed84cb5490d714","d86613d2a9c9bfc4d3991653128089935ba5c683","d69d4dbf9910f65bed124e90d3f8ea1aed62558c","bef50959534e7ff9cf04125f4bf4e9c8f0841083","6903c8b7b1e3f2d8abe9eb903eb7faaf56aa5197","f90b3508f31f03c2950d73249973105c0d9495b8","07d5ae69fed920256ed03e945c55eee21c29b152","1bd9be549a309deb84b274361253e1705930affe","4431b256b8a1fce0280190e8c110752314a37d2b","93d306caa1020faec2001ddbd84bccb6027f89be","807f74523cade87e37860dfde524b6cd4f6c691a","5912326d6d9ec5f09200041210a325e1563cff27"],"id":"f6276b1e59ee0b1015801d3a6d7969db2793965c","s2Url":"https://semanticscholar.org/paper/f6276b1e59ee0b1015801d3a6d7969db2793965c","authors":[{"name":"Xuefeng Wang","ids":["26339524"]},{"name":"Yaowei Liu","ids":["1898459"]},{"name":"Shibao Li","ids":["2557155"]},{"name":"Maosheng Cui","ids":["33162052"]},{"name":"Mingzhu Sun","ids":["1750604"]},{"name":"Xin Zhao","ids":["1708899"]}],"doi":"10.1109/IROS.2017.8206133"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206026","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"An adaptive trajectory tracking problem for a spherical rolling robot driven by a 2DOF pendulum is considered in this paper. A feedback controller is proposed for the goal of tracking the trajectory for the full configuration of the spherical robot. To deal with time-varying uncertainty of the system dynamics, an adaptation mechanism is included in the trajectory tracking controller by parameterizing the system uncertainty with a polynomial function, weighted by unknown constant parameters adjustable by the defined update law. The constructed controller is first tested for the planar hoop-pendulum and then applied to the ball-pendulum system. The convergence to the desired trajectories under the control law is proved and then verified by simulations for tracking circular motions under perturbation.","inCitations":[],"pmid":"","title":"Adaptive trajectory tracking control for the ball-pendulum system with time-varying uncertainties","journalPages":"2083-2090","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206026"],"entities":["Algorithm","Algorithmic efficiency","Basis function","Computation","Control theory","Double pendulum","File Allocation Table","Hoops 3D Graphics System","Optimal control","Polynomial basis","Robot","Simulation","System dynamics","Testbed","Underactuation"],"journalVolume":"","outCitations":["fc88e51fac88fe4ca88cc9f2a4729d00f0cd3267","ca442cc2eef5d9bd5cd5cb4c516cd15991569d93","d4c60ec24db7b52d1ff61771f262c155e5dea0e8","9cee23863e9d6edb3c85549bc6d816cad596fdad","c3656059102441d43218a5797aba22e9fdaba222","4a7bb96cbb772ec7ff09b5d9e94b14c3f58c9fdf","88bdd16fb6cb43f4105d578493c9d7bdecf6ac34","fe8027c45857363eabfc987bd559eda7c63280f8","79e9ff5c237d546bb08e1c1223b5767a8b2b9d85","cebde9cee57f1d05086d0061c7f63fe3115b872d","53c8300369d9dc26316495c44cab36e29c862f99","9831c0ac5ae1a4313d9ca3f95756c08048ddbd6c","e4597419bed3bace1d15cbfa50f60d2670ccc1df","c0d256c841c3c58c2b2dee26511013bdaaf951fa","e74882f21c265e7e8b81f3b11670796fc3cf21ac","db48529390b349c62d2a510a1575a96089beff3e"],"id":"8c69ae1d801cda06b71a75c62afb8d856d7745fa","s2Url":"https://semanticscholar.org/paper/8c69ae1d801cda06b71a75c62afb8d856d7745fa","authors":[{"name":"Yang Bai","ids":["48442689"]},{"name":"Mikhail M. Svinin","ids":["1776707"]},{"name":"Motoji Yamamoto","ids":["35107638"]}],"doi":"10.1109/IROS.2017.8206026"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545590","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Age estimation is an important task in computer vision and is widely used in applications. However, such a technology is largely affected by the resolution of face, and it would be a challenge if one has to estimate the age of a person at a distance. While body image of a person is often captured more clearly, when and how to use body-based visual cues for age estimation are largely under studied. In this work, we argue that body-based visual cues are better for estimating the age group and can assist the estimation of exact age value. For this purpose, we develop a Body-based Age Net (BAN) that unifies selective local convolution features and contextual convolution features. The network is designed based on two assumptions: 1) a person's wearing is closely related to his/her age group property; 2) some selective local parts of a body are more discriminative for age group estimation. We have contributed a large-scale and publicly available Body Age (BAG) dataset. We have quantitatively evaluated the proposed model on BAG.","inCitations":[],"pmid":"","title":"Does A Body Image Tell Age?","journalPages":"2142-2147","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545590"],"entities":[],"journalVolume":"","outCitations":["a53d13b9110cddb2a5f38b9d7ed69d328e3c6db9","ec6cef66d5548514c2df6dc5d365793deb8958d5","10195a163ab6348eef37213a46f60a3d87f289c5","2f2406551c693d616a840719ae1e6ea448e2f5d3","341ed69a6e5d7a89ff897c72c1456f50cfb23c96","9201bf6f8222c2335913002e13fbac640fc0f4ec","2cbb4a2f8fd2ddac86f8804fd7ffacd830a66b58","3d94f81cf4c3a7307e1a976dc6cb7bf38068a381","14318685b5959b51d0f1e3db34643eb2855dc6d9","614a7c42aae8946c7ad4c36b53290860f6256441","c7f242483d68b8deee34752de7add6e29c4f6b08","3b2697d76f035304bfeb57f6a682224c87645065","3cb488a3b71f221a8616716a1fc2b951dd0de549","0c741fa0966ba3ee4fc326e919bf2f9456d0cd74","324608bf8fecc064bc491da21291465ab42fa6b6","2b632f090c09435d089ff76220fd31fd314838ae","5b9d9f5a59c48bc8dd409a1bd5abf1d642463d65","8039c28bb693b872c732bad66eab34663df88f35","8355d095d3534ef511a9af68a3b2893339e3f96b","68c4a1d438ea1c6dfba92e3aee08d48f8e7f7090"],"id":"a0cce54ef20ad75dee4769c2ecaaac690d0eb21d","s2Url":"https://semanticscholar.org/paper/a0cce54ef20ad75dee4769c2ecaaac690d0eb21d","authors":[{"name":"Baoyu Yuan","ids":["35217093"]},{"name":"Ancong Wu","ids":["2084013"]},{"name":"Wei-Shi Zheng","ids":["3333315"]}],"doi":"10.1109/ICPR.2018.8545590"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206161","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper considers the problem of active object recognition using touch only. The focus is on adaptively selecting a sequence of wrist poses that achieves accurate recognition by enclosure grasps. It seeks to minimize the number of touches and maximize recognition confidence. The actions are formulated as wrist poses relative to each other, making the algorithm independent of absolute workspace coordinates. The optimal sequence is approximated by Monte Carlo tree search. We demonstrate results in a physics engine and on a real robot. In the physics engine, most object instances were recognized in at most 16 grasps. On a real robot, our method recognized objects in 2\u20139 grasps and outperformed a greedy baseline.","inCitations":[],"pmid":"","title":"Active end-effector pose selection for tactile object recognition through Monte Carlo tree search","journalPages":"3258-3265","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1703.00095v3.pdf","https://natanaso.github.io/ref/Zhang_ActiveTouch_IROS17.pdf","http://www.seas.upenn.edu/~zmen/MabelZhang_iros2017.pdf","https://doi.org/10.1109/IROS.2017.8206161","http://www.seas.upenn.edu/~zmen/MabelZhang_iros2017_pres.pdf","http://www.cis.upenn.edu/~kostas/mypub.dir/mabel17iros.pdf"],"entities":["Active object","Approximation algorithm","Baseline (configuration management)","Greedy algorithm","Instance (computer science)","Monte Carlo method","Monte Carlo tree search","Outline of object recognition","Physics engine","Robot end effector","Workspace"],"journalVolume":"","outCitations":["a7110449b8eab8db29336d98410ba481e1e2bbcf","979ac902daa976ab6ce691b73180bc4413281a89","699bafd19e1d549d121642b0ff839ee4ece1f399","a03710c3c6545b0f3d4399b68664f9ea7cdb349b","a8fe64d5ef3283bd60ffc5300304f24c775fae64","10f45b1768a23883d24e4c8f624369484172aa21","db867a3c0da9933dc49504ddffc28ed490bef4a8","16cd14828b3fabe5050a63ff9a616c7c6347a819","937e0b9d7e18a4e80ece5abc4c1d45370049932a","93d949bf9fe90d3495f491a0d970713be8a94fc4","0353c95b7e7cac147f915ff11eccbf87252b3849","fad57c71d7231507da98b5f7f160cace9946307e","03f2e44e0ed866f7714bb0741d8450487cd1d9cd","2bdbec74d70371b09c830706fb4b1f4e61874766","c4a529baa6e7540de4efe5426e1877ab49b68c52","03613bbadde699e9e47f0a9cd2da6c8ef8dd44cf","5e7e0a8d666f724003537ffb093e857934a4a436","3fe099d6ec49529017f0949583b68a6f6bebfd66","6f97847847bb4dd7f2194dd4add14e2a30904624","20df8df52b0dc0ec3c3db1d9c8c5a554a7571cc8","67f9923f060e2a0e147c48ec9ae8ddf6999d58cb","12d1d070a53d4084d88a77b8b143bad51c40c38f","2a83c466c5d57b711258dccbc0ac12c14171e1f0","37661e7aa554cffb41d6c96c5c0b9339a07dd22e","221e269474bd0dc08f0a3f3d652b75a04d1ddfee","1120862afe84a80db85e1f997d1b7e7255d73b31","614cef67ef201f3cd0a78ee058ef8e58d16fe8f2","c93b8813f2a0eb31703be2c621b2ce18dc226aaa","4a86d435ea4d5b779ba75c88368f5177ba3926ec","1c9da6cef6b1be9c116b26dd52c341c0adcf7db2","03fb8b2c510a53a806492dc754285d4e0140dc65","278ee1afabacf2a153da267ddac20abfb01a3232","11edac6632bf3feb18d7fc0dedca93ea1b990bf7","6661e57237e4e8739b7a4946c4d3d4875376c068","96cfc6038130e35b1dd8571b374d70a6424ca320"],"id":"74c958a8d1a633405dd78f02b3cd427c5b14b77c","s2Url":"https://semanticscholar.org/paper/74c958a8d1a633405dd78f02b3cd427c5b14b77c","authors":[{"name":"Mabel M. Zhang","ids":["2106891"]},{"name":"Nikolay Atanasov","ids":["50365495"]},{"name":"Kostas Daniilidis","ids":["1751586"]}],"doi":"10.1109/IROS.2017.8206161"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593764","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Nascent endoscopic therapeutic procedures, such as endoscopic submucosal dissection, enable unparalleled access to and removal of mid-size cancerous neoplasia from within the gastrointestinal tract. However, the remote locations of these lesions often incur substantial distal dexterity which imparts appreciable cognitive loading on the clinician and opens up the possibility of adverse events such as intestinal perforation due to limited dexterity and a lack of sensory feedback. In this work, we introduce a mm-scale, tip-mounted robotic system, EndoMODRA (Endoscopic Module for On-Demand Robotic Assistance), which interfaces with commercially-available endoscopic tools and provides additional dexterity and feedback sensing using on-board actuators and sensors, decoupling tool motion from endoscope motion. Leveraging alternative high-energy-density actuation strategies and monolithic, printed-circuit-inspired manufacturing processes, all actuation and sensing is fully contained within the distally-mounted module, obviating the need for a continuous mechanical transmission to a proximal motor package. We develop a fuzzy-tuned PID/PWM controller for closing the loop distally to enable closed-loop position-controlled trajectory execution using onboard actuation and sensing, realizing fully -distal loop closure in an endoscope-mounted robotic module with no proximal actuation or sensing component. Controller performance is validated on a fully-integrated module with on-board sensing, demonstrating the ability to execute pre-determined trajectories as well as real-time rate-based teleoperation.","inCitations":[],"pmid":"","title":"Fuzzy-Based Feedback Control of a Tip-Mounted Module for Robot-Assisted Endoscopy*This material based on work supported by Defense Advanced Research Projects Agency (DARPA), A2P (Grant No. FA8650-15-C-7548). This work was also partially funded by the Wyss Institute for Biologically Inspired Engineer","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593764"],"entities":[],"journalVolume":"","outCitations":[],"id":"acccd9af32e61894fa2dc31ebf98e656e4c925a7","s2Url":"https://semanticscholar.org/paper/acccd9af32e61894fa2dc31ebf98e656e4c925a7","authors":[{"name":"Joshua B. Gafford","ids":[]},{"name":"Hiroyuki Aihara","ids":[]},{"name":"Christopher Thompson","ids":[]},{"name":"Conor J. Walsh","ids":[]},{"name":"Robert J. Wood","ids":[]}],"doi":"10.1109/IROS.2018.8593764"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593643","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a novel robocentric formulation of visual-inertial navigation systems (VINS)within a multi-state constraint Kalman filter (MSCKF)framework and develop an efficient, lightweight, robocentric visual-inertial odometry (R-VIO)algorithm for consistent localization in challenging environments using only monocular vision. The key idea of the proposed approach is to deliberately reformulate the 3D VINS with respect to a moving local frame (i.e., robocentric), rather than a fixed global frame of reference as in the standard world-centric VINS, and instead utilize high-accuracy relative motion estimates for global pose update. As an immediate advantage of using this robocentric formulation, the proposed R-VIO can start from an arbitrary pose, without the need to align its orientation with the global gravity vector. More importantly, we analytically show that the proposed robocentric EKF-based VINS does not undergo the observability mismatch issue as in the standard world-centric frameworks which was identified as the main cause of inconsistency of estimation. The proposed R-VIO is extensively tested through both Monte Carlo simulations and real-world experiments using different sensor platforms in different environments and shown to achieve competitive performance with the state-of-the-art VINS algorithms in terms of consistency, accuracy and efficiency.","inCitations":[],"pmid":"","title":"Robocentric Visual-Inertial Odometry","journalPages":"6319-6326","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1805.04031v1.pdf","http://export.arxiv.org/pdf/1805.04031","https://doi.org/10.1109/IROS.2018.8593643","http://arxiv.org/abs/1805.04031"],"entities":[],"journalVolume":"","outCitations":["2c8e95bc331024105cbde6f6918cda8493f263c8","2d5678dd885b4d7fd8ba84573f685eecc663f956","d6aa4f77dd5eb3dd3bc203014c3a16ec4c569df6","56a4a8a7937f0dd2d38e4d2903f0baf4fb189102","12b9b781d200d69bf6df4fd9a80a07958f98a43c","194e1eea20451d02eac96b74e61b5d780b59b3c9","6f5c364b0216175c2e09113684792b3e2c95a651","2295fda13d5118808557d04310b0e176a1341063","1b957ef775e3f319f4457322478d46cfa7241584","2cbc2797efe322f8ac286d90cbc2c4852f0f6bde","f681f56aa31a38ad9ec1241e9f5af1629ea7c43c","3f803ccf2d038cb9930006ed598dab327806528c","0be0c13803cd08e81b7adaada537e91222eb1491","3a9ccbf83c143114e32f39511c858f053cfc94d6","6f7892e29b76ba107ca3e6a923ce56dd3a7cd580","a13dc9739e4637599359d792fd60d511ab8a016e","a6374ee834201b2c1ac3cf8c0097f503def431df","61ccc9c1aed93193d97810d69d68667fb1e2c795","53226f929c8affbf49c1bdb4cbe5e8ff1b05e0d4","18dfee622303def1e721c483395f9844e7c1d9f1","d591682c056fe9d082a180c14324ee46e5a618f0","1f2ee7714c3018c1687f9b17f2bc889c5c017439","4b19be501b279b7d80d94b2d9d986bf4f8ab4ede","25e26d4db49e280669e26cd05305d27f35a0791a","4a0fd215a2aff831a278cc4af1212c2898f1738d","0a202f1dfc6991a6a204eaa5e6b46d6223a4d98a","6269c027bb8707c292ef5364bb6c6e983c68da63","190b8feb7ed7bb7bba430a23c459b1cb9d27ad9b","7200dce06a410959760aca287c53557830f43fcf","281c6a02097e94c88d67078b2c9a4a6da29e1872","2e7975fb0351638bd2646a217e5885ae56ca5cff","763d7bcca50943f58ab3967368271ba4a2a7a878","335f0596a696e52c2746ece0a49f2a89bd5a4d77","19a5e5b31e741d0f9cacf9c93de782184d24c947","299d554e837e9a30ae3d6d6f2268ed41f5d6facf","6cfffb91f23b6744b003ec629f87199dc42f6383"],"id":"5d62561b21a1796713cba47cac99fa7ca40f5029","s2Url":"https://semanticscholar.org/paper/5d62561b21a1796713cba47cac99fa7ca40f5029","authors":[{"name":"Zheng Huai","ids":["31192903"]},{"name":"Guoquan Huang","ids":["39680655"]}],"doi":"10.1109/IROS.2018.8593643"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546025","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a structured image inpainting method employing an energy based model. In order to learn structural relationship between patterns observed in images and missing regions of the images, we employ an energy-based structured prediction method. The structural relationship is learned by minimizing an energy function which is defined by a simple convolutional neural network. The experimental results on various benchmark datasets show that our proposed method significantly outperforms the state-of-the-art methods which use Generative Adversarial Networks (GANs). We obtained 497.35 mean squared error (MSE) on the Olivetti face dataset compared to 833.0 MSE provided by the state-of-the-art method. Moreover, we obtained 28.4 dB peak signal to noise ratio (PSNR) on the SVHN dataset and 23.53 dB on the CelebA dataset, compared to 22.3 dB and 21.3 dB, provided by the state-of-the-art methods, respectively. The code is publicly available.11https:llgithub.com/cvlab-tohoku/DSEBImageInpainting.","inCitations":[],"pmid":"","title":"Deep Structured Energy-Based Image Inpainting","journalPages":"423-428","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1801.07939v2.pdf","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546025","http://arxiv.org/abs/1801.07939"],"entities":[],"journalVolume":"","outCitations":["6e3f79b017ce4ac2be9cff75bc7489708b7b62e1","7fc604e1a3e45cd2d2742f96d62741930a363efa","3694f2c87d22007f6aa619880155f2291140f0af","10d39702b2b969ff8ba8645988910954864ccbec","4b18885cd459d5b1b038374d03111ed6a70adcee","55206f0b5f57ce17358999145506cd01e570358c","6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","3fc79dfbc36c9b64ee82dc897d6cf5442d652c5c","7e383307edacb0bb53e57772fdc1ffa2825eba91","26d4ab9b60b91bb610202b58fa1766951fedb9e9","69b647afe6526256a93033eac14ce470204e7bae","272216c1f097706721096669d85b2843c23fa77d","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","2c03df8b48bf3fa39054345bafabfeff15bfd11d","42f422a9a67ba71a9ac699205940d8cc2dca8317","446fbff6a2a7c9989b0a0465f960e236d9a5e886","58ea3abc2a4e08461e0683405c1ae72ffaef77dd","8a3bf4d403a39ed33f0fa8cf78dc906d6130595f","501d99e392783e4acafb220136d32ea68a921282","c52f2a00fdbfb7fb10252796dbede6403e780da6","5317e47cb32876c3136b7205cccecc98afff6f99","4512037737ab217cee118a07c5fdb22821eaf3fb","162d958ff885f1462aeda91cd72582323fd6a1f4","d21ebaab3f715dc7178966ff146711882e6a6fee","02227c94dd41fe0b439e050d377b0beb5d427cda","35756f711a97166df11202ebe46820a36704ae77"],"id":"622107e35327a8759b967ae6a6c654bdab738f91","s2Url":"https://semanticscholar.org/paper/622107e35327a8759b967ae6a6c654bdab738f91","authors":[{"name":"Fazil Altinel","ids":["35523920"]},{"name":"Mete Ozay","ids":["2159942"]},{"name":"Takayuki Okatani","ids":["1718872"]}],"doi":"10.1109/ICPR.2018.8546025"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206474","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a method to achieve online gait adaptation of a dynamically walking biped when collaborating with an external agent \u2014 either a human or a robot \u2014 acting as a leader. Adaptation occurs without any explicit information on the leader's intended motion; only implicit information is used through the interaction force developed between the leader and the biped. An adaptive supervisory control scheme is proposed and guarantees for boundedness of the state despite switching under external force are provided. The supervisory controller leverages the availability of a library of exponentially stable limit-cycle gaits, and orchestrates switching among them in an online fashion to achieve adaptation. As a result, the range of leader speeds that the biped can adapt to is drastically enlarged while the leader's effort is reduced.","inCitations":["35bcad67f0a7fa4c8314c5be2aa4d955d057e885","876166a5b2926ab708287ce822271fe6f294e2c2","660fb1b250589f2686a373b179380f79124d5ce8","0888183253e72e00936d18b539a7151eee0ac085","c223ef9f915941f8c4ad4af8ea6776131e666d60"],"pmid":"","title":"Adaptation of limit-cycle walkers for collaborative tasks: A supervisory switching control approach","journalPages":"5840-5845","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206474"],"entities":["Adaptive switching","Interaction","Limit cycle","Mathematical induction","Precomputation","Robot"],"journalVolume":"","outCitations":["921525b5ed9993541e53fb40ad4c833097ff1ee1","c2fe4c41b04f67fe1ef662baf669f7ff3e265a6f","602a2ffaf4ff247e6d0bc4e05b223b10b203d79e","2845211725fce10387756116a443236534ea26da","1c30858379114b4ff4c6785fc75ad961037a9106","214ebf5df64bbda2945c7d85ffd6875dba6f4f3b","35143167f5e0c6c7cd2279828cd26f19f981b269","e52c92965d65058045652ce64477ebec07d63d95","b3790829cbb0bda4195122bf3fcd2863d1d38044","8dc904a75fe59a04b7b146a984abeddcafb398da","760af5c9d900b6d5c6a358a814f6848ee54605b3","36256596180fe57c345210057bc360a7024bfc9d","6c267aafaceb9de1f1af22f6f566ba1d897054e4","971e9fbbbb9c16254fa96bc5a8bb69e8c98036f7","b344bb74e0844c2d47272f7d69616df4870bc5ca","0042692299644177cb591f72df68550b3824dbb9","9ddc7b1753f39b480eccc1f0d6dbc9b8eb6ee9d7","4d1f274b25085e692720d7c5ad2aed319283a86b","f84cbfd45a5d68888bd17311b02adb4a58f83ff9"],"id":"d7acbea1fa262d846ff5779bf50088a93302d52d","s2Url":"https://semanticscholar.org/paper/d7acbea1fa262d846ff5779bf50088a93302d52d","authors":[{"name":"Sushant Veer","ids":["2083048"]},{"name":"Mohamad Shafiee Motahar","ids":["2457902"]},{"name":"Ioannis Poulakakis","ids":["1698307"]}],"doi":"10.1109/IROS.2017.8206474"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205946","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Feedback controller for robotic bipedal walking models can be multi-layered, consisting of low-level continuous-time controller and high-level event-based controller. Stimulated by the success in our preceding study that demonstrates the validity of the l∞-induced norm as an adequate performance measure, we suggest a systematic methodology to design optimal event-based feedback controller for bipedal models walking on rough terrains. More precisely, we first assume that the system is already equipped with a low-level continuous-time feedback controller, capable of stable flat-ground-walking, and then formulate the design problem of the high-level event-based feedback control as the l1-optimal control problem for discrete-time linear systems defined on the linearized Poincarè map. In order to validate the proposed methodology, nonlinear dynamic simulations are conducted with a simple biped model walking on rough terrain. The terrain slope randomly varies at each footstep while the magnitude of slope variation is bounded by some maximum value. Simulation results indicate that the optimal system equipped with the proposed controller can successfully overcome a rough terrain on which the original system could not walk and fall.","inCitations":[],"pmid":"","title":"A method for robust robotic bipedal walking on rough terrain: L1-optimal event-based feedback controller","journalPages":"1443-1448","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205946"],"entities":["Approximation error","Feedback","High- and low-level","Linear system","Nonlinear system","Numerical method","Randomness","Robot","Simulation"],"journalVolume":"","outCitations":["35143167f5e0c6c7cd2279828cd26f19f981b269","3d5a60f73c31f006e5a964e4a90ff0ddf8675ca8","3443eb396e4a15e269ca6a7a7fab215f6b62cebb","54ef0d07abd9b0f39c1f39e58c05c5e0def4355b","199fc8aa963d6373fe4f5936d1727630021f157d","eab10ee2f134d1c9b807c22a3e8f1eacbcddf83c","a77150dd62b1d85902adefee0e59e040d0a1f6e4","b3d0182db0637a2b795b12629caa3f57af3bb15c","07afa5b65469f48b9e36c87ec02a8412d2effe24","7d0bfb58d6f8efccc128889c2ffbfffa3f217a78","52de1849579e0a8a03f977157a05d8d90894bf2f","2f31e2a92999f2ff66e94f3f4eab21ae859c9a5d","b3790829cbb0bda4195122bf3fcd2863d1d38044","6dfcad1998910643364523be7fd330eac239dadb","906c38679482b06ff0ed4efd025b59230c35ab7d","02f3972ed9de9bfa06a64e304e14189147012247","fac8ca897a84198518c5551c8969f2e0755129e7","55fac7a4de0d4b62066892b93c31c2276ecc4dcd","2a81c733e3a8273dc75d90f55657c967d053bc32","2de43ef84d9b532b50745709027bc93e17e41868","82e9df8c0c7d2c8e10ebe155f97d0571872a53d9","f1417ca4fdbe45db729193af924174f8e9acc4f6","b2527e8f02ad211be906dfe55741730570121bd6","73b0f858f7faa1692b46dbeee2575c0a3cae467f","1c1f270a477718c3d176edc0e759416def9862dc","ed362d1f470b122d45306ca62dc389f48e24598b"],"id":"d4d3fb8a34cdb9a8cc823580b1455cc45367aa1c","s2Url":"https://semanticscholar.org/paper/d4d3fb8a34cdb9a8cc823580b1455cc45367aa1c","authors":[{"name":"Jongwoo Lee","ids":["9174305"]},{"name":"Jung Hoon Kim","ids":["49476628"]},{"name":"Yonghwan Oh","ids":["1752136"]}],"doi":"10.1109/IROS.2017.8205946"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00097","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a cooperative model combined the multi-task reverse sparse representation model (MTRSR) and the AdaBoost classifier, which were used to cope with the disturbing of target gradient information caused by motion blur or target serious occlusion, and a descriptive dictionary were used to estimate the weights of each candidates. First, we use the MTRSR model to get the blur kernel which were used to get the blur target template set, meanwhile the confidence of the candidates is also obtained by the reconstruction error. Then we use the HOG features of the target templates to get the descriptive dictionary to calculate the weights of the candidates, and a AdaBoost classifier is used to calculate the confidences of all candidates. Finally, the best target is retrieved by the sum of production of weight value and the two confidences. The experimental data show that the proposed algorithm can fully cope with the target's information change which were caused by motion blur and target occlusion in the complex scene, and our algorithm can further improve the accuracy and robustness in visual tracking.","inCitations":[],"pmid":"","title":"Visual Tracking Based on Cooperative Model","journalPages":"614-620","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00097"],"entities":["AdaBoost","Algorithm","Computer multitasking","Dictionary","Gaussian blur","Gradient descent","Interference (communication)","Naive Bayes classifier","Sparse approximation","Sparse matrix","Statistical classification","Tree accumulation","Video tracking"],"journalVolume":"","outCitations":["c0b7d14afd4c7fb231bd95d5644edcc87647e1ef","c828fcec67581126230ab1be9607d6d045e586d5","1812c98ad2a9aed7ef3e662449297d9b8807a3d9","9cf3c67529085d31c646091b97be1a1e3dc191f2","7dfc24b8e21fa7a397d5557574efedd218864d10","552a06c09c49a91956e0bb3a69d7aae688dbcfd0","de9ba54f6e5099350cbbae5104183b7cf153dcc4","40b8458788dfa3ea1d8f62e8a5aafc1e429daf46","6d7671593b2564aa4e35fe13f1e437dd12821026","5b7de103a0beea8b5a0fb9384a5c2172867fcbea","0c972ce7e7969a4e351056bac9ba75478b7fddcd"],"id":"20af30d0ced10e56c904b0ba2df6330b93b9db45","s2Url":"https://semanticscholar.org/paper/20af30d0ced10e56c904b0ba2df6330b93b9db45","authors":[{"name":"Bobin Zhang","ids":["27339086"]},{"name":"Weidong Fang","ids":["2038426"]},{"name":"Wei Chen","ids":["47482457"]},{"name":"Fangming Bi","ids":["38819886"]},{"name":"Chaogang Tang","ids":["1678942"]},{"name":"Xiaohua Huang","ids":["47932625"]}],"doi":"10.1109/FG.2018.00097"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594177","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Many children on autism spectrum have atypical behavioral expressions of engagement compared to their neu-rotypical peers. In this paper, we investigate the performance of deep learning models in the task of automated engagement estimation from face images of children with autism. Specifically, we use the video data of 30 children with different cultural backgrounds (Asia vs. Europe) recorded during a single session of a robot-assisted autism therapy. We perform a thorough evaluation of the proposed deep architectures for the target task, including within- and across-culture evaluations, as well as when using the child-independent and child-dependent settings. We also introduce a novel deep learning model, named CultureNet, which efficiently leverages the multi-cultural data when performing the adaptation of the proposed deep architecture to the target culture and child. We show that due to the highly heterogeneous nature of the image data of children with autism, the child-independent models lead to overall poor estimation of target engagement levels. On the other hand, when a small amount of data of target children is used to enhance the model learning, the estimation performance on the held-out data from those children increases significantly. This is the first time that the effects of individual and cultural differences in children with autism have empirically been studied in the context of deep learning performed directly from face images.","inCitations":[],"pmid":"","title":"CultureNet: A Deep Learning Approach for Engagement Intensity Estimation from Face Images of Children with Autism","journalPages":"339-346","s2PdfUrl":"","pdfUrls":["https://dam-prod.media.mit.edu/x/2018/08/04/iros2018_RudovicEtAl.pdf","https://doi.org/10.1109/IROS.2018.8594177"],"entities":[],"journalVolume":"","outCitations":["b8d47b0c0b12dd77543e82e6bf6636ddd335cfea","ac11dda8332a01b421c5f25ee7314a6f85049e31","9ea0f6ecee81e0013b67b2e27649b65c453cd355","dc7fccf29eea743c13fe319f861578e3b9962923","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","0b2a6a9599640b97eeff40fea24fcc6a53fcbd06","4ff4f6f462fb9e982ead3db857df5149aed8ba96","a365245525cbb4a8adc396ab787d8c0d2c98f746","c2d439dd891553659e1805ea8d98be5a9be7732a","464e8ac9026540e3f9611c762e0b2a483d11e88a","4954fa180728932959997a4768411ff9136aac81","758d7e1be64cc668c59ef33ba8882c8597406e53","9288c1f24d0479946275ff55ea84e38231feb421","da2a98aa6cf3120d393fd71932de4df5c9206042","15cf63f8d44179423b4100531db4bb84245aa6f1","9dd569a8fb5432e8a1c8f117d85594f4bee35ee0","4357c4be7b67e0d19bd93885eab5a1f55fecaeb4","7dec92e95cca6bddc4d1d7233509cc687f1ad64e","328def172844548b9bdb53a537ee8d9fd449463c","a21ff5f5a9b1aa2d02b8f6afaa5a5c92163eba01","7c2964512e2a10eb8a91528f2fa4d7148ffc88a3","e9b43cd29c4761223273cfec6ee9d475a167aa69","18c39ba04333d31c6cb10faf79d1f18692c38d0f","90a34722ea6af475271b344004dba7ed059246b4","5cc637eff1c2dc15a5208ce238f26f7b819c8a29","5e35aa827e081c1bd7216fbf99e7bd1257601132","24f88a22662538c2eaeee6568a8b1cd771bb04dd","2c03df8b48bf3fa39054345bafabfeff15bfd11d","9fd20dad7d69b20e5025fa20c5036b0af96aa45c","469f803a65c8ee26af061d7e2a4fb2b58af14f24"],"id":"a7dfe697959ae8b15eb795d4692a52c2a5856d88","s2Url":"https://semanticscholar.org/paper/a7dfe697959ae8b15eb795d4692a52c2a5856d88","authors":[{"name":"Yuria Utsumi","ids":["35874557"]},{"name":"Jaeryoung Lee","ids":["1756029"]},{"name":"J. Hernández","ids":["50474992"]},{"name":"Eduardo Castelló Ferrer","ids":["10721219"]},{"name":"Björn W. Schuller","ids":["9460079"]},{"name":"Rosalind W. Picard","ids":["1719389"]}],"doi":"10.1109/IROS.2018.8594177"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206140","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Differential drive robots, such as robotic vacuums, often have at least two motion primitives: the ability to travel forward in straight lines, and rotate in place upon encountering a boundary. They are often equipped with simple sensors such as contact sensors or range finders, which allow them to measure and control their heading angle with respect to environment boundaries. We aim to find minimal control schemes for creating stable, periodic \u201cpatrolling\u201d dynamics for robots that drive in straight lines and \u201cbounce\u201d off boundaries at controllable angles. As a first step toward analyzing high-level mobile robot dynamics in more general environments, we analyze the location and stability of periodic orbits in regular polygons. The contributions of this paper are: 1) proving the existence of periodic trajectories in n-sided regular polygons and showing the range of bounce angles that will produce such trajectories; 2) an analysis of their stability and robustness to modeling errors; and 3) a closed form solution for the points where the robot collides with the environment boundary while patrolling. We present simulations confirming our theoretical results.","inCitations":["56d85eb036b8f6b77927b888480d1c84bc7aa0ee"],"pmid":"","title":"Periodic trajectories of mobile robots","journalPages":"3020-3026","s2PdfUrl":"","pdfUrls":["http://msl.cs.uiuc.edu/~lavalle/papers/NilBecLav17.pdf","https://doi.org/10.1109/IROS.2017.8206140"],"entities":["Course (navigation)","Fixed point (mathematics)","High- and low-level","Internationalization and localization","Limit cycle","Mobile robot","Robustness (computer science)","Sensor","Simulation","XFP transceiver"],"journalVolume":"","outCitations":["3303b29b10ce7cd76c799ad0c796521751347f9f","1f4db5b7e9ec8275437f6d47fa17d33ae3c987ef","d2426026eaaf382e17a33af0d81ad1d98aed374e","fd3b6354a04adb9526f93452088b9964489f330c","11252232a9ef2f674c726bba22b5ef4de24df702","30f67b7275cec21a94be945dfe4beff08c7e004a","35a6bfa74a157d7a48b1b333025cadd2bb1f2801","0348cc294883adf24b48c00f50df9870983317d2","242b71aaa26d696e8cf6c7ecda30a91df504ddb4","c1fcad9f829205aff618e13ed949f99c89046496","5f3109a8c954f5843388e54cdeaa4d69ce36acff","aaee9d46a4e7190af4e9dff20e810c830bfc31ab","57f397b8d1380f77cb3cd377bc85cd06a24e6d0e","b2ff901f6d1b1dad5bb441fb23dcb3b148ef1726","5c467f1765dd18cd96513b0f899299285d1062a1"],"id":"1ca5bab528421cd60e4cf4fc6d7c211de3c3e293","s2Url":"https://semanticscholar.org/paper/1ca5bab528421cd60e4cf4fc6d7c211de3c3e293","authors":[{"name":"Alexandra Q. Nilles","ids":["31962876"]},{"name":"Israel Becerra","ids":["2919580"]},{"name":"Steven M. LaValle","ids":["1683060"]}],"doi":"10.1109/IROS.2017.8206140"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594141","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"People engage in social coordination without explicitly communicating when they are conflicting over spatial resources, e.g., a shop clerk who yields to customers the best place to view products. In this study, we proposed a method that achieves such social coordination with a robot. Our idea is that the social coordination between two agents can be represented as utility-maximizing behavior for joint utility rather than just by a single agent utility. That is, given that each agent's reasonable behavior can be represented as utility-maximizing behavior for single agent utility, we model each agent's plans for himself as well as for the partner agent. Moreover, superiority relationships exist in this joint-utility computation. Since each agent knows such superiority relationships, social coordination can be modeled as utility-yielding behavior based on informed superiority. We specifically focus on looking-together situations for which we developed a utility model. With simulations, we investigate whether the above joint-utility-based modeling successfully reproduces social coordination in looking-together situations. We conducted an experiment in a situation where a tele-operated robot and a customer together look at products in a shop environment. Our experimental results show that our proposed method enables the robot to socially coordinate spatial resources, yielding significantly more thoughtful, less-self-centered, and appropriate impressions than the alternate robot.","inCitations":[],"pmid":"","title":"Social Coordination for Looking-Together Situations","journalPages":"834-841","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594141"],"entities":[],"journalVolume":"","outCitations":[],"id":"b673160791edb56950ce653917a8e7c23f41491a","s2Url":"https://semanticscholar.org/paper/b673160791edb56950ce653917a8e7c23f41491a","authors":[{"name":"Shohei Akita","ids":[]},{"name":"Satoru Satake","ids":[]},{"name":"Masahiro Shiomi","ids":[]},{"name":"Michita Imai","ids":[]},{"name":"Takayuki Kanda","ids":[]}],"doi":"10.1109/IROS.2018.8594141"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546131","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Human action recognition in video is an active yet challenging research topic due to high variation and complexity of data. In this paper, a novel video based action recognition framework utilizing complementary cues is proposed to handle this complex problem. Inspired by the successful two stream networks for action classification, additional pose features are studied and fused to enhance understanding of human action in a more abstract and semantic way. Towards practices, not only ground truth poses but also noisy estimated poses are incorporated in the framework with our proposed pre-processing module. The whole framework and each cue are evaluated on varied benchmarking datasets as JHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art performance on these datasets and show the strength of complementary cues.","inCitations":[],"pmid":"","title":"Multi-Modal Three-Stream Network for Action Recognition","journalPages":"3210-3215","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546131"],"entities":[],"journalVolume":"","outCitations":["0c56f414251d6c9f43623ee683dc6cae3be1045a","2f66086e4cbb22c5736c836614830489f9594b91","070874b011f8eb2b18c8aa521ad0a7a932b4d9ad","3aa822de87b9e7ba4b09a9bf4dd2c5dce373edb2","2d83ba2d43306e3c0587ef16f327d59bf4888dc3","b5f2846a506fc417e7da43f6a7679146d99c5e96","a470a81f989d5354239f1044c90e07b78c6beed7","25f5df29342a04936ba0d308b4d1b8245a7e8f5c","e1c7564dff61993b354972fd0e925030f46c1a8f","24115d209e0733e319e39badc5411bbfd82c5133","7950d67f7104e9bd82d957f0ed80f11982802397","c6241e6fc94192df2380d178c4c96cf071e7a3ac","d9c536addc6f84e8b6f48a555b0320992f950b1b","53698b91709112e5bb71eeeae94607db2aefc57c","36c473fc0bf3cee5fdd49a13cf122de8be736977","57a217ce76ff857fb660b39c2b3c22dd5e4185e1","e2d969752015f90d50a2e85c19d5b391011bc9a3","a8879cf5cb423e5217e77dda35e2f3cfbe189cea","236171d2c673194045b4c2e2837ddcc4a2041b8a","48186494fc7c0cc664edec16ce582b3fcb5249c0","1858690b56e10cad51d8be29ddbb55a41c794ff7","b0e7c177084be76fb73df3c4bcf1846676a2d615","081304950e098d3ec89761280f443eb9ff41e950","5ad0e283c4c2aa7b9985012979835d0131fe73d8","44f23600671473c3ddb65a308ca97657bc92e527"],"id":"59478365451b82f5227bc4b694a2ff319025cc33","s2Url":"https://semanticscholar.org/paper/59478365451b82f5227bc4b694a2ff319025cc33","authors":[{"name":"Muhammad Usman Khalid","ids":["49065707"]},{"name":"Jie Yu","ids":["46380968"]}],"doi":"10.1109/ICPR.2018.8546131"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545780","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Correlation filter based trackers have been extensively investigated for their superior efficiency and fairly good robustness. However, it remains challenging to achieve longterm tracking when the object is under occlusion and severe deformation. In this paper, we propose a tracker named Complementary Learners with Instance-specific Proposals (CLIP). The CLIP tracker consists of three main components, including a translation filter, a scale filter, and an error correction module. Complementary features are incorporated into the translation filter to cope with illumination changes and deformation, and an adaptive updating mechanism is proposed to prevent model corruption. The translation filter aims to provide an excellent real-time inference. Furthermore, the error correction module is activated to correct the localization error by an instance-specific proposal generator, especially when the target suffers from dramatic appearance changes. Experimental results on the OTB, Temple-Color 128 and UAV20L datasets demonstrate that the CLIP tracker performs favorably against existing competitive trackers in term of accuracy and robustness. Moreover, our proposed CLIP tracker runs at the speed of 33 fps on the OTB. It is highly suitable for real-time applications.","inCitations":[],"pmid":"","title":"Long-term Object Tracking with Instance Specific Proposals","journalPages":"1628-1633","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545780"],"entities":[],"journalVolume":"","outCitations":["2a2af16483c0264c2afe1ed00fa8c9a44501a5b2","03484414f657a6cce8e08dc8a560ac9db65088f4","47b5e4d564f36bf322c14893b51ae4ecf782b53b","f1d80d6adef260500ebb4eb6347fd9a8adf8961e","fffe5e4d2c82667be54241e87a9339dfd993be00","549910b8450e56f3001321f6db0c0352f34592db","57139f647fe3009cfe30f365ae33959260206bdc","4c020c0866610848a59ee0c80e00db925e2f13ca","a9a414604cff39f1a03c5547385dc421e6c8452e","18422ae57ec0318fb4c200fdcf7c6e145208d792","7b69739682b8edbd3d9ae5af141800fb2a89e751","b7d540cd0de72e984cdec44afa4a4d039cfd5eea","1c42b5543c315556c8a961b1a4ee8bc027f70b22","8262facae51097b3004f492d6de456286a2a58a2","149e5e5eeea5a9015ab5ae755f62c45ef70fa79b","65c9b4b1d49f46b3f8f64a5f617acfc14f85d031","3a186019b67871548eb00bda0d3032826a12ab1f","eebc4a70509aca8491c2302c8a4241fd82c9007f","1af794673d64112edf60f071ca24066e6a227d2b","27850781e39df9f750e05409b8072261124068e8","36fa34edccea48d5b689675cb75a2d7f32146f6c","4de68b1a0d26002fc0220a8b1e7a33b9cff74fa7","0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5","caa0fd34e50bb417fae3ee32f667e78fe5b198bc","b183947ee15718b45546eda6b01e179b9a95421f"],"id":"bcfc93fa80f35fbc0621a6df2ecb1d1119a1ebb9","s2Url":"https://semanticscholar.org/paper/bcfc93fa80f35fbc0621a6df2ecb1d1119a1ebb9","authors":[{"name":"Hao Liu","ids":["40496417"]},{"name":"Qingyong Hu","ids":["8163866"]},{"name":"Biao Li","ids":["49729898"]},{"name":"Yulan Guo","ids":["2985328"]}],"doi":"10.1109/ICPR.2018.8545780"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545782","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The application cost machine learning methods often rely on the availability of large-scale data collection and annotation, especially in the cases of cross-domain learning. One way to circumvent this cost is constructing models to synthesize data and provide automatic annotation. Although these models are attractive, they often can not be generalized from synthetic images to real-world images. Therefore, domain adaptive algorithm is needed to improve these models, so that they can be applied successfully. In this paper, we propose a novel unsupervised domain adaptive framework codenamed D-DANN inspired by the theory of adversarial learning. We apply the discriminator to diverse the features extracted from dual branch CNN. We can obtain more sufficient shared representation across domains by the proposed dual feature extractors. The framework can be easily adapt to most popular CNN models to improve the representation power. We implement the D-DANN with several popular CNN models including LeNet, AlexNet and so on. Using these D-DANN enhanced neural networks, we conduct extensive experiments on several pairs of domain adaptive validation datasets. The results show that our approach can efficiently enhance domain adaptive capability of general CNN models for unlabeled data.","inCitations":[],"pmid":"","title":"Diversified Dual Domain-Adversarial Neural Networks","journalPages":"615-620","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545782"],"entities":[],"journalVolume":"","outCitations":["4008f2c35a63b7de841b757b32d66fa9e8d04106","51a4d658c93c5169eef7568d3d1cf53e8e495087","326a0914dcdf7f42b5e1c2887174476728ca1b9d","a27f9defe663cca0793c21d9c6063e20743618bb","ca937a9ea84bfed5001cd91d339bcc38e2e9820c","7da1c2d2ea8f51011b1a3006eabc03b342f792d4","162d958ff885f1462aeda91cd72582323fd6a1f4","6b4da897dce4d6636670a83b64612f16b7487637","3bf2efc97513f95d0fa6609f47e306f4fe4a5ef0","0a59337568cbf74e7371fb543f7ca34bbc2153ac","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","3b2697d76f035304bfeb57f6a682224c87645065","3a307b7e2e742dd71b6d1ca7fde7454f9ebd2811","4cd547ae45556e7cf232eee261219872a8073649","7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7","6c8b30f63f265c32e26d999aa1fef5286b8308ad","02d17701dd346311197c3f1553ae9e0d6376fd43","ba753286b9e2f32c5d5a7df08571262e257d2e53","38b457f765979495e33ff37859460350a92bf33c","55c22f9c8f76b40793a8473248873f726abd8ce9","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","107010b7f2abe3c0c9df62bcef35eb77f6fc76df","30416bc0760f463fa90bd8a92a388fb6710fe589","2c91aaefc57acbd1e9e4dfe83b4bedb2fc42bec1","01cb4071a0a43aeef63e5d568ad5afe1fb8b2411","1c734a14c2325cb76783ca0431862c7f04a69268","9a700c7a7e7468e436f00c34551fbe3e0f70e42f","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","35756f711a97166df11202ebe46820a36704ae77","223319a93dcf3912bbc1e5f949e5ab4d53906e62","072fd0b8d471f183da0ca9880379b3bb29031b6a","05dafb4bc699d19152dabfb744f52e88ec7e029a","5d9a3036181676e187c9c0ff995d8bed1db3557d","815abff110b8f4713e634c3c58a2017328e1ee4e","d0e51833b3db3af1c762ab723efd08f117a497c8"],"id":"3c42f71febb290c4a42c64267593ac8386408bfc","s2Url":"https://semanticscholar.org/paper/3c42f71febb290c4a42c64267593ac8386408bfc","authors":[{"name":"Yuchun Fang","ids":["8589942"]},{"name":"Qiulong Yuan","ids":["30438417"]},{"name":"Wei Zhang","ids":["40430159"]},{"name":"Zhaoxiang Zhang","ids":["48805516"]}],"doi":"10.1109/ICPR.2018.8545782"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593923","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Performing skillfull manipulation is a very challenging task for robots. So far, even experts could barely program them to e.g. perform the well known peg-in-hole problem in the real world. Autonomously acquiring such skills, let alone generalizing them to new tasks, is still a major challenge. Typically, manipulation learning is approached with the help of large computation power, very long learning times, or possibly both. However, the performance achieved up to now is still far from human performance. We show the results of our new paradigm to robot manipulation. It bridges and unifies basic motor control, simple and complex manipulation strategies and high-level manipulation planning. The robots show autonomous skill learning, intra-class and inter-class generalization of insertion skills at human-level performance.","inCitations":[],"pmid":"","title":"The Art of Manipulation: Learning to Manipulate Blindly","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593923"],"entities":[],"journalVolume":"","outCitations":[],"id":"a39bc0bf0bbcd28c7b2444fe66f519ca83519dc9","s2Url":"https://semanticscholar.org/paper/a39bc0bf0bbcd28c7b2444fe66f519ca83519dc9","authors":[{"name":"Sami Haddadin","ids":[]},{"name":"Lars Johannsmeier","ids":[]}],"doi":"10.1109/IROS.2018.8593923"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206299","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper explores a stochastic approach for controlling swarms of independent robots toward a target distribution in a bounded domain. The robot swarm has no central controller, and individual robots lack both communication and localization capabilities. Robots can only measure a scalar field (e.g. concentration of a chemical) from the environment and from this deduce the desired local swarm density. Based on this value, each robot follows a simple control law that causes the swarm as a whole to diffuse toward the target distribution. Using a new holonomic drive robot, we present the first confirmation of this control law with physical experiment. Despite deviations from assumptions underpinning the theory, the swarm achieves the theorized convergence to the target distribution in both simulation and experiment. In fact, simulated and experimental performance agree with one another and with our hypothesis that the error from the target distribution is inversely proportional to the square root of the number of robots. This is evidence that the algorithm is both practical and easily scalable to large swarms.","inCitations":["4475ef4289f8c8d5f26cc8c0ba056d4bd25430fa"],"pmid":"","title":"Decentralized stochastic control of robotic swarm density: Theory, simulation, and experiment","journalPages":"4341-4347","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206299","http://faculty.engineering.asu.edu/acs/wp-content/uploads/2017/08/Li_IROS2017_DecentralizedStochCtrl.pdf","http://www.math.ucla.edu/~bertozzi/papers/IROS2017.pdf"],"entities":["Algorithm","Experiment","No-communication theorem","Optimal control","Robot","Scalability","Simulation","Stochastic control","Swarm robotics","Theory"],"journalVolume":"","outCitations":["e6e217867c54a3a51e9b4d289371d4c92ba61d47","0c2942d71b6e4ed9eb288fa2421a5d3ade3b5764","2e9ad557c3cde7cb10358bf3ee42d02af9a9cff3","645d90ede11053f795ad2e6c142d493a5a793303","332648a09d6ded93926829dbd81ac9dddf31d5b9","5fe850ce0f9f77f6d689641be4df9e575f15ea90","eb1c3115f30bf67b4a20830b20d85640977e11b9"],"id":"b807a48edac9f8b0f08c59c43422a5abdeede9c5","s2Url":"https://semanticscholar.org/paper/b807a48edac9f8b0f08c59c43422a5abdeede9c5","authors":[{"name":"Hanjun Li","ids":["47892700"]},{"name":"Chunhan Feng","ids":["32370004"]},{"name":"Henry Ehrhard","ids":["47831567"]},{"name":"Yijun Shen","ids":["2639080"]},{"name":"Bernardo Cobos","ids":["32369179"]},{"name":"Fangbo Zhang","ids":["47191039"]},{"name":"Karthik Elamvazhuthi","ids":["3348500"]},{"name":"Spring Berman","ids":["4453738"]},{"name":"Matt Haberland","ids":["33702926"]},{"name":"Andrea L. Bertozzi","ids":["1852040"]}],"doi":"10.1109/IROS.2017.8206299"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545506","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Effective training of the deep neural networks requires much data to avoid underdetermined and poor generalization. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data by for example, flipping, distorting, adding noise to, cropping a patch from the original samples. In this paper, we introduce the adversarial autoencoder (AAE) to impose the feature representations with uniform distribution and apply the linear interpolation on latent space, which is potential to generate a much broader set of augmentations for image classification. As a possible \u201crecognition via generation\u201d framework, it has potentials for several other classification tasks. Our experiments on the ILSVRC 2012, CIFAR-10 datasets show that the latent space interpolation (LSI) improves the generalization and performance of state-of-the-art deep neural networks.","inCitations":["b6919d278aa46648ecf6f64218efbd6b0fbc53b8"],"pmid":"","title":"Data Augmentation via Latent Space Interpolation for Image Classification","journalPages":"728-733","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545506"],"entities":[],"journalVolume":"","outCitations":["d5e9832d602a4e6bc92248de691b9ed349301e6f","1193317829bfcc9b9dffa5ae85a2e2114254b37e","83bfdd6a2b28106b9fb66e52832c45f08b828541","82070bef06578a24e63b2b739ec86d4d31bb576a","0f88de2ae3dc2ec1371d1e9f675b9670902b289f","572dd2d5d75227bb878430c9375b9be92cc7e6e9","3b2697d76f035304bfeb57f6a682224c87645065","04640006606ddfb9d6aa4ce8f55855b1f23ec7ed","a1cda8e30ce35445e4f51b47ab65b775f75c9f18","8033e0edb3b43c4ba3605d70d0de14efbe69c976","c0672186ed39c276a476dc5dd4d3ccf548d29eee","4185286ec9d65086803c3ddf1cae1b27a9d6b5bb","2679e4f84c5e773cae31cef158eb358af475e22f","15174d135305c7865b584ca2bbb725159f6efef4","a36b028d024bf358c4af1a5e1dc3ca0aed23b553","c6850869aa5e78a107c378d2e8bfa39633158c0c","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","35756f711a97166df11202ebe46820a36704ae77","b6919d278aa46648ecf6f64218efbd6b0fbc53b8","321514a76a09d7a6fca00d27eda92663236c0f66","6ce1922802169f757bbafc6e087cc274a867c763","17bdb111948e04b2496cb153ae77e51a6c79476d","1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f","501d99e392783e4acafb220136d32ea68a921282","3ba179bceb9692d4d21109d0b87b120195761148","0626908dd710b91aece1a81f4ca0635f23fc47f3","d0e51833b3db3af1c762ab723efd08f117a497c8","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","2c03df8b48bf3fa39054345bafabfeff15bfd11d","2ae378e934f7f4720f44cc5586cecaad30b2f55b"],"id":"4f4feb145b7551e11eea0cf4d5db8ce018afd120","s2Url":"https://semanticscholar.org/paper/4f4feb145b7551e11eea0cf4d5db8ce018afd120","authors":[{"name":"Xiaofeng Liu","ids":["49544204"]},{"name":"Yang Zou","ids":["46810327"]},{"name":"Lingsheng Kong","ids":["34876535"]},{"name":"Zhihui Diao","ids":["31535504"]},{"name":"Junliang Yan","ids":["32886613"]},{"name":"Jun Wang","ids":["37199474"]},{"name":"Site Li","ids":["5903136"]},{"name":"Ping Jia","ids":["46990998"]},{"name":"Jane You","ids":["1748883"]}],"doi":"10.1109/ICPR.2018.8545506"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.34","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Speech is the most used communication method between humans and it involves the perception of auditory and visual channels. Automatic speech recognition focuses on interpreting the audio signals, although the video can provide information that is complementary to the audio. Exploiting the visual information, however, has proven challenging. On one hand, researchers have reported that the mapping between phonemes and visemes (visual units) is one-to-many because there are phonemes which are visually similar and indistinguishable between them. On the other hand, it is known that some people are very good lip-readers (e.g: deaf people). We study the limit of visual only speech recognition in controlled conditions. With this goal, we designed a new database in which the speakers are aware of being read and aim to facilitate lip-reading. In the literature, there are discrepancies on whether hearing-impaired people are better lip-readers than normal-hearing people. Then, we analyze if there are differences between the lip-reading abilities of 9 hearing-impaired and 15 normal-hearing people. Finally, human abilities are compared with the performance of a visual automatic speech recognition system. In our tests, hearing-impaired participants outperformed the normal-hearing participants but without reaching statistical significance. Human observers were able to decode 44% of the spoken message. In contrast, the visual only automatic system achieved 20% of word recognition rate. However, if we repeat the comparison in terms of phonemes both obtained very similar recognition rates, just above 50%. This suggests that the gap between human lip-reading and automatic speech-reading might be more related to the use of context than to the ability to interpret mouth appearance.","inCitations":["8ac69ed7dd2d046c20140a5bbb8cbe41bdff7670"],"pmid":"","title":"Towards Estimating the Upper Bound of Visual-Speech Recognition: The Visual Lip-Reading Feasibility Database","journalPages":"208-215","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.34","https://arxiv.org/pdf/1704.08028v1.pdf","http://arxiv.org/abs/1704.08028"],"entities":["One-to-many (data model)","Speech recognition","Speech synthesis","Word error rate"],"journalVolume":"","outCitations":["166204d5e1ea7c48c238ea0c9444e542b4bd2ab1","8434844720bc3a55b33809df4172eae05ca2f134","0d5980ebcc8986aa3943285983ab7d192b868192","81dec1d7d4170e4323efda5632faf239627a82ce","b62628ac06bbac998a3ab825324a41a11bc3a988","68449bd37e7c2b95a0cd507e74801cb458547d86","39a7284502b382a587b80c7cba10781a7acf755a","53ac494b6785336b3753e6e3a6d885f4289bf87c","4ab3f2b81588805d12158eb3ca2be72e67d98218","7543e6e5af72ec1db951f776a90f9ad8c3d7149f","6374e5307c699da00875a858134653c16d5fd035","452e3a679c532e845b51322ae6dfc12d143f9141","63b9ef62e1a290a096cac46e982d840c92559705","03f98c175b4230960ac347b1100fbfc10c100d0c","944592fcb4ab1d254ea55bd32960b6482556a1e6","0c252434ba0405f3818960c47cbb27acf33c067d","7fab18a98909a2777104cdcf90a90eff693915a7","101412092a775012260a4c8bd5372c68dd1a1acf","f6bcba74059bb091e62027bde7d29da051255484","22e94a160c26ef2febd05dd1fb3589ee1e0c6780","b82babc5a64336aaf5463857c0a8dd3ddcf462b3","ee7109b1f600f944e1a6c18cd268302855abce3b","5aa1d92777a7aa29e279ef1dc9f25864aaa43a12","0b370962301354d6f8572eb32610128ce69f1444","5129350ec0bd8f1fe78a9b864865709f8d8de058","81b17388633e0144ffa24d282acc89a44c40d2ce","e3bc7b874c9bc7e76cb6b7375e5e0fcedcc01e14","653b957d4c70d6cbf8de443df497e47edcab77b4","8f3ea4eedb0cee67298b4e04d2cbbff20726cef8","08635266c783a2b7158a8c46bde42655d036c0f1","169927e68301d881da6585dd09e1bc9a88adf147","b74c410ea8db3babb9a307d1fb8f08b90ce8b490","eef41ae597a20ea377461d522fd5100da6a7a9b7","4d1d5dbfccb2219c547afa347148cfc372c4d998","8dd62c3a6803aa1f9a102c947cb848affa18f155","2f663f368444026a79c93af09dba809e207e9b74","e74e2004d8b7357c35d727cb4c92ca97142759f0","a551d3ab87f4adedd15597714cc1721b2856e859","18cc14fd2a0c4d341c7653a02f1b8516e819b726","8edbee3390b1a607320973ba66f5699b2e4dd080","a8bbbf3ff7757e2c963997ab07229518b04a1db6","a066b79ba0254e19a6dea518c5ae4c9f3e377f75","6fac3e9a31ed176209251e65bba568d5c0f04875","3cc268ed40d453ed007657172ba2bc5f6509ed65"],"id":"176bc3f528d3d87a7543e377f9b7e4c04b5a9408","s2Url":"https://semanticscholar.org/paper/176bc3f528d3d87a7543e377f9b7e4c04b5a9408","authors":[{"name":"Adriana Fernandez-Lopez","ids":["35114261"]},{"name":"Oriol Martínez","ids":["2642774"]},{"name":"Federico Sukno","ids":["2012978"]}],"doi":"10.1109/FG.2017.34"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594267","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Robots and automated systems are increasingly being introduced to unknown and dynamic environments where they are required to handle disturbances, unmodeled dynamics, and parametric uncertainties. Robust and adaptive control strategies are required to achieve high performance in these dynamic environments. In this paper, we propose a novel adaptive model predictive controller that combines model predictive control (MPC) with an underlying $\\mathcal{L}_{1}$ adaptive controller to improve trajectory tracking of a system subject to unknown and changing disturbances. The $\\mathcal{L}_{1}$ adaptive controller forces the system to behave in a predefined way, as specified by a reference model. A higher-level model predictive controller then uses this reference model to calculate the optimal reference input based on a cost function, while taking into account input and state constraints. We focus on the experimental validation of the proposed approach and demonstrate its effectiveness in experiments on a quadrotor. We show that the proposed approach has a lower trajectory tracking error compared to non-predictive, adaptive approaches and a predictive, nonadaptive approach, even when external wind disturbances are applied.","inCitations":[],"pmid":"","title":"Adaptive Model Predictive Control for High-Accuracy Trajectory Tracking in Changing Conditions","journalPages":"7831-7837","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594267","http://arxiv.org/abs/1807.05290","http://export.arxiv.org/pdf/1807.05290","http://www.dynsyslab.org/wp-content/papercite-data/pdf/pereida-iros18.pdf","https://arxiv.org/pdf/1807.05290v2.pdf"],"entities":[],"journalVolume":"","outCitations":["7888ce2e70b9db38e9f970e80064bd88489d1829","4fa8e378876e716f64f53b3c0ff26f14b45ce8c0","6bad4549c14a39dee0cdd3a29455fba584d61e0a","9c156e62dd04a86224cab1f78c9e09d857736cfd","5425d7bd33ed5643a4c24c41245829ff1cb963d7","9cd6ed0e056463f536be0758f2cf399f7ae86ec8","954725f69b18b3635fc8c87c712db8b8ecef65be","0ecf1594b8a3fada8bd0bb45c55f2e6c8b300746","fdae4101ec42fb91fa1ff1fd6d5e4d840588f33e","abe3167ff50408ea3b89890f63526c1f2fbd7087","3937674c9d84dd8ea9e7fb102f8f27f8547cc2e7","b5bfc70be36bc06e49827b65bb07d2ec275fc043","ddbad2f61168e0fede4c42f6741602cb29478698","fe1c6df309603adb5edf266cf282615a725fa4d7","b6d7f52452749152895d87d1e48f58e87f4f062f","3a26dbc6f15ea43bfa81288b51a2f476dbe46dba","e633e7e997e6c1cc6814d085eff700ba7e79fc3f"],"id":"82520bd6219cbbc2268d983f0072d58c7eaefae0","s2Url":"https://semanticscholar.org/paper/82520bd6219cbbc2268d983f0072d58c7eaefae0","authors":[{"name":"Karime Pereida","ids":["2498435"]},{"name":"Angela P. Schoellig","ids":["3008301"]}],"doi":"10.1109/IROS.2018.8594267"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593941","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this tutorial, we provide principled methods to quantitatively evaluate the quality of an estimated trajectory from visual(-inertial) odometry (VO/VIO), which is the foundation of benchmarking the accuracy of different algorithms. First, we show how to determine the transformation type to use in trajectory alignment based on the specific sensing modality (i.e., monocular, stereo and visual-inertial). Second, we describe commonly used error metrics (i.e., the absolute trajectory error and the relative error) and their strengths and weaknesses. To make the methodology presented for VO/VIO applicable to other setups, we also generalize our formulation to any given sensing modality. To facilitate the reproducibility of related research, we publicly release our implementation of the methods described in this tutorial.","inCitations":["a944085eaf8146a81c6242feac4cc921d6206058"],"pmid":"","title":"A Tutorial on Quantitative Trajectory Evaluation for Visual(-Inertial) Odometry","journalPages":"7244-7251","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593941","http://rpg.ifi.uzh.ch/docs/IROS18_Zhang.pdf"],"entities":[],"journalVolume":"","outCitations":["215fe6ebcf44a364edca5e9a19ff79d3360d0e86","9af5d0d7d6106863629ea0a643ffa05f934e0ee7","2295fda13d5118808557d04310b0e176a1341063","c496d0733d316c9cc6955ddee52fe1c457f80be5","1dcae5e5e9fc1b4c0315b6e68f19b9bb2d9dba05","c1f921333f308d0d57bc64cd48ff4a2f499bbd79","55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1","d107231cce2676dbeea87e00bb0c587c280b9c53","05d3b4cfc83c1ccfeb75872f82aac75235c25c2d","6f7892e29b76ba107ca3e6a923ce56dd3a7cd580","44f874b7a754bee99f9907c3ae0ad9a69fef375b","230ad73e8bd1d3268d56c66a83442d24176b864d","5549a981a3c0bf5afb46fdbbb9a5163bb0295d6d","5eb4c55740165defacf08329beaae5314d7fbfe6","9da965529ee3da77178ed99cf13d97be0fa85f6e","19a5e5b31e741d0f9cacf9c93de782184d24c947"],"id":"b076c7b6e10ba80799a7bf28af06e178f1cc760b","s2Url":"https://semanticscholar.org/paper/b076c7b6e10ba80799a7bf28af06e178f1cc760b","authors":[{"name":"Zichao Zhang","ids":["3414578"]},{"name":"Davide Scaramuzza","ids":["2075371"]}],"doi":"10.1109/IROS.2018.8593941"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594085","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"To improve the movement ability of humanoid robots, instead of traditional methods dependent on only power of actuators, there is possibility that utilizing elasticity inspired from collaboration of muscle and tendon of human is effective to achieve high-power movement. In this study, we aimed to realize a jumping motion that accumulates energy more appropriately in spring by combining active joint driving with spring behavior like human tendons and muscles. We proposed a countermovement jump method using the resonance with the leg's active pushing-off movement and leg stiffness. To achieve active pushing-off and joint stiffness, we developed a new joint mechanism using leaf springs and an actuator unit with a worm gear. We then performed experiments to evaluate the effectiveness of the proposed mechanism and methods. Finally, the robot achieved a countermovement jump using active kicking and leg's elasticity.","inCitations":[],"pmid":"","title":"Jumping Motion Generation of a Humanoid Robot Utilizing Human-Like Joint Elasticity","journalPages":"8707-8714","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594085"],"entities":[],"journalVolume":"","outCitations":[],"id":"0e3354c7b0cda32b093bfd772c45a1fa437f8da7","s2Url":"https://semanticscholar.org/paper/0e3354c7b0cda32b093bfd772c45a1fa437f8da7","authors":[{"name":"T. Otani","ids":[]},{"name":"Kenji Hashimoto","ids":[]},{"name":"H. Ueta","ids":[]},{"name":"M. Sakaguchi","ids":[]},{"name":"Yasuo Kawakami","ids":[]},{"name":"Hun-ok Lim","ids":[]},{"name":"Atsuo Takanishi","ids":[]}],"doi":"10.1109/IROS.2018.8594085"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593522","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The Heterogeneous Extensible Robot Open (HERO) platform is designed for autonomous robotic research. While bringing in the flexible computational capacities by CPU and FPGA, it addresses the challenges of heterogeneous computing by embracing OpenCL programming. We propose heterogeneous computing approaches for three fundamental robotic tasks: simultaneous localization and mapping (SLAM), motion planning and convolutional neural network (CNN) inference. With FPGA acceleration, the SLAM and motion planning tasks are performed 2\u20134 times faster on the HERO platform against fine-tuned software implementation. For CNN inference, it can process 20\u201330 images per second with the network of VGG-16 or ResNet-50. We expect the open platform and the developing experiences shared in this paper can facilitate future robotic research, especially for those compute intensive tasks of perception, movement and manipulation.","inCitations":[],"pmid":"","title":"HERO: Accelerating Autonomous Robotic Tasks with FPGA","journalPages":"7766-7772","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593522"],"entities":[],"journalVolume":"","outCitations":[],"id":"2e840fd963a6ec2bde79312d91736164fca8fc70","s2Url":"https://semanticscholar.org/paper/2e840fd963a6ec2bde79312d91736164fca8fc70","authors":[{"name":"Xuesong Shi","ids":[]},{"name":"Lu Cao","ids":[]},{"name":"Dawei Wang","ids":[]},{"name":"Ling Liu","ids":[]},{"name":"Ganmei You","ids":[]},{"name":"Shuang Liu","ids":[]},{"name":"Chunjie Wang","ids":[]}],"doi":"10.1109/IROS.2018.8593522"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594000","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Fast inspection and light maintenance capability is already a clear demand to control the tokamak condition and improve the efficiency of the experimental campaigns. EAST remote maintenance system has been developed to implement inspection and grasping tasks during plasma. The paper presents design description of EAMA (EAST articulated maintenance arm) robot, the gripper and the CASK. The field commissioning was performed both in mockup and EAST tokamak to demonstrate the availability and functionalities of EAMA system. To be able to realize fully routine operation on EAST, improvement of EAMA control system was proposed with integration developed algorithm, such as the robot flexible model modeling, vision servo, motion planning, etc. Finally, thoughts for CFETR In-Vessel Inspection System (CIVIS) are given.","inCitations":[],"pmid":"","title":"Progress and Prospects of EAST Remote Maintenance System","journalPages":"3593-3598","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594000"],"entities":[],"journalVolume":"","outCitations":[],"id":"5b85053f2a3f327b8f33a5d0ff4bed17fe249d43","s2Url":"https://semanticscholar.org/paper/5b85053f2a3f327b8f33a5d0ff4bed17fe249d43","authors":[{"name":"Hongtao Pan","ids":[]},{"name":"Shanshuang Shi","ids":[]},{"name":"Yong Cheng","ids":[]},{"name":"Wenlong Zhao","ids":[]}],"doi":"10.1109/IROS.2018.8594000"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545416","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Pre-processing, Feature Extraction, Feature Selection and Classification are the four sub modules of the Signal Processing module of a typical BCI system. Pattern recognition is mainly involved in this Signal Processing module and in this paper, we experimented with different state-of-the-art algorithms for each of these submodules on two separate datasets we acquired using Emotiv EPOC and the Muse headband from 38 college-aged young adults. For our experiment, we used two artefact removal techniques, namely Stationary Wavelet Transform (SWT) based denoising technique and an extended SWT technique (SWTSD). We found SWTSD improves average classification accuracy up to 7.2 % and performs better than SWT. However, that does not state that SWTSD will outperform SWT when implemented on other BCI paradigms or on other EEG-based applications. In our study, the highest average accuracy achieved by the data of the Muse headband and Emotiv EPOC were 77.7% and 66.7% respectively and from our results we conclude that, the performance of different BCI systems depends on several different factors including artefact removal techniques, filters, feature extraction and selection algorithms, classifiers, etc. and appropriate choice and usage of such methods can have a significant positive impact on the end results.","inCitations":[],"pmid":"","title":"Effect of Artefact Removal Techniques on EEG Signals for Video Category Classification","journalPages":"3513-3518","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545416"],"entities":[],"journalVolume":"","outCitations":["cfc1e5c490ccc3ee09cd4fdc9d0f43a343cd0d75","33450b64e1c80045e27ef074e333a97f8c6a766f","7d9b506ee54d949c7f80c294e8c6262f15073349","667ab2dfe4ffbda11cda88d8057ae86ed444d020","524b707e6904f56104a539a8989194296181bac6","a3f9bcfbacecc41b75ded96814578ba9a2994ea7","85576b60d01eb94077db12cfa907ac88fcfec6e1","a1aa188ef055f9b1d163ffd4c3feb3ac73de9601","b2451d7a84477eee22825c565ba42c0597828575","a1fb5fec1207a2283d59cc108993b24e920c9579","b23e0e0f596030ae6dabbb16792584a16555ad52","41a306e416489a0b17ed36e7b6453b7285890876","019521a0794aeeb7f496a77a98892508a0bc11d9","ff84832f6656045de19c4412407eaf34fb1a3883","42fc0fa9d5fa86968ae165a221d34a3beef17369","b92e6d6b4ad164eb6b31afb1e142ce4cae2f750a","1f87a96fb7a35bfa0ad827fc7158a3fdc5c7c73a","bef994544227d50d8c78c6019b3569da8934555d","49f16a7429e4d62b23f44ace3c4a44276a52786f","a32e510a55957325ae05cdcdfff9023ce59c1629"],"id":"0580da7ae858a948324cc1e7730660a50c5c14e0","s2Url":"https://semanticscholar.org/paper/0580da7ae858a948324cc1e7730660a50c5c14e0","authors":[{"name":"Aunnoy K. Mutasim","ids":["24371750"]},{"name":"M. Raihanul Bashar","ids":["9219927"]},{"name":"Rayhan Sardar Tipu","ids":["12564594"]},{"name":"Md Kafiul Islam","ids":["1825338"]},{"name":"M. Ashraful Amin","ids":["49962766"]}],"doi":"10.1109/ICPR.2018.8545416"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206213","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present a novel approach to trajectory generation that enables an autonomous vehicle to accurately follow a lead vehicle tracked by on-board sensors. In contrast to other approaches, we ignore the structure of the environment (e.g., lane markings), focusing purely on following the path driven by the vehicle ahead. Based on the leader's velocity, its distance to the ego vehicle and its recorded path, a continuous-curvature trajectory is generated using Sequential Quadratic Programming. As the optimization process takes the ego vehicle's kinematic and dynamic constraints into account, the resulting trajectory is guaranteed to be feasible and safe. The algorithm was tested extensively during practical experiments with an actual autonomous car. Our tests were conducted in both on- and off-road environments, with speeds ranging from 1 m/s to 15 m/s. Ground truth data shows that the system achieves a high degree of accuracy even in difficult scenarios.","inCitations":["b006de4d45da599aad0724340e94515f94927622"],"pmid":"","title":"An optimization approach to trajectory generation for autonomous vehicle following","journalPages":"3675-3680","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206213"],"entities":["Algorithm","Autonomous car","Autonomous robot","Experiment","Ground truth","Mathematical optimization","On-board data handling","Sensor","Sequential quadratic programming","Velocity (software development)"],"journalVolume":"","outCitations":["0e2387639950041b52c7c3dcd935f001e64dce79","0e8c927d9c2c46b87816a0f8b7b8b17ed1263e9c","95077a0cb8e947614f9565d86738fae51614c233","29bd49f4648b45ee5b2fedc3dea55d3effa003fc","1a6a7bb49428c468d2a31a14987e53ad4c92a540","2a8857b5f1ca38b5999d29215cf2be36f1d3c556","b78da75686e9630ec97bd41fe317434d219a727f","2bc59bb3210340b9dcfbabed9d3d043be5134513","2309b4ab424e174e4559b7fd3223c538c35d7d56","ec27ea83cc1b7f18880e1fc52948091337d10a96","8cb95555089c00b0a27b307d0c5577d52549ce10","deb8fb0074e670f9e3639e673c67d7f46818cd3d","18520721525ed81a6ffa6d8b1c7dcbd771e4a64b","ec26d7b1cb028749d0d6972279cf4090930989d8","01a97cd4e640beb1d8990a978bac7c618574de5c"],"id":"1d9f36778527a5beae654af9c46b784c6e7aac14","s2Url":"https://semanticscholar.org/paper/1d9f36778527a5beae654af9c46b784c6e7aac14","authors":[{"name":"Dennis Fassbender","ids":["33951864"]},{"name":"Benjamin C. Heinrich","ids":["38502346"]},{"name":"Thorsten Luettel","ids":["2376187"]},{"name":"Hans-Joachim Wünsche","ids":["2206680"]}],"doi":"10.1109/IROS.2017.8206213"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593567","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This work presents the concept of a robotic gripper designed for the disassembly of electromechanical devices that comprises several innovative ideas. Novel concepts include the ability to interchange built-in tools without the need to grasp them, the ability to reposition grasped objects in-hand, the capability of performing classic dual arm manipulation within the gripper and the utilization of classic industrial robotic arms kinematics within a robotic gripper. We analyze state of the art grippers and robotic hands designed for dexterous in-hand manipulation and extract common characteristics and weak points. The presented concept is obtained from the task requirements for disassembly of electromechanical devices and it is then evaluated for general purpose grasping, in-hand manipulation and operations with tools. We further present the CAD design for a first prototype.","inCitations":[],"pmid":"","title":"The KIT Swiss Knife Gripper for Disassembly Tasks: A Multi-Functional Gripper for Bimanual Manipulation with a Single Arm","journalPages":"4590-4597","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593567","http://h2t.anthropomatik.kit.edu/pdf/Borras2018.pdf"],"entities":[],"journalVolume":"","outCitations":["7c1db2f68859de5094b94d08fbfec4460619e67f","c1558124848ac86aa099c802a5ce0300d35f5299","0c0564c0a9a8167f03cdbcf0dcdc703a3d299acb","261eaec4a8db089d5d437ec0c4cf773dcecd0519","abaabb822fa7834c95fd7d4632a64e97d28f0710","c7905e30014dea2482030430e64c773f46f8a349","2f9352ca18f300645f7242f8e02885e310a8660c","ab9280bdb5459d7d7e33ae9a049407f8f5238992","93e349e7e25aec5ac399c49a17f7fca5140d8b92","03a283ca72dd7fb9f544e3a049efba6bf4203520","257c8f64686893a5d368312d992309f0093f6003","b1ea881990b50d4e01947879bfc7fc17a21848d3","f92240ae699798649eb94b758f07430ad2bb6f29","26f75576e065f0939e416a78cafdd00221425ae0","037f7c63962b20ad3ab47aa4b16c9f7484a91528","9fb6ea4c66ca39f66a01986b939c7daf19dcae50","d993b98a70e1c281f7b990deedf38269290d0349","ca1dafd4ca37bd9096ced00e10f1d68a6638dba8"],"id":"b18857d20b07f325cdf91cb32fff43a612f75941","s2Url":"https://semanticscholar.org/paper/b18857d20b07f325cdf91cb32fff43a612f75941","authors":[{"name":"Júlia Borr̀as","ids":["35147091"]},{"name":"Raphael Heudorfer","ids":[]},{"name":"Samuel Rader","ids":["34787598"]},{"name":"Peter Kaiser","ids":["25108190"]},{"name":"Tamim Asfour","ids":["1722677"]}],"doi":"10.1109/IROS.2018.8593567"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202251","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Haptic feedback for minimally invasive surgery is a widely debated topic, especially force sensors for this kind of application are thoroughly researched. The current state-of-the-art includes several realization options, but none of them are suitable to be integrated in small diameter instruments. In this work, we present a new approach for the generation of haptic feedback by using two separate feedback mechanisms for spatial kinesthetic and unidirectional tactile feedback in a teleoperation system with a parallel kinematic slave robot and the design of the corresponding user interfaces. Kinesthetic forces are measured with proximal sensors in the robot kinematic and displayed with a delta-type kinematic mechanism. Tactile signals are acquired with an acceleration sensor placed closely at the tip of the robot manipulator and conveyed open-loop by a vibrating cylinder to the user's palm. Kinesthetic forces up to 15 N can be displayed up to a frequency of 50 Hz. The tactile interface displays accelerations up to 15m/s for frequencies from 50 Hz to 1000 Hz. The combination of the two interfaces allows the display of high-fidelity haptic feedback with greatly reduced requirements on the interface components.","inCitations":[],"pmid":"","title":"User-interface for teleoperation with mixed-signal haptic feedback","journalPages":"892-898","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202251"],"entities":["Cylinder seal","Haptic technology","Kinematic chain","Mixed-signal integrated circuit","Requirement","Robot","Sensor","User interface"],"journalVolume":"","outCitations":["5714a5d75f1e2be7fbf9c5afbce73c7e2f5194e5","363ffe99fa25d06cd7fd0351a73f1d1b01671dfe","71d15181f049a049f7762fafd93bece0ac9b75c0","327400dbd9031b7f07faed060be8a97fb1106a2e","b1c86d69b374cdb2521696fd91a8f964bd7c7f23","262576c7269771e27bc44dcfeff4e01cf19f7143","0fca9f8ddeaf38e3d10b7ff6a9894ee62b35f12f","bcd0d72828d7d2cdb8d2a423cc8cf52eb743ec90","3036d82ca78b55c5805ec4ec3dfa2d60dc0769a5","c962b793a0c7028c1afe21c813c4d10cf5036217","3a7fc41825d35c97aa52149e5239977a050724dd","0853f8561b3693d263d94d35301bde937d11acf6","c00eba23074ee34159f3809adfea29fc2b2b862a","76667c56a53cbd40537be62fbb39b783b8bf90bd","13c0fb2ba6ffe39085a467e29e21de3b4f710eea","4d38c171e912d56069d183d716556cf30cc824a4","b56be8bc9b1015f68f1c68bdb3c2f4743a109df3","8b8553928fe0e122ad3ae3c255f95aac7a106884","0d00b4c7c55a445cb007d5d7ce57cfbac31ff651","02ab92692487c254f90cbb30cbd2bf3b179b9ec8","5af2b194c1e2ed9204750023e48781f99c5e40dd","13b3d9190b72e93c50cec196657c5e7fa16f3e78","46c1c5021ccc023300f3d30558f7c6e512e5a873","99ec9c470e8fcf71da4da1deca601c343348440f"],"id":"e6b75c90dc8a4e754f98474da15f25bf51c9a9aa","s2Url":"https://semanticscholar.org/paper/e6b75c90dc8a4e754f98474da15f25bf51c9a9aa","authors":[{"name":"Daniel B. Thiem","ids":["32210120"]},{"name":"Carsten Neupert","ids":["3335004"]},{"name":"Johannes Bilz","ids":["3423996"]},{"name":"Sebastian Matich","ids":["1830503"]},{"name":"Julian Polzin","ids":["35012996"]},{"name":"Roland Werthschützky","ids":["1807273"]},{"name":"Mario Kupnik","ids":["1886203"]},{"name":"Helmut F. Schlaak","ids":["3054259"]},{"name":"Andreas Kirschniak","ids":["2952678"]},{"name":"Markus Hessinger","ids":["50400023"]},{"name":"Christian Hatzfeld","ids":["38194304"]}],"doi":"10.1109/IROS.2017.8202251"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594086","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"A new lizard-inspired robot is presented in this paper, which enables to maintain its moving direction by lateral body motions even during high-speed bipedal running. First, a dynamic model for lizard-inspired robot is derived to simulate the lateral body motion of real lizard. Based upon the simulation using dynamic model, the lizard-inspired robot is tactfully built so that its hind leg is optimally designed on a 4-bar mechanism and its body is simplified to consist of two body links and a tail connected by two revolute joints. The experiments verify that the proposed robot can maintain its moving direction via proper lateral motions during high-speed bipedal running similar to that of real lizard.","inCitations":[],"pmid":"","title":"Design of Lizard-Inspired Robot with Lateral Body Motion","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594086"],"entities":[],"journalVolume":"","outCitations":[],"id":"8f89bf9ee8a7c09178519c8f2c8b05a0e1a6b138","s2Url":"https://semanticscholar.org/paper/8f89bf9ee8a7c09178519c8f2c8b05a0e1a6b138","authors":[{"name":"Jeongryul Kim","ids":[]},{"name":"Hongmin Kim","ids":[]},{"name":"Youngsoo Kim","ids":[]},{"name":"Hwa Soo Kim","ids":[]},{"name":"JongWon Kim","ids":[]}],"doi":"10.1109/IROS.2018.8594086"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206281","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Bio-inspired underwater robots have several benefits compared to traditional underwater vehicles such as agility, efficiency, and environmentally friendly body. However, bio-inspired underwater robots developed so far have a single swimming mode, which may limit their capability to perform different tasks. This paper presents a re-configurable bio-inspired underwater robot that can change the morphology to enable multiple swimming modes: octopus-mode and fish-mode. The robot is 60 cm long and 50 cm wide, weighing 2.1 kg, and consists of a re-configurable body and 8 compliant arms that are actuated independently by waterproof servomotors. In the robot, the octopus-mode is expected to perform unique tasks such as object manipulation and ground locomotion as demonstrated in literature, while the fish-mode is promising to swim faster and efficiently to travel long distance. With this platform, we investigate the effectiveness of adaptive morphology in bio-inspired underwater robots. For this purpose, we evaluated the robot in terms of the cost of transport and the swimming efficiency of both the morphologies. The fish-mode exhibited a lower cost of transport of 2.2 and higher efficiency of 1.2 % compared to the octopus-mode, illustrating the effect of the multiple swimming modes by adaptive morphology.","inCitations":[],"pmid":"","title":"Development of bio-inspired underwater robot with adaptive morphology capable of multiple swimming modes","journalPages":"4197-4202","s2PdfUrl":"","pdfUrls":["https://infoscience.epfl.ch/record/231420/files/Paper_for_IROS_F.pdf","https://doi.org/10.1109/IROS.2017.8206281"],"entities":["Adaptive grammar","British Informatics Olympiad","Coat of arms","Galaxy morphological classification","Mathematical morphology","Robot"],"journalVolume":"","outCitations":["327f4ab07d7892ec8b5c707aa496d7266fc23296","273b5753803417e548e41b05df9c2cec77cd108a","8256b4f1b3021f60faacafbbe1ff6de7d85d8acf","0ee48b0d063bf2bb2650bee104e3f6e6e5bb8614","e8c5dd6cc7fff6920916f29479069f1fc8123f94","569a19c78e13ca4af6ebd488625e88579ff1873f","e251b655573bfb8f20dd720fe164c94855309c9f","aca36d7597ee210949a6f719daeb006865be7a27","5c25dfa7a7eab1962490d550ffd962edfa1e8c4f","05fa34bc16e9f5f2c2a0c2753319ffd2453d39cc","9149483019b47ef01ef51fba234be19e59161702","7ce5b5f1bf9c7dd84f3fc4c6ff0ec377182abcde"],"id":"f5367815d95deb4969fb7787765b3475f17b942d","s2Url":"https://semanticscholar.org/paper/f5367815d95deb4969fb7787765b3475f17b942d","authors":[{"name":"Thibaut Paschal","ids":["30614000"]},{"name":"Jun Shintake","ids":["2365583"]},{"name":"Stefano Mintchev","ids":["2425039"]},{"name":"Dario Floreano","ids":["1742820"]}],"doi":"10.1109/IROS.2017.8206281"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593590","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Sensed information greatly helps a robot to adjust its motion or modulate the locomotory behavior. While many sensing components have been developed for macroscale robots, such off-the-shelf sensors are hardly integrated with a mesoscale (i.e., 0.1 mm to 10 mm) robot due to the size limitation. In this work, we propose a Compliant Mechanosensory Composite (CMC) to fabricate a small compliant mechanism with embedded sensing ability. As the first demonstration of CMC, we directly print a conductive polymer PEDOT:PSS onto the flexible joint of a compliant mechanism to sense the motion of the flexible joint itself. Owing to the variation of electric contact resistance (ECR) upon bending, the CMC could estimate its bending angle. The performance of the CMC was verified by analyzing the cyclic bending, transient and stationary response. Overall, a sparsely printed serpentine pattern with thicker line exhibited consistent response without a noticeable hysteresis. To demonstrate the applicability of the CMC process, a small gripper actuated by a SMA (shape memory alloy) coil was fabricated, and its motion was successfully measured using the embedded sensors. We expect the proposed CMC will enable a small robot to become sensible at its self motion, external load, and physical contacts in future.","inCitations":[],"pmid":"","title":"Design of Compliant Mechanosensory Composite (CMC) and its Application Toward the Sensible Mesoscale Robotics","journalPages":"1470-1475","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593590"],"entities":[],"journalVolume":"","outCitations":[],"id":"35eea32102199096d08b9dd56d21c5765c924839","s2Url":"https://semanticscholar.org/paper/35eea32102199096d08b9dd56d21c5765c924839","authors":[{"name":"Bokeon Kwak","ids":[]},{"name":"Joonbum Bae","ids":[]}],"doi":"10.1109/IROS.2018.8593590"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206119","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this work, we propose a search-based planning method to compute dynamically feasible trajectories for a quadrotor flying in an obstacle-cluttered environment. Our approach searches for smooth, minimum-time trajectories by exploring the map using a set of short-duration motion primitives. The primitives are generated by solving an optimal control problem and induce a finite lattice discretization on the state space which can be explored using a graph-search algorithm. The proposed approach is able to generate resolution-complete (i.e., optimal in the discretized space), safe, dynamically feasibility trajectories efficiently by exploiting the explicit solution of a Linear Quadratic Minimum Time problem. It does not assume a hovering initial condition and, hence, is suitable for fast online re-planning while the robot is moving. Quadrotor navigation with online re-planning is demonstrated using the proposed approach in simulation and physical experiments and comparisons with trajectory generation based on state-of-art quadratic programming are presented.","inCitations":["54df9c1fae567370e71e34b51a90b0e9a747b56c","2fca64bfc89d18b0e20cf4ea8cf2dd3952958618","6a3bfe11d8486a438fb765a6bac8b076beb1edb8","1f8b463196b7b994604d02d9b7b2802cf2256ad9","eee4ed5e3fa6295c29ffad4ba0e0bbb5df63bb2c","ed18ea422cd8c049b772c63246f74bdb8158d9a7","94d6c3aafa73b1c36489441bf8ed183da4ed60e3"],"pmid":"","title":"Search-based motion planning for quadrotors using linear quadratic minimum time control","journalPages":"2872-2879","s2PdfUrl":"","pdfUrls":["https://natanaso.github.io/ref/Liu_DynamicTrajectoryPlanning_IROS17.pdf","https://arxiv.org/pdf/1709.05401v1.pdf","http://arxiv.org/abs/1709.05401","https://doi.org/10.1109/IROS.2017.8206119"],"entities":["Discretization","Experiment","Global Positioning System","Graph traversal","Initial condition","Motion planning","Optimal control","Quadratic programming","Robot","Sampling (signal processing)","Search algorithm","Simulation","State space","The Matrix"],"journalVolume":"","outCitations":["d7eab36e58d5450d8b150c478ccf9597bca84d60","16cb057888f1ddb7a3bbeb4737c07513f55c2759","d46a3efd1b85c11cc9376fc8b48388eae5221d2b","65d297d4d8f7316392fa8cb9750495e5566a6595","4ebd9474f18eb7e7db47cbf1cf3b175367f557e2","088e21d6931729e005e6a622c6a92695e3c9dce8","9d1f17baafa1bf923839a53a3cbc83da7e25186a","995ff74cd998396d083f41678d8232b8fed27f0c","9fc479131d651c856c9c3d7c0dd8c79277eb66b2","d967d9550f831a8b3f5cb00f8835a4c866da60ad","373471553fadd638de24fe73bb286cfe06a940e2","114465c1a8d507ce729718c3b03885fce151fc20","2e268b70c7dcae58de2c8ff7bed1e58a5e58109a","5dd64a9f3293aedccf356419fa439338f7b0e27d","1e0fa5563ee08f5c54befe03420bc83dc0516679","421f4fec1d1b2d4947de2b5eab681ae20d44dbd8","80c3f33f08c56789f9aa9050e5b8ddadec52a2e6","303b2b5196b76a6e56f2375b27a5d476dff6cb03","e7bb90ab20b1d438f13e703d35112aa3ad2d5bc0","9dfd9554e948b95cc92a64f4d16c3369cdde82de","2376078d13761387cabb933798b93a706c2ea7ef","6facbe854c57343710d7a4de32cb8ab8d7b1c951","9f85c8f83c835b997f4c613525836f6b53cfdb7f","661e755613b7abf6b2252ae530285629f888a70d","aa8a324cb165e6b9b6fb9929e96b73693e255893","23f7df66eb3220c1866e7da33071a908646fdcd5"],"id":"a6da050db876d1a99abd98a1cd72378dd85c2a7f","s2Url":"https://semanticscholar.org/paper/a6da050db876d1a99abd98a1cd72378dd85c2a7f","authors":[{"name":"Sikang Liu","ids":["1868543"]},{"name":"Nikolay Atanasov","ids":["50365495"]},{"name":"Kartik Mohta","ids":["1899477"]},{"name":"Vijay Kumar","ids":["39806219"]}],"doi":"10.1109/IROS.2017.8206119"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545271","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Recently, facial attribute classification (FAC) has attracted significant attention in the computer vision community. Great progress has been made along with the availability of challenging FAC datasets. However, conventional FAC methods usually firstly pre-process the input images (i.e., perform face detection and alignment) and then predict facial attributes. These methods ignore the inherent dependencies among these tasks (i.e., face detection, facial landmark localization and FAC). Moreover, some methods using convolutional neural network are trained based on the fixed loss weights without considering the differences between facial attributes. In order to address the above problems, we propose a novel multi-task learning of cascaded convolutional neural network method, termed MCFA, for predicting multiple facial attributes simultaneously. Specifically, the proposed method takes advantage of three cascaded sub-networks (i.e., S_Net, M_Net and L_Net corresponding to the neural networks under different scales) to jointly train multiple tasks in a coarse-to-fine manner, which can achieve end-to-end optimization. Furthermore, the proposed method automatically assigns the loss weight to each facial attribute based on a novel dynamic weighting scheme, thus making the proposed method concentrate on predicting the more difficult facial attributes. Experimental results show that the proposed method outperforms several state-of-the-art FAC methods on the challenging CelebA and LFWA datasets.","inCitations":[],"pmid":"","title":"Multi-task Learning of Cascaded CNN for Facial Attribute Classification","journalPages":"2069-2074","s2PdfUrl":"","pdfUrls":["https://export.arxiv.org/pdf/1805.01290","https://arxiv.org/pdf/1805.01290v1.pdf","http://arxiv.org/abs/1805.01290","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545271"],"entities":[],"journalVolume":"","outCitations":["3fb26f3abcf0d287243646426cd5ddeee33624d4","759a3b3821d9f0e08e0b0a62c8b693230afc3f8d","6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","4c170a0dcc8de75587dae21ca508dab2f9343974","6baaa8b763cc5553715766e7fbe7abb235fae33c","2bcd9b2b78eb353ea57cf50387083900eae5384a","e988be047b28ba3b2f1e4cdba3e8c94026139fcf","076d3fc800d882445c11b9af466c3af7d2afc64f","19d4855f064f0d53cb851e9342025bd8503922e2","370b5757a5379b15e30d619e4d3fb9e8e13f3256","7a3d46f32f680144fd2ba261681b43b86b702b85","24205a60cbf1cc12d7e0a9d44ed3c2ea64ed7852","1d696a1beb42515ab16f3a9f6f72584a41492a03","41951953579a0e3620f0235e5fcb80b930e6eee3","3312eb79e025b885afe986be8189446ba356a507","42e3dac0df30d754c7c7dab9e1bb94990034a90d","061356704ec86334dbbc073985375fe13cd39088","0ad4a814b30e096ad0e027e458981f812c835aa0","614a7c42aae8946c7ad4c36b53290860f6256441","3958db5769c927cfc2a9e4d1ee33ecfba86fe054","87147418f863e3d8ff8c97db0b42695a1c28195b"],"id":"b6fe64f953c5e0b8776bd4e0e1a2fb29d8e63294","s2Url":"https://semanticscholar.org/paper/b6fe64f953c5e0b8776bd4e0e1a2fb29d8e63294","authors":[{"name":"Ni Zhuang","ids":["41034942"]},{"name":"Yan Yan","ids":["40461734"]},{"name":"Si Chen","ids":["47336404"]},{"name":"Hanzi Wang","ids":["37414077"]}],"doi":"10.1109/ICPR.2018.8545271"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593868","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper proposes a novel aerial manipulator with tandem ducted fans, which takes both trafficability and effective loading into account. The aerial manipulator is particularly suitable for grasping in complex and narrow environment, in which traditional multi-rotor and helicopter would be inaccessible. The comprehensive integrated dynamic model is established by taking the aerial vehicle dynamics and manipulator dynamics as a whole. On this basis, a multilayer composite controller with feedforward compensation is designed, considering the mutual reactive influence between the aerial vehicle and the manipulator to improve the stability of the system under the motion of the manipulator. The simulation and actual flight tests verify the effectiveness of the design and show good stability and tracking performance of the system.","inCitations":[],"pmid":"","title":"Design and Implementation of a Novel Aerial Manipulator with Tandem Ducted Fans","journalPages":"4210-4217","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593868"],"entities":[],"journalVolume":"","outCitations":[],"id":"90d77551c80272222bdbc0c709ea2dd421fa9d78","s2Url":"https://semanticscholar.org/paper/90d77551c80272222bdbc0c709ea2dd421fa9d78","authors":[{"name":"Yibo Zhang","ids":[]},{"name":"Changle Xiang","ids":[]},{"name":"Bin Xu","ids":[]},{"name":"Yang Wang","ids":[]},{"name":"Xiaoliang Wang","ids":[]}],"doi":"10.1109/IROS.2018.8593868"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594233","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a novel agricultural robot for greenhouse applications. In many greenhouses, including the greenhouse used in this work, sets of pipes run along the floor between plant rows. These pipes are components of the greenhouse heating system, and doubles as rails for trolleys used by workers. A flat surface separates the start of each rail set at the greenhouse headland. If a robot is to autonomously drive along plant rows, and also be able to move from one set of rails to the next, it must be able to locomote both on rails and on flat surfaces. This puts requirements on mechanical design and navigation, as the robot must cope with two very different operational environments. The robot presented in this paper has been designed to overcome these challenges and allows for autonomous operation both in open environments and on rails by using only low-cost sensors. The robot is assembled using a modular system created by the authors and tested in a greenhouse during ordinary operation. Using the robot, we map the environment and automatically determine the starting point of each rail in the map. We also show how we are able to identify rails and estimate the robots pose relative to theses using only a low-cost 3D camera. When a rail is located, the robot makes the transition from floor to rail and travels along the row of plants before it moves to the next rail set which it has identified in the map. The robot is used for UV treatment of cucumber plants.","inCitations":[],"pmid":"","title":"A Novel Autonomous Robot for Greenhouse Applications","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594233"],"entities":[],"journalVolume":"","outCitations":[],"id":"d79910aba01e8f40375d3f4a2d20b32d857f117b","s2Url":"https://semanticscholar.org/paper/d79910aba01e8f40375d3f4a2d20b32d857f117b","authors":[{"name":"Lars Grimstad","ids":[]},{"name":"Remy Zakaria","ids":[]},{"name":"Tuan-Dung Le","ids":[]},{"name":"Pål Johan From","ids":[]}],"doi":"10.1109/IROS.2018.8594233"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594289","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In a human-robot interaction, a robot needs to move to a position where the robot can obtain high reliability data of people, such as positions, postures, and voice. This is because the human recognition reliability depends on the positional relation between the people and the robot. In addition, the robot should choose the sensor data which is necessary to perform the interaction task. Therefore, it is necessary to navigate the robot to the position to obtain the data for initiation of the interaction task. Accordingly, we need to design a path-planning method considering sensor characteristics, human recognition reliability, and task contents. Although previous studies proposed path-planning methods using an interaction potential considering sensor characteristics, they did not consider the task contents and the human recognition reliability, which are important for practical application and did not applied to interaction with multiple people. Consequently, we present a path-planning method considering the task contents and the human recognition reliability using multimodal potential field integrating these information. We verified effectiveness of the path-planning method for interaction with multiple people.","inCitations":[],"pmid":"","title":"Autonomous Navigation Using Multimodal Potential Field to Initiate Interaction with Multiple People","journalPages":"7654-7659","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594289"],"entities":[],"journalVolume":"","outCitations":[],"id":"74cf4df20e20bc3e2de5745884d4e817b2041535","s2Url":"https://semanticscholar.org/paper/74cf4df20e20bc3e2de5745884d4e817b2041535","authors":[{"name":"Yosuke Kawasaki","ids":[]},{"name":"Ayanori Yorozu","ids":[]},{"name":"Masaki Takahashi","ids":[]}],"doi":"10.1109/IROS.2018.8594289"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206426","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"There are many applications where robots have to operate in environments that other agents can change. In such cases, it is desirable for the robot to achieve a given high-level task despite interference. Ideally, the robot must decide its next action as it observes the changes in the world, i.e. act reactively. In this paper, we consider a reactive planning problem for finite robotic tasks with resource constraints. The task is represented using a temporal logic for finite behaviors and the robot must achieve the task using limited resources under all possible finite sequences of moves of other agents. We present a formulation for this problem and an approach based on quantitative games. The efficacy of the approach is demonstrated through a manipulation case study.","inCitations":["9975c621076256963d8951f511a8c5a4110bcb39","a1da468f0bc6b9133284fdad5857ecaedd756680"],"pmid":"","title":"Reactive synthesis for finite tasks under resource constraints","journalPages":"5326-5332","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206426","http://qav.comlab.ox.ac.uk/papers/hlk+17.pdf","http://www.kavrakilab.org/publications/he-lahijanian2017reactive-manipulation.pdf"],"entities":["Algorithm","CUPS","High- and low-level","Interference (communication)","Motion planning","Reactive planning","Reduction (complexity)","Robot","Run time (program lifecycle phase)","Stacking","Temporal logic"],"journalVolume":"","outCitations":["2387b8880e989197e5d2e6c5642fdde4f3561600","3ed0705ec286bc907abb6dc40484a3f57c0a3978","584e5799b359f41f70fa6f0f7044bf4003bccc6f","1e90953131b400ee78801580df5503fdf763226d","7d70ec460c5a8b4a35ab2cbab0ded3187fc1cc1d","74f7629431d975d0a6981b00f154fc43a21c5598","3b3c6b39831e48ed98995cc987f7ec2e3e39745c","904387be48410f0323c7fa31740151b62ae047f9","07bc31afac569aa07f22cccd1663e92685e78ccc","cb8f9119c2b794834b900e13ead73638d3172ec3","642ef69cae7eb6eff384bae489ddea045e8fee38","27731122173f7cfd97cac49b98bf43b0398c18c2","69b62d416e27b5900e266c4b9da95762c5b66904","790503cb3700cadc4db985b9cacf472d48b81ac5","414dd143e1580d86d94c3a1e5dd1d5019fb76947","2bb5051f5409856f9cea23856d0ccbc7fb83c8e8","5530b9b712016ab4ffe5734f2a3e97bcdedca7d5","48e969504bdebdb403a690e77ef3cc95d1a63499","9ac25e08a3cd81ed1e59bdf7c4edf533fd81af31","bc86106bccbcfdfc23d0e3f5a24066d567856319","b9bc9a32791dba1fc85bb9d4bfb9c52e6f052d2e","5c60def91632b2494bd39b84b22bf925087b898b","6d1e97df31e9a4b0255243d86608c4b7f725133b","246d823764cbbe251e4bd32c878d7048d6da2288","972b7116e75a62730d449feafb017411bbb2fa1f","a3a43efddaaec7fda35e5fc28551e69a9ccafb17"],"id":"250e9be90b7e6edb905829e198b541285e62cb52","s2Url":"https://semanticscholar.org/paper/250e9be90b7e6edb905829e198b541285e62cb52","authors":[{"name":"Keliang He","ids":["47699632"]},{"name":"Morteza Lahijanian","ids":["2766159"]},{"name":"Lydia E. Kavraki","ids":["1780248"]},{"name":"Moshe Y. Vardi","ids":["9083969"]}],"doi":"10.1109/IROS.2017.8206426"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546301","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Person re-identification (Re-ID) is an important technique for video surveillance and security systems. Most existing Re-ID methods assume fixed size of training data and the models have to be re-trained from scratch given newly collected data, which is time-consuming. Accelerating the training speed with ever-increasing data is desired and critical for Re-ID. In this work, we propose to apply incremental learning to address this problem. We build the Re-ID model based on the null Foley-Sammon transform (NFST) method. Our idea is to extract new information from newly-added data and integrate it with the existing NFST trained model by an efficient updating scheme. We derived the incremental learning algorithm for both the non-kernelized and kernelized version of NFST. Extensive experiments have been carried on three public datasets, including VIPeR, PRID2011 and CUHK01. The results show that our proposed method can achieve comparable accuracy to the batch learning method while significantly reduces the computational complexity.","inCitations":[],"pmid":"","title":"Incremental Kernel Null Foley-Sammon Transform for Person Re-identification","journalPages":"1683-1688","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546301"],"entities":[],"journalVolume":"","outCitations":["16c7c31a7553d99f1837fc6e88e77b5ccbb346b8","6273b3491e94ea4dd1ce42b791d77bdc96ee73a8","1a0a06e659eb075d414286d61bd36931770db799","ebde14a5d0137e970bfea350eb0ccc7d4eec2394","d501ddc75b53591ebfb6975ae4b9a8f9db6f4793","fecd60d149118c447e70510d4040eade7082cc57","3f45d73a7b8d10a59a68688c11950e003f4852fc","1b2f7fed4632d37d535fa52cb1c79ecb9ab633d8","0dd1bd231cd35db5296a31a51f0fabd5b0766978","36cbcd70af6f2fd3e700e0a710acd5f1f6abebcf","7c73b95790d8939146da94ff8c76bf8494e7e428","38b55d95189c5e69cf4ab45098a48fba407609b4","207e0ac5301a3c79af862951b70632ed650f74f7","2a29ad33d8a878554008e3f0cad7d3607451d16f"],"id":"447848afc79a7f78be49450cd330bfdaabdd64d3","s2Url":"https://semanticscholar.org/paper/447848afc79a7f78be49450cd330bfdaabdd64d3","authors":[{"name":"Xinyu Huang","ids":["2257812"]},{"name":"Jiaolong Xu","ids":["2470198"]},{"name":"Gang Guo","ids":["50469536"]}],"doi":"10.1109/ICPR.2018.8546301"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00083","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"We introduce an immersive system prototype that integrates face, gesture and speech recognition techniques to support multi-modal human-computer interaction capability. Embedded in an indoor room setting, a multi-camera system is developed to monitor the user facial behavior, body gesture and spatial location in the room. A server that fuses different sensor inputs in a time-sensitive manner so that our system knows who is doing what at where in real-time. When correlating with speech input, the system can better understand the user intention for interaction purpose. We evaluate the performance of core recognition techniques on both benchmark and selfcollected datasets and demonstrate the benefit of the system in various use cases.","inCitations":["2d36a909f1725378ecaa308ce81fdc8645cddd9a","282c9d8e49b2faafdc778663a82f1c1bb4b36000"],"pmid":"","title":"An Immersive System with Multi-Modal Human-Computer Interaction","journalPages":"517-524","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00083","http://homepages.rpi.edu/~wangk10/papers/Zhao2018a.pdf"],"entities":["Benchmark (computing)","Cognition","Committed information rate","Embedded system","Emotion recognition","Human\u2013computer interaction","Immersive technology","Modal logic","Multi-user","Prototype","Real-time clock","Server (computing)","Speech recognition","Usability testing"],"journalVolume":"","outCitations":["0fdeb6104649af74b1a418bec933366aaa02e332","62feb51dbb8c3a94cbfc91c950f39dc2c7506e1a","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","05d821d27623946a6ed1759945b12b914dab8e2c","636cf802d73421d83d98e84d6db4361b4d36afa2","1d9326f737dbe99f8caedda017fc2a4ebefa9d2f","1a998506899da3bc703455bde45114bd4c228947","0f2fd434ee0cce863517f719702215f6a3b71579","6873a4db9703c9bf38ddabf9abed17ac5b673b59","7fc62b438ca48203c7f48e216dae8633db74d2e8","fd647eb53390261fc279660048d2f2478a3cadd4","50e983fd06143cad9d4ac75bffc2ef67024584f2","5d8094a0f3621d758f74f0c298627ebc8622d38b","f7bd19a763e02ea70980d5429683557a793cec27","4f7a0ab878930183b2d17b0512be4f72cb10b4f6","5ba7042c5220548c9d5636df3cc2c84bb8641e02","80d088324bf9326815b50496fed3cd401572cbff","033fb6c30817e36dce787928cd821d1e6adaad8d","3a97662c4e239f159397d4ee8966ed5b1078311c","7a97de9460d679efa5a5b4c6f0b0a5ef68b56b3b","a6f1dfcc44277d4cfd8507284d994c9283dc3a2f","36c46078f4dd07457d32c89cccd6132aaf24b1ac","341323d36b932b099b787ec5d825526a0509a054","146016dd81a9de199b16857f0aed3b6b334dc24d","ec37378bf8634383943eaeb9513e1ac54a926939","2485c98aa44131d1a2f7d1355b1e372f2bb148ad","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","03f98c175b4230960ac347b1100fbfc10c100d0c"],"id":"7fbd2cf032ca4f390f1699ec6743f55b60e2bb4a","s2Url":"https://semanticscholar.org/paper/7fbd2cf032ca4f390f1699ec6743f55b60e2bb4a","authors":[{"name":"Rui Zhao","ids":["49832912"]},{"name":"Kang Wang","ids":["1771700"]},{"name":"Rahul R. Divekar","ids":["38685234"]},{"name":"Robert Rouhani","ids":["46239229"]},{"name":"Hui Su","ids":["46444371"]},{"name":"Qiang Ji","ids":["1726583"]}],"doi":"10.1109/FG.2018.00083"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206217","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present a novel method for synthesizing collision-free, dynamic locomotion behaviors for legged robots, including jumping, going down a very steep slope, or recovering from a push using the arms of the robot. The approach is automatic and generic: non-gaited motions, comprising arbitrary contact postures can be generated along any environment. At the core of our framework is a new steering method that generates trajectories connecting two states of the robot. These trajectories account for the state-dependent, centroidal dynamic constraints inherent to legged robots. The method, of low dimension, formulated as a Linear Program, is really efficient to compute, and can find an application in various problems related to legged locomotion. By incorporating this steering method into an existing sampling-based contact planner, we propose the first kinodynamic contact planner for legged robots.","inCitations":["b8ed14b7aad60034cf64b41aee189b2aa2f7193a","a13184735787a82aeaa85ad28230da4e9e918ef3"],"pmid":"","title":"A kinodynamic steering-method for legged multi-contact locomotion","journalPages":"3701-3707","s2PdfUrl":"","pdfUrls":["https://hal.laas.fr/hal-01486933v2/document","https://hal.laas.fr/hal-01486933/file/A%20Kinodynamic%20steering-method%20for%20legged%20multi-contact%20locomotion.pdf","https://doi.org/10.1109/IROS.2017.8206217"],"entities":[],"journalVolume":"","outCitations":["20ffe2e75e02ad3be04aa6735a257b248cb15ab1","16ff7d5ce93075750630c0dc14d0302837a66f68","6f09af195f4846e9aedbb18ccf995a68aca2a8fa","8fc8eb4ad8b9b69d270ff15eff80ab657823f15e","a13d83e348f382b8fa659d814becc185dbd327c5","72e58c0583689e6644ec589a119920297f40b2cd","03aa21a8b2f0b4855c03471ed2336c33f53207b7","6ff182dc67f73166ae24092fbd2feb873bdce452","9301198e0edf4bd27cad39fd50c0a6e6e2e0d825","daa50f6ab4d25c74bf0e228e83ab9e643c9e02a9","214e86899cc5a42fb5e60e7d5865c48957dd33c2","088e21d6931729e005e6a622c6a92695e3c9dce8","6c267aafaceb9de1f1af22f6f566ba1d897054e4","3186f82000e19c76c114dd4a3f764389705bb5c7","384f4022ad4981e0d676420f53f4509356a62aa6","3b979cca666dcb0d7f2cfb49223227772458c38b","13889d8bd7e46bea71c0cabac4417b41b28ed0c3","87a48fab7dc3e100061fb4123899e112fef39cb1","3f58960606d664e0ad8013a6d57e5a88263bb2e8","b9bc9a32791dba1fc85bb9d4bfb9c52e6f052d2e","5995c16f6d7eea8dcb5ebaca5599c5a6f51513f3","387e0d200604c3f05306b28dcd59b4d9b46ec645","2b77a09a1ea6aab696e11940401a514b5590bb18","31716c2bf0184eba27d87fb56fdbc447e3b19fd5","1942d31b5277efe0174ddde117d99c2f87691a71","c98f5b4a7dc8e4c0601da44bf36ddf4e8f562564"],"id":"8fe94bfc708d3cedcc9d6db86daf40f78ca7796b","s2Url":"https://semanticscholar.org/paper/8fe94bfc708d3cedcc9d6db86daf40f78ca7796b","authors":[{"name":"Pierre Fernbach","ids":["3493353"]},{"name":"Steve Tonneau","ids":["3084582"]},{"name":"Andrea Del Prete","ids":["3153720"]},{"name":"Michel Taïx","ids":["1725117"]}],"doi":"10.1109/IROS.2017.8206217"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545627","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The ultimate goal of this paper is to develop a novel personalized comprehensive computer aided diagnostic (CAD) system for precise diagnosis of autism spectrum disorder (ASD) based on the 3D shape analysis of the cerebral cortex (Cx), To achieve the main goal of the proposed system, we used structural MRI modality (sMRI) to be able to extract the shape features of the brain cortex. After segmenting the brain cortex from sMRI, we used a spherical harmonics analysis to measure the surface complexity, in addition to studying surface curvatures. Finally, a multi-stage deep network based on several autoencoders and softmax classifiers is constructed to provide the final global diagnosis. The presented CAD system was tested on several datasets, achieving an average accuracy of 92.15%. In addition to its global diagnostic accuracy, the local diagnostic accuracies of the most significant areas also demonstrated the ability of the proposed system to construct very promising local maps of ASD-related brain abnormalities, which can be considered an important step towards personalized medicine for autistic individuals.","inCitations":[],"pmid":"","title":"Towards Personalized Autism Diagnosis: Promising Results","journalPages":"3862-3867","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545627"],"entities":[],"journalVolume":"","outCitations":["f0851ad3da49f6f4d6bffff2614d928a889e577d","4f02ae98f7e69880099cdd4f3f6625c1c727ddfc","46ca9b8fbab737dbefa3a17953e4475dad5b9a9d","6fc7cf8e31c7563e29dfef72610b58423ca0c5b9","ac1742df9b68da295dd8ebdb307c66630190f651","270a1476b656d0f63d6e48ae39bcc87ec5de10d0","60033741fb13c1444131501def44dec942c4c969","8953465165b369a0a356d1707611ac00cec12f4a","26a02d4dc3ad3263a3f1b9284468f473011a4339","2e5594825e96fdd9b021fa05b7943b313e2fb442","0a0afaf4b0ae98415f7c92df2110a2c77afa5efc","01f5d179bd01181f4c804cbeb46fd7435ce03959","97db6c2e4192dcd9222d46a671a709d58de4c2d7","a209af0611a09ee02a90014ae857f6d9e2ea0022","470b89e2c5248eb58e09129aa9b4d8bc77497e7e"],"id":"16438c512dd8ff3112f8e2347151b70db7b66ab3","s2Url":"https://semanticscholar.org/paper/16438c512dd8ff3112f8e2347151b70db7b66ab3","authors":[{"name":"Y. Elnakieb","ids":["2927008"]},{"name":"Matthew Nitzken","ids":["3346999"]},{"name":"Ahmed Shalaby","ids":["2239392"]},{"name":"Omar Dekhil","ids":["46190759"]},{"name":"Ali M. Mahmoud","ids":["50534963"]},{"name":"Andy Switala","ids":["40304674"]},{"name":"Adel Said Elmaghraby","ids":["1688139"]},{"name":"Robert Keynton","ids":["40551066"]},{"name":"Mohammed Ghazal","ids":["2452259"]},{"name":"Ashraf Khalil","ids":["47178308"]},{"name":"Gregory Barnes","ids":["32817842"]},{"name":"Ayman El-Baz","ids":["2919029"]}],"doi":"10.1109/ICPR.2018.8545627"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202276","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Powered ankle-foot prostheses must provide high power and torque while satisfying stringent size and weight requirements. Previous works have focused on improving the prosthesis torque/weight ratio with novel actuation systems such as series and parallel elastic actuators, clutchable leverages, and pneumatic artificial muscles. In this paper, we propose an alternative design approach to minimize the prosthesis size and weight, while improving torque generation and electrical efficiency, based on a polycentric kinematic chain. The proposed approach is implemented on a novel powered polycentric ankle prosthesis (p2Ankle). Kinematics and mechatronic design are presented together with bench-top testing. Preliminary validation in standing and walking is conducted with an able-bodied user using a bypass orthosis.","inCitations":["512aae7b0cfa1d48ba66889b507f4d6c9c2d4c80","bbc70c058e00fb5d1f581290d5998d8e053b0e20","f8bf1e0f4c29fcc2bdfc51e0723b6d4dc954f21f"],"pmid":"","title":"Design, development, and bench-top testing of a powered polycentric ankle prosthesis","journalPages":"1064-1069","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202276"],"entities":["Kinematic chain","Mechatronics","Pneumatic artificial muscles","Requirement"],"journalVolume":"","outCitations":["bb707a5d349c523da59ef8143f196e7b3e831869","ec65abe384449da185af78d4a2d7e69857991968","dedb7551c3dff33bf7aac5ddb0de50cac2a2c900","ce54cbde87a759b466d5122b45b6e9ede330a1fe","27045de487a920fa91960116d6c841038839c398","3dc2c6e88cbd9be57160fe386cb261afc746783c","9b1be615c1146108078c6772b7e7f463b27e6792","eef339857bf55b9e07294a865db6ef9d160c6a16","04ae6cde070231036e5993ed10e30add1f0ff463","fa2182fa00b6802e3f4523fb45f8c0b024bd00ad","526cefe13fb823721f898d4ab0f24f5051394e60","952af62cc0d42d6a457d3e1f2b092c9d230044c0","023cb49da339109b9e58ff54d017f660b35e5dab","3354893945331a22fc099fe41e7afe24cbc58ca0","eafb0cddc12cf5d48ea7666e8aa2c21172cabf85","4e982dfdb4976fcbd4bc75acaf11cd36a4761cd5","7651a4cdfcec956752b2ced3169bcd2e38fb56a7","78f6963e6ee5b3800ba82bd2141ec53e8b9f2769"],"id":"9bde277b6008deb8d26423a3b773fe807dfc4a8b","s2Url":"https://semanticscholar.org/paper/9bde277b6008deb8d26423a3b773fe807dfc4a8b","authors":[{"name":"Marco Cempini","ids":["1712856"]},{"name":"Levi J. Hargrove","ids":["1682249"]},{"name":"Tommaso Lenzi","ids":["1682634"]}],"doi":"10.1109/IROS.2017.8202276"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206522","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, we address the problem of using a camera with limited field of view for controlling the motion of a quadrotor in aggressive flight regimes. We present a minimum time trajectory planning method that guarantees visibility of the image features while allowing the robot to undertake aggressive motions for which the usual near-hovering assumption is violated. We exploit differential flatness and B-Splines to parametrize the system trajectories in terms of a finite number of control points, which can then be optimized by Sequential Quadratic Programming (SQP). The control strategy is similar to a Receding Horizon Control (RHC) approach for modifying online the reference trajectory in order to account for noise, disturbances and any non-modeled effect. The algorithm is validated in a physically realistic simulation environment.","inCitations":["450c037f017abce407cb009dd7ec095fe0d6e643","46dd6703491db81fb1fc8e1f93297dca068ccc43"],"pmid":"","title":"Vision-based minimum-time trajectory generation for a quadrotor UAV","journalPages":"6199-6206","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206522","http://rainbow-doc.irisa.fr/pdf/2017_iros_penin.pdf"],"entities":["Algorithm","B-spline","Control theory","Mathematical optimization","Numerical analysis","On-board data handling","Optimization problem","Recueil des historiens des croisades","Robot","Sequential quadratic programming","Simulation","Unmanned aerial vehicle"],"journalVolume":"","outCitations":["2e83b6f1d6da2694dd029597911599c03b690afc","3a9116b028fe3f2743859a0e1c03ded22442e363","1a0190a9a5bd95604cf0664befed7635c38d6e9d","30bcd4597c6feecfa87c4fd099e5487adb2ebbc0","3dbe952772dd0fde673c5ff9341e7285a9a7b50d","ccfb3ce17d27d239ac39458987f67fcade966b62","38a1cf98161a8bfa1838b90f3e704b7cd77df079","6f5c364b0216175c2e09113684792b3e2c95a651","3ed62e62e6371808010a0a0b989916adc3c6b87c","59b77cbbefac84a6cff25488d1558f62cc2f14c2","3efb75b09e5cde28ae8efc576d0624fe7f404d45","e374bda968b1cd6ade55d1ff4ce916f28e4326f4","8e7e0a6f1a9291bc053f696e862a12fe8d7b6ab9","391474223c3a9d7a83a14701a6831e873f39a53b","6ffd47d2a40cc50d6015e1630147067aab057876","83479d07ae7702ed4e1dff8bc78e7e5a52058362","ab7dd35aa2438d79a316042b4d8b57772e602b0e","09c091c56a217935575371feb5fd57e9e99b2cce","fb451482724037f0d6033e484a13a55b5cc02478","a5293bbe6f5b4d5c9b9a9ce0d7e95d8b7234aa83","087eb40d97dd6bd28444809bc1e4633451a95a8b","0b8cf17ce967094bf1407092e36711d15a25282e","0796433dc34b6ac21c6c2e8a390c72323a3b5d3c","6facbe854c57343710d7a4de32cb8ab8d7b1c951","f0e7f26d9b8b06faa30788cf0105ff19ac19b8b3","17118f86e873d283e844e1d4593e07d0a83a7112","8a4313d27cafbe8f5cd471ffdff6cc08fd09aaab","1457f7dbd7294cea1e7745820c5ed634420f87a8","63558ceaf7e12efb62dd732e15960e2c9d9f179d","7031bd7cbfd63a29489ee2f4409afb1fb0a45537","4babe5b061e02351f920e82d1417e877dca1cb5f","b1e5907533b7bf87f0900e7ed9c0955568c07977"],"id":"be2b9c906e3a1f35418debecc45e6b503c8fbf88","s2Url":"https://semanticscholar.org/paper/be2b9c906e3a1f35418debecc45e6b503c8fbf88","authors":[{"name":"Bryan Penin","ids":["32912400"]},{"name":"Riccardo Spica","ids":["2859420"]},{"name":"Paolo Robuffo Giordano","ids":["2499654"]},{"name":"François Chaumette","ids":["1755969"]}],"doi":"10.1109/IROS.2017.8206522"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594095","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Planning for autonomous driving in complex, urban scenarios requires accurate prediction of the trajectories of surrounding traffic participants. Their future behavior depends on their route intentions, the road-geometry, traffic rules and mutual interaction, resulting in interdependencies between their trajectories. We present a probabilistic prediction framework based on a dynamic Bayesian network, which represents the state of the complete scene including all agents and respects the aforementioned dependencies. We propose Markovian, context-dependent motion models to define the interaction-aware behavior of drivers. At first, the state of the dynamic Bayesian network is estimated over time by tracking the single agents via sequential Monte Carlo inference. Secondly, we perform a probabilistic forward simulation of the network's estimated belief state to generate the different combinatorial scene developments. This provides the corresponding trajectories for the set of possible, future scenes. Our framework can handle various road layouts and number of traffic participants. We evaluate the approach in online simulations and real-world scenarios. It is shown that our interaction-aware prediction outperforms interaction-unaware physics- and map-based approaches.","inCitations":["abcd14caaa2221436abff6da1c6f6cc1d7978058","88babcb7cfa8e46f814c241e441c890285f6f9d4"],"pmid":"","title":"Interaction-Aware Probabilistic Behavior Prediction in Urban Environments","journalPages":"3999-4006","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1804.10467v2.pdf","http://arxiv.org/abs/1804.10467","https://doi.org/10.1109/IROS.2018.8594095"],"entities":[],"journalVolume":"","outCitations":["5d05aad3241323e8da2959b6499c294e2b900798","9ad4026c49580e2336c17482e82c3e8f0c61911c","3fdebce9e1bac5ba22cd669e44ae01875b7524d9","e0bc019fcb363a2e8d03b43b750dc6b9c22bd471","2e97db7ef85cb74526f75bc4b9e52ab3e4e2825d","43dd2df0a123fedb022d353892713cd2eec7a6b4","751f898e3cfb27eeaf42109a38b9029f8c75a3f6","47fa870154c653ba1c05e3487e1911363bfc98de","29ea16c2b4e504ea63a223bf3e1389a8a7daa441","f7d3b7986255f2e5e2402e84d7d7c5e583d7cb05","355590cf8da5dbf597eded1684ae749909f6e987","ec767112ab194d955059edb5718b8ec2a0a2ac1b","5adf34e44d51659edf4437d602aaf22ec0a50d46","0ae705501edfa94f610f10424faa7d4e7615a1f8","4c87cf268bef27a01b34eac7566770102c877cfa","e4c02985a0622e7dda57b07469b8530844304f21","bdff874d3eaa2f74018072f1bb8894024d15beb8","a96b5981c2d747ade22ff1933224dc0e88522b52","a42b08c2cd40780c7198cbe77e7f90284ae1ea1f","c521be082cea91e23ccbd468b661d9ae14753d83","43a9ee217f98f0cfbadff7fc84c6091a70dd45d8"],"id":"ce39cfd41e3f3a76c2264df04672ac316afc87d9","s2Url":"https://semanticscholar.org/paper/ce39cfd41e3f3a76c2264df04672ac316afc87d9","authors":[{"name":"Jens Schulz","ids":["28522963"]},{"name":"Constantin Hubmann","ids":["3408606"]},{"name":"Julian Löchner","ids":["41021710"]},{"name":"Darius Burschka","ids":["1791260"]}],"doi":"10.1109/IROS.2018.8594095"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545141","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"On the off-the-shelf navigational assistance devices, the localization precision is limited to the signal error of global navigation satellite system (GNSS). During travelling outdoors, the inaccurately localization perplexes visually impaired people, especially at key positions, such as gates, bus stations or intersections. The visual localization is a feasible approach to improving the positioning precision of assistive devices. Using multiple image descriptors, the paper proposes a robust and efficient visual localization algorithm, which takes advantage of priori GNSS signals and multi-modal images to achieve the accurate localization of key positions. In the experiments, we implement the approach on the wearable system and test the performance of visual localization under practical scenarios.","inCitations":[],"pmid":"","title":"Visual Localization of Key Positions for Visually Impaired People","journalPages":"2893-2898","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1810.03790v1.pdf","https://export.arxiv.org/pdf/1810.03790","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545141","http://arxiv.org/abs/1810.03790"],"entities":[],"journalVolume":"","outCitations":["80f2545c21324755917e653d2522b75adbebdcee","38dfb367a3358258a289fa6914eef69de652b38c","55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1","cb15303aff027fc53f6044890a1fc8a42d94430e","0f78fcc5f954835e4f3102e6f73d235d07e82e2e","00498a081478924c86597dc5d252562b545c7df7","038f17f45498a3744cab113f91dc05376bf7ad02","fa6cbc948677d29ecce76f1a49cea01a75686619","2ab5cd957f3dddfad9a9e82772794a188d7b2167","65c540d9683b5422bbe6d837ef52d6271c8387a6","978e7f8557774fa1ebd228e181c4c154449423c9","39030337d0c198cf1238759445b8be7a5b067ecd","b6c8e5922026ca3f8b48edcc7046143a905c70a0","8af1af95b6dac2481081e3e84c68ee91fe9d5403","c1e2719a60ce6d811bf9e409da0e04fceae7c5b2","b91180d8853d00e8f2df7ee3532e07d3d0cce2af","5d30efa7eebb98ceaaeda87785e1b782b3d022ca","c47b50de9d709a2fa499178240960d415a9010b8","4bcaff3735371f462c4e185953667f416f223eb9"],"id":"4ac6bf4e987ebc76b769a83e0cd8cd053c617821","s2Url":"https://semanticscholar.org/paper/4ac6bf4e987ebc76b769a83e0cd8cd053c617821","authors":[{"name":"Ruiqi Cheng","ids":["26019609"]},{"name":"Kaiwei Wang","ids":["7200505"]},{"name":"Guangyu Na","ids":["51462720"]},{"name":"Kailun Yang","ids":["8689702"]}],"doi":"10.1109/ICPR.2018.8545141"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594221","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper we present the design of a hybrid robotic arm using soft, inflatable bladders for actuation. Low cost switching valves are used for pressure control, where the valve model is identified experimentally. A model of the robotic arm is derived based on system identification and used to derive a linear quadratic Gaussian controller. A method to solve limitations of the employed switching valves is proposed and experimentally proven to improve tracking performance. The closed loop control performance of the robotic arm is demonstrated by stabilizing a rotational inverted pendulum known as the Furuta pendulum.","inCitations":[],"pmid":"","title":"Design, Modeling and Control of a Soft Robotic Arm","journalPages":"1456-1463","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594221"],"entities":[],"journalVolume":"","outCitations":[],"id":"842fcd089ab9a35f706ec6a8efdc06a5c37fa33a","s2Url":"https://semanticscholar.org/paper/842fcd089ab9a35f706ec6a8efdc06a5c37fa33a","authors":[{"name":"Matthias Hofer","ids":[]},{"name":"Raffaello D'Andrea","ids":[]}],"doi":"10.1109/IROS.2018.8594221"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205950","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In order to be able to perform complex and arduous tasks, stiffness articular identification of industrial robots is a current approach to predict the deflection under static or dynamic loading. Manufacturers propose new features to take the loading into account and a new generation of industrial robot equiped with double encoding systems are proposed. However, current methods brings some drawbacks when the ratio between the stiffness arm and the wrist one is too high. In this paper, we propose a new approach to take this aspect into account by decoupling the arm identification and the wrist one. We compare then our method regarding two current methods and applied it on this new industrial robot. The results highligh the stability and the quality of the stiffness articular estimation with and without activating the double encoding system. On our data, we are able to take into account 84% of the global deflection.","inCitations":[],"pmid":"","title":"New method for decoupling the articular stiffness identification: Application to an industrial robot with double encoding system on its 3 first axis","journalPages":"1478-1483","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205950"],"entities":["ARM architecture","Apache Axis","Coupling (computer programming)","Double encoding","Dynamic loading","Industrial robot","Spatial variability","Stiffness","Workspace"],"journalVolume":"","outCitations":["60bc22cf90c593ccdc78cdeb8bfdf05f885ac8bd","46d6b14d5415208c6e970ce5bc7b0ff4abc14964","f566a736ede160425b507ae98a76622143baf309","85895cae6a7633d6b9b9d88ed5d88b94ce131ca5","644daeaabd9f70e1a815ec9843edd1db054c2907","1d14270c366a1a98672226b2cb32a08857ec5c98"],"id":"40e595a1a1e22eaafd80f3736a5a09d229cfbd06","s2Url":"https://semanticscholar.org/paper/40e595a1a1e22eaafd80f3736a5a09d229cfbd06","authors":[{"name":"Alexandre Ambiehl","ids":["32209818"]},{"name":"Sébastien Garnier","ids":["2609251"]},{"name":"Kévin Subrin","ids":["2916514"]},{"name":"Benoît Furet","ids":["2672126"]}],"doi":"10.1109/IROS.2017.8205950"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545709","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In order to make use of unlabeled data in hyperspectral images (HSIs), a simple but effective semi-supervised learning method based on convolutional neural network (CNN) is proposed for HSIs classification. First, we define a loss function by integrating a clustering loss function for unlabeled data with softmax loss function for labeled data. Here, the labeled features extracted from CNN are not only used for training classifiers, but also providing anchors to initialize a set of clustering centers via K-means method. Then, all data are used to jointly train deep network for HSIs classification. The experimental results demonstrate that our method can achieve comparative results over the traditional supervised learning method based on CNN. Meanwhile, our method has simple network structure and can be easily trained.","inCitations":[],"pmid":"","title":"Semi-Supervised Learning via Convolutional Neural Network for Hyperspectral Image Classification","journalPages":"1-6","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545709"],"entities":["3-D Image","Artificial neural network","Biological Neural Networks","Cluster analysis","Convolutional neural network","Extraction","HTML element","K-means clustering","Loss function","Semi-supervised learning","Semiconductor industry","Softmax function","Supervised learning","Tracer","statistical cluster"],"journalVolume":"","outCitations":["d639fc711f5c22da1995c1bc0f24300aa70bf4ec","e2769447c40c94e0ec7f3ac46ee6d612edd4dbd9","a1af18f07d3e34887ec82d85826371c1ba8017a1","24a759e723ef8f03326c91f671f3996f05bab9da","3e32b4775f3aeff0e1eaf37cc46771376a0c8d08","c5389f0d9751ce9cd39160320ef17ae968a79edf","f9d119346b0773ea83251598fa5305bc75bac8ab","e8e87430f7c76abf80081c73ee3dd95e2770c9d9","44d26f78bcdee04a14896c8c6f3681313402d854","4cf28fc05744d9cfd3206b6b6e8a81c823e740f6","ef8ae1effca9cd45677086034d8c7b06a69c03e5","2badd8953397d757693859918cf9318fe7ec5e3b","3e6551acb2cbe4e83aa5fa18ac15b6401688f5f3","8bcc400da06e3f619c7814f8656886212549da28","407181823b808264e941fb2be6d9b615f5ec7010","64fd6b7139be4b8b104a9aed768deaf62f71f5c0","587bae5778c1fb4a99ccd7c52ddd294352be8737"],"id":"fe0aefc285dccf9c13696fddc86ab7ca07562210","s2Url":"https://semanticscholar.org/paper/fe0aefc285dccf9c13696fddc86ab7ca07562210","authors":[{"name":"Zhigang Ling","ids":["2817492"]},{"name":"Xiuxin Li","ids":["9277406"]},{"name":"Wen Zou","ids":["48564429"]},{"name":"Siyu Guo","ids":["47508392"]}],"doi":"10.1109/ICPR.2018.8545709"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545798","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"This paper presents a novel, yet compact texture descriptor for plant species identification based on bark texture images. Termed Statistical Macro Binary Pattern (SMBP), the descriptor is informative, rotation invariant, and it is designed to encode texture information from a large support area. The main novelty of this approach is the use of statistical description to represent the intensity distribution in the large support area, and an LBP-like encoding scheme to derive a statistical macro pattern by thresholding it against its adaptive statistical prototype. We propose to test three neighborhood sampling schemes according to the angular quantization at each level of the macrostructure. The comprehensive experiments on three challenging bark datasets (BarkTex, Trunk12, AFF) show that our descriptor achieves high and more consistent identification rates when compared with LBP-like texture descriptors.","inCitations":[],"pmid":"","title":"Plant identification from bark: A texture description based on Statistical Macro Binary Pattern","journalPages":"1530-1535","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545798"],"entities":[],"journalVolume":"","outCitations":["23e7d3999b97009f94a972114829e0dab6e494e5","33fad977a6b317cfd6ecd43d978687e0df8a7338","47ab659f74caf27f92526e7c0b42a54e234c0ab1","114d05c0cb8ae34a1c2e851c1a2a1a892c3c6618","2a57cf3e15b20708595737a499a5f6fedc7fac94","a0f0ecdf17284cd3f6f7ce5cb63f99e94f534175","cad40a18165556c45874c1f576568e2468962134","bd8907634d0acac5d5406111dbd9c65f11e73b02","59abbc3175e002b3e6cffdeb8ff934467519ad71","0eac91e81ceffa178702f775370b17594e80a47c","53692881ac5b1c149adf6d22ccb2c414d7fdcf4e","236b7f345639b2c6fa1beb8995cb674cfc1ba434","5a83061de32a3940a96eb3d65f25d005bec8156f","e4c19e4fcd1be1e7003b1c4825502f3292fce514","19f46583bc22935739b5233678182f1953003221","b323aa63247c6a1f3076bee55e49188a09d6c6ab","41927f98e0b21aa50dcb3a7396990fd988dab073","c8ea49c1b96886d02965acbabc25227ce6254422","2a65066f13a62d6f7e551a899d7f8a2fed063765"],"id":"6c5a99bb6c820f374e6009725030a6d3fa8bddf1","s2Url":"https://semanticscholar.org/paper/6c5a99bb6c820f374e6009725030a6d3fa8bddf1","authors":[{"name":"Safia Boudra","ids":["1864751"]},{"name":"Itheri Yahiaoui","ids":["1731057"]},{"name":"Ali Behloul","ids":["2488395"]}],"doi":"10.1109/ICPR.2018.8545798"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546304","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The embedding based framework handles the multiple-instance learning (MIL) via the instance selection and embedding. It is how to select instance prototypes that becomes the main difference between various algorithms. Most current studies depend on single criteria for selecting instance prototypes. In this paper, we adopt two kinds of instance-selection criteria from two different views. For the combination of the two-view criteria, we also present an empirical estimator under which the two criteria compete for the instance selection. Experimental results validate the effectiveness of the proposed empirical estimator based instance-selection method for MIL.","inCitations":[],"pmid":"","title":"Multiple- Instance Learning with Empirical Estimation Guided Instance Selection","journalPages":"770-775","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546304"],"entities":["Embedding","Genetic Selection","Multiple-instance learning","Software prototyping","algorithm"],"journalVolume":"","outCitations":["ea498eac27d6d7e153231e6ecb55ca53143eba7e","4b79c231dd6215fd4237361c956cb215356942fb","0f16f6f478b5c788dce466eb50e36c612273c36e","4d8340eae2c98ab5e0a3b1a7e071a7ddb9106cff","0471834190f14aafc59d92b5b648d11e440b519f","3447fe054f6af70403cfc39b4d21076337a71128","89dd979043cbda9f3964c31acc69873c63040552","247b527f0a071eed4e5c43dda90b52fb01cd3eef","40d7c9c6fa0e0c1073ce422b4dd4a3c8c101e61d","64372501affd8571db20dc606b0146a76c266303","7a8665ada37b8a26f2a68c0b9c3f3ceb1e0f655a","0b104e517e0440e3bdace01b5f6706c5fa944149","05531fda7a82b18c42c7ae9d053349fdb8281b33","0fd1bffb171699a968c700f206665b2f8837d953","5d982c10ba2ad3d5d10986adeb79b5b64b15be32","1c7d38f68fe1150895a186e30b60c02dd89a676a","521768f7772163bc7c57ae2c9855889abb747fbb","4a3684ee0c64d22386a44c0ddfbf8d609cd2de48","3096e5c4b1cc970a5ef79ba30992ba2f6e3f8a41","2d53f86dae40f0995e9950c1aa9d27b584d3544f","8c07b0b03dd6dfb58eaccb5bb11ae44fdef033c1"],"id":"d006694fb3a2dbdb265ca9d5e183a32c36424e14","s2Url":"https://semanticscholar.org/paper/d006694fb3a2dbdb265ca9d5e183a32c36424e14","authors":[{"name":"Liming Yuan","ids":["46762692"]},{"name":"Xianbin Wen","ids":["10207434"]},{"name":"Haixia Xu","ids":["49507207"]},{"name":"Lu Zhao","ids":["30881139"]}],"doi":"10.1109/ICPR.2018.8546304"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206353","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this work a constraint-based control approach is proposed in order to perform a cap rotation task with a dual-arm robot. The method relies on the introduction of a robust specification for the constraint on the interaction force arising during the task, accounting for robot-environment contact model uncertainties, in addition to force measurement noise and surface uncertainties. Experiments have been performed on an ABB dual-arm prototype robot to validate the proposed approach in a cap assembly task, employing a model-based sensorless observer of the interaction forces.","inCitations":[],"pmid":"","title":"Robust constraint-based robot control for bimanual cap rotation","journalPages":"4785-4790","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206353"],"entities":["Experiment","Prototype","Robot control"],"journalVolume":"","outCitations":["147146c93d780e0cb6024ca82043a1cc90ba871a","ecaacd7ff8279dad6214b74250af2ee7b4d2acb7","7a07e4f8f4d8ebfd6a2bd861a0d390189a0fcd61","06ccec88ac72937997a1c3e024de0787806c9032","367fc85c0267516a2a91d8314c1ca50465a8cdac","a430f454666b3f9fb45125228bad846ed926c893","835b79cf461dbed14b893c2a4f82c0d49ab67075","b4ccfecbfd4832a230fec65b4457c782a3867dad","ee01cab880283da071416dd4958d6273940fa56f","e1789dc2ae8859c2f7a31c613c7ff513a59f4bf4","b27c78d9d951107f68e3ace4e7aa7f1c15c03b4a","d5a2ccd6dcd549d4284bd56c67c75d71afac96d1","95ac2dee3497073f6f139ba0f16cf64c5e9d327c","55c833fd0ae94da638c5ee93a838e004b517898f","84f12114e4a5bd48227057b6534ca63c9d183dcb","1c0c239f0d1ae602915dab8b148bfecab4d8019f","9ce3e89c49239401932fe41a9ac6b1d5dc7c1fca","c3cb3614cc0d670d2220db231c752892c5987ef1"],"id":"244f2f71f48e3bbf384ca0b18d06b9db1048f3e7","s2Url":"https://semanticscholar.org/paper/244f2f71f48e3bbf384ca0b18d06b9db1048f3e7","authors":[{"name":"Matteo Parigi Polverini","ids":["2482278"]},{"name":"Andrea Maria Zanchettin","ids":["2650655"]},{"name":"Francesco Incocciati","ids":["31522857"]},{"name":"Paolo Rocco","ids":["7408413"]}],"doi":"10.1109/IROS.2017.8206353"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00091","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Driver fatigue is one of the main causes of road accidents. It is essential to develop a reliable driver drowsiness detection system which can alert drivers without disturbing them and is robust to environmental changes. This paper explores yawning behaviour as a sign of drowsiness in spontaneous expressions of drowsy drivers in simulated driving scenarios. We analyse a labelled dataset of videos of sleep-deprived versus alert drivers and demonstrate the correlation between hand-over-face touches, face occlusions and yawning. We propose that face touches can be used as a novel cue in automated drowsiness detection alongside yawning and eye behaviour. Moreover, we present an automatic approach to detect yawning based on extracting geometric and appearance features of both mouth and eye regions. Our approach successfully detects both hand-covered and uncovered yawns with an accuracy of 95%. Ultimately, our goal is to use these results in designing a hybrid drowsiness-detection system.","inCitations":[],"pmid":"","title":"Analysis of Yawning Behaviour in Spontaneous Expressions of Drowsy Drivers","journalPages":"571-576","s2PdfUrl":"","pdfUrls":["https://www.cl.cam.ac.uk/~mmam3/pub/FG2018-HBU-yawining.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2018.00091"],"entities":["Algorithm","Autonomous car","Autonomous robot","Bonjour Sleep Proxy","Color","Device driver","Machine learning","Multimodal interaction","Nonlinear system","Personalization","Spontaneous order"],"journalVolume":"","outCitations":["df8ca3abc82c74a012782c4252a0cd8663bd6124","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","16b0c152494fc632b1b937e17c9fe7e8e984b99f","6cd05e10be5dfcf863641c03eee1092a1a278be0","16404d38775e2e8ddcd28e2ca6e79cf8a82d0d9f","64def1348746938a3133252c457b23a7b1b11672","57a3692507804ea548474a9e76763cb07ca54e0c","497396fd8872f115476cbc52169fa7b68c8f847d","43a3010ec3890c3b8649aba1419dda46edaacd0b","d312c721bde7b60aafe05c28e7e8d1cf18e3ef80","1be16d8c557b15cdf2db9e7eb4453f2274fd60af","0f071caf0731abc96d882c59c9164ff0f9bea5e3","2fda461869f84a9298a0e93ef280f79b9fb76f94","a94cae786d515d3450d48267e12ca954aab791c4","944d5d5338cf7f723a0b92c3c184e6a1e3f160c1","0e986f51fe45b00633de9fd0c94d082d2be51406","42fc1edfb80e9977e1a60f02bfdec70ed0a9a046","1d2302072cbd836ce43acc7f909e99af64d02775"],"id":"8a8127a06f432982bfb0150df3212f379b36840b","s2Url":"https://semanticscholar.org/paper/8a8127a06f432982bfb0150df3212f379b36840b","authors":[{"name":"Zhuoni Jie","ids":["46232768"]},{"name":"Marwa Mahmoud","ids":["2022940"]},{"name":"Quentin Stafford-Fraser","ids":["2809420"]},{"name":"Peter Robinson","ids":["39626495"]},{"name":"Eduardo Dias","ids":["49981899"]},{"name":"Lee Skrypchuk","ids":["2540001"]}],"doi":"10.1109/FG.2018.00091"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593998","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We propose an end-to-end real time framework to generate high resolution graphics grade textured 3D map of urban environment. The generated detailed map finds its application in the precise localization and navigation of autonomous vehicles. It can also serve as a virtual test bed for various vision and planning algorithms as well as a background map in the computer games. In this paper, we focus on two important issues: (i) incrementally generating a map with coherent 3D surface, in real time and (ii) preserving the quality of color texture. To handle the above issues, firstly, we perform a pose-refinement procedure which leverages camera image information, Delaunay triangulation and existing scan matching techniques to produce high resolution 3D map from the sparse input LIDAR scan. This 3D map is then texturized and accumulated by using a novel technique of ray-filtering which handles occlusion and inconsistencies in pose-refinement. Further, inspired by human fovea, we introduce foveal-processing which significantly reduces the computation time and also assists ray-filtering to maintain consistency in color texture and coherency in 3D surface of the output map. Moreover, we also introduce texture error (TE) and mean texture mapping error (MTME), which provides quantitative measure of texturing and overall quality of the textured maps.","inCitations":[],"pmid":"","title":"Real Time Incremental Foveal Texture Mapping for Autonomous Vehicles","journalPages":"3233-3240","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593998"],"entities":[],"journalVolume":"","outCitations":[],"id":"e8a4e3b3af90c84ebd6c40ad759a80729a81230d","s2Url":"https://semanticscholar.org/paper/e8a4e3b3af90c84ebd6c40ad759a80729a81230d","authors":[{"name":"Ashish Kumar","ids":[]},{"name":"James R. McBride","ids":[]},{"name":"Gaurav Pandey","ids":[]}],"doi":"10.1109/IROS.2018.8593998"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206032","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Autonomous trajectory generation for unmanned aerial vehicles (UAVs) in unknown environments continues to be an important research area as UAVs become more prolific. In this paper, we develop a trajectory generation algorithm for a vehicle in an unknown environment with wind disturbances, that relies only on the vehicle's on-board distance sensors and communication with other vehicles within a finite region to generate a smooth, collision-free trajectory. The proposed trajectory generation algorithm can be used in conjunction with high-level planners and low-level motion controllers, as demonstrated. The algorithm provides guarantees that the trajectory does not violate the vehicle's thrust limitation, sensor constraints, or a user-defined clearance radius around other vehicles and obstacles. Simulation results of two quadrotors moving through an unknown building environment with obstacles demonstrates the trajectory generation performance.","inCitations":["93137ddbff913bc29aaa8155735200d339a6c0ac"],"pmid":"","title":"Reactive trajectory generation in an unknown environment","journalPages":"2151-2157","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1703.00461v2.pdf","http://arxiv.org/abs/1703.00461","https://arxiv.org/pdf/1703.00461v1.pdf","https://doi.org/10.1109/IROS.2017.8206032"],"entities":["Aerial photography","Algorithm","High- and low-level","Motion controller","On-board data handling","Sensor","Simulation","Thrust","Unmanned aerial vehicle"],"journalVolume":"","outCitations":["fcdc2324f80d3db06a41c519a29a6172176fd816","8be4178e8190888092cce982a6be6abb2da0d3e4","0048d85dd2eefdcaf46e64c513f5f845915ee322","b489f555fe6db9a63979e8ba7b7c27cc792d4fb4","a3dde9f67353f7f000011351e757a0aaaf3ec941","617f7926358629afe71a8aea445e8e7bd3d3c51e","30491aa1eba92206914cfddf8e6007badc8120b3","7d0f543a687739a73fed6be22f460e41fb3ac32b","fbb143e5f575407ed468e18b845f4d4c23530f3e","2ecad43e05f326ba2ed847f2afd9540b3f156859","ea10fbe542b8c42396a8251b885c2fd3fa3aee9f","2a30fc213eb36577f7a8db812a138a68025c894a","5b198644773c5c85caf4ce4c375c52f0a95a5b56","0475bf49f8eeaac9675739edfc6e004e10f67a86","7bfef3038424420eccabbe32520d2080aaebbef1","a969d8c9b3e1ddf3ad0dd354356d4b2f5d7f0b18","30e84bd51fceb26b1abcea9a9226e2b1772b01fd","b0d61ba24b83b8d6ea3ec4d5a19ab164996de494","1ac3f218c014650c1a35f8cd3c08e78b443236af","87c3a519a6d95e2a7d20cbb6ae230a41411f4b77","6d6f5c7937a51443ad8aa5d0b440aae2c7c8d850","a52d7c7bba44a91697c7bfd114117ef6af1de7c5","4a18f6232f6d7fb4f3395df95d0dc62e03f9f586"],"id":"751e76ede00cf7e5b5d1d396eed4cd788ca7500d","s2Url":"https://semanticscholar.org/paper/751e76ede00cf7e5b5d1d396eed4cd788ca7500d","authors":[{"name":"Kenan Cole","ids":["40190045"]},{"name":"Adam M. Wickenheiser","ids":["3414275"]}],"doi":"10.1109/IROS.2017.8206032"}
{"doiUrl":"","venue":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","journalName":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","sources":[],"year":2017,"text":"Automatic License Plate Recognition (ALPR) is base of many Intelligent Transformation Systems (ITS) services. Many ALPR systems have usually three steps, License Plate Detection (LPD), character segmentation and character recognition. LPD is the first and main step in ALPR. There are many algorithms for LPD, while detecting a license plate in different conditions is still a complex task. The goal of this paper is proposing an algorithm to extract license plate in different conditions. The proposed approach has three following steps, adaptive morphological closing, local adaptive thresholding and morphological opening. Experimental results using some real dataset show that the detection rate of the proposed approach is higher than some related works. In addition, the computational time of the proposed approach is less than other techniques.","inCitations":["fb5f8b13efa49ccbeb6bd5a01977f74916bebcd8","96cae62e947305b5f88f8699425f8dd4c55ca259"],"pmid":"","title":"License plate detection using adaptive morphological closing and local adaptive thresholding","journalPages":"146-150","s2PdfUrl":"","pdfUrls":[],"entities":["Algorithm","Automatic number plate recognition","Closing (morphology)","Computation","Less Than","Mathematical morphology","Opening (morphology)","Optical character recognition","Sensor","Silo (dataset)","Thresholding (image processing)","Time complexity","biologic segmentation"],"journalVolume":"","outCitations":["bc2bab2c38948caa1a04caa50fdddfbd543bc8ba","f0be501f57e04f1b642b68ebbcccf3fef1aa90e5","514329846d5b8e27e6646b524aa2b60d2bd365ab","25604bdbc092ad67b77024062edb15c0e7b474dc","3d35929ce968b73f45adfb165314d94baa9872a1","a08f7f29d85167a523e8d08501bb6442ed953544","a8c67ceeac5d3f9550a2f3ab2497ab094a4af380","05d3f5f6e29b94dcfbae65e8f193f61440eff570","af4c99c07ca3292ac1ccb71ce7d1d9fc0a7ff2a9","ab5fce8f49b40a5ddbe84409dce2515ac621f3dd","8d74418ec3c4e2ff45b72e723ac0fbe5fcd58620","fd83fb888d887e356669030b93b3a25a27dd74d7","f4719989049f131a0d517b00281938daf1c88504","1963ec5d972319dd9182ce5a0ad85fb1925fc14f"],"id":"17482f01446fba0638e6fd2c8e872ab6c5cbbe99","s2Url":"https://semanticscholar.org/paper/17482f01446fba0638e6fd2c8e872ab6c5cbbe99","authors":[{"name":"Babak Abad Fomani","ids":["21212138"]},{"name":"Asadollah Shahbahrami","ids":["1696247"]}],"doi":""}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594243","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Since the resurgence of CNNs the robotic vision community has developed a range of algorithms that perform classification, semantic segmentation and structure prediction (depths, normals, surface curvature) using neural networks. While some of these models achieve state-of-the art results and super human level performance, deploying these models in a time critical robotic environment remains an ongoing challenge. Real-time frameworks are of paramount importance to build a robotic society where humans and robots integrate seamlessly. To this end, we present a novel real-time structure prediction framework that predicts depth at 30 frames per second on an NVIDIA-TX2. At the time of writing, this is the first piece of work to showcase such a capability on a mobile platform. We also demonstrate with extensive experiments that neural networks with very large model capacities can be leveraged in order to train accurate condensed model architectures in a \u201cfrom teacher to student\u201d style knowledge transfer.","inCitations":["7acd36ba0b59849d49bfc01cef11d18d2bcad76f","12c5179c88f6ebf42a5b965a7440461ff67eaed8"],"pmid":"","title":"CReaM: Condensed Real-time Models for Depth Prediction using Convolutional Neural Networks","journalPages":"540-547","s2PdfUrl":"","pdfUrls":["https://export.arxiv.org/pdf/1807.08931","https://doi.org/10.1109/IROS.2018.8594243","http://arxiv.org/abs/1807.08931","https://arxiv.org/pdf/1807.08931v1.pdf"],"entities":[],"journalVolume":"","outCitations":["84c01c9760cd294718bd7c4b4c93596db1e5e068","026e3363b7f76b51cc711886597a44d5f1fd1de2","a1543975098f8ec14f4402f761eefb473100beee","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","b9b5866e4df383a7fbb780e8f822b905e3fbef6d","af2bd1d50b492d6b862d7d1ff3e3ea94bd16d753","9201bf6f8222c2335913002e13fbac640fc0f4ec","f375bc91a5f7b1f2d36e41841ccc22f202be2dcf","74fc396d0b8ec548d600395182f12c9b06cc84e9","06feba1ffd596b41884cea6e8ef0da89b6dd2233","293b47fc6c1d83e72484debf7b4faadc5d75cce7","daf74c34f7da0695b154f645c8b78a7397a98f16","833fbf0e4be3ba82e7a1efdbc16813ee849d9942","218b4c5f1c14bd8fe57a849490f64a0832434a0c","78331c884611987cf62a2104d52dd3ca9efab88d","3e7ffb5658cf99968633ede18785c5cfdd6aa9eb","53c9d4729c8a40ee32f2452ff6b8023b5de48f0e","501d99e392783e4acafb220136d32ea68a921282","420c46d7cafcb841309f02ad04cf51cb1f190a48","3ba179bceb9692d4d21109d0b87b120195761148","21b58c8aba44c173493e418a797a1f36c6dae8a9","272216c1f097706721096669d85b2843c23fa77d","0d5fa5be4bfe085de8f88dbee1c3b2a6e5ab9ee2","7449cff9a37e1c64080d97b6971f94c29b31fd30","5eb4c55740165defacf08329beaae5314d7fbfe6","3d2126066c6244e05ec4d2631262252f4369d9c1","b16ce4d35c754f41d26cd7237d423763f9509bef","2c03df8b48bf3fa39054345bafabfeff15bfd11d","230ad73e8bd1d3268d56c66a83442d24176b864d","bf5f67ebbe41f2fb1726a7c3c0be707366d5a4fb","63623c63ffddd08d266d884680d3493e8b7705f1","07f77ad9c58b21588a9c6fa79ca7917dd58cca98","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","ce6d34010f04afa4cf3018f51bad8f480ebc759c"],"id":"91cd2360f964406d9ba9ce3ff1b99a9d5d9ce2a7","s2Url":"https://semanticscholar.org/paper/91cd2360f964406d9ba9ce3ff1b99a9d5d9ce2a7","authors":[{"name":"Andrew Spek","ids":["37827889"]},{"name":"Thanuja Dharmasiri","ids":["7206611"]},{"name":"Tom Drummond","ids":["1742942"]}],"doi":"10.1109/IROS.2018.8594243"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545748","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Modern smart grids rely on advanced metering infrastructure (AMI) networks for monitoring and billing purposes. However, such an approach suffers from electricity theft cyberattacks. Different from the existing research that utilizes shallow, static, and customer-specific-based electricity theft detectors, this paper proposes a generalized deep recurrent neural network (RNN)-based electricity theft detector that can effectively thwart these cyberattacks. The proposed model exploits the time series nature of the customers' electricity consumption to implement a gated recurrent unit (GRU)-RNN, hence, improving the detection performance. In addition, the proposed RNN-based detector adopts a random search analysis in its learning stage to appropriately fine-tune its hyper-parameters. Extensive test studies are carried out to investigate the detector's performance using publicly available real data of 107,200 energy consumption days from 200 customers. Simulation results demonstrate the superior performance of the proposed detector compared with state-of-the-art electricity theft detectors.","inCitations":[],"pmid":"","title":"Deep Recurrent Electricity Theft Detection in AMI Networks with Random Tuning of Hyper-parameters","journalPages":"740-745","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1809.01774","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545748","https://arxiv.org/pdf/1809.01774v1.pdf"],"entities":[],"journalVolume":"","outCitations":["9edf1a25ebc182355c5584fb7f5d234e75ccf3d1","2fadc3557956a4ab87cbcac611164faedcffbf24","95120fdbe16f61a3b2aea91dab5846fd693ea6c0","0e67bac2937f5f53f310564efa547efd82c0371d","48234756b7cf798bfeb47328f7c5d597fd4838c2","37c970435423c27979b38f4ff6693497f9c6ec4d","06a9942be8850a40f44a10a537705804cb7e84c2","66abdcb2f348ee88403a5ca32c201aad607dac80","81f40a75802dcff2278f24c22205952b0b330285","017d6ee319f699274dbd15739d599e954ff2f4fa","8a66099266512166d8c2481ddf73a81916d18dec","48352fdf0f1fc622e870d747e08b0e530465e5d3","0cae9c0a6c57e64d2dbdd5c574e435dd33622d96","2f92f9b04a49a75b714db7e35e5dca01b3330274","bc9fad8bbf62591ef5295b778c7d3ab30f21e38a","db0c1d03b24396f0916e56ac78b663f2ca132c64","7acc947a379a98c0281372fc2e3b5cbab3dece94"],"id":"4e1e91f26d5d0d58ef3db9f157837f8e3599f73a","s2Url":"https://semanticscholar.org/paper/4e1e91f26d5d0d58ef3db9f157837f8e3599f73a","authors":[{"name":"Mahmoud Nabil","ids":["2289325"]},{"name":"Muhammad Ismail","ids":["4450578"]},{"name":"Mohamed M. E. A. Mahmoud","ids":["49768284"]},{"name":"Mostafa Shahin","ids":["3004194"]},{"name":"Khalid A. Qaraqe","ids":["1727654"]},{"name":"Erchin Serpedin","ids":["1741609"]}],"doi":"10.1109/ICPR.2018.8545748"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545062","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Background modeling in complex scenes is a challenging problem. In this paper, a novel background subtraction method is proposed to address it. First, the textures are modeled with local compact binary patterns (LCBP), which have excellent robustness, strong discriminative power, and fast computation speed. To make LCBP more effective to appearance changes in complex scenarios, spatiotemporal local compact binary patterns (STLCBP) are then considered in which spatial texture information and temporal motion information are combined together. Multiple color spaces are also presented to separate foreground pixels more accurately from the background. To our knowledge, this is the first time that LCBP have been used for background modeling. Extensive experimental results on a widely used dataset clearly show that the proposed method outperforms other state-of-the-art methods and works effectively in complex scenes.","inCitations":[],"pmid":"","title":"Local Compact Binary Patterns for Background Subtraction in Complex Scenes","journalPages":"1518-1523","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545062"],"entities":[],"journalVolume":"","outCitations":["1790ae874429c5e033677c358cb43a77e4486f55","49dab04cc871add0c374c4826ee56b34e76d3e03","4ea5d320fa35ccc669a328ca0fc3a7ba842083d3","4135919550e98b53c145036cb602101105812b21","455686876c21a7de450543f20482a729ef72bb56","5fca285e4e252b99f8bd0a8489c984280e78a976","0830921178cc0912c3cd78e9f339cddc402bcda3","d322f2d31ab49c25298ad2a28906483e59c74ad0","56b1eee82a51ce17d72a91b5876a3281418679cc","e73eccd6dd998d7c2a1068243b6938eb5a525e86","2a29ad33d8a878554008e3f0cad7d3607451d16f","9fc993aeb0a007ccfaca369a9a8c0ccf7697261d","e7f02a44f7ad34195cb327561e89f9c00500a9aa","c39cd8b0fac1e78e71674edaeceb3d9f2c974913","5be53a4a5bfc75dfea2a1aed962098473126e9a5","22a13d517eb1b79407bd43763ff7f0144b5ccc8a","4f2d62eaf7559b91b97bab3076fcd5f306da57f2","4a101b7dd12ba4cad578bd649b68359877d8ad87","6502cf30c088c6c7c4b2a05b7777b032c9dde7cd","51e322b8f0778ecff5286a8f08dfe73cb628b9dc","40b0e1e852084a79ba77ee1c2f392cfdde2eebee","5507060ad65205536d1361b93b45278955792707"],"id":"b67ded1b9b6dae57109238ba7ab94377ec621ca2","s2Url":"https://semanticscholar.org/paper/b67ded1b9b6dae57109238ba7ab94377ec621ca2","authors":[{"name":"Wei He","ids":["50101686"]},{"name":"Yongkwan Kim","ids":["2785722"]},{"name":"Jianhui Wu","ids":["40455173"]},{"name":"Guoyun Zhang","ids":["9070416"]},{"name":"Qi Qi","ids":["40806187"]},{"name":"Longyuan Guo","ids":["2256936"]},{"name":"Bing Tu","ids":["46770258"]},{"name":"Feng Huang","ids":["50187811"]}],"doi":"10.1109/ICPR.2018.8545062"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.69","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Automatic understanding and analysis of groups has attracted increasing attention in the vision and multimedia communities in recent years. However, little attention has been paid to the automatic analysis of group membership \u2013 i.e., recognizing which group the individual in question is part of. This paper presents a novel two-phase Support Vector Machine (SVM) based specific recognition model that is learned using an optimized generic recognition model. We conduct a set of experiments using a database collected to study group analysis from multimodal cues while each group (i.e., four participants together) were watching a number of long movie segments. Our experimental results show that the proposed specific recognition model (52%) outperforms the generic recognition model trained across all different videos (35%) and the independent recognition model trained directly on each specific video (33%) using linear SVM.","inCitations":["fcc82154067dfe778423c2df4ed69f0bec6e1534"],"pmid":"","title":"Generic to Specific Recognition Models for Membership Analysis in Group Videos","journalPages":"512-517","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.69","https://www.cl.cam.ac.uk/~hg410/MouEtAl_FG17_Camera.pdf"],"entities":["Database","Experiment","Mathematical optimization","Multimodal interaction","Pattern recognition","Support vector machine","Two-phase locking"],"journalVolume":"","outCitations":["34022637860443c052375c45c4f700afcb438cd0","d660abfbe5f84c1c49f1e7174eb166b8b23e53c4","7090fbafa970a7b5c87f388c86ae16d046395677","1ab881ec87167af9071b2ad8ff6d4ce3eee38477","d166c56ed4031e217cc4c623dd9850905244b577","384bb3944abe9441dcd2cede5e7cd7353e9ee5f7","8d95317d0e366cecae1dd3f7c1ba69fe3fc4a8e0","27a299b834a18e45d73e0bf784bbb5b304c197b3","ecfeeebbec8391ebc6e0803334699ff801564282","13b1b18b9cfa6c8c44addb9a81fe10b0e89db32a","4595838980348561522c9a91807728b73c61fc1f","047b2e2e987348a4e74bf103564c97b0725bf5b1","4f3a7d5615cb4154dd3e7c79c20edfc5939af26a","0f16f6f478b5c788dce466eb50e36c612273c36e","572dbaee6648eefa4c9de9b42551204b985ff863","799af7872a63573711565f2b9bf1b9c9cd89b764","0014fcda839ce8155999d2e0b65faa8331e1e694","62eff39e58fa64ba4871750e1f487d8e00de1ab6","ebc9e751e3056a17f1d40a146fa39bb82ded6c8a","21d9d0deed16f0ad62a4865e9acf0686f4f15492","3e9219fdcbc17772041456cf8dbfd361a82cbdd4","6dda6bee01c68b7d8a5d1d08d5ef94a592f5ff1b","6144edcfb074cc065156d37900848c4685f04325","3fbb1498257e4cac39a03a4b98b745238bcf2f9e","68102f37d6da41530b63dfd232984f93d8685693","ffd152065390103497e29f00acc040567e1481b6","06c8fcb0429afd3aee153ba42e1fd8aa93f7214f","0a68747d001aba014acd3b6ec83ba9534946a0da","023740e2b732448132ecc67594c55f809f2884a9"],"id":"ff81f2b4ff19f043b61b5f720643448711ebdb6d","s2Url":"https://semanticscholar.org/paper/ff81f2b4ff19f043b61b5f720643448711ebdb6d","authors":[{"name":"Wenxuan Mou","ids":["2734386"]},{"name":"Christos Tzelepis","ids":["1694090"]},{"name":"Vasileios Mezaris","ids":["1737436"]},{"name":"Hatice Gunes","ids":["1781916"]},{"name":"Ioannis Patras","ids":["1744405"]}],"doi":"10.1109/FG.2017.69"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593657","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents orientation planning algorithms respecting the requirements of task space trajectory generation, particularly in robotics applications. The proposed algorithms fulfill the following conditions: (i) permitting to impose constraints at angular velocity and acceleration in addition to orientation at endpoints; (ii) rendering continuous acceleration profiles even when interpolating multiple orientations; and (iii) being computationally fast enough for realtime implementation. The generated spline trajectories are essentially a concatenation of polynomial in time curves parameterized by quaternion coefficients. To impose the unitariness condition critically required for quaternion representation of orientation, we develop an on-line update mechanism which successively reparameterizes the polynomials constructing the spline, towards suppressing distortions that the normalization operation might incur. Experiments on an anthropomorphic robot upper-body are carried out to demonstrate the efficacy and real-time compatibility of the proposed algorithms in comparison with a standard spherical interpolation method.","inCitations":[],"pmid":"","title":"On the Orientation Planning with Constrained Angular Velocity and Acceleration at Endpoints","journalPages":"7033-7038","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593657"],"entities":[],"journalVolume":"","outCitations":[],"id":"7ec60dc07b09e31a51f0486f57e237ce16144ae5","s2Url":"https://semanticscholar.org/paper/7ec60dc07b09e31a51f0486f57e237ce16144ae5","authors":[{"name":"Mohammad Shahbazi","ids":[]},{"name":"Navvab Kashiri","ids":[]},{"name":"Darwin G. Caldwell","ids":[]},{"name":"Nikolaos G. Tsagarakis","ids":[]}],"doi":"10.1109/IROS.2018.8593657"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206379","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The planar rimless wheel (PRW) is a classic and simple passive dynamic mechanism to simulate biped walking, different simplified PRW models have respective descriptions and limited applications. This paper focus on constructing the general PRW model, and analyzing the intrinsic and mathematical relation between different PRW models. Based that, the limit cycle of symmetric PRW and asymmetric PRW are proposed separately. Moreover, the stability in universal domain are investigated in detail. Furthermore, simulation and experiment results show that the 2-period limit cycle motion of the asymmetric PRW is more effective, flexible and self-adaptive compared with regular rimless wheel, and more actual applications would be achieved by adopting the general model.","inCitations":[],"pmid":"","title":"Modelling and analysis of the passive planar rimless wheel mechanism in universal domain","journalPages":"4969-4975","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206379"],"entities":["Interference (communication)","Limit cycle","Simulation"],"journalVolume":"","outCitations":["5c800c555ecb68dd892a0ab7d7efd4f15fc41e38","42d694a096640b0e4b982d7dd3ba79e97d08dc01","85410c3aa92ab89673434fc54aac7990e9369d44","2478fc88a1ae05122223775f02bda065886d4d7e","1b7bafe2e7d5d8300116451b4ce0fab69c279437","04199580ac5898022d1d27bbe43c6ea139c39cc7","cdabe8fa08d032f2b70fc66b32bdf5e8f62572cc","418aa70f6cb4451247d06fe1dd4bfe5287c3fb3e","2a81c733e3a8273dc75d90f55657c967d053bc32","3e5fad2f206176d6e29917ccccaa1380847b7b9a","02982aa302641c21e213af957ff19fe94a8716bb","5567709b83c7cb1e3c8655b36e8ca8080aecc096","8d9a984c4fdc445bc869f75c608d85410c6b95c3"],"id":"fb5078e45946e85c9f7f5d0d52fe084349ef3e81","s2Url":"https://semanticscholar.org/paper/fb5078e45946e85c9f7f5d0d52fe084349ef3e81","authors":[{"name":"Wenchuan Jia","ids":["2553553"]},{"name":"Jiang Yang","ids":["2071053"]},{"name":"Liangyu Bi","ids":["23555387"]},{"name":"Quan Zhang","ids":["37509862"]},{"name":"Yi Sun","ids":["50876739"]},{"name":"Huayan Pu","ids":["2653882"]},{"name":"Shugen Ma","ids":["1679353"]}],"doi":"10.1109/IROS.2017.8206379"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593700","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a future-focused approach for robot programming based on augmented trajectories. Using a mixed reality head-mounted display (Microsoft Hololens) and a 7-DOF robot arm, we designed an augmented reality (AR) robotic interface with four interactive functions to ease the robot programming task: 1) Trajectory specification. 2) Virtual previews of robot motion. 3) Visualization of robot parameters. 4) Online reprogramming during simulation and execution. We validate our AR-robot teaching interface by comparing it with a kinesthetic teaching interface in two different scenarios as part of a pilot study: creation of contact surface path and free space path. Furthermore, we present an industrial case study that illustrates our AR manufacturing paradigm by interacting with a 7-DOF robot arm to reduce wrinkles during the pleating step of the carbon-fiber-reinforcement-polymer vacuum bagging process in a simulated scenario.","inCitations":[],"pmid":"","title":"Robot Programming Through Augmented Trajectories in Augmented Reality","journalPages":"1838-1844","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593700"],"entities":[],"journalVolume":"","outCitations":[],"id":"45048984874e7d8951fd470a4b9d4736fc2e9c0d","s2Url":"https://semanticscholar.org/paper/45048984874e7d8951fd470a4b9d4736fc2e9c0d","authors":[{"name":"Camilo Perez Quintero","ids":[]},{"name":"Sarah Li","ids":[]},{"name":"Matthew K. X. J. Pan","ids":[]},{"name":"Wesley P. Chan","ids":[]},{"name":"H. F. Machiel Van der Loos","ids":[]},{"name":"Elizabeth A. Croft","ids":[]}],"doi":"10.1109/IROS.2018.8593700"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00044","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Pain is a symptom of many disorders associated with actual or potential tissue damage in human body. Managing pain is not only a duty but also highly cost prone. The most primitive state of pain management is the assessment of pain. Traditionally it was accomplished by self-report or visual inspection by experts. However, automatic pain assessment systems from facial videos are also rapidly evolving due to the need of managing pain in a robust and cost effective way. Among different challenges of automatic pain assessment from facial video data two issues are increasingly prevalent: first, exploiting both spatial and temporal information of the face to assess pain level, and second, incorporating multiple visual modalities to capture complementary face information related to pain. Most works in the literature focus on merely exploiting spatial information on chromatic (RGB) video data on shallow learning scenarios. However, employing deep learning techniques for spatio-temporal analysis considering Depth (D) and Thermal (T) along with RGB has high potential in this area. In this paper, we present the first state-of-the-art publicly available database, 'Multimodal Intensity Pain (MIntPAIN)' database, for RGBDT pain level recognition in sequences. We provide a first baseline results including 5 pain levels recognition by analyzing independent visual modalities and their fusion with CNN and LSTM models. From the experimental evaluation we observe that fusion of modalities helps to enhance recognition performance of pain levels in comparison to isolated ones. In particular, the combination of RGB, D, and T in an early fusion fashion achieved the best recognition rate.","inCitations":["86dcc4bbb35dc754e04a63430908c42b7d238030","d3b9ef82c999f16641c7dd41efe6695bf44d34f0"],"pmid":"","title":"Deep Multimodal Pain Recognition: A Database and Comparison of Spatio-Temporal Visual Modalities","journalPages":"250-257","s2PdfUrl":"","pdfUrls":["http://sergioescalera.com/wp-content/uploads/2018/05/FG2018_MIntPAIN_Poster01.pdf","http://vbn.aau.dk/files/279732218/FG2018_MIntPAIN_CameraReady.pdf","http://vbn.aau.dk/files/271461978/FG2018_MIntPAIN_manuscript01_submitted.pdf","http://sergioescalera.com/wp-content/uploads/2018/05/FG2018_MIntPAIN-Presentation01.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2018.00044"],"entities":["Baseline (configuration management)","Database","Deep learning","Long short-term memory","Multimodal interaction","Pain","Usability","Visual inspection"],"journalVolume":"","outCitations":["51e4ea58950dd6356ab966980fefc41f394ab26e","c19113b63aa3c273cfa33841d8be59d259a2b239","7eb8a3b55bb02867b959461e3577a2d6f22fda49","45a5e32edb42795eccf0ed433149089369119e7e","fb6f957eb38bd92527ffd4a35e99a372e4361e24","53bb52eb910c3a0ac5dc7f379b1f3f7c29af529d","6c5a6a3742294f8616d0a97f5e1ea68048bbaf9c","56ffa7d906b08d02d6d5a12c7377a57e24ef3391","41df6b9618bd930d6bf2c4704ff123b5fa5dcb2a","f74211fb48f92739e70e6e066ee4c569c6ec5f95","46d39dd5b2d94ea67e2f49527fe3a2205b6bc1ec","961a5d5750f18e91e28a767b3cb234a77aac8305","2ad7cef781f98fd66101fa4a78e012369d064830","bdebb77783a838a3da84c67509d0d73a0f72806c","317553d0c479cb9893b3884c96cfa681c80296a0","0917b6fcd660d3269091f20beb1be5e03a23e38f","739d0476edd6d348679e4b08c4ab95c1484ee02c","f38904f42caa7dccd71381dacb28e19734b1e23d","0cff6fe25622c3da377d073c256203b98a2169a8","0daaa56d724c11e64338996e99a257fa69900236","47d4838087a7ac2b995f3c5eba02ecdd2c28ba14","f68a81b5190c17613527502c552b8d7135d01019","2c285dadfa6c07d392ee411d0213648a8a1cf68f","8c66378df977606d332fc3b0047989e890a6ac76","44f23600671473c3ddb65a308ca97657bc92e527","8fe5feeaa72eddc62e7e65665c98e5cb0acffa87","53ae5a31eb1b31ae8f30dfa55508c4862604e61f","36b8a34b53dc680c7f92ebc001f055d3ba42da54","2e7e1ee7e3ee1445939480efd615e8828b9838f8","6613b09add63ec18c4a24589bd9272ce2a5b42f3","2be24e8a3f2b89bdaccd02521eff3b7bb917003e","c6dc6f92833061c754786678da0ecc39eac73baa","04c230b978e2374477cebaaf74900a2bd2c94395","087002ab569e35432cdeb8e63b2c94f1abc53ea9","0f4dfdeca316a5d27d61ab6bcdf907cd44686941","1485b72151d53895a1e251d2e42d84f78c1a5009","1afbd7d0e5fa4f1add15b068ddc61b57999a8de8","747c25bff37b96def96dc039cc13f8a7f42dbbc7","ec90738b6de83748957ff7c8aeb3150b4c9b68bb","c531a0105a597c9c3d1491daad8977d04b6f238e"],"id":"185ade6026c291166cbe4d36a444ee71fbe7f873","s2Url":"https://semanticscholar.org/paper/185ade6026c291166cbe4d36a444ee71fbe7f873","authors":[{"name":"Mohammad A. Haque","ids":["3032980"]},{"name":"Ruben B. Bautista","ids":["38817001"]},{"name":"Fatemeh Noroozi","ids":["4204765"]},{"name":"Kaustubh Kulkarni","ids":["38370357"]},{"name":"Christian B. Laursen","ids":["49218349"]},{"name":"Ramin Irani","ids":["37541412"]},{"name":"Marco Bellantonio","ids":["9762642"]},{"name":"Sergio Escalera","ids":["7855312"]},{"name":"Gholamreza Anbarjafari","ids":["3087532"]},{"name":"Kamal Nasrollahi","ids":["1803459"]},{"name":"Ole Kæseler Andersen","ids":["40137619"]},{"name":"Erika G. Spaich","ids":["2101741"]},{"name":"Thomas B. Moeslund","ids":["1700569"]}],"doi":"10.1109/FG.2018.00044"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00021","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"In this paper, we present a novel open-source pipeline for face registration based on Gaussian processes as well as an application to face image analysis. Non-rigid registration of faces is significant for many applications in computer vision, such as the construction of 3D Morphable face models (3DMMs). Gaussian Process Morphable Models (GPMMs) unify a variety of non-rigid deformation models with B-splines and PCA models as examples. GPMM separate problem specific requirements from the registration algorithm by incorporating domain-specific adaptions as a prior model. The novelties of this paper are the following: (i) We present a strategy and modeling technique for face registration that considers symmetry, multi-scale and spatially-varying details. The registration is applied to neutral faces and facial expressions. (ii) We release an open-source software framework for registration model-building demonstrated on the publicly available BU3D-FE database. The released pipeline also contains an implementation of an Analysis-by-Synthesis model adaption of 2D face images, tested on the Multi-PIE and LFW database. This enables the community to reproduce, evaluate and compare the individual steps of registration to model-building and 3D/2D model fitting. (iii) Along with the framework release, we publish a new version of the Basel Face Model (BFM-2017) with an improved age distribution and an additional facial expression model.","inCitations":["e8875b317c2e0ed6fba0c908d599b3772a400bdd","643d11703569766bed0a994941ae5f7b3e101659","6a3e43e397aa9a4a329c22eef7359a2959064f44","ae7155dee93d994594c8f44c65029325bd0a9165","5a2eaba870d962896a09fdd220b46980831c07e9","d1633dc3706580c8b9d98c4c0dfa9f9a29360ca3","b3e521baceadee36ac22b6a06266e8abd6a701f7","1f9e7b699cf2db3888a56ece481ed2f05c88520f","96279b995b3f6ab5b6c1b53836b9dfcedf73fd9f","0081e2188c8f34fcea3e23c49fb3e17883b33551","1aaecbb841588fd6de7d23665bb0f1cc629bafb1"],"pmid":"","title":"Morphable Face Models - An Open Framework","journalPages":"75-82","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00021","http://arxiv.org/abs/1709.08398","https://arxiv.org/pdf/1709.08398v2.pdf"],"entities":["Algorithm","B-spline","Computer vision","Curve fitting","Gaussian process","Image analysis","Neutral monism","Open-source software","Requirement","Software framework","Speech coding"],"journalVolume":"","outCitations":["370b5757a5379b15e30d619e4d3fb9e8e13f3256","46d39b0e21ae956e4bcb7a789f92be480d45ee12","cc589c499dcf323fe4a143bbef0074c3e31f9b60","87ad10252a5298ec1612c249e942bb685d6ae56c","2358752dd189f76f16c5addfc6cf88fab1924054","e9fb3522f3577b7d76ecc9ec55d673c1a2bac4fb","639937b3a1b8bded3f7e9a40e85bd3770016cf3c","4d2b40531aa2738455977b81f409e0b6001f7c3c","e6ca412a05002b51d358c2e3061913c3dab6b810","7a0b78879a13bd42c63cd947f583129137b16830","5f0d86c9c5b7d37b4408843aa95119bf7771533a","da59f4fa6dc73b2b8b041e7d4e0e7f121297658a","9944c451b4a487940d3fd8819080fe16d627892d","3eeac7f6e800024fa73bc74d35f107da90a9d440","10cb7f4f86c6437b496a1c98955ba413c7540cd4","7caa3a74313f9a7a2dd5b4c2cd7f825d895d3794","f50bb87a850ad0f44eef9c2acae5b1dcccb945da","42505464808dfb446f521fc6ff2cfeffd4d68ff1","650b04ffc4e5911137c86263e11ea3cdb474e501","575110b7eeb5c24e2b99a3736c539ef32fba8cb7","a9c120de41679fe336e2779f3e6fe4b04945cb3a","03406ec0118137ca1ab734a8b6b3678a35a43415","a0eb31cd4124a4e3ed49bef1b70ffeb19f9f377c","e632221f702eafc4cf108f6bf3955c2c6be2d827","0c79485f64733bd128ef8c395034b6bc77abf94d"],"id":"ac86ccc16d555484a91741e4cb578b75599147b2","s2Url":"https://semanticscholar.org/paper/ac86ccc16d555484a91741e4cb578b75599147b2","authors":[{"name":"Thomas Gerig","ids":["3277377"]},{"name":"Andreas Forster","ids":["47494075"]},{"name":"Clemens Blumer","ids":["39550224"]},{"name":"Bernhard Egger","ids":["34460642"]},{"name":"Marcel Lüthi","ids":["1731460"]},{"name":"Sandro Schönborn","ids":["1987368"]},{"name":"Thomas Vetter","ids":["1687079"]}],"doi":"10.1109/FG.2018.00021"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206538","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents the design and evaluation of a new sensorized underactuated self-adaptive finger. Our design incorporates a two degrees-of-freedom (DOF) parallel based underactuated mechanism with an embedded load cell for contact force measurement and a trimmer potentiometer to acquire the joint variables. Integration of the sensors leads to tactile feedback fidelity without compromising the finger size and complexity which results in efficient and robust functionality. The particular rounded shape of the distal phalanx and high equilibrium position enable the finger to deliver both precision and power grasps. The effectiveness of our design is verified theoretically and through experimental results demonstrating its shape adaptability, and tactile capability.","inCitations":["8b84a6a740dcabf55c08dd09474d689e1f57fe89"],"pmid":"","title":"Development and grasp analysis of a sensorized underactuated finger","journalPages":"6331-6336","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206538"],"entities":["Complexity","Embedded system","Image resolution","Jacobian matrix and determinant","Nonlinear system","Phalanx CIWS","Potentiometer","Sensor","Underactuation"],"journalVolume":"","outCitations":["6df8653fc8758d0d1534207e34000a5189abf212","d4611f42f5b82385f11c518154c6765096f008ad","4977242eec1bca6836854570a40f82f719ccb83f","9f26aa38e5ac5d6f4ce07c2a76aba84270593159","7f585926187f7730e680df63508d3a255495a45f","7c73262b7fa024a7d081f7d98da8b7a0b1311308","1076e776422d85a370024e1765d05b0b67616595","29a7e15081c63f0e28359a8feea4dea4054f601e"],"id":"4b5ca742bd749e1b8e8ab796cda8ed49d080608c","s2Url":"https://semanticscholar.org/paper/4b5ca742bd749e1b8e8ab796cda8ed49d080608c","authors":[{"name":"Mahyar Abdeetedal","ids":["32216005"]},{"name":"Mehrdad R. Kermani","ids":["35002122"]}],"doi":"10.1109/IROS.2017.8206538"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594057","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a novel wireframe map structure for resource-constrained robots operating in a rectilinear 2D environment. The wireframe representation compactly represents geometry, in addition to transient situations such as occlusions and boundaries of unexplored regions. We formulate a particle filter to suit this sparse wireframe map structure. Functions for calculating the likelihood of scans, merging wireframes, and resampling are developed to accommodate this map representation. The wireframe structure with the particle filter allows for severe discrete map errors to be corrected, leading to accurate maps with small storage requirements. We show in a simulation study that the algorithm attains a map of an environment with 1 % error, compared to an occupancy grid map obtained with GMapping which attained 23% error with the same storage requirements. A simulation mapping a large environment demonstrates the algorithms scalability.","inCitations":[],"pmid":"","title":"Wireframe Mapping for Resource-Constrained Robots*This research was supported in part by NSF grant CMMI-1562335 and ONR grant N00014-12-1-1000. We are grateful for this support","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594057"],"entities":[],"journalVolume":"","outCitations":[],"id":"de99fcf761876bc9e504adf1bfea5cb1472e8004","s2Url":"https://semanticscholar.org/paper/de99fcf761876bc9e504adf1bfea5cb1472e8004","authors":[{"name":"Adam Caccavale","ids":[]},{"name":"Mac Schwager","ids":[]}],"doi":"10.1109/IROS.2018.8594057"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206584","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"3D laser scanners are frequently used sensors for mobile robots or autonomous cars and they are often used to perceive the static as well as dynamic aspects in the scene. In this context, matching 3D point clouds of objects is a crucial capability. Most matching methods such as numerous flavors of ICP provide little information about the quality of the match, i.e. how well do the matched objects correspond to each other, which goes beyond point-to-point or point-to-plane distances. In this paper, we propose a projective method that yields a probabilistic measure for the quality of matched scans. It not only considers the differences in the point locations but can also take free-space information into account. Our approach provides a probabilistic measure that is meaningful enough to evaluate scans and to cluster real-world data such as scans taken with Velodyne scanner in urban scenes in an unsupervised manner.","inCitations":[],"pmid":"","title":"Analyzing the quality of matched 3D point clouds of objects","journalPages":"6685-6690","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206584","http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/bogoslavskyi17iros.pdf"],"entities":["Autonomous car","Autonomous robot","CT scan","Experiment","Karmarkar's algorithm","Mobile robot","Point cloud","Point-to-Point Protocol","Robotics","Sensor","Similarity measure","Unsupervised learning"],"journalVolume":"","outCitations":["ff85bdff5d5d000a6a9f4f08490ca3e9a529c40f","b4d666efd54b485e0e387153532b3e1b88d66365","bb8123b5f19c05e63be57e8439ebab483174eac4","ed1235a6a54d0f541efa0137d9495df05a9bce5d","5f0d86c9c5b7d37b4408843aa95119bf7771533a","2503a1055ac0d45a2a388da716062788b42a6c5e","91e121162d0a0fe0368a2727043bf21bdd66510a","7842c2f0dedf9f891685a0abf6958435149eeea6","d1db5c78ce2107e21624b5774a4275b8ec4d3edd","1f122c66cd907a526cf47723e7da3c03868c76b8","38bef5fc907299a1a9b91346eec334b259d5b3eb","026e3363b7f76b51cc711886597a44d5f1fd1de2","07121fa5e8e881aa1217d7f590c3e2cbf16f6855","e0d2861a9022667a93a8a0573d44f238f7c3a027","5701755ce4dc580ca5bb7fa9d91249fb15a07d7c","9f4c032918f323f4ca3a1f083e144e7cb8857e56","a0b5022edae9d24c0b06999c6ab8ce727d1212a9"],"id":"e39fc6ff1145924c7987ca07c1f2df80577f0646","s2Url":"https://semanticscholar.org/paper/e39fc6ff1145924c7987ca07c1f2df80577f0646","authors":[{"name":"Igor Bogoslavskyi","ids":["1736861"]},{"name":"Cyrill Stachniss","ids":["1722062"]}],"doi":"10.1109/IROS.2017.8206584"}
{"doiUrl":"","venue":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","journalName":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","sources":[],"year":2017,"text":"Several methods are exploited to watermark digital images as a safety measure for storing information, but an attacker can destroy the information by cropping a segment of the watermarked image. In recent years, numerous schemes were proposed that reduce the impact of such attacks. A new method has been proposed to confront cropping attack that is carried out using two sudoku tables. In this method, the watermark image is scattered in two sudoku table layouts with different solutions and is watermarked in the host image. Using this method, the watermark image is repeated 81 times in the host image, and to this effect the watermark image can be reconstructed using other segments when cropped by the attacker. Both sudokus used in this paper are in the classic 9×9 form and using this method, resistance to cropping attacks increases up to 98.8%.","inCitations":["357bdf8f75ec4b04cc687b47ff1a3d5f8dfe342d"],"pmid":"","title":"Introducing a new method robust against crop attack in digital image watermarking using two-step sudoku","journalPages":"237-242","s2PdfUrl":"","pdfUrls":[],"entities":["Avian crop","Data Table","Digital image","Digital watermarking","Solutions","Sudoku","Watermark (data file)"],"journalVolume":"","outCitations":["edfa4acecc22a05299c31c17d8d8159678e7f530","10988ba8722d1fbb542b22cb9ea6dc47a02a3d8f","0ee07349282944118b6151cf90eead38eeaf0f52","d2cc125c0117b34dc8ea21cd7887287b54d15f4a","da516cd080524ee70f5ba9de7b7e0f53c6967227","0ad4debcd13bfb651a03f15b7c3ab573b084409c","a9eee45e1ce6b208f7d4002f5dec3f04c333c909"],"id":"610bfb07fa5a1b2d5597883945f21fb653e5a121","s2Url":"https://semanticscholar.org/paper/610bfb07fa5a1b2d5597883945f21fb653e5a121","authors":[{"name":"Mohammad Shahab Goli","ids":["21189268"]},{"name":"Alireza Naghsh","ids":["2216989"]}],"doi":""}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594431","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We want to enable the robot to reorient its feet in order to face its direction of motion. Model Predictive Control schemes for biped walking usually assume fixed feet rotation since adapting them online leads to a nonlinear problem. Nonlinear solvers do not guarantee the satisfaction of nonlinear constraints at every iterate and this can be problematic for the real-time operation of robots. We propose to define safe linear constraints that are always inside the intersection of the nonlinear constraints. We make simulations of the robot walking on a crowd and compare the performance of the proposed method with respect to the original nonlinear problem solved as a Sequential Quadratic Program.","inCitations":[],"pmid":"","title":"Adaptive step rotation in biped walking","journalPages":"720-725","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594431"],"entities":[],"journalVolume":"","outCitations":[],"id":"36a3f697f1bc00e648da1de2b05117ac7b98d639","s2Url":"https://semanticscholar.org/paper/36a3f697f1bc00e648da1de2b05117ac7b98d639","authors":[{"name":"Nestor Bohorquez","ids":[]},{"name":"Pierre-Brice Wieber","ids":[]}],"doi":"10.1109/IROS.2018.8594431"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594172","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Reducing material spillage by robotic mining mobile manipulators, such as front-end loaders, is necessary to improve mining operations. To this end, the present work proposes an approach to reduce disturbances on the end-effector induced by the terrain and propagated through the wheels and arm links of the machine. The proposed approach is based on an $H$∞ control strategy that includes a feedforward action, computed using the pitch rate of the mobile base, and considers the hydraulic arm dynamics, as well as the reaction forces in the contact points of the mobile base, which is modeled as a floating body with non-permanent ground contacts. Alternative control schemes based on the classic proportional-derivative (PD) control, and the Active Disturbance Rejection Control (ADRC), with and without feedforward action, were also implemented and experimentally evaluated using a semiautonomous Cat® 262C compact skid-steer loader equipped with inclination and inertial sensors. The proposed method reduces disturbances by at least 70% when climbing ramps at 25% of the machine's maximum speed, and by at least 20% when driving over speed bumps which produce disturbances similar to that caused by stones. The proposed disturbance attenuation strategy should help reducing the spillage of material when driving over mounds, inclines or spilled rocks, especially considering that even if existing autonomous machines are able to drive with little operator supervision along mining galleries, they are often unable to avoid disturbing material on the ground or the characteristic unevenness of mining terrains.","inCitations":[],"pmid":"","title":"Ground Disturbance Rejection Approach for Mobile Robotic Manipulators with Hydraulic Actuators","journalPages":"5980-5986","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594172"],"entities":[],"journalVolume":"","outCitations":[],"id":"82dc675e9c412c1bffa9a853836017be6df75e95","s2Url":"https://semanticscholar.org/paper/82dc675e9c412c1bffa9a853836017be6df75e95","authors":[{"name":"Mattia Rigotti-Thompson","ids":[]},{"name":"Miguel Torres-Torriti","ids":[]},{"name":"Fernando Alfredo Auat Cheeín","ids":[]},{"name":"Giancarlo Troni","ids":[]}],"doi":"10.1109/IROS.2018.8594172"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594160","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Biped robots have not achieved the efficient and harmonious locomotion of the human beings, capable of walking and running on unstructured terrains, with obstacles, holes and slopes. With this in mind, researchers started the development of biomimetic solutions to control the locomotion of biped models. This work presents a new solution of motion control of bipedal robots with adaptable stiffness, by exploring effects of joint stiffness in modulating walking behavior. Further, torque adjustment is achieved through a biomimetic controller that mimics and adjusts the natural dynamics of the robot to the environment. Specifically, the torque adjustment is made using AFOs (adaptive frequency oscillator) to generate the correct equilibrium positions that will be applied to the impedance control that computes the torque of each joint. Results show that the biped model is capable of walking in several types of terrain, including flat terrain, ramps, stairs and flat terrain with obstacles.","inCitations":[],"pmid":"","title":"Torque Controlled Biped Model Through a Bio-Inspired Controller Using Adaptive Learning","journalPages":"4369-4374","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594160"],"entities":[],"journalVolume":"","outCitations":[],"id":"03e0bdd4116a8a144364ffa638ddf7a0cbca1658","s2Url":"https://semanticscholar.org/paper/03e0bdd4116a8a144364ffa638ddf7a0cbca1658","authors":[{"name":"César Ferreira","ids":[]},{"name":"Tomas Cunha","ids":[]},{"name":"Cristina P. Santos","ids":[]},{"name":"Luís Paulo Reis","ids":[]}],"doi":"10.1109/IROS.2018.8594160"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202308","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present an Omnidirectional Visual-Inertial Odometry (OVIO) approach based on Multi-State Constraint Kalman Filtering (MSCKF) to estimate the ego-motion of a moving platform. Instead of considering visual measurements on image plane, we use individual planes for each point that are tangent to the unit sphere and normal to the corresponding measurement ray. This way, we combine spherical images captured by omnidirectional camera with inertial measurements within the filtering method MSCKF. The key hypothesis of OVIO is that a wider field of view allows incorporating more visual features from the surrounding environment, thereby improving the accuracy and robustness of the motion estimation. Moreover, by using an omnidirectional camera, it is less likely to end up in a situation where there is not enough texture. We provide an evaluation of OVIO using synthetic and real video sequences captured by a fish-eye camera, and compare the performance with MSCKF using a perspective camera. The results show the superior performance of the proposed OVIO.","inCitations":[],"pmid":"","title":"Omnidirectional visual-inertial odometry using multi-state constraint Kalman filter","journalPages":"1317-1323","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202308"],"entities":["Autonomous robot","Computer-integrated manufacturing","Global Positioning System","Ground truth","Image plane","Information model","Kalman filter","Motion estimation","Odometry","Omnidirectional camera","Precise Point Positioning","Streaming media","Synthetic data","Synthetic intelligence"],"journalVolume":"","outCitations":["d2b0742c91747fa82b3e2070a6f8edbc52c7ec4d","fb87d22f2657345e460fecce6d3608ed5555ac42","2ebffd9b5d5661af87371d5897f33498cc4a9bc1","b7631470c99be17c65ce90fedb521348d370a2e4","f3697f7c93df1abd031cf88b30b431995ced91ab","bb52e0ece9afda068aaa8bf52cf524e9869efdab","487911cb8a5da9a6237d5f73c6f2af735d3d3737","248cb96ecdf0fb5345cac799f209db92f63c2b01","2e377bb05224a7a64def38e27e8f438c5cb7b0c7","5e6ef2f83d1049999b27a84c9c45e1c5919f79ed","c5430c25854eb38fd875713d317cc0b59441551f","58905efce3bc96246580f30259878d473cf4b895","343d93c11043740dd8177074b0810e9141aebd51","261cf699e44f71285e25c161258a32f8a04c639a","d377cc552732ebc8f4eed65e71f0b0746ec4d391","f2992fd8c2828d4606b10c795abe2a8ba9979cb8","70464856c4dad5711e457f3f6623bdfd8e1fa05f","7200dce06a410959760aca287c53557830f43fcf","0be0c13803cd08e81b7adaada537e91222eb1491","25e26d4db49e280669e26cd05305d27f35a0791a","1aadcb32b70b0087e0c91b48c8ef37bf97df21a2","3a5f93a61e36bf0dd5b4c143d28101db40e078a1","86d3ccf90dbeabfc34ad760295718e4d7bab51a2","c8965cc5c62a245593dbc679aebdf3338bb945fc","2e7975fb0351638bd2646a217e5885ae56ca5cff","11d18a36ba3a1970398ca668937f7a00e38dc663","badf73b0415e03bbd86ef1abcc3d7ef3da8f6ef9","abf192388cf027221dc2af556efa9097f6d3230f","3b119ed30d711a06ae7f2683c7829d39e455e862","3be23e51455b39a2819ecfd86b8bb5ba4716679f","5889c5ba353272e81d5c604aed04ec024a5cbd0e","406e130160eb49d3808af30d097defbe81ea0949"],"id":"0e03149c8b0ec8cc9c15ccb1adb2724713aecfad","s2Url":"https://semanticscholar.org/paper/0e03149c8b0ec8cc9c15ccb1adb2724713aecfad","authors":[{"name":"Milad Ramezani","ids":["31367273"]},{"name":"Kourosh Khoshelham","ids":["3056521"]},{"name":"Laurent Kneip","ids":["1727013"]}],"doi":"10.1109/IROS.2017.8202308"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206017","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Self-folding is capable of forming complex three-dimensional structures from planar sheets. However, existing self-folding machines cannot generate large forces or bear high loads. In contrast, traditional robots are expected to operate under large loads but these robots are generally dense, resulting in heavy machines with relatively low strength-to-weight ratios. In this paper, we present a new self-folding technique that is meter-scale and load-bearing. We demonstrate the design and fabrication of both structural beams and pneumatically actuated joints. We model and measure the stiffness and strength of self-folded structural beams. Finally, we integrate these results into a robotic arm that weighs 0.3 kg and can lift up to 1.0 kg. These results indicate that functional, load-bearing, self-folding robots are possible.","inCitations":[],"pmid":"","title":"A self-folding robot arm for load-bearing operations","journalPages":"1979-1986","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206017"],"entities":["Robot","Robotic arm"],"journalVolume":"","outCitations":["04e0f565142ed0ada976dd3960687ef6c181fae4","181fc1da57b32f192d6a034444efec845b36b4d6","e8ca31722709ff0dcb1cbab23455d8de44566b58","26832dc37dab021a4c61c90e2496d6c7c65bfca8","75cbb367578d24afb7253f30656ffeb382c0508d","0b5b02d6b55c0cf6926bc406462d8b72a763ce98","563f7a03ac7708fcbf0a23923a9ea07e5aec1c2d","ff3b17761ac466dedcfe50f6dda9857ccb9cd1ba","4454dd21852a3017c6ee2efdea231ef2131f20a7","046f860b72ac1300eb1266b296d0b977b0f8a161","587f8411391bf2f9d7586eed05416977c6024dd0","201b71061ae8b4d8f28ae97c956f7396028d0ec7","fc471587a4333b70fc9ee93234c00d2a46ad7084","cd5994d7151e743295ad4897942d11e9e1ddf41b","9de485229367ea8276f2a9f303952e05eb7a9e39","59adaed2393dfcb73a36e04fcf15c6bda051834e","045b92cd4c21353ae58bedf5ce5c76e42b7f5bd9"],"id":"acc67f3ab065c8f80410efe2ab1136b100365905","s2Url":"https://semanticscholar.org/paper/acc67f3ab065c8f80410efe2ab1136b100365905","authors":[{"name":"Chang Liu","ids":["1692867"]},{"name":"Samuel M. Felton","ids":["35570500"]}],"doi":"10.1109/IROS.2017.8206017"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593631","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Currently, the manufacturing of self-actuating and self-sensing robots requires non-standard manufacturing techniques and assembly steps to integrate electrical and mechanical systems. In this work, we developed a novel manufacturing technique, where such robots can be produced at a flexible electronics factory. We developed the technique using standard industrial machines, processes, and materials. Using a lamination process, we were able to integrate air pouches or shape memory alloy (SMA) inside a polyamide-based flexible circuit to produce bending actuators. The bend angle of the actuators is sensed with a chain of inertial measurement units integrated on the actuator. Air-pouch actuators can produce a force of a 2.24N, and a maximum bend angle of 74 degrees. To demonstrate, we manufactured a five-legged robot with the developed actuators and bend sensors, with all the supporting electronics (e.g., microcontrollers, radio) directly integrated into the flexible printed circuit. Such robots are flat and lightweight (15 grams) and thus conveniently compact for transportation and storage. We believe that our technique can allow inexpensive and fast prototyping and deployment of self-actuating and self-sensing robots.","inCitations":[],"pmid":"","title":"Mass Manufacturing of Self-Actuating Robots: Integrating Sensors and Actuators Using Flexible Electronics","journalPages":"6099-6104","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593631","https://dam-prod.media.mit.edu/x/2018/09/28/IROS_Circuit_Robots_FINAL_cxaT3iu.pdf"],"entities":[],"journalVolume":"","outCitations":["40975d415b39697ab866ce6bd061050629c96fd0","557d873f50b72f678869c7918a8f743e4108cc30","563f7a03ac7708fcbf0a23923a9ea07e5aec1c2d","04e0f565142ed0ada976dd3960687ef6c181fae4","528eed628b232d2c92d7ed901fd1adb52ee30f45","75cbb367578d24afb7253f30656ffeb382c0508d","ff3b17761ac466dedcfe50f6dda9857ccb9cd1ba","e369c6d56589928a3c26498b63dc8e1018ba1217","3be7fe3f13cfd568330abb02e2842ff4bab809d7","082c40cc7a16eba131ef6f75e12bf0bb4a6cfee1","1bc4cbf6363560de444168b1802417b8579c8f4f","49a4cefeebf8bb771b0f1efda6ad5d68194eeb35","0b5b02d6b55c0cf6926bc406462d8b72a763ce98","3f2ea5b6b2169f4bc9357f17ecabafa7284aa089"],"id":"cf48dd6e18b142aadade7485893bec41c1dee9a5","s2Url":"https://semanticscholar.org/paper/cf48dd6e18b142aadade7485893bec41c1dee9a5","authors":[{"name":"Artem Dementyev","ids":["49172131"]},{"name":"Jie Qi","ids":["47825967"]},{"name":"Jifei Ou","ids":["39577419"]},{"name":"Joseph A. Paradiso","ids":["1733194"]}],"doi":"10.1109/IROS.2018.8593631"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206327","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents the mechatronic design of a robotic arm that is mounted on a ground rover and used to deploy and recover small-scale unmanned aerial vehicles. The arm and rover are part of a network of collaborative robotic agents aiming to enhance current rescue operations by supporting human operators without burdening them with servicing tasks. The robust autonomy of the system, guaranteed in part through the addition of this robotic arm, is a main contributing factor. Design requirements are derived from the context of the rescue mission and a kinematic analysis is provided that leads to a customized design, including variable stiffness joints for compliant interaction with the environment. Experiments demonstrate the system's ability to perform the required Cartesian trajectory control and manipulation tasks, and to achieve a desired variable end-effector compliance.","inCitations":["01df112f239b088706d33b31834d7a34c3af6b3b","931942e3ff14fa9800134c25862bead8e3616536","897b91c670a77dc462ac12f9a306ca4b2af99759"],"pmid":"","title":"Mechatronic design of a variable stiffness robotic arm","journalPages":"4582-4588","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206327"],"entities":["Aerial photography","Cartesian closed category","Experiment","Mechatronics","Requirement","Robot end effector","Robotic arm","Rover (The Prisoner)","Stiffness","Unmanned aerial vehicle","Velocity (software development)","Workspace"],"journalVolume":"","outCitations":["275eda8c6cf0d289212384d0f8a85a52daed7882","a06ceb2744a799b522275af6f2b3c0d3c238fe4f","2eef0a9dcd7f495b12841b5bfd8e26167017d757","28f34080618df94e4c644b97482a8d7adbfee0f6","ad452231d3be171d30974909b43253b81d065ad7","1da6b297cf880604bf6019117e029754443ff932","0358a1178cf9c434ed3252f721956efcfe351ecf","5d8433aee68d9a9cb40d6115145ff4974fe1bc01","6fed81469bacb23dc5e13e531017962f092da859","62f53766d5cb8cd3f0ac6f70a623b1d92c4be265","350721ab6f02a092f974f1c6a140a16372bfe815","64d38980d5b381c58c07c52b969adb5aa49ccd9e"],"id":"71535755968c862553d652a53ff170f5f69a327a","s2Url":"https://semanticscholar.org/paper/71535755968c862553d652a53ff170f5f69a327a","authors":[{"name":"Eamon Barrett","ids":["36086818"]},{"name":"Mark Reiling","ids":["8440245"]},{"name":"Giuseppe Barbieri","ids":["1878837"]},{"name":"Matteo Fumagalli","ids":["47474115"]},{"name":"Raffaella Carloni","ids":["1770216"]}],"doi":"10.1109/IROS.2017.8206327"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593610","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"External force observer for humanoid robots has been widely studied in the literature. However, most of the proposed approaches generally rely on information from six-axis force/torque sensors, which the small or medium-sized humanoid robots usually do not have. As a result, those approaches cannot be applied to this category of humanoid robots, which is widely used nowadays in education or research. In this paper, we improve the external force observer in [1] to handle the case of an external force applied in any direction and at an arbitrary point of the robot structure. The new observer is based on Kalman filter formulation and it allows the estimation of the three force components. The observer is simple to implement and can easily run in real time using the embedded processor of a medium-sized humanoid robot such as Nao or Darwin-OP. Moreover, the observer does not require any change to the robot hardware as it only uses measurements from the available force-sensing resistors (FSR) inserted under the feet of the humanoid robot and from the robot inertial measurement unit (IMU). The proposed observer was extensively validated on a Nao humanoid robot. In all conducted experiments, the observer successfully estimated the external force within a reasonable margin of error.","inCitations":[],"pmid":"","title":"Kalman Filter Based Observer for an External Force Applied to Medium-sized Humanoid Robots","journalPages":"1204-1211","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593610"],"entities":[],"journalVolume":"","outCitations":[],"id":"298cdfc529be5b6c6be850811ecc91cb55ffd0bf","s2Url":"https://semanticscholar.org/paper/298cdfc529be5b6c6be850811ecc91cb55ffd0bf","authors":[{"name":"Louis Hawley","ids":[]},{"name":"Remy Rahem","ids":[]},{"name":"Wael Suleiman","ids":[]}],"doi":"10.1109/IROS.2018.8593610"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545863","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we mainly propose a Robust Adaptive Projective Dictionary Pair Learning (RA-DPL) framework based on the adaptive discriminative representations. Our formulation can seamlessly integrate the robust projective dictionary pair learning and the adaptive sparse representation learning into a unified model. RA-DPL improves the existing DPL algorithm in threefold. First, RA-DPL aims at computing the robust projective dictionary pairs by employing the sparse and robust $l_{2,1}$ -norm to encode the reconstruction error. Second, RA-DPL regularizes the robust $l_{2,1}$ -norm on the analysis dictionary so that the analysis dictionary can extract sparse coefficients from the given samples explicitly. More importantly, the optimization of $l_{2,1}$ -norm is so efficient, that is, the sparse coding step will be time-saving. Third, RA-DPL can clearly preserve the local neighborhood relationship of the sparse coefficients within each class, which can make the learnt representations discriminating and can also improve the discriminating power of learnt dictionary. Extensive simulations on image databases demonstrate that our RA-DPL can obtain the superior performance over other state-of-the-arts.","inCitations":[],"pmid":"","title":"Robust Discriminative Projective Dictionary Pair Learning by Adaptive Representations","journalPages":"621-626","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545863"],"entities":["Algorithm","Coefficient","Computation (action)","Data dictionary","Database","Dictionary [Publication Type]","ENCODE","Feature learning","Machine learning","Mathematical optimization","Neural coding","PRND gene","Privilege level","Rheumatoid Arthritis","Simulation","Sparse approximation","Sparse matrix","Traffic collision avoidance system","Unified Model"],"journalVolume":"","outCitations":["98720e3b5b36160a73c09086afc88405f7ed517f","6d96f946aaabc734af7fe3fc4454cf8547fcd5ed","24662ce3f3499ec8c5ecc546dac69dbffad578c6","4d423acc78273b75134e2afd1777ba6d3a398973","f88ce52c5042f9f200405f58dbe94b4e82cf0d34","9067db319ecb338312070a92b081341e6f03a6c6","23aba7b878544004b5dfa64f649697d9f082b0cf","075bc988728788aa033b04dee1753ded711180ee","7692613c83bed22f8d1bca7c722c878a3237c901","2e5b6fb6d6fe250e99a2af47949932eba01f5b8d","91b3aeca88e910be86f23a5bb6c1a2351eb23fae","f2eab39cf68de880ee7264b454044a55098e8163","54ffc4c83974d5915025f80e54e350cd30ef96d7","16b37855a02f702b0e57416de4781e95e5e3b406","774dd71f42cca20aa0eaf93fc337ab48fb123fd1","13313124277b42dc61096edd170e74b87592ddab","af625de277975bf9dae17531683002ddd7db79dc","96e626c8d03ff83b10adb45901e2e7fe247f2cd4","df8684817ada3d604c98f12b6b22512b2e29a6cd"],"id":"d36155b6f5143c784e203df82cd0de77669ecba3","s2Url":"https://semanticscholar.org/paper/d36155b6f5143c784e203df82cd0de77669ecba3","authors":[{"name":"Yulin Sun","ids":["49697536"]},{"name":"Zhao Zhang","ids":["40630838"]},{"name":"Weiming Jiang","ids":["49408546"]},{"name":"Guangcan Liu","ids":["50084588"]},{"name":"Meng Wang","ids":["47446553"]},{"name":"Shuicheng Yan","ids":["1698982"]}],"doi":"10.1109/ICPR.2018.8545863"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593978","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Many agricultural tasks are known to be dangerous for human operators, the environment, and human health in general. The increasing pressure both on safety and on production levels motivates the development of new methodologies and technologies. The rising of off-road mobile robots for agricultural application appears to be a promising contribution to required innovations. It both permits to limit the exposure of people to hazardous products and to achieve difficult and repetitive tasks. Nevertheless, to be fully efficient, autonomous robots have to ensure a high level of accuracy, while carrying potentially heavy tools, possibly in harsh conditions. It is especially the case of spraying, for which accuracy is a key challenge for reducing environmental impacts. The use of huge robots for spraying might seem to be a straightforward solution, by simply automating existing machines. Nevertheless, a simple automation does not reduce directly the environmental impact of human activities (soil compaction, energy, reduction of the use of chemical products). Moreover, huge machines are not necessarily an advantage when considering safety aspects (rollover risk and maneuverability). As a result, a solution based on the cooperation of at least two mobile robots, moving from either side of a vine row, is investigated in this paper thanks to Ultra Wide Band (UWB) technology.","inCitations":[],"pmid":"","title":"Close Coordination of Mobile Robots Using Radio Beacons: A New Concept Aimed at Smart Spraying in Agriculture","journalPages":"7727-7734","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593978"],"entities":[],"journalVolume":"","outCitations":[],"id":"a2875ea2364e5dea8264d4885abfb3337753158a","s2Url":"https://semanticscholar.org/paper/a2875ea2364e5dea8264d4885abfb3337753158a","authors":[{"name":"Thibault Tourrette","ids":[]},{"name":"Mathieu Deremetz","ids":[]},{"name":"Olivier Naud","ids":[]},{"name":"Roland Lenain","ids":[]},{"name":"Jean Laneurit","ids":[]},{"name":"Vincent De Rudnicki","ids":[]}],"doi":"10.1109/IROS.2018.8593978"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593642","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Risk-aware Multi-Policy Decision Making (MPDM)is a powerful framework for reliable navigation in a dynamic social environment where rather than evaluating individual trajectories, a \u201clibrary\u201d of policies (reactive controllers)is evaluated by anticipating potentially dangerous future outcomes using an on-line forward roll-out process. There is a core tension in Multi-Policy Decision Making (MPDM)systems - it is desirable to add more policies to the system for flexibility in finding good policies, however, this increases computational cost. As a result, MPDM was limited to small (perhaps 5\u201310)discrete policies - a significant performance bottleneck. In this paper, we radically enhance the expressivity of MPDM by allowing policies to have continuous-valued parameters, while simultaneously satisfying real-time constraints by quickly discovering promising policy parameters through a novel iterative gradient-based algorithm. Our evaluation includes results from extensive simulation and real-world experiments in semi-crowded environments.","inCitations":[],"pmid":"","title":"C-MPDM: Continuously-Parameterized Risk-Aware MPDM by Quickly Discovering Contextual Policies","journalPages":"7547-7554","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593642"],"entities":[],"journalVolume":"","outCitations":[],"id":"37af91cd391004e5b883c37414e113ded0483390","s2Url":"https://semanticscholar.org/paper/37af91cd391004e5b883c37414e113ded0483390","authors":[{"name":"Dhanvin Mehta","ids":[]},{"name":"Gonzalo Ferrer","ids":[]},{"name":"Edwin Olson","ids":[]}],"doi":"10.1109/IROS.2018.8593642"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202237","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Deep learning has significantly advanced computer vision and natural language processing. While there have been some successes in robotics using deep learning, it has not been widely adopted. In this paper, we present a novel robotic grasp detection system that predicts the best grasping pose of a parallel-plate robotic gripper for novel objects using the RGB-D image of the scene. The proposed model uses a deep convolutional neural network to extract features from the scene and then uses a shallow convolutional neural network to predict the grasp configuration for the object of interest. Our multi-modal model achieved an accuracy of 89.21% on the standard Cornell Grasp Dataset and runs at real-time speeds. This redefines the state-of-the-art for robotic grasp detection.","inCitations":["2f3bc6ea87199385538efa41ddbb95c03d67b955","f55487c8e3f6380c95355ac03c41dcc0a000df00","37c5b5717fdbcb8f714c761c73448ed6924423ea","c68c391be18920ea1c065b714692dd968bf5a15d","3dec5cbb8cb790d955ff534453a921e70dcc7123","9b4d32bcf0e887468de09796c310ffde69fd61db","0066dc9131c25e93111fc092098f6c9db6255f1a","2d069885943862c4ba59910b5ea46496c63c2dee","a29cf6dd566d30f6cd1cb3f25bde925a9c09ef37","478433a69b228810f4ef633e9013bb4fde27f0b5","59fdd6af3621f9828468d4832d1068790d2ff2bf","118408e38f25750d743a09b137459368f3f8b632","139763a0b2e78b63b245a456f2b9dbde2d1d573c","bc6a01ea112d45bdded0bb2d34a4782e4f6f16be","4587b05be6b53be74a4dbde57481c2b465ff42c9","7799d596f07ab001922220d88a86cace69663902","06788f2b13aa1673707780d9531b8318a3942cde","a09d1ea248e56f9d82346f8df07be36a4594f49d","b419fa675c6e5ec6a489033570c2926f3d8ee4f9","da180cecf42e4c34cb5ae7b08a32be4fe4b47b8c","4f854a07565be21aa8d3da90066f9b43e1790c8b","f10e2e4aafc37b08def0de1e3f097c73f155e6c9","94968968221289accb5b8e537b4f906ec400bd48","be53a0665b3f29a9459704c2851c85b12fa763ed"],"pmid":"","title":"Robotic grasp detection using deep convolutional neural networks","journalPages":"769-776","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202237","https://arxiv.org/pdf/1611.08036v2.pdf","https://arxiv.org/pdf/1611.08036v1.pdf","https://arxiv.org/pdf/1611.08036v4.pdf","https://arxiv.org/pdf/1611.08036v3.pdf","http://arxiv.org/abs/1611.08036"],"entities":["Artificial neural network","Computer vision","Convolutional neural network","Deep learning","Definition","Modal logic","Natural language processing","Real-time clock","Real-time computing","Real-time transcription","Robot end effector","SMT placement equipment"],"journalVolume":"","outCitations":["93cd30083dfbbee2a6b4f1661988a4f549645fa7","3c104b0e182a5f514d3aebecc93629bbcf1434ac","8582fd09981cb5fe9729b2014112674c2e991c1f","25a6b2bbccab3f6fa07ab5dd11243b4e5a4c3b94","07edaa3b52141dcd376a54490df3a91529f1bde3","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","a252992313e9e022fb284d7caed05d3e2d607acb","efa3e8826aab1a79d05b1f3ab55b277c0120a092","14ce7635ff18318e7094417d0f92acbec6669f1c","1407ae8725ccef768d634b2f6ec2baa2197b0bbb","3884168a29353fe2a0107454a18a1c3dc645d391","43cc625f0ddeadd6735dbf2b56902ea89498ddc1","42d3001b4b3e0a0e459715737e18153a9b16217c","3b2697d76f035304bfeb57f6a682224c87645065","27ac2adee50ccc98b502eb9165d0010699b850cd","8a9cf53fa232d912272e143801c88c57072015fb","2c55b3bc8b3f4fbbe53f878c93641fed266beb65","3ba79761192aa4bddd3342db03aa8187516c0fab","8796d9f0ba5199c628d0f8fe0a26055531b02a0c","39dba6f22d72853561a4ed684be265e179a39e4f","167abf2c9eda9ce21907fcc188d2e41da37d9f0b","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","2c03df8b48bf3fa39054345bafabfeff15bfd11d","3fe70222056286c4241488bf687afbd6db02d207","081651b38ff7533550a3adfc1c00da333a8fe86c","dbebc3af7f644116d50ee350adc910c3a2354b6e","09ac8added26307b358b83884b55af29de8b5bf9","a006fb462fcd73b8fd534c97c5e89adaa52436d6","0b544dfe355a5070b60986319a3f51fb45d1348e","09d6610dff6bc170f5b9006b7e401f378a4cd305","15cf63f8d44179423b4100531db4bb84245aa6f1","7d57b4908f212233b94cfeb8c532da14a8b71007","8fa9c9568d8de9cd3536d6f99d99fe957d45e0a1","48775433f43c824aa206a8fd6991f630945a0036"],"id":"0961912f74510efef0474c27c81e1b26bbf7f8a8","s2Url":"https://semanticscholar.org/paper/0961912f74510efef0474c27c81e1b26bbf7f8a8","authors":[{"name":"Sulabh Kumra","ids":["3140548"]},{"name":"Christopher Kanan","ids":["3290098"]}],"doi":"10.1109/IROS.2017.8202237"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202169","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Recently in tandem with the spread of portable devices for reading electronic books, devices for digitizing paper books, called book scanners, are developed to meet the increased demand for digitizing privately owned books. However, conventional book scanners still have complex components to mechanically turn pages and to rectify the acquired images that are inevitably distorted by the curvy book surface. Here, we present the multi-scale mechanism that turns pages electronically using electroadhesive force generated by a micro-scale structure. Its another advantage is that perspective correction of image processing is applicable to readily reconstruct the distorted images of pages. Specifically, to turn one page at a time not two pages, we employ a micro-scale structure to generate near-field electroadhesive force that decays rapidly and accordingly attracts objects within tens of micrometers. We analyze geometrical parameters of the micro-scale structure to improve the decay characteristics. We find that the decay characteristics of electroadhesive force definitely depends upon the geometrical period of the micro-scale structure, while its magnitude depends on a variety of parameters. Based on this observation, we propose a novel electrode configuration with improved decay characteristics. Dynamical stability and kinematic requirements are also examined to successfully introduce near-field electroadhesive force into our digitizing process.","inCitations":[],"pmid":"","title":"Automatic page-turning mechanism with near-field electroadhesive force for linearly correctable imaging","journalPages":"279-285","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202169"],"entities":["Algorithm","Book","Dynamical system","E-book","Image processing","Instability","Mobile device","PAC-PAD 1","Rectifier (neural networks)","Requirement"],"journalVolume":"","outCitations":["772daef4f4f1276b68710b5e835c7b0a282d3a88","41643aba6d385c92b6229f5d286de13e0acc3873","65698a00b5cec9096c1ad7307237879c10ebb1a8","03398e0f63a5c53b32cd5e543838ad5010a2eefc","f355b45a6047b917778f985a35a30cf949aac396","2ea1f7a5be0741050cfb4dedd8ed9017937db6de","0a6c3943b6110fbd24ef03686bbf2676b4e8b9c8","d4ac29f11a8ed7cbdddd000c564591bc60928dad","c2b287b215bedbffcc09119f6ec88e6cb8640092","332648a09d6ded93926829dbd81ac9dddf31d5b9","c00a911d955bd81d88ea055ec675a3617232313f","0ebbb44ee09326bb3f7203632c93775873cb569b","693778f7cd0f68a633ece88851b5abe2bb953538","a95ee7eb13c624d9eb2e60ef83627a1688726879","ea1279e9769f0b7238952c771997d008e7dc4942","46771ba6d902853abfe9778186f94a6384b05701"],"id":"34cee6b6530dc7e2309008e035bea18124107cdc","s2Url":"https://semanticscholar.org/paper/34cee6b6530dc7e2309008e035bea18124107cdc","authors":[{"name":"Junseok Lee","ids":["2117612"]},{"name":"Wonseok Jeon","ids":["2350223"]},{"name":"Youngsu Cha","ids":["48743827"]},{"name":"Hyunseok Yang","ids":["2635180"]}],"doi":"10.1109/IROS.2017.8202169"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593438","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Humans and mobile robots will be increasingly cohabiting in the same environments, which has lead to an increase in studies on human robot interaction (HRI). One important topic in these studies is the development of robot navigation algorithms that are socially compliant to humans navigating in the same space. In this paper, we present a method to learn human navigation behaviors using maximum entropy deep inverse reinforcement learning (MEDIRL). We use a large open dataset of pedestrian trajectories collected in an uncontrolled environment as the expert demonstrations. Human navigation behaviors are captured by a nonlinear reward function through deep neural network (DNN) approximation. The developed MEDIRL algorithm takes feature inputs including social affinity map (SAM) that are extracted from human motion trajectories. We perform simulation experiments using the learned reward function, and the performance is evaluated comparing it with the real measured pedestrian trajectories in the dataset. The evaluation results show that the proposed method has acceptable prediction accuracy compared to other state-of-the-art methods, and it can generate pedestrian trajectories similar to real human trajectories with natural social navigation behaviors such as collision avoidance, leader-follower, and split-and-rejoin.","inCitations":[],"pmid":"","title":"Learning How Pedestrians Navigate: A Deep Inverse Reinforcement Learning Approach","journalPages":"819-826","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593438"],"entities":[],"journalVolume":"","outCitations":[],"id":"8eeec65866ac8d9ec7fd9e90570b690f4808a7fc","s2Url":"https://semanticscholar.org/paper/8eeec65866ac8d9ec7fd9e90570b690f4808a7fc","authors":[{"name":"Muhammad Fahad","ids":[]},{"name":"Zhuo Chen","ids":[]},{"name":"Yi Guo","ids":[]}],"doi":"10.1109/IROS.2018.8593438"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206264","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We introduce a novel pose graph-based localization technique for autonomous driving that incorporates both sparse point features as well as lane markings on the road. Unlike many commonly used filter methods, our graph-based localization takes a much larger history of the trajectory into account. In addition, we utilize a multiple-hypothesis approach for data association for increased robustness against the presence of outliers. By incorporating both sparse point features and lane markings, we are able to take advantage of both kinds of features widely available in urban environments as well as highways. Furthermore, by using high level features, we avoid having to store and match against large amounts of data such as dense 3D point clouds. Finally, we evaluate our approach on real driving sequences on city roads.","inCitations":[],"pmid":"","title":"Precise pose graph localization with sparse point and lane features","journalPages":"4077-4082","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206264"],"entities":["Autonomous car","Autonomous robot","Correspondence problem","High-level programming language","Item Unique Identification","Lateral thinking","Point cloud","Satellite navigation","Sensor","Sparse matrix"],"journalVolume":"","outCitations":["9d50b7187557ef9738f689b4dd3d280e1c2f6613","040f9907a894a7c828489fd67ea4188e0dcf0884","5878a711a026256da906baafe5dc6f9ab60a51c2","06bc88d584a702f330bfcad7b5a43c2b5630a19c","6a94c226d9b482fbab76c11546b287ef620997a6","37be06697cb90d8392546b1365699e447984ab2d","22412b731bfe087bb510af9072079b5d1d4733e3","38aeb9e48692c066e1aacaba7f619ed2fbde55fa","924f7268d592d327f97ad4e96f48ad774d982ef3","44f874b7a754bee99f9907c3ae0ad9a69fef375b","55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1","2e9ebe0cd16e046910a8fb3f39f33a921e07bc6d","24c807f1be9857908d13bc82c6997e9f26f48354","8dfe13156288a877e2db0a79398d047e27946b99"],"id":"130f047c7abed46284aa70c812e5344e4024944f","s2Url":"https://semanticscholar.org/paper/130f047c7abed46284aa70c812e5344e4024944f","authors":[{"name":"Cong Wu","ids":["3160178"]},{"name":"Tiffany A. Huang","ids":["26943451"]},{"name":"Maximilian Muffert","ids":["2760813"]},{"name":"Tilo Schwarz","ids":["40606270"]},{"name":"Johannes Grater","ids":["32465185"]}],"doi":"10.1109/IROS.2017.8206264"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00051","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Over the past few years, Convolutional Neural Networks (CNNs) have shown promise on facial expression recognition. However, the performance degrades dramatically under real-world settings due to variations introduced by subtle facial appearance changes, head pose variations, illumination changes, and occlusions. In this paper, a novel island loss is proposed to enhance the discriminative power of deeply learned features. Specifically, the island loss is designed to reduce the intra-class variations while enlarging the inter-class differences simultaneously. Experimental results on four benchmark expression databases have demonstrated that the CNN with the proposed island loss (IL-CNN) outperforms the baseline CNN models with either traditional softmax loss or center loss and achieves comparable or better performance compared with the state-of-the-art methods for facial expression recognition.","inCitations":["1174b869c325222c3446d616975842e8d2989cf2","4739b47af26137ec52bbfb582e6f37e9e9f5aba0","6ac1dc59e823d924e797afaf5c4a960ed7106f2a","7334987d38cc09b02d291de188426eb593e8f53c","9100c3dfaf25ecce78a9cb0d0191515fddd273b7","11aa92ecea8674138c375d2bfea3ddc4f5811b45","3266b06bcbb7ab12fd8a595c3bdf3d244735a8d4","90e89d2251ea2de123c29912b07bb89efb9dd677","53d3738e867cae5f19f2bd9bb96d0d06eb0362b0"],"pmid":"","title":"Island Loss for Learning Discriminative Features in Facial Expression Recognition","journalPages":"302-309","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1710.03144v3.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2018.00051","http://arxiv.org/abs/1710.03144","http://export.arxiv.org/pdf/1710.03144"],"entities":["Baseline (configuration management)","Benchmark (computing)","Convolutional neural network","Database","Neural Networks","Softmax function"],"journalVolume":"","outCitations":["0e3fcfe63b7b6620e3c47e9751fe3456e85cc52f","10eb7bfa7687f498268bdf74b2f60020a151bdc6","2f25bdbb72990c76c12565c4aa7a6d9ee3c9045a","24f1e2b7a48c2c88c9e44de27dc3eefd563f6d39","1bcbf2a4500d27d036e0f9d36d7af71c72f8ab61","d36a1e4637618304c2093f72702dcdcc4dcd41d1","48de3ca194c3830daa7495603712496fe908375c","4cfd770ccecae1c0b4248bc800d7fd35c817bbbd","f9c431f58565f874f76a024add2aa80717ec5cf5","0278acdc8632f463232e961563e177aa8c6d6833","4fbf0b611741b1dc3d9c29738c9a498e7509cb56","81513764b73dae486a9d2df28269c7db75e9beb3","2c285dadfa6c07d392ee411d0213648a8a1cf68f","41b997f6cec7a6a773cd09f174cb6d2f036b36cd","0c67ae07ccfef4b39d161f1bcd9eb34f42f6eb13","92527ace7f75188b5ec209ff7d59f431343075e4","8e461978359b056d1b4770508e7a567dbed49776","d57982dc55dbed3d0f89589e319dc2d2bd598532","39a6cc80b1590bcb2927a9d4c6c8f22d7480fbdd","3be8964cef223698e587b4f71fc0c72c2eeef8cf","2911e7f0fb6803851b0eddf8067a6fc06e8eadd6","2a75f34663a60ab1b04a0049ed1d14335129e908","2e8a0cc071017845ee6f67bd0633b8167a47abed","f472cb8380a41c540cfea32ebb4575da241c0288","42e0127a3fd6a96048e0bc7aab6d0ae88ba00fb0","3e34632cd69be865fc4639cdd1f43a8b5f2a3ea6","ad29afdbce160d3749a3685da75bc35b16d64b95","0359f7357ea8191206b9da45298902de9f054c92","23fc83c8cfff14a16df7ca497661264fc54ed746","865d4ce1751ff3c0a8eb41077a9aa7bd94603c47","2666dd9801fe9d70ee23ab9173cbf68bee67f718","e71bf6c404cc9634ab2e0bdea3ee517911012001","24bf94f8090daf9bda56d54e42009067839b20df","235cb2c5e800dfff5f3800fdd1d195c07de7416a","33c68d6bc73e74ea042dc5b9fe966625565c475e","14efb131bed66f1874dd96170f714def8db45d90","eac2a468de0c4f241fad7696ca930bb2e11545b3","b6c53891dff24caa1f2e690552a1a5921554f994","b9db409535714b2cd59b7413bb024e9aca2bf118","3661a34f302883c759b9fa2ce03de0c7173d2bb2","4d9a02d080636e9666c4d1cc438b9893391ec6c7","162ea969d1929ed180cc6de9f0bf116993ff6e06","56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719","c5366f412f2e8e78280afcccc544156f63b516e3","044fdb693a8d96a61a9b2622dd1737ce8e5ff4fa","59b83666c1031c3f509f063b9963c7ad9781ca23","c0c0b8558b17aa20debc4611275a4c69edd1e2a7","dd600e7d6e4443ebe87ab864d62e2f4316431293","cf86616b5a35d5ee777585196736dfafbb9853b5","0334a8862634988cc684dacd4279c5c0d03704da","32a9569f90ee6254ca7e21116ffc0082422260f7","7be60f8c34a16f30735518d240a01972f3530e00","ad0863aa16301d7c6617f8b965a64cf58f38594f","4dff129a6f988d78c457ece463b774c3d81ac5c7","0568fc777081cbe6de95b653644fec7b766537b2","17a8d1b1b4c23a630b051f35e47663fc04dcf043","061356704ec86334dbbc073985375fe13cd39088","757f7f1a1cd62cf00150cdcfbdc5cd02c63553f1","160259f98a6ec4ec3e3557de5e6ac5fa7f2e7f2b"],"id":"081fb4e97d6bb357506d1b125153111b673cc128","s2Url":"https://semanticscholar.org/paper/081fb4e97d6bb357506d1b125153111b673cc128","authors":[{"name":"Jie Cai","ids":["33348020"]},{"name":"Zibo Meng","ids":["3091647"]},{"name":"Ahmed-Shehab Khan","ids":["7348573"]},{"name":"Zhiyuan Li","ids":["46947384"]},{"name":"James O'Reilly","ids":["48762636"]},{"name":"Yan Tong","ids":["1686235"]}],"doi":"10.1109/FG.2018.00051"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594050","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In recent years, various shadow detection methods from a single image have been proposed and used in vision systems; however, most of them are not appropriate for the robotic applications due to the expensive time complexity. This paper introduces a fast shadow detection method using a deep learning framework, with a time cost that is appropriate for robotic applications. In our solution, we first obtain a shadow prior map with the help of multi-class support vector machine using statistical features. Then, we use a semantic-aware patch-level Convolutional Neural Network that efficiently trains on shadow examples by combining the original image and the shadow prior map. Experiments on benchmark datasets demonstrate the proposed method significantly decreases the time complexity of shadow detection, by one or two orders of magnitude compared with state-of-the-art methods, without losing accuracy.","inCitations":["5baa96e8d0bac9096a33224a45c65695fd9af569","5b9782527082dbe986b70c988148968f20d20624","bd205857decfaea7ef60b0097298a15b297c98e7","3d9be83e1b94764a884354097e8287cb6d9f3171"],"pmid":"","title":"Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network","journalPages":"3124-3129","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594050","https://arxiv.org/pdf/1709.09283v2.pdf","http://arxiv.org/abs/1709.09283"],"entities":[],"journalVolume":"","outCitations":["3e80edfbc578652b7a62a44dda4b611170d741c9","68da903b2237cd500b98f152dd851189cad1b344","7fe7f66531ee900e57e5711cd7f9866662049e2e","3d8822970272cc01e2fd9fa0b36231a8009430b3","87b47a7a8a163cb477a46a85c20073567cccd7e8","2b5bfb6ce348aaaa744bc943d2a06cb750e233af","1a9ca36b38b8638a7c4785818d3569b329dbeaa9","49000038c77cc6b3f3318135126d22ad49b9cddb","1314e6ea34a8d749ca6190a0d2dd00b3a1879cc6","0c7f52c753a65ceaf3755e20b906ffd0c05c994a","4deb1df2bdfec06f786214bc81146181d5dd5fe7","c0e4b6a993a35ded0d17a5e751bd135b795244ae","dca73edfdfd20e08032ad3edc8348aed06cc3577","33a7a59f785ef46091c30c4c85ef88c6bdabab51","6f6bec30477559fbaa1a764879be7524e55f629c","bd580fad7a14f93d6d59765a5fe91974e2653281","1cf8464425870c125b4f3a0560651c97c3911a8e","879de3fd53210ff74a96db3a53b695fec6a015ac","0f16f6f478b5c788dce466eb50e36c612273c36e"],"id":"2fa24bedc5bc3a97664fff586dc7a84b5e027914","s2Url":"https://semanticscholar.org/paper/2fa24bedc5bc3a97664fff586dc7a84b5e027914","authors":[{"name":"Sepideh Hosseinzadeh","ids":["25864102"]},{"name":"Moein Shakeri","ids":["2627414"]},{"name":"Hong Zhang","ids":["46197387"]}],"doi":"10.1109/IROS.2018.8594050"}
{"doiUrl":"","venue":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","journalName":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","sources":[],"year":2017,"text":"Nowadays, image processing is getting more popular due to the daily increase of diverse data acquisition methods such as digital scanners and cameras. Due to the high volume of archived documents, automatic document classification methods can help to save the time and space in digital document organization. Logos in official and business documents are used to identify document identities. Different approaches have been used for logo recognition yet, many of which has complex computations to achieve a high level of precision. In this paper, a novel algorithm for accurate logo recognition with low level of computational complexity is proposed based on Local Binary Pattern (LBP). We proposed PerLogo dataset consisting 850 images of 10 different classes of logos has been proposed in this paper. Through 3 separate experiments over 50, 60, 70 images per each class the proposed system has been evaluated. Experimental results show that recognition rate is increased with increasing the number of training images per class. Experimental results show the recognition accuracy of 98% when 0.09 salt and pepper noise are added to the test images, which is more than 95% accuracy proposed by the state-of-the-art approaches achieving 95% accuracy.","inCitations":["beed7297ae7ee951ccaa8ae8760f354867eeaa24"],"pmid":"","title":"Persian logo recognition using local binary patterns","journalPages":"258-261","s2PdfUrl":"","pdfUrls":[],"entities":["Algorithm","Archive","Chili Pepper (dietary)","Class","Computation","Computational complexity theory","Data acquisition","Document classification","Experiment","Greater Than","High-level programming language","Image processing","Local binary patterns","Logo","Salt (cryptography)","Salt-and-pepper noise","Sense of identity (observable entity)","Silo (dataset)","negative regulation of trichome patterning"],"journalVolume":"","outCitations":["d8fe903697e3db05a3a620b060149a01cf702786","8e60840c391cba5f1a5f90f1d1a3092d18607e78","faf8444bad76e8aa727c8b2df42fefe7b8242957","01778409958b99e1c0f2cf8c5c27d3d08386c3fd","3a869bcdadd1e1b1b7bd092b964a7ccd2d7a7097","0d184a3444e103bb714c8acd0abd557f21611f64"],"id":"5997d9d45480c09abfddc92b32c54624bea8102b","s2Url":"https://semanticscholar.org/paper/5997d9d45480c09abfddc92b32c54624bea8102b","authors":[{"name":"Afsoon Asghari Shirazi","ids":["20998866"]},{"name":"Alireza Dehghani","ids":["47477345"]},{"name":"Hassan Farsi","ids":["1928919"]},{"name":"Mehran Yazdi","ids":["2320021"]}],"doi":""}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593389","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper proposes a wearable robotic stick for walking assistance, called \u201cHand-Free-Stick\u201d (HFS), for people with non-serious dysfunction in their gait. The basic idea of the proposed HFS is to enlarge ZMP (Zero moment point) area of a user under hands free conditions and to augment his/her body balance ability in walking. A boots type prototype of the HFS is developed with a lightweight robotic stick using a servomotor, in which the slider-link mechanism works to regulate the stick angle and length at the same time. The stick motion is controlled by a single-board computer based on the distribution of foot/feet pressures measured by the sensor system using eight load cells attached at the sole of boots. A set of walking tests with/without the prototype of HFS is carried out for four healthy subjects and demonstrates the effectiveness of the proposed HFS to expand the ZMP area leading to walking balance assistance.","inCitations":[],"pmid":"","title":"Robotic Hand-Free-Stick for Walking Balance Assistance","journalPages":"5445-5450","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593389"],"entities":[],"journalVolume":"","outCitations":[],"id":"c08c7daca714fb38e01ae9675a82f64f2c1d217c","s2Url":"https://semanticscholar.org/paper/c08c7daca714fb38e01ae9675a82f64f2c1d217c","authors":[{"name":"Yoshiyuki Tanaka","ids":[]},{"name":"Naoki Oyama","ids":[]},{"name":"Takazumi Takenaka","ids":[]}],"doi":"10.1109/IROS.2018.8593389"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593909","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents the foundations of a MATLAB framework for dynamic modeling and simulation of closed kinematic chain (CKC) mechanisms, with a particular focus on implementation with legged locomotive mechanisms. As such, the framework supports both floating-base and fixed-base systems. Through the use of singular perturbation theory, various CKC mechanisms can be modeled so that constraint errors asymptotically converge to zero, thus avoiding the numerical drift that plagues commonly used methods. A functional API and the relevant core commands necessary to construct a model are presented. Two robotic legs incorporating CKC mechanisms are utilized as case studies, and simulations of each leg performing a dynamic monopedal gait are illustrated.","inCitations":[],"pmid":"","title":"A Framework for Modeling Closed Kinematic Chains with a Focus on Legged Robots","journalPages":"2733-2738","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593909"],"entities":[],"journalVolume":"","outCitations":[],"id":"39695b8429d3b9d30ed1f4a6beb9272211105b7f","s2Url":"https://semanticscholar.org/paper/39695b8429d3b9d30ed1f4a6beb9272211105b7f","authors":[{"name":"Vinay R. Kamidi","ids":[]},{"name":"Adam Williams","ids":[]},{"name":"Pinhas Ben-Tzvi","ids":[]}],"doi":"10.1109/IROS.2018.8593909"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202139","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In the present work, we propose an active learning framework based on optimal query paths to efficiently address the problem of tactile object shape exploration. Most previous approaches perform active touch probing at discrete query points, which leads to inefficient touch-and-retract motions. In contrast, in this paper we propose to query information efficient sliding paths instead of only touch locations. This is realized by three components: A Gaussian process implicit surface model represents the shape and uncertainty of the object. A compliant task/force controller framework fuses the information of this GP model into the parameterization of its tasks, which enables the robot to slide over the unknown object safely and robustly. Thirdly, we develop two strategies to solve the proposed active path querying learning problem. Sliding along those query paths not only creates more dense data than touch probing, but additionally greatly reduces the uncertainty of the object. We demonstrate the effectiveness of our proposed framework both in simulation and on the PR2 robot platform. Furthermore, it is shown that our methodology can be extended to other learning tasks, such as finding a desired surface normal on an unknown object, e.g. for pushing.","inCitations":["1e2ec73184f2ee8918d404c10b8775c91260646c","732bf05a9a31e0b658cb9f4d29b9f8d5523294eb","fbcfaa1902a7059d596a1d378e63ed48c0014c93"],"pmid":"","title":"Active learning with query paths for tactile object shape exploration","journalPages":"65-72","s2PdfUrl":"","pdfUrls":["http://ipvs.informatik.uni-stuttgart.de/mlr/papers/17-driess-IROS.pdf","https://doi.org/10.1109/IROS.2017.8202139"],"entities":["Active learning (machine learning)","Digraphs and trigraphs","Emoticon","Execution unit","Gaussian process","Greedy algorithm","Implicit surface","Normal (geometry)","Robot","Simulation","Tactile sensor"],"journalVolume":"","outCitations":["0257bb33abcfd44c51011a33795a4d1693807456","1e1c631b51116a784f096dd10972018082911ec4","35e03bf99ed0b6e72633131507e1527b474db094","24bcb200832359f133508fb693fc449fdaa80f60","35bcda5a6304c1b0122e0efe8bf23b7d8de00132","1b440ab940ef64d676c7a63572b6a1544f4c4f76","f95df2c4e69c6c13b252facb0d7a9a90d7ad2699","104792634bf5dbbeb70de4f01e1e0b900fb68222","174f184f7b8d694628b9dd0e3d6b443641434d18","5ef3555a83a7b0ef00e1e2a47a2d41ae2a900517","47598c6267a065ad0f9226c0a130728fedf18b81","04dfefe75f2b1dbd9e00f263057216917f932fea","5b69f06478f227a3b8ac809c52666f049c7d40bc","b2ee1acb674533d18f26648465d25593666ed60e","e48dfad31e7499de6dc759e619344168d549af6e"],"id":"13a594063e22ce036c6e979b2faebfd779903b62","s2Url":"https://semanticscholar.org/paper/13a594063e22ce036c6e979b2faebfd779903b62","authors":[{"name":"Danny Drieß","ids":["30837327"]},{"name":"Peter Englert","ids":["3060230"]},{"name":"Marc Toussaint","ids":["1731096"]}],"doi":"10.1109/IROS.2017.8202139"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594288","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper we present a new robotic control approach inspired in the animal nervous system control. The system implements the binomial Brain-Peripheral Nervous System (CNS-PNS) combining the use of microprocessors as the high level control, or brain, and FPGAs as the low level control, or nervous system. Thanks to the new open source tools for FPGAs, we are able to apply them in the field of robotics in new ways that were impossible before. In this paper, we will demonstrate that our approach is not only able to control the movements of robots using digital circuits built inside an FPGA, but is also capable of generating, synthesizing and uploading them inside the FPGA in real time and on demand.","inCitations":[],"pmid":"","title":"Hybrid Bio-Inspired Architecture for Walking Robots Through Central Pattern Generators Using Open Source FPGAs","journalPages":"7071-7076","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594288"],"entities":[],"journalVolume":"","outCitations":[],"id":"c3bb16e046bacb44d1f025df3b62fd75dcf17308","s2Url":"https://semanticscholar.org/paper/c3bb16e046bacb44d1f025df3b62fd75dcf17308","authors":[{"name":"Julian Caro Linares","ids":[]},{"name":"Antonio Barrientos","ids":[]},{"name":"Enric Mayas Marquez","ids":[]}],"doi":"10.1109/IROS.2018.8594288"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593874","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The enhanced dexterity and manipulability offered by continuum manipulators makes them the robots of choice for complex procedures inside the human body. However, without tailored analytical tools to evaluate their manipulability, many capabilities of continuum robots such as safe and effective manipulation will remain largely inaccessible. This paper presents a quantifiable measure for analysing force/velocity manipulability of continuum robots. We expand classical measures of manipulability for rigid robots to introduce three types of manipulability indices to continuum robots, namely, velocity, compliance, and unified force-velocity manipulability. We provide a specific case study using the proposed method to analyse the force/velocity manipulability for a concentric-tube robot. We investigate the application of the manipulability measures to compare performance of continuum robots in terms of compliance and force-velocity manipulability. The proposed manipulability measures enable future research on design and optimal path planning for continuum robots.","inCitations":[],"pmid":"","title":"Force/Velocity Manipulability Analysis for 3D Continuum Robots","journalPages":"4920-4926","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593874"],"entities":[],"journalVolume":"","outCitations":[],"id":"c67f0b41b521978c9e2497801c62b155a85faee9","s2Url":"https://semanticscholar.org/paper/c67f0b41b521978c9e2497801c62b155a85faee9","authors":[{"name":"Mohsen Khadem","ids":[]},{"name":"Lyndon Da Cruz","ids":[]},{"name":"Christos Bergeles","ids":[]}],"doi":"10.1109/IROS.2018.8593874"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593617","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"For a robot to manipulate an object, it has to understand the functions and the actions that can be subjected to the object. This set of information is known as affordance of the object. Affordances are generally defined by the geometrical structures and physical properties of the objects. In this paper, we present an affordance detection network (ADNet) for detecting object affordances using multimodal input i.e., RGB-D data. The method is based on the state-of-the-art fully convolutional network with two encoding streams and one decoding stream. In the presented formulation, the network learns powerful discriminative features independently from the RGB and depth images, which enables it to abstract rich photometrical and geometrical properties of the objects. The multimodal encoding is combined at multiple stages of the network using the late-fusion strategy and used is for predicting the potential affordances of the objects.","inCitations":[],"pmid":"","title":"Predicting Part Affordances of Objects Using Two-Stream Fully Convolutional Network with Multimodal Inputs","journalPages":"3096-3101","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593617"],"entities":[],"journalVolume":"","outCitations":[],"id":"32351e6d9397cc65c9c4d4a9b3ae022a32ce1153","s2Url":"https://semanticscholar.org/paper/32351e6d9397cc65c9c4d4a9b3ae022a32ce1153","authors":[{"name":"Krishneel Chaudhary","ids":[]},{"name":"Kei Okada","ids":[]},{"name":"Masayuki Inaba","ids":[]},{"name":"Xiangyu Chen","ids":[]}],"doi":"10.1109/IROS.2018.8593617"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545047","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a novel integrated framework for learning both text detection and recognition. For most of the existing methods, detection and recognition are treated as two isolated tasks and trained separately, since parameters of detection and recognition models are different and two models target to optimize their own loss functions during individual training processes. In contrast to those methods, by sharing model parameters, we merge the detection model and recognition model into a single end-to-end trainable model and train the joint model for two tasks simultaneously. The shared parameters not only help effectively reduce the computational load in inference process, but also improve the end-to-end text detection-recognition accuracy. In addition, we design a simpler and faster sequence learning method for the recognition network based on a succession of stacked convolutional layers without any recurrent structure, this is proved feasible and dramatically improves inference speed. Extensive experiments on different datasets demonstrate that the proposed method achieves very promising results.","inCitations":[],"pmid":"","title":"A Novel Integrated Framework for Learning both Text Detection and Recognition","journalPages":"2233-2238","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545047","http://arxiv.org/abs/1811.08611","https://arxiv.org/pdf/1811.08611v1.pdf"],"entities":[],"journalVolume":"","outCitations":["3470684522ba013135a61fd6644a102e2f14cc7c","8011e446f3043357b1b7d5a2e502dc02a8ca5a02","2950be66e7b4c94dbb16e3319d8bece5da4e799f","64ff7f81f066a26a40f52e41931a97c166db094d","14318685b5959b51d0f1e3db34643eb2855dc6d9","8e9149ab00236d04db23394774e716c4f1d89231","21a1654b856cf0c64e60e58258669b374cb05539","2c4c976fdf3d8191caeb097c07f64e500b2f6712","83db1afa7f0f693db0b596b88b8533abea08ee45","b73099c82c16e829320a2c50b44a9fd8f3ec5b4f","2a94c84383ee3de5e6211d43d16e7de387f68878","04a10e1b25f35a9ac1a4d4344bfbdb34b253cb59","20a78d3145279dcd799cd7a856ae2714f4863a16","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a","b1a25f665cd18b2a22e5804a2a012af282d59f1b","a07b88a0417c3803d87f57138a72dd11c0370e2c","261a056f8b21918e8616a429b2df6e1d5d33be41","2c03df8b48bf3fa39054345bafabfeff15bfd11d","a8f24fcc1eb0354ffd91f0e3031f5c4dc3e02dd6","031dcef770fc3b56810d2d11d69bfc73ec4b5a62","4013e08f5a581663c00f527872563a88b707b195","061356704ec86334dbbc073985375fe13cd39088","424561d8585ff8ebce7d5d07de8dbf7aae5e7270"],"id":"75c410bb51c5151f2642ce43035ba7bc879634bb","s2Url":"https://semanticscholar.org/paper/75c410bb51c5151f2642ce43035ba7bc879634bb","authors":[{"name":"Wanchen Sui","ids":["2603501"]},{"name":"Qing Zhang","ids":["39145751"]},{"name":"Jun Yang","ids":["47987957"]},{"name":"Wei Chu","ids":["2015610"]}],"doi":"10.1109/ICPR.2018.8545047"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593732","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a novel method for the control of the elbow joint of the humanoid robot TEO, based on a fractional order PD controller. Due to the graphical nature of the proposed method, a few basic operations are enough to tune the controller, offering very competitive results compared to classic methods. The experiments show a robust performance of the system to mass changes at the tip of the humanoid right arm.","inCitations":[],"pmid":"","title":"A Robust Control Method for the Elbow of the Humanoid Robot TEO Based on a Fractional Order Controller","journalPages":"6378-6383","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593732"],"entities":[],"journalVolume":"","outCitations":[],"id":"d6c2cae80ed89428c1a8fbc73bc64ac044fabf8c","s2Url":"https://semanticscholar.org/paper/d6c2cae80ed89428c1a8fbc73bc64ac044fabf8c","authors":[{"name":"Jorge Munoz","ids":[]},{"name":"Concepción A. Monje","ids":[]},{"name":"Fernando Martin","ids":[]},{"name":"Carlos Balaguer","ids":[]}],"doi":"10.1109/IROS.2018.8593732"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206418","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper we introduce a biped foot placement controller based on capture point (CP) feedback control. This capture point feedback controller generates desired Zero Moment Point (ZMP) according to current capture point error. We call this desired ZMP as Control-ZMP (cZMP). Using this cZMP, we constructed (i) a disturbance adapting walking pattern generator, (ii) an ankle torque reference generator, (iii) a landing foot position adjustor and, (iv) a step time adjustor. By applying these controllers, the biped system becomes robust over uneven terrain and external pushing disturbances. We considered cZMP as a key indicator of bipedal walking stability. If the cZMP is within the support polygon, the robot will track the cZMP by using (i) and (ii). If the cZMP is outside of the support polygon, the robot will change footstep position and time by using (iii) and (iv) (COM pattern will be changed by (i) too). Capture point dynamics is described based on a linear inverted pendulum (LIP) model. The performance of the suggested control algorithm is validated in the Choreonoid simulator with a model of DRC-HUBO+ [11], [12].","inCitations":[],"pmid":"","title":"Biped walking stabilization based on foot placement control using capture point feedback","journalPages":"5263-5269","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206418"],"entities":["Algorithm","Capture the flag","Chaos theory","Control theory","Feedback","Inverted pendulum","Linear logic","Robot","The Walking Dead: Season Two","Zero moment point"],"journalVolume":"","outCitations":["84824f4fb57b36cb9a10a20a170e7c4d9a081730","efb290c2beff46114dea3b90e1e3d06bceac6941","2b780e58cc2ca41f4944de8765e7e242cd13fe5c","5b9c2ab77edcd97f4f36a9138e8411fc6dc390a3","8a03dfd25b1ee9003acb57cdd659a6682e87dfa5","d8b4d3779dc3d6ccfb70d62b7e2c720ce482edf2","6c267aafaceb9de1f1af22f6f566ba1d897054e4","531962c15688082e94b916869575d64463f72686","b9b0dee991d4feb701d23b1fcf4687139a46b212","c8a020532acf240d28686ca55b368e7c28f6e8ec","3b979cca666dcb0d7f2cfb49223227772458c38b","55e9f0598a3c61753c12043719e2c69724faa84d","4e673086be8581fd90ab8f02c8a7e428ce398056","35a4f4f0f764ee0301e7e8f2f5c3f2ed02ddc267","791e78fc07fe0cd34d78318b17dee712d36db9d8","ae2eafcdf872cebbb2d902183f1aff72b90c604e","4f534e3244e01976ebe03e890fe4739f272c27e7","6a316e0d44e35a55c41a442b3f0d0eb1f9d4d0ca","5f12826e560141add9470a36349a93c6b31b2640","388c5e783f8d8cda66b324b456cbb5a5111a6153"],"id":"1138bcb019bad9d3b607f0f7df08897b4b8a6ff0","s2Url":"https://semanticscholar.org/paper/1138bcb019bad9d3b607f0f7df08897b4b8a6ff0","authors":[{"name":"Hyobin Jeong","ids":["49146574"]},{"name":"Okkee Sim","ids":["3124306"]},{"name":"Hyoin Bae","ids":["34646057"]},{"name":"Kang Kyu Lee","ids":["3239593"]},{"name":"Jaesung Oh","ids":["34707606"]},{"name":"Jun-Ho Oh","ids":["1742518"]}],"doi":"10.1109/IROS.2017.8206418"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594439","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Nine degrees of freedom (DOF) inertial measurement units (IMUs) comprised of three-axis magnetometers, three-axis accelerometers, and three-axis angular rate sensors are commonly used in attitude and heading reference systems (AHRSs). Two classes of AHRSs exist: systems that estimate true-North heading and systems that estimate magnetic-North heading. True-North heading AHRSs require high-end angular rate sensors which are sensitive enough to dynamically estimate Earth-rate (typically fiber-optic and ring-laser gyros), while magnetic-North AHRSs employ gyros that are not sensitive enough to dynamically estimate Earth-rate (i.e. all MEMS gyros). Thus, magnetic-North AHRSs employ magnetometers for estimating heading. This paper will focus on this class of magnetic-North AHRSs. These systems fuse IMU measurements to generate estimates of the instrument's roll, pitch, and magnetic heading. However, their accuracy is limited by sensor measurement bias that is unknown a priori. Hence, accurate sensor bias estimation and compensation is essential for true attitude estimation. This paper reports a novel adaptive sensor bias observer for sensor measurement biases in 9-DOF IMUs. The algorithm requires smaller angular movements of the instrument than other reported sensor bias calibration methods, does not require a priori knowledge of local fields like the local magnetic field or the local gravity vector, and does not require knowledge of the attitude of the instrument. Stability proofs, preliminary simulations, and a full-scale vehicle experimental evaluation are reported.","inCitations":[],"pmid":"","title":"Adaptive Sensor Bias Estimation in Nine Degree of Freedom Inertial Measurement Units: Theory and Preliminary Evaluation","journalPages":"5555-5561","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594439"],"entities":[],"journalVolume":"","outCitations":[],"id":"5583875b110b2de339ff3f1b2b46219327a6491b","s2Url":"https://semanticscholar.org/paper/5583875b110b2de339ff3f1b2b46219327a6491b","authors":[{"name":"Andrew R. Spielvogel","ids":[]},{"name":"Louis L. Whitcomb","ids":[]}],"doi":"10.1109/IROS.2018.8594439"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206480","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Six-degree of freedom (DOF) inertial measurement units (IMUs) are widely used for attitude estimation. However, such systems' accuracy is limited by the accuracy of calibration of bias, scale factors, and non-orthogonality of sensor measurements. This paper reports a stable adaptive estimator of measurement bias in six-DOF IMUs and preliminary simulation results employing a commercially available IMU comprising a 3-axis fiber optic gyroscope (FOG) with a 3-axis micro-electro-mechanical systems (MEMS) accelerometer. A stability proof of the adaptive estimator and preliminary numerical simulation results are reported. The simulation results for a rotating IMU configuration are promising, and further experimental evaluation and extension of the algorithm for the case of a translating IMU configuration typically found on moving robotic vehicles are needed.","inCitations":["c41715803c46e02b3686e10b53e142dc9c0369f9"],"pmid":"","title":"Adaptive estimation of measurement bias in six degree of freedom inertial measurement units: Theory and preliminary simulation evaluation","journalPages":"5880-5885","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206480","https://dscl.lcsr.jhu.edu/wp-content/uploads/2017/09/spielvogel2017iros.pdf"],"entities":["Algorithm","Fibre optic gyroscope","Microelectromechanical systems","Numerical weather prediction","Optical fiber","Robot","Simulation"],"journalVolume":"","outCitations":["a25489c669123f1c4d81b926eacbcdb1863bfce9","52d1f3c115c869ec399b3d1a35c0c4440af01d19","ca2ae065dad2dcdb0685763d945c29c25c2cbf2d","87411dc74a5d7b6c977d01d870e889bea054a6c9","147ba5f06219004b704ac4c71ed84e5a152e83f4","7c037a591ca9ac0c8fa2ff0079d41518731c84d4","33f50e844b19f7b005654a341035d8f87e0f3a9e","14a99874bc497ecc33396c5153900511cea4df68","f7e7eb0c9dc8e6e89a476f201c1d4b6f3dca57b7","a97e99d958e48fdc427dd981753cb1accab57b91","99739065ebe9182e7698ab0f1edf904dcf48172f","083f73bdb41e38974802967e7ead1c7b66faad76","7061bad0ab23b065690ec858ab7bda94181c1a2f","45245511cd406ae45202293ee36d5163c1f50442","4da5e9c6a49237b8b411c1947cc8370d51000f4f","6e647113a3fc32b819d7da1384dae2ed21808f58","2f786d106953536c839c0d21efe20ab34e86d12e"],"id":"23fab09a02a17d86ef8604ad0e8a44a295ed0f97","s2Url":"https://semanticscholar.org/paper/23fab09a02a17d86ef8604ad0e8a44a295ed0f97","authors":[{"name":"Andrew R. Spielvogel","ids":["39209874"]},{"name":"Louis L. Whitcomb","ids":["1696120"]}],"doi":"10.1109/IROS.2017.8206480"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545528","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Many works in the literature have used machine learning techniques to solve their classification problems in different knowledge areas, e.g., medicine, agriculture, and remote sensing. Since there is no a single machine learning technique that achieves the best results for all kind of applications, a good alternative is the fusion of classification techniques, also known as multiple classifier systems (MCS). A common challenge in MCS is the selection of a few classifiers among many classifiers that are available in the literature; using all possible classifiers is not a feasible alternative. The choice of the classifiers becomes an essential factor, i.e., we need an ensemble selection approach. In this work, we propose a novel graph-based approach for static ensemble selection (GASES) to find or choose the best classifier set for remote sensing image classification. Experiments demonstrate that GASES improves performance by up to 70% over different baseline approaches when fusing classifiers. It decreases the number of classifiers used while retaining the effectiveness of using all of the classifiers. Furthermore, our proposed method is a more straightforward and intuitive technique for static ensemble selection scheme than other baseline approaches such as Consensus and Kendall.","inCitations":[],"pmid":"","title":"A Graph-based Approach for Static Ensemble Selection in Remote Sensing Image Analysis","journalPages":"344-349","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545528"],"entities":[],"journalVolume":"","outCitations":["8d98b5fe982464c8eb1c14d075ee896eb23902bb","c4eb2d12d5e96af200bcd2b345c0fdc4bf7a4d4b","1928764ba5aa95881f058f93d02a0944af4eab54","3796bc36d459f60c439efa4547f70e14509f96cb","08ed7d03b934dd7efdf2e476e84be6d6f7ef477a","0a448be0cab627ac8d901f90a771599a9309b444","d1ee87290fa827f1217b8fa2bccb3485da1a300e","8b78aa7954d05a3dd5062c2dc2f6e16112b62a88","7c722776d5b156987cedcde1e0b02cef5ea956c4","1bbb6384076ff6aeb0d1a2c499a6026959671a9c","403b4f22d9a68fceebb19f683fa10a139152ec32","5cadf2a39a144354ba7a0e213ec4c3696830b477","73b874ba6aba479977ce73d5f94fd8dfabdcafad","6542124901a5eded025d60f4e6489b6f94058ff0","604954a3600f749b25a9f52317a42d13a8ec0339","0a11cd64f46b34fee230840684dae3cc8e1905a8","12efe7121afc747afb89b85feed96a7d301051e9","6bbfe86cf101117e43650cfbcf1684830d1e21c0","9dcc9bdebb5f62ff1bc0ac5c817d672fb8045de2","b41d0fa5fdaadd47fc882d3db04277d03fb21832","5b1e6d9c51f356cd2b90269c0dbe02f0d742f5a9","92f727da41038c350989cb932236b107df5fb7f4","2ad10e773a95d182902a8ea2faa0cf7f1053f566","b3124b15f786064c33b979a630be0685f43c4900","71451b67a47603f68e3cb22cae45b79512a9204d"],"id":"10f60deeb4cace39e78890e0873619e38c7656b3","s2Url":"https://semanticscholar.org/paper/10f60deeb4cace39e78890e0873619e38c7656b3","authors":[{"name":"Fábio Augusto Faria","ids":["1989350"]},{"name":"S. Sarkar","ids":["46859637"]}],"doi":"10.1109/ICPR.2018.8545528"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593810","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"3D visual understanding of the surrounding environment is vital for successful mobile robotic tasks such as autonomous navigation or general object interaction. However, current systems have limited perceptual capabilities in the sense that they are not very well adaptable to unknown environments. Human operators, on the other hand, are experts in adapting to previously unknown information. Hence, human-robot teaming in which the human helps the robot to adapt to new environments and the robot assists in automated object recognition to efficiently feed the control environment of the operator is advantageous. In this work, we propose an object recognition and localization system for mobile robots, based on deep learning, and we study the adaptation of the resulting robotic perception to a new environment. We propose two methods to teach the robot a new object category: using prior knowledge and using limited operator input. We conducted several experiments to show the feasibility of proposed methods.","inCitations":[],"pmid":"","title":"Incremental Learning-Based Adaptive Object Recognition for Mobile Robots","journalPages":"6263-6268","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593810"],"entities":[],"journalVolume":"","outCitations":[],"id":"6f7639aafb30f1651b0526eb96eeafe5f4d076f9","s2Url":"https://semanticscholar.org/paper/6f7639aafb30f1651b0526eb96eeafe5f4d076f9","authors":[{"name":"Mehmet Ozgur Turkoglu","ids":[]},{"name":"Frank B. ter Haar","ids":[]},{"name":"Nanda van der Stap","ids":[]}],"doi":"10.1109/IROS.2018.8593810"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545146","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Attribute representations became relevant in image recognition and word spotting, providing support under the presence of unbalance and disjoint datasets. However, for human activity recognition using sequential data from on-body sensors, human-labeled attributes are lacking. This paper introduces a search for attributes that represent favorably signal segments for recognizing human activities. It presents three deep architectures, including temporal-convolutions and an IMU centered design, for predicting attributes. An empiric evaluation of random and learned attribute representations, and as well as the networks is carried out on two datasets, outperforming the state-of-the art.","inCitations":["03046f4dd5c1535a9d97662f75d1e3fc9bda407d","73c4d79a41354ccf5ff30d781f27c4fb5b510627","096a4b3fb836dd9fa8f83889bb64d2b62266ff46"],"pmid":"","title":"Learning Attribute Representation for Human Activity Recognition","journalPages":"523-528","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1802.00761","https://arxiv.org/pdf/1802.00761v1.pdf","http://patrec.cs.tu-dortmund.de/pubs/papers/MoyaRueda2018_ICPR.pdf","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545146"],"entities":[],"journalVolume":"","outCitations":["0566bf06a0368b518b8b474166f7b1dfef3f9283","39a1abcbb87d8ff48ee47d446411a3455451f25b","8ff67b8bb53883f2e1c30990bdf44ec2d8b502b9","3408dd51ced6e046bb89040ac2a363e683d32304","9201bf6f8222c2335913002e13fbac640fc0f4ec","856c09ab10efbc8c61a84a951746654d947370f3","0170c5a8a212ae5b2c10aa7ee66d7e944e8fc96b","3ef1b0469b43acc8ede2e56d8f001ad090b04826","baafe3253702955c6904f0b233e661b47aa067e1","8f81b587105026f1718f3b5353241b7aee39ba26","6f9f143ec602aac743e07d092165b708fa8f1473","0b3cfbf79d50dae4a16584533227bb728e3522aa","d99bd4ab6b799fe45a88438d349fb94225468b50","13ffeaf5f41484f1204a53fb31235e6350da37f3","add39272e8762cea5a24c95ad238af5d61c3bd54","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","84f7b9d697ad786a6ca6e4170c072b5882803229","93a487990aea7ad4b72ffd47b5eb362494c1666f","08c71fe89af5f168b373e92b44a7cf9d1755fb8c","57a4071ad8a563b1a84bacea29fb33c0b0d2cc7d"],"id":"622e44206b4f0fb358ba61bab5c476e5e6c6b05a","s2Url":"https://semanticscholar.org/paper/622e44206b4f0fb358ba61bab5c476e5e6c6b05a","authors":[{"name":"Fernando Moya Rueda","ids":["40380855"]},{"name":"Gernot A. Fink","ids":["1749475"]}],"doi":"10.1109/ICPR.2018.8545146"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593863","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Precise Point Positioning (PPP) offers advantages over Real-Time Kinematic (RTK) positioning in that it needs no ground base station or a rover to provide a communication channel with the base station. Therefore, PPP is expected to be applied in various fields, such as precise agriculture, intelligent transportation, construction, and mining. One major drawback of PPP is that it takes several tens of minutes for initial convergence to reach an adequate level of accuracy, and how to shorten the convergence time remains the key issue regarding the proliferation of PPP. In our previous work, we proposed a method of drastically reducing this initial convergence time of PPP (called \u201cARIADNE\u201d) by using an accurate fiducial marker whose position in Earth-fixed coordinates has been accurately measured using an onboard camera. In this contribution, in order to improve both the positioning accuracy and reliability, we introduce some new techniques to ARIADNE: (1) carrier phase ambiguity resolution (AR); and (2) estimating displacement of the marker's orientation. AR results in doubling the positioning accuracy, while displacement estimation enables the detection of any change in the marker's installation orientation and compensation for the effect thereof in the initialization process. Analytical results based on actual data acquired with a prototype system show that the above method and techniques work very well with a realistic setup of PPP-AR and marker performance, and successfully reduce the initial convergence time from tens of minutes to less than a minute.","inCitations":[],"pmid":"","title":"ARIADNE with Ambiguity Resolution: Visual Marker Based Rapid Initialization of PPP-AR","journalPages":"7362-7368","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593863"],"entities":[],"journalVolume":"","outCitations":[],"id":"d6de81b66a13e57fd6687dbcb10c92e48bb97103","s2Url":"https://semanticscholar.org/paper/d6de81b66a13e57fd6687dbcb10c92e48bb97103","authors":[{"name":"Keisuke Watanabe","ids":[]}],"doi":"10.1109/IROS.2018.8593863"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593841","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"One of the safest and most reliable strategies for vehicle's collision avoidance is embedded control at low level to guarantee safe motion in all situations using on-board sensors. In this paper, we propose a novel lightweight collision avoidance strategy that can be implemented as a low level motion control to achieve safe motion while simultaneously tracking the robot's reference control input. This strategy is designed to be general so that it can be easily integrated with most control designs, with the primary target of resource-constrained robot swarms that act in real-time, dynamic environments. The main advantages of our approach are a very simple structure and low computational requirements. We verified the effectiveness of the proposed collision avoidance strategy through two simulated scenarios and with physical robots. We believe our design can be directly used in many areas, such as autonomous driving, intelligent transportation and planetary exploration.","inCitations":[],"pmid":"","title":"Lightweight Collision Avoidance for Resource-Constrained Robots","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593841"],"entities":[],"journalVolume":"","outCitations":[],"id":"044c6f575723a251485fab70410b6e4ad0588c55","s2Url":"https://semanticscholar.org/paper/044c6f575723a251485fab70410b6e4ad0588c55","authors":[{"name":"Mohammadali Shahriari","ids":[]},{"name":"Ivan Svogor","ids":[]},{"name":"David St-Onge","ids":[]},{"name":"Giovanni Beltrame","ids":[]}],"doi":"10.1109/IROS.2018.8593841"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594176","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Objects in space often exhibit a tumbling motion around the major inertial axis. In this paper, we address the image based visual servoing of a robotic system towards an uncooperative tumbling object. In contrast to previous approaches that require explicit reconstruction of the object and an estimation of its velocity, we propose a novel controller that is able to minimize the feature error directly in image space. This is achieved by observing that the feature points on the tumbling object follow a circular path around the axis of rotation and their projection creates an elliptical track in the image plane. Our controller minimizes the error between this elliptical track and the desired features, such that at the desired pose the features lie on the circumference of the ellipse. The effectiveness of our framework is exhibited by implementing the algorithm in simulation as well on a mobile robot.","inCitations":[],"pmid":"","title":"Image Based Visual Servoing for Tumbling Objects","journalPages":"2901-2908","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594176"],"entities":[],"journalVolume":"","outCitations":[],"id":"452061fe2e31803751d3070aecc37252388bf53b","s2Url":"https://semanticscholar.org/paper/452061fe2e31803751d3070aecc37252388bf53b","authors":[{"name":"P. Mithun","ids":[]},{"name":"Harit Pandya","ids":[]},{"name":"Ayush Gaud","ids":[]},{"name":"Suril V. Shah","ids":[]},{"name":"K. Madhava Krishna","ids":[]}],"doi":"10.1109/IROS.2018.8594176"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593948","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Highly accurate and robust real-time localization is an essential technique for various autonomous driving applications. Numerous localization methods have been proposed that combine various types of sensors, including an environmental sensor, IMU and GPS. However, the usage of a single environmental sensor is rather fragile. Although the use of multi-environment sensors is a better alternative, fusion methods from previous studies have not adequately compensated for shortcomings in dissimilar sensors or have not considered errors in the pre-built map. In this paper, we propose a decentralized localization framework using heterogeneous map-matching sources. Decentralized localization performs two independent map-matchings and integrates them with a stochastic situational analysis model. By applying a stochastic model, the reliability of the two map matchings is collected and system stability is verified. A number of experiments with autonomous vehicles within the actual driving environment have shown that combining multiple map-matching sources ensures more robust results than the use of a single environmental sensor.","inCitations":[],"pmid":"","title":"Decentralized Localization Framework using Heterogeneous Map-matchings","journalPages":"2183-2189","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593948"],"entities":[],"journalVolume":"","outCitations":[],"id":"06fcf9560ce8cf54708ddf08d5324df8ace5c562","s2Url":"https://semanticscholar.org/paper/06fcf9560ce8cf54708ddf08d5324df8ace5c562","authors":[{"name":"Soomok Lee","ids":[]},{"name":"Jung-Roon Kim","ids":[]},{"name":"Jung-Woo Kim","ids":[]},{"name":"Gyu-Min Oh","ids":[]},{"name":"Seung-Woo Seo","ids":[]}],"doi":"10.1109/IROS.2018.8593948"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594327","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We address the problem of audio-visual gaze control in the specific context of human-robot interaction, namely how controlled robot motions are combined with visual and acoustic observations in order to direct the robot head towards targets of interest. The paper has the following contributions: (i) a novel audio-visual fusion framework that is well suited for controlling the gaze of a robotic head; (ii) a reinforcement learning (RL) formulation for the gaze control problem, using a reward function based on the available temporal sequence of camera and microphone observations; and (iii) several deep architectures that allow to experiment with early and late fusion of audio and visual data. We introduce a simulated environment that enables us to learn the proposed deep RL model without the need of spending hours of tedious interaction. By thoroughly experimenting on a publicly available dataset and on a real robot, we provide empirical evidence that our method achieves state-of-the-art performance.","inCitations":[],"pmid":"","title":"Deep Reinforcement Learning for Audio-Visual Gaze Control","journalPages":"1555-1562","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594327"],"entities":[],"journalVolume":"","outCitations":[],"id":"6f8b824efc17bf95336de936acfb84730bfa8856","s2Url":"https://semanticscholar.org/paper/6f8b824efc17bf95336de936acfb84730bfa8856","authors":[{"name":"Stéphane Lathuilière","ids":[]},{"name":"Benoit Massé","ids":[]},{"name":"Pablo Mesejo","ids":[]},{"name":"Radu Horaud","ids":[]}],"doi":"10.1109/IROS.2018.8594327"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546106","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Lung cancer has been the most prevalent cancer in the world and an effective way to diagnose the cancer at the early stage is to detect the pulmonary nodule by computer-aided system. However, the size of the pulmonary nodules varies and the one with small diameter is generally one of the most difficult cases to diagnose. Under this condition, traditional convolution network based nodule classification methods fail to achieve satisfied result due to the miss of tiny but vital features by the pooling operation. To tackle this problem, we propose a novel 3D spatial pyramid dilated convolution network to classify the malignancy of the pulmonary nodules. Instead of using the pooling layers, we utilize the 3D dilated convolution to capture and preserve more detailed characteristic information of the nodules. Moreover, a multiple receptive field fusion strategy is applied to extract the multi-scale features from the nodule CT images. Extensive experimental results show that our model achieves a better result with an accuracy of 88.6% which outperforms other state-of-the-art methods.","inCitations":[],"pmid":"","title":"Spatial Pyramid Dilated Network for Pulmonary Nodule Malignancy Classification","journalPages":"3911-3916","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546106"],"entities":[],"journalVolume":"","outCitations":["db299ad09f629a0fcd45b74fa567da476d83a4f3","53b5e53427f632021cc056005bfe1fcfb0834cbf","91b7b1ad437c888e63a3d175c35eb2d501bbd7d0","671e5d7429cca3bc42ade048bfa48760c088f7ef","cab372bc3824780cce20d9dd1c22d4df39ed081a","d3cce6f51b7e64bd0fb785d6c7d4abf14da94e2f","c6ddfcc5a18d30cac8dd1b1e66e073bd5b076a3e","06c8b4c1dd0c045bc6a08d0062c5042b5588d55b","fe7616d0e58ccd3debfd937f21813621fb923479","55e70aa8d6388a1e1d76e81ce1933e08a16d2940","8257167186837ff6840d6c2f552b4d23ff26ec81","6fa670f36e877a3a4093893502bc13ad3df2c73a","4f61779a850eff1fe544613a1769f02f0b145441","9a69b2d7e15e0ebd66d4aaef1bc91b06bed7bf65","a5075a9a0d30ea5e5ef1816f25b64a109436a966","9585eed348e7a49ceb063fe0391a5ca65ff71fcc","2c8f979c0dc3d9ee1e00570792ce2ce22382ce66","973c85f16d02927829c538a999093e038a2028ce","3c86dfdbdf37060d5adcff6c4d7d453ea5a8b08f","068fe7d1f404990dc1bde61c7b667d3afa3fa792","86ab4cae682fbd49c5a5bedb630e5a40fa7529f6","a68de41a9fcf997be71167bdfc079273a2a6f230","8ed5968495a4948e0fc1bd7e55e21c294dbeced7","ccef83b8255d1eb954f8b7caa3574a6d04cd5a06"],"id":"6bd03e8eccd12a06fa23b380586eb31eaaf64428","s2Url":"https://semanticscholar.org/paper/6bd03e8eccd12a06fa23b380586eb31eaaf64428","authors":[{"name":"Guokai Zhang","ids":["9070436"]},{"name":"Ye Luo","ids":["37082461"]},{"name":"Dandan Zhu","ids":["50792440"]},{"name":"Yixuan Xu","ids":["9653455"]},{"name":"Yunxin Sun","ids":["15087036"]},{"name":"Jianwei Lu","ids":["49301770"]}],"doi":"10.1109/ICPR.2018.8546106"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593706","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Navigation in unknown indoor environments with fast collision avoidance capabilities is an ongoing research topic. Traditional motion planning algorithms rely on precise maps of the environment, where re-adapting a generated path can be highly demanding in terms of computational cost. In this paper, we present a fast reactive navigation algorithm using Deep Reinforcement Learning applied to multi rotor aerial robots. Taking as input the 2D-laser range measurements and the relative position of the aerial robot with respect to the desired goal, the proposed algorithm is successfully trained in a Gazebo-based simulation scenario by adopting an artificial potential field formulation. A thorough evaluation of the trained agent has been carried out both in simulated and real indoor scenarios, showing the appropriate reactive navigation behavior of the agent in the presence of static and dynamic obstacles.","inCitations":[],"pmid":"","title":"Laser-Based Reactive Navigation for Multirotor Aerial Robots using Deep Reinforcement Learning","journalPages":"1024-1031","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593706"],"entities":[],"journalVolume":"","outCitations":[],"id":"cb54ba102a9ef1f3c6df8160781a4d316391af9a","s2Url":"https://semanticscholar.org/paper/cb54ba102a9ef1f3c6df8160781a4d316391af9a","authors":[{"name":"Carlos Sampedro","ids":[]},{"name":"Hriday Bavle","ids":[]},{"name":"Alejandro Rodriguez-Ramos","ids":[]},{"name":"Paloma de la Puente","ids":[]},{"name":"Pascual Campoy","ids":[]}],"doi":"10.1109/IROS.2018.8593706"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00013","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Deep convolutional networks have generated significant performance improvements in the domain of face recognition. However, these improvements do not provide insight into which facial features lead to classification decisions. In this paper, we explore the problem of visualizing discriminative information in faces, to show which properties of images and subjects influence classification. We compare six different techniques for computing a network saliency map, which identifies influential local features in an image, using a metric called the \"hiding game\" to directly evaluate these techniques on classification performance. Results show that contrastive excitation backprop (cEBP) [26] best localizes features that lead to face identification. However, these maps are nearly identical across subjects, which can result in an unstable network saliency map. We introduce a robust improvement called truncated cEBP and demonstrate the capability to predict the performance of a given map. Our evaluation provides the first application of network saliency to face recognition, and we provide a robust new tool for face recognition analysts to explore which facial regions lead to changes in match scores.","inCitations":[],"pmid":"","title":"Visualizing and Quantifying Discriminative Features for Face Recognition","journalPages":"16-23","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00013"],"entities":["1:1 pixel mapping","Backpropagation","Biometrics","Control theory","Convolutional neural network","Facial recognition system","Map"],"journalVolume":"","outCitations":["0b2cbe47a9bdea2898bce630165ec04a304aed53","8c4c723a74fe479c2b8af7d911817377dd6d85c9","13b6b5b2e96b2b63f0d35f6e91e22a08d2f4e52c","20e210bb6b1d3e637e2b2674aeead3fad8c2c70e","376b078694f0c183e4832900debda4dfed021a9a","03cea06797f4432419e3189f0a64b8fd4adcf783","17a273bbd4448083b01b5a9389b3c37f5425aac0","05f9e6737b7c8b5b0e0cc34c7e1250f92d72f2f5","3b2697d76f035304bfeb57f6a682224c87645065","0f84a81f431b18a78bd97f59ed4b9d8eda390970","14b5e8ba23860f440ea83ed4770e662b2a111119","9b678aa28facf4f90081d41c2c484c6addddb86d","172625369ff68a24d9ef4aeadb198fa2156e7f0a","8481305acddd4ca10adbcc366007cc0f45be7aec","061356704ec86334dbbc073985375fe13cd39088","162ea969d1929ed180cc6de9f0bf116993ff6e06","5c4b89581b0de09c016f17f8af120b9ebecbd0d6","a79a7af6e6d15a76084710f1c79b68f0e1e6f419","dbf777403156adda2d551a973379edd3e2bc5aaa"],"id":"f94867160e0885bc397892e4db56fa7ce9ebc83a","s2Url":"https://semanticscholar.org/paper/f94867160e0885bc397892e4db56fa7ce9ebc83a","authors":[{"name":"Gregory Castañón","ids":["31330906"]},{"name":"Jeffrey Byrne","ids":["49121167"]}],"doi":"10.1109/FG.2018.00013"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202254","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper proposes an approach to enlarge the impedance range of admittance-type haptic interfaces. Admittance-type haptic interfaces have advantages over impedance-type haptic interfaces in the interaction with high impedance virtual environments. However, the performance of admittance-type haptic interfaces is often limited by the lower boundary of the impedance that can be achieved without stability issue. Especially, it is well known that low value of inertia in an admittance model often causes unstable interaction. This paper extends recently proposed input-to-state stable approach [1] to further lower down the achievable impedance in admittance-type haptic interfaces with less conservative constraint compared with the passivity-based approaches. The primary challenge was identifying the nonlinear hysteresis components which are essential for the implementation of the input-to-state stable approach. Through experimental investigation and after separating and merging the admittance model and the position controller, the partial admittance model (from the measured human force to the desired velocity) and the velocity controller (from the velocity tracking error to the controller force) were found having counter-clockwise hysteresis nonlinear behavior. Therefore, it allows implementing the one-port input-to-state stable (ISS) approach for making both components dissipative and ISS. An additional advantage of the proposed ISS approach is the easiness of the implementation. No model information is required, and the network representation is not necessary, unlike the passivity-based approaches. Series of experiments verified the effectiveness of the proposed approach in term of significantly lowering the achievable impedance value compared with what the time-domain passivity approach can render.","inCitations":[],"pmid":"","title":"Realizing low-impedance rendering in admittance-type haptic interfaces using the input-to-state stable approach","journalPages":"914-919","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202254"],"entities":["Characteristic impedance","Coefficient","Control theory","Dissipative system","Experiment","Haptic technology","High impedance","Hysteresis","Nonlinear system","Port (circuit theory)","Velocity (software development)","Virtual reality"],"journalVolume":"","outCitations":["796160a63b44d9ff54774bc39be3751f0fe0f7e2","2f81e4683ce7d8074aa7a1e3dd53b214020c20df","666e4ea184b0d093faacb0dd301e8e598a62603b","b22bed2c23f1aab98303b6903b606a468e1dc039","1bbac84ea85b8823f2bf936c66c060367fcb56c5","245669262e62fae06fc2a3409d2ae00aad855f15","ee4dc692437173f483d90e806bd210aad1615709"],"id":"f459405e85b099f174687f79904222861b71be1d","s2Url":"https://semanticscholar.org/paper/f459405e85b099f174687f79904222861b71be1d","authors":[{"name":"Muhammad Nabeel","ids":["9390230"]},{"name":"Aghil Jafari","ids":["2562900"]},{"name":"Jee-Hwan Ryu","ids":["1771235"]}],"doi":"10.1109/IROS.2017.8202254"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594382","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We propose a collision avoidance method that incorporates the interactive behavior of agents and is proactive in dealing with the uncertainty of the future behavior of obstacles. The proposed method considers interactions that will be experienced by an autonomous surface vessel (ASV) in an environment governed by the international regulations for preventing collisions at sea (COLREGs). Our approach aims at encouraging dynamic obstacles to cooperate according to COLREGs. Therefore, we propose a strategy for assessing the cooperative behavior of obstacles, and the result of the assessment is used to adapt collision avoidance decisions within the Reciprocal Velocity Obstacles (RVO) framework. Moreover, we propose a predictive approach to solving known limitations of the RVO framework, and we present computationally feasible extensions that enable the use of complex dynamic models and objectives suitable for ASVs. We demonstrate the performance and potentials of our method through a simulation study, and the results show that the proposed method leads to proactive and more predictable ASV behavior compared with both Velocity Obstacles (VO) and RVO, especially when obstacles cooperate by following COLREGs.","inCitations":[],"pmid":"","title":"Proactive Collision Avoidance for ASVs using A Dynamic Reciprocal Velocity Obstacles Method","journalPages":"2402-2409","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594382"],"entities":[],"journalVolume":"","outCitations":[],"id":"6c0aad63e465c976a07bd0e8b3b4665136de0e64","s2Url":"https://semanticscholar.org/paper/6c0aad63e465c976a07bd0e8b3b4665136de0e64","authors":[{"name":"D. Kwame Minde Kufoalor","ids":[]},{"name":"Edmund Førland Brekke","ids":[]},{"name":"Tor Arne Johansen","ids":[]}],"doi":"10.1109/IROS.2018.8594382"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545447","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Human motion and behaviour in crowded spaces is influenced by several factors, such as the dynamics of other moving agents in the scene, as well as the static elements that might be perceived as points of attraction or obstacles. In this work, we present a new model for human trajectory prediction which is able to take advantage of both human-human and human-space interactions. The future trajectory of humans, are generated by observing their past positions and interactions with the surroundings. To this end, we propose a \u201ccontext-aware\u201d recurrent neural network LSTM model, which can learn and predict human motion in crowded spaces such as a sidewalk, a museum or a shopping mall. We evaluate our model on a public pedestrian datasets, and we contribute a new challenging dataset that collects videos of humans that navigate in a (real) crowded space such as a big museum. Results show that our approach can predict human trajectories better when compared to previous state-of-the-art forecasting models.","inCitations":["e05f4e761aded7f9feefafc802bbd28b15a4690d","e596753824ed56f17927984f78f51713b321588d","dbe2f62c086b2e6617c7744f8c242de401f17cc0","7c739dddc905c967e0b432dc7515cb1f4b82e580","782bec46053aa93ebbbf5d1d27497966b2407848","64c1aa13fecfc1cdd4231e39a40a5f6db91b90f8"],"pmid":"","title":"Context-Aware Trajectory Prediction","journalPages":"1941-1946","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1705.02503v1.pdf","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545447","http://arxiv.org/abs/1705.02503"],"entities":["Artificial neural network","Interaction","Kinesiology","Long short-term memory","Recurrent neural network"],"journalVolume":"","outCitations":["8cb6daba2cb1e208e809633133adfee0183b8dd2","3d48e6ed5af44bd6650601b8d9b91b4fa662b9a2","f0ca09be64e57a473be91ffd6cc63c40b54fe72a","0d8a5addbd17d2c7c8043d8877234675da19938a","67906211cf1482ac3a46c379d76adaa13c899d1f","b18f94c5296a9cebe9e779d50d193fd180f78ed9","13e847d085af4260526e683a94804aa4a078e531","ed62a56b81511d7fcf6d247014987163d9668982","d927a7b282c8a5896a72fc1efac6ace7a0fc0b8a","19dd12ccc9c6997c9cecdaf12f937812066827aa","b7005f76d0b43c11349e846297a0d71b77e65d01","156a76529ada8c120ba80cda783c821599ce0d4a","bb08d1570685a20861e9b8c15c57aae9c01d3eac","4dd744b647523c4851e2d922aed0eb6e132cb7b5","22493d8d4d7b4604cae23638dce4981b36e30147","20ca3dc873d7c986d7b1b233fdcf85e78b92914e","798c55fdf210024fe0c58cb3314b9e14af7fbb3b","3921f459a9ee26827963abc4abf013b4cc9cbd32","c7b160f762ff472bfaa800e60b030761231e7975","77a53d4141a8081657ce08b13dc3328ac4a4e689","2bcec23ac1486f4106a3aa588b6589e9299aba70","12dc84ff8f98d6cb4b3ea0c1d4b444900d4bbb5c","06890d068a7fb82fa78443038ad26ca7623f7a98","782bec7177abadf8b8fd31392edf2477caad8a15","570f37ed63142312e6ccdf00ecc376341ec72b9f","df339fb7e56704e16a0622305208bcd1e54ed1b9","0b8c9acb478c856eb157c648b25b3d60117392fd"],"id":"cbfb8a1592c30a8cc765ceeef2c64c7280435f84","s2Url":"https://semanticscholar.org/paper/cbfb8a1592c30a8cc765ceeef2c64c7280435f84","authors":[{"name":"Federico Bartoli","ids":["36971654"]},{"name":"Giuseppe Lisanti","ids":["2973738"]},{"name":"Lamberto Ballan","ids":["1795847"]},{"name":"Alberto Del Bimbo","ids":["8196487"]}],"doi":"10.1109/ICPR.2018.8545447"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202271","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Stereo vision is commonly used for local obstacle avoidance of autonomous mobile robots: stereo images are first processed to yield a dense 3D reconstruction of the observed scene, which is then used for navigation planning. Such an approach, which we term Sequential Perception and Planning (SPP), results in significant unnecessary computations as the navigation planner only needs to explore a small part of the scene to compute the shortest obstacle-free path. In this paper, we introduce an approach to Joint Perception and Planning (JPP) using stereo vision, which performs disparity checks on demand, only as necessary while searching on a planning graph. Furthermore, obstacle checks for navigation planning do not require full 3D reconstruction: we present in this paper how obstacle queries can be decomposed into a sequence of confident positive stereo matches and confident negative stereo matches, which are significantly faster to compute than the exact depth of points. The resulting complete JPP formulation is significantly faster than SPP, while still maintaining correctness of planning. We also show how the JPP works with different planners, including search-based and sampling-based planners. We present extensive experimental results from real robot data and simulation experiments, demonstrating that the JPP requires less than 10% of the disparity computations required by SPP.","inCitations":["cb2af878199bbcbd288c483b85dd35d811ffe81f"],"pmid":"","title":"Joint perception and planning for efficient obstacle avoidance using stereo vision","journalPages":"1026-1031","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202271"],"entities":["3D reconstruction","Algorithm","Autonomous robot","Binocular disparity","Computation","Computational resource","Correctness (computer science)","Experiment","Mobile robot","Obstacle avoidance","Sampling (signal processing)","Self-propelled particles","Simulation","Stereopsis"],"journalVolume":"","outCitations":["bc835f0e2efb72fc2b488f434d65a6c47db9b040","9ddeb0da24c5a460ccff17f226d98264dbc074e3","42ba030410f037e1c6a86cd7531651027d5ee2c1","1d3b7546a9c45bad6bb0f53c8ce2b776e31af724","79ba9b1e1b16f166da84e206b71bbb4e6b9be9c5","50411faedf26bf1f449b36743cb0234bc3b9ddcc","ecd5ea80a9903ef915b22cd20cb27b4a77e7f006","49e196655e5c3953306cd03707c80e65c467bf32","889e6ee95eba2d0ce8535b8e9b7bd61914de5946","6e194bea77ee836ebc4212b82bdea1c64dfe58a8","0957467f2d9ccac1a46d8a3bf28225900b292c35","421e6c7247f41c419a46212477d7b29540cbf7b1","d05b18627326ecd12c32d20232a0307a6795c1cf","5af349ab5acdf0117d52dbd2936e02113d82cc75","891afbd2df799801e43d8809bbdab1fb4c3baada","048053a2c841d34f48774219576867994a1ff6f6","3ebc7ec2a9b89ebcc00328b11a1984d69a57f2a7"],"id":"d2e32678b3573cfab2985d25c635f95136fd7cbd","s2Url":"https://semanticscholar.org/paper/d2e32678b3573cfab2985d25c635f95136fd7cbd","authors":[{"name":"Sourish Ghosh","ids":["3756874"]},{"name":"Joydeep Biswas","ids":["3045593"]}],"doi":"10.1109/IROS.2017.8202271"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206486","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, we propose a classification method for single views of 3D objects with missing data retrieval. A mobile robot equipped with range sensors basically obtains only single view information of a 3D scene. Therefore, large amount of information is missing by self-occlusion, which leads to severe restriction on the object classification exploiting the whole shapes of 3D objects. Humans can precisely identify the objects from single view, since they already have concepts of the entire shape of 3D objects by learning process. Based on these concepts, humans can infer the entire shape and category of the object from a single view. Inspired from this, the proposed algorithm learns concepts in abbreviated form for the shapes of 3D objects, then infers the entire shape and object category from these concepts simultaneously. We apply a generative model based on variational auto-encoder (VAE) to learn the concepts for complex shapes of 3D objects. Our method is evaluated on 3D CAD model dataset, and also compared with other state-of-the-art methods.","inCitations":[],"pmid":"","title":"A variational approach for 3D object classification with retrieval of missing data","journalPages":"5922-5927","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206486"],"entities":["Algorithm","Autoencoder","Calculus of variations","Coherence (physics)","Computer-aided design","Data retrieval","Encoder","Experiment","Generative model","Hidden surface determination","Humans","Latent variable","Missing data","Mobile robot","Sensor","Simultaneous localization and mapping","Variational principle"],"journalVolume":"","outCitations":["4cdcf2ae5e1fafebd9b3613247a7b1962584da34","519f306c6f16f041edaf9f0e962e105e5b58535f","8db9926d0a6e4e26e0a5cc8661931ed09e68ba63","0e6cb161465d3305142b440937116894f8a91671","89bf8f9b83bf52c5a28dd6bfa9010a8eec3f6408","0f88de2ae3dc2ec1371d1e9f675b9670902b289f","087d87eb6fc37245b7867cde27235c2f23574949","354c60f54b0d7c774ebcedc2098196d6ce2c65ec","5a1b5d31905d8cece7b78510f51f3d8bbb063063","09d2f3467f171f253499773d8c1d71f21ac4f983","27ac2adee50ccc98b502eb9165d0010699b850cd","9a01ad5d531d9662795f4d65fdf4c06116dc871c","5c3e3c9eb353c2662333b2cafbb67bfce4baa371","059ff0ee2e0ac95d5cc54fd20ded2d33e5d5dbd0","b246df9be5a5590a7e9f8122ba5d86bb0b8acf83","3f1c6749edfaf4f89bac38b2d15a0493bd9aa253","0b8759d61e93b809df16d9fe9010d2a2d7241c74","15074808e5409638e7735c45ee0c1b7825405617","adc55a30b17060d68b5092dfaba52243e39c68f5","2e4e83ec31b43595ee7160a7bdb5c3a7dc4a1db2","24091189b5e80d89b9708fab56a3cd7ba7107d12"],"id":"40416e487b8460c20d1047de52fad94bba302581","s2Url":"https://semanticscholar.org/paper/40416e487b8460c20d1047de52fad94bba302581","authors":[{"name":"Haoping Yu","ids":["49514567"]},{"name":"B. H. Lee","ids":["49132226"]}],"doi":"10.1109/IROS.2017.8206486"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202298","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents methodologies for dexterous manipulation by an underactuated robotic soft gripper with a low DOF. While the softness and passive joints provide a stable grasp, the manipulations become challenging owing to the complexities of modeling and state estimation. To investigate the drawbacks, target manipulations using a support surface and gravity are defined in this study. By realizing the target manipulations, methodologies for utilizing both the support surface and gravity in manipulation are validated. For manipulation utilizing the support surface, the key states \u2014 at which the type of gripper motion primitive changes \u2014 are defined, and gripper motion at the key states is produced with a DNN-based motion generator. For manipulation utilizing gravity, the fall risk of the object is formulated as a constraint for a criterion function, and a parameters-online-exploration based methodology is adopted. The validity of the methodologies is demonstrated through several experiments.","inCitations":[],"pmid":"","title":"Thin plate manipulation by an under-actuated robotic soft gripper utilizing the environment","journalPages":"1236-1243","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202298","http://zkks.w3.kanazawa-u.ac.jp/papers/IROS2017.pdf"],"entities":["Experiment","Loss function","Robot end effector","Thin plate spline"],"journalVolume":"","outCitations":["b6717f12c4cf4c1625222853486ccf0e7e021650","a11139ef44108280c6e17f9dc43042b77826b655","f34fea58f4a911ff3e2e41f6490a9e1d0e19c952","d4cbb08752810e275c0007015b0ef474536dfa3b","7a76697bcf441f1123e5018cbabca46b6ab48df2","31cb25628e54dbcb147c0ef6d4391bff273313ca","23e8d29d7d828f70d2e3e594d3a3c793b7341f04","3b195d8b0e67fe694671ed688fbc19e4a03d8e1a","2b0d2277a2628f6a8e3e4caa44ef0d3a3955b704","4b9ba9c4d1128ebe8d81056b3128e7cd0743d8d5","aeebeab818d68073c0caf9eafcda58374a734949","20e996d0b19a106aaf10eec596255aa5e44dd232","27ed7cec1bb9c92dc388c0f28a01f02ef48432bf","257c8f64686893a5d368312d992309f0093f6003","183357d25d6bad84e16bdb8db8f57ad616f3a9ce","5ef3555a83a7b0ef00e1e2a47a2d41ae2a900517","dc22d08a65286949f2834618e83d88bbd9f303c0","12f5d07603819f82fe815ca0f74311074ffcfc93","daa63f57c3fbe994c4356f8d986a22e696e776d2","ae380c2577e3a3b45a7ce64862021cbdf12c5e93","3fe70222056286c4241488bf687afbd6db02d207"],"id":"167d73e6a671b01b169fcd56073f43d829dfdbf7","s2Url":"https://semanticscholar.org/paper/167d73e6a671b01b169fcd56073f43d829dfdbf7","authors":[{"name":"Toshihiro Nishimura","ids":["2383250"]},{"name":"Kaori Mizushima","ids":["46329232"]},{"name":"Yosuke Suzuki","ids":["1891897"]},{"name":"Tokuo Tsuji","ids":["1681918"]},{"name":"Tetsuyou Watanabe","ids":["33709365"]}],"doi":"10.1109/IROS.2017.8202298"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594193","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Millirobots are a promising robotic platform for many applications due to their small size and low manufacturing costs. Legged millirobots, in particular, can provide increased mobility in complex environments and improved scaling of obstacles. However, controlling these small, highly dynamic, and underactuated legged systems is difficult. Hand-engineered controllers can sometimes control these legged millirobots, but they have difficulties with dynamic maneuvers and complex terrains. We present an approach for controlling a real-world legged millirobot that is based on learned neural network models. Using less than 17 minutes of data, our method can learn a predictive model of the robot's dynamics that can enable effective gaits to be synthesized on the fly for following user-specified waypoints on a given terrain. Furthermore, by leveraging expressive, high-capacity neural network models, our approach allows for these predictions to be directly conditioned on camera images, endowing the robot with the ability to predict how different terrains might affect its dynamics. This enables sample-efficient and effective learning for locomotion of a dynamic legged millirobot on various terrains, including gravel, turf, carpet, and styrofoam. Videos and further details can be found at https://sites.google.com/view/imageconddyn","inCitations":["488f81f4aa4d06834ae8d4870b265098926d3de8","572dd013395abba54d7f00bb4814870cfe8d3a41"],"pmid":"","title":"Learning Image-Conditioned Dynamics Models for Control of Underactuated Legged Millirobots","journalPages":"4606-4613","s2PdfUrl":"","pdfUrls":["https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-43.pdf","https://arxiv.org/pdf/1711.05253v3.pdf","https://people.eecs.berkeley.edu/~ronf/PAPERS/anagabandi-iros18.pdf","https://doi.org/10.1109/IROS.2018.8594193"],"entities":[],"journalVolume":"","outCitations":["54dd77bd7b904a6a69609c9f3af11b42f654ab5d","620412c443e85a1fc2f9ab950ddfada8d18d63b4","125d262f1c66f75f3af9c3c8759abcc02ebb4ef9","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","1554d6787b88b24cde34f398a330e1ad4d30b634","1763fa2c20d2a7038af51a97fddf37a1864c0fe2","0635d24efdcae7df54031edfc91be04f7e9970d1","f6a4142bb55786d719be74f69ab543c5d3adba14","127316fbe268c78c519ceb23d41100e86639418a","4252bf93fc99a5dba8cc58fab5455e31535853a2","36189372690be88a15cd3330a91fd5290e9575d2","64bb1647bbd2540c077c2ff675c5398c8d978e51","cfa29ede7c2471e7e0700de80654142fd6110871","140a3877a10f9eaf320029fb942e1527a5001607","86cf98c8ad25b3cb185b7524bccf32f77a91b616","ca8860612636582614a8d03554f21bf51f4c750b","6c5a3691643abbdc063fd2505951ac3573d431c9","3cd84af9196000e15e2704f7e347946f7bf41863","1bf0b56d7e93819a98f0209200263596d29eee22","e15526e05d479e26538312edab4fcc728c227720","dc5ce0713b69e2644b6e9c2abd5caf95a32e56d3","1aeb1d745a87849c568c854fd8f247c953068eb5","3e42557a8a8120acbe7ab9c1f43c05a529254e04","340f48901f72278f6bf78a04ee5b01df208cc508","407d73084279e69f59c98199d06357ee05c807d5","371ad992f337d327d9a231e201b738f16187a945","0176d366c47643dcb2adf307cba2effac10a98c0","807401ebc3eca7befe9042d03b12ca82f22cdb5a","8f89f992fdcc37e302b9c5b9b25c06d7f1086cf9","60a0128f02585995d22967ba5d2bd71b0e5a332a","14a2d7c405e022fe2ee5b427dad9141d80353e1c","7bd8a0153f62549ab21d320e7dc80414ea630a90","bf6c35413528e274136619c9033102452ee8517c","272216c1f097706721096669d85b2843c23fa77d","1e48535de9242f78fbf6defda1bf59b0eacccde5","2313ad2f9176028a73076e87ab7b24c9dc0f3747","b729427daa0cd63b02b90f8a7c5d00049ac64175","632f4d095c2fed351520f27b841e275ff195616e","52aee36c292072b115cb4c1d7c5e755ad62ddd06","afbe59950a7d452ce0a3f412ee865f1e1d94d9ef","8a224c0a261f7998bcc2e5a9060bc40cdfef2b21","0a8d77adfba482327dc3193cd0a035f5fb62f396","84745363e21625b24bb46ee8007f3242cd49d569","5129a9cbb6de3c6579f6a7d974394d392ac29829","3372740b4006675b79d49bd7da1b487eecac7c86","053dec3537df88a1f68b53e33e1462a6b88066f6","298500897243b17fa2ebe7bde0a1b8ebc00ea07f","79b814d2d0d157b646bf29f0fcc0f9adaba5dc8d","016a4d80be54ed13d7b7ac5e94fc4a181e7b096a","1e6bcda276bb1681125eb18ca4d62a44e94c25c9"],"id":"c975e103413189d68f25ee7dc525d70cd62201be","s2Url":"https://semanticscholar.org/paper/c975e103413189d68f25ee7dc525d70cd62201be","authors":[{"name":"Anusha Nagabandi","ids":["3195183"]},{"name":"Guangzhao Yang","ids":["48636552"]},{"name":"Thomas Asmar","ids":["30052125"]},{"name":"Ravi Pandya","ids":["8118986"]},{"name":"Gregory Kahn","ids":["46292812"]},{"name":"Sergey Levine","ids":["1736651"]},{"name":"Ronald S. Fearing","ids":["1773013"]}],"doi":"10.1109/IROS.2018.8594193"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546283","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Segmentation methods only work for a single imaging modality usually suffer from the low spatial resolution in positron emission tomography (PET) or low contrast in computed tomography (CT) when the tumor region is inhomogeneous or not obvious. To address this problem, we develop a segmentation method combining the advantages and disadvantages of PET and CT. Firstly, the initial contours are obtained by the pre-segmentation of PET images using region growing and mathematical morphology. The initial contours can be used to automatically obtain the seed points required for random walk on PET and CT images, at the same time, they can be also used as a constraint in the random walk on CT images to solve the shortcoming that the tumor areas are not obvious if the CT images have not been enhanced. For the reason that CT provides essential details on anatomic structures, the anatomic structures of CT can be used to improve the weight of random walk on PET images. Finally, the similarity matrices obtained by random walk on PET and CT images are weighted to obtain identical results on PET and CT images. Our methods achieve an average DSC of $\\pmb{0.8456\\pm} \\pmb{0.0703}$ on 14 patients with lung cancer. Our method has much better performance when the tumors are inhomogeneous on PET images and not obvious on CT images.","inCitations":[],"pmid":"","title":"A Method for PET-CT Lung Cancer Segmentation based on Improved Random Walk","journalPages":"1187-1192","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546283"],"entities":[],"journalVolume":"","outCitations":["75d9d1e7c89c4474e7dd6ac3c3b718bffdcbfda2","d8b14d6b6375d9ba0af3163e201d03f646275bd7","06829a53d534260df70282316f72deff625c7fcd","4dd56c8b28efcae79c3f9b51dfd2effdf24d08a5","14cabc4f3b3dfbb44168420b3259296643497d54","363850a782ad92811a5f4a72b89d6136db32b8a3","00eddf30e1668a33d6c70580d5b76d4ddb7cd61b","3c318f18b61e9cc28e21f833c852f1142dd6f8a9","adef6fe0b8384117313d04a638790be4c5790a53","0d0d3cf83a69a5b1de7cd44d04d050e171b2c51b","573edc20ae85e708f3570113723ab443e98d3c56","92ee839db7a31f65b04d2300cef84b6c5d5ae2cf","9b94d9b50b731f708046a5b52236e70e73e8cfb6","41f0d9593c8b8e0a53a5435a10bac6e6dec0ab11","4baf39a94a124b0a62c8a07205cd6f35255cb7bf","41260972ab641cd3b34056161f5791ae7a0a5def","a736c0c11546ea7e9312cd724f218dfc99074211"],"id":"aaf59f93b1de5ca84f6fbd019a54bb0190605e9a","s2Url":"https://semanticscholar.org/paper/aaf59f93b1de5ca84f6fbd019a54bb0190605e9a","authors":[{"name":"Zhe Liu","ids":["47781634"]},{"name":"Yuqing Song","ids":["40607664"]},{"name":"Charlie Maere","ids":["52183539"]},{"name":"Qingfeng Liu","ids":["47362009"]},{"name":"Y. Zhu","ids":["1853827"]},{"name":"Hu Lu","ids":["2361700"]},{"name":"Deqi Yuan","ids":["24911549"]}],"doi":"10.1109/ICPR.2018.8546283"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.109","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"In this paper, a robust system for viewindependent action unit intensity estimation is presented. Based on the theory of sparse coding, region-specific dictionaries are trained to approximate the characteristic of the individual action units. The system incorporates landmark detection, face alignment and contrast normalization to handle a large variety of different scenes. Coupled with head pose estimation, an ensemble of large margin classifiers is used to detect the individual action units. The experimental validation shows that our system is robust against pose variations and able to outperform the challenge baseline by more than 35%.","inCitations":["b1a3b19700b8738b4510eecf78a35ff38406df22","88ad82e6f2264f75f7783232ba9185a2f931a5d1"],"pmid":"","title":"Support Vector Regression of Sparse Dictionary-Based Features for View-Independent Action Unit Intensity Estimation","journalPages":"854-859","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.109"],"entities":["Approximation algorithm","Baseline (configuration management)","Color image","Dictionary","K-SVD","Machine learning","Margin classifier","Neural coding","Singular value decomposition","Sparse approximation","Sparse matrix","Support vector machine"],"journalVolume":"","outCitations":["42e0127a3fd6a96048e0bc7aab6d0ae88ba00fb0","b91f54e1581fbbf60392364323d00a0cd43e493c","45836122e1a8ed3de05ce0446a93a5adc5ef84ae","aed124c053b9c510487d68e0faf32aff2a84c3b5","0f22251fa9c4bb120f00767053430fbab141fac3","6cac3f5009b5728bda2457fef6180d28357b3ae6","66b9d954dd8204c3a970d86d91dd4ea0eb12db47","2661f38aaa0ceb424c70a6258f7695c28b97238a","1824b1ccace464ba275ccc86619feaa89018c0ad","005aea80a403da18f95fcb9944236a976d83580e","940a675de8a48b54bac6b420f551529d2bc53b99","f6532bf13a4649b7599eb40f826aa5281e392c61","00568ab7c7ee96bcf1c5d2ceba1471404c7be2b2","03f98c175b4230960ac347b1100fbfc10c100d0c","0021f46bda27ea105d722d19690f5564f2b8869e","4d9a02d080636e9666c4d1cc438b9893391ec6c7","2a9b398d358cf04dc608a298d36d305659e8f607","377aad6a81eef83898fa8e384d52ce20ad4dccc5","8050ae0128b4a6b268142f6c1d22e2992d992af7","4998462014c907c519ae801af39cf58a4c538bc9","353fe6fd56ecaf186a5d3e3364b4eb06d98bfdb7","12a376e621d690f3e94bce14cd03c2798a626a38","075bc988728788aa033b04dee1753ded711180ee","235cb2c5e800dfff5f3800fdd1d195c07de7416a"],"id":"99cd84a62edb2bda2fc2fdc362a72413941f6aa4","s2Url":"https://semanticscholar.org/paper/99cd84a62edb2bda2fc2fdc362a72413941f6aa4","authors":[{"name":"Mohammadreza Amirian","ids":["1985672"]},{"name":"Markus Kächele","ids":["2144395"]},{"name":"Günther Palm","ids":["1774384"]},{"name":"Friedhelm Schwenker","ids":["1685857"]}],"doi":"10.1109/FG.2017.109"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593886","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Given the description of an object, s physical attributes, humans can determine a proper strategy and grasp an object. This paper proposes an approach to determine grasping strategy for an anthropomorphic robotic hand simply based on natural-language descriptions of an object. A learning-based approach is proposed to help a robotic hand learn suitable grasp poses starting from the natural language description of the object. Object features are parsed from natural-language descriptions by using a customized natural-language processing technique. The most likely grasp type for the given object is learned from the human grasping taxonomy based on the parsed features. The grasping strategy generated by the proposed approach is evaluated both by simulation study and execution of the grasps on an ARl0 robotic hand.","inCitations":[],"pmid":"","title":"Learning Robotic Grasping Strategy Based on Natural-Language Object Descriptions","journalPages":"882-887","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593886"],"entities":[],"journalVolume":"","outCitations":[],"id":"9e345cf95555d189df97f2ac437e5e5c00fe839b","s2Url":"https://semanticscholar.org/paper/9e345cf95555d189df97f2ac437e5e5c00fe839b","authors":[{"name":"Achyutha Bharath Rao","ids":[]},{"name":"Krishna Krishnan","ids":[]},{"name":"Hongsheng He","ids":[]}],"doi":"10.1109/IROS.2018.8593886"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593568","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Prescribed fires can lessen wildfire severity and control invasive species, but they can also be risky and costly. Unmanned aerial systems can reduce those drawbacks by, for example, dropping ignition spheres to ignite the most hazardous areas. Existing systems, however, lack awareness of the fire vectors to operate autonomously, safely, and efficiently. In this work we address that limitation, introducing an approach that integrates a lightweight fire simulator and a planner for trajectories and ignition sphere drop waypoints. Both components are unique in that they are amenable to input from the system's sensors and the fire crew to increase fire awareness. We conducted a preliminary study that confirms that such inputs improve the accuracy of the fire simulation to counter the unpredictability of the target environment. The field study of the system showed that the fire-aware planner generated safe trajectories with effective ignitions leveraging the fire simulator predictions.","inCitations":[],"pmid":"","title":"Fire-Aware Planning of Aerial Trajectories and Ignitions","journalPages":"685-692","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593568","http://cse.unl.edu/~carrick/papers/BeachlyDEDHTA-2018IROS.pdf"],"entities":[],"journalVolume":"","outCitations":["af4b8460aa2631bee345b12c14a3b8b0fefd0873","c21c72ac919e280ebdb8b56609b5bbd6a1b338e7","b460478f854396aafd43e1211ca104840ef3a509","6abe7536ae31f2fa37ee3f6e05c6601183f4ca0a","161ebf64b6366f7a863c877b26a75042ef5142ac","aa86826752b2231b7a71c541722592343e142533","407395de5f396d8c9689d99cfa0a790d427e2bd1","799c56caa55e00f3b27b4a53de3c3cc979649621","67389e37f54ede439b68354b431c4e235c4b840b","1ca5b806f54552b86b6d7b0e1fab0f7ab2acaed3","3b67ad5aeb77bd760fd025c08fe6483fddb6ad5e"],"id":"d2f98d8f76793e94837cbcfcdfb99221b67d3ed1","s2Url":"https://semanticscholar.org/paper/d2f98d8f76793e94837cbcfcdfb99221b67d3ed1","authors":[{"name":"Evan Beachly","ids":["27851159"]},{"name":"Carrick Detweiler","ids":["1694389"]},{"name":"S. Elbaum","ids":["48113494"]},{"name":"Brittany A. Duncan","ids":["2666206"]},{"name":"Carl Hildebrandt","ids":[]},{"name":"Dirac Twidwell","ids":["6552482"]},{"name":"Craig Allen","ids":["24635770"]}],"doi":"10.1109/IROS.2018.8593568"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545760","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The main challenge of single image super resolution (SISR) is the recovery of high frequency details such as tiny textures. However, most of the state-of-the-art methods lack specific modules to identify high frequency areas, causing the output image to be blurred. We propose an attention-based approach to give a discrimination between texture areas and smooth areas. After the positions of high frequency details are located, high frequency compensation is carried out. This approach can incorporate with previously proposed SISR networks. By providing high frequency enhancement, better performance and visual effect are achieved. We also propose our own SISR network composed of DenseRes blocks. The block provides an effective way to combine the low level features and high level features. Extensive benchmark evaluation shows that our proposed method achieves significant improvement over the state-of-the-art works in SISR.","inCitations":[],"pmid":"","title":"An Attention-Based Approach for Single Image Super Resolution","journalPages":"2777-2784","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1807.06779v1.pdf","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545760","http://arxiv.org/abs/1807.06779"],"entities":[],"journalVolume":"","outCitations":["45f2c264c496296b6600fafc340b8f0dbc4ac52f","6e8b1bd63e0e33fe633d00742560de1a4ea8e30f","ca5766b91da4903ad6f6d40a5b31a3ead1f7f6de","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","a538b05ebb01a40323997629e171c91aa28b8e2f","18cc17c06e34baaa3e196db07e20facdbb17026d","81d7a3b7a250045cbe65cdd0273e69f8e5bb4763","03a5b2aac53443e6078f0f63b35d4f95d6d54c5d","ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","146f6f6ed688c905fb6e346ad02332efd5464616","06c06885fd53b2cbd407704cf14f658842ed48e5","49f517ffab84e576ca0cef98396e0bd65cd96297","071b16f25117fb6133480c6259227d54fc2a5ea0","25fc7c60a47546eb8c5d1404df366ef625004641","853c2c56ec2b7e827f13dd000769cc9bc6fdb335","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","915c4bb289b3642489e904c65a47fa56efb60658","26d4ab9b60b91bb610202b58fa1766951fedb9e9","39c3ba33f6ec8f9de36f0a8461c5f78e4f911d98","546b3592f59b3445ef12fba506b729c832198c33","1e3ea7a70c9d8984fe8f0d0988b672e411162d43","189c1a6d0047476d89dc3ceef7d57e809329a32b","7da7678882d06a1f93636f58fe89635da5b1dd0c","b82b0badc69573df4c2881f1aa662e21b3c2da4f","41d8d35fd8ff2d82c7b32f2d59c0c1ba64ba699a","c225ce4a58b7693919dcdc62871ca303e02e003e","7ba5d3808e117e7a68dc40331ce1d483ceeedcb2","272216c1f097706721096669d85b2843c23fa77d","0b0cf7e00e7532e38238a9164f0a8db2574be2ea","5054edca22325ddd3507b860f9af4a961baea009","2c03df8b48bf3fa39054345bafabfeff15bfd11d","26c8d040bef85ad6dde55a8f71af936fb38356ad","2f160ce71f01ac2043de67536ff0e413ff6f58c5","0875fc92cce33df5cf7df169590dbf0ca00d2652","5763c2c62463c61926c7e192dcc340c4691ee3aa","ee7c4f765e3743b324ee2d74f056a01f7e54320d","501d99e392783e4acafb220136d32ea68a921282","b13b40d94e06723401963342dd67e104132fe489"],"id":"2555ffe399b99600ee2295fc433ca768c4fe2b65","s2Url":"https://semanticscholar.org/paper/2555ffe399b99600ee2295fc433ca768c4fe2b65","authors":[{"name":"Yuan Liu","ids":["49420316"]},{"name":"Yuancheng Wang","ids":["5196783"]},{"name":"Nan Li","ids":["50599127"]},{"name":"Xu Cheng","ids":["48684315"]},{"name":"Yifeng Zhang","ids":["2284679"]},{"name":"Yongming Huang","ids":["1713582"]},{"name":"Guojun Lu","ids":["1748411"]}],"doi":"10.1109/ICPR.2018.8545760"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545571","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Melanoma is known to be the most fatal form of skin cancers. In order to achieve automated diagnosis of such disease, a system is needed to accurately locate suspicious skin lesions using images captured by standard digital cameras. Recently, there exists a trend for the use of Fully Convolutional Net-work(FCN) to perform image segmentation task. In this paper, we propose a FCN-based processing pipeline that incorporates a deep neural net and a graphical model, to attain a segmentation mask of lesion region from normal skin. Our method extends the residual network by adding a transposed convolution layer to yield a FCN architecture. We demonstrate that the noisy outcome from FCN can be refined by a fully connected Conditional Random Field(CRF). Our model enjoys three major advantages over existing algorithms: simpler process pipeline, state-of-art accuracy in terms of segmentation sensitivity(95.6%) and fast inference time.","inCitations":[],"pmid":"","title":"Fast Skin Lesion Segmentation via Fully Convolutional Network with Residual Architecture and CRF","journalPages":"1438-1443","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545571"],"entities":[],"journalVolume":"","outCitations":["5e6dee39e29c0d74452deca07d8b20e70576aa3e","9201bf6f8222c2335913002e13fbac640fc0f4ec","46018a894d533813d67322827ca51f78aed6d59e","0690ba31424310a90028533218d0afd25a829c8d","95e7df02bcf50a6575c1556d759ffe3a0c3f3314","b7bbd5030b39ab97d2dfdccd5b6ecf9174d8cb9a","306b3d24b2147b55e048ad733f5b8556b0b7204e","272216c1f097706721096669d85b2843c23fa77d","835fd564d0e6ebf04ddade5410ca826fda26e912","d497d7df0b8b2bdff6c93c10d7e3e519a8d23532","42407d338639b1484236ef58f7ea226164fcfcb0","2c03df8b48bf3fa39054345bafabfeff15bfd11d","c4aa3d3df6acff478d5d0b9648818532571c1e91","37bda16c9f204c98d4c57a6be00cee3db520643d","17d1949e6653ec81eb496b38f816e99572eabcb1","703eea50a0f6d4b1ff9d040671e26b1494297ce7","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","ebe2d55d3eb5c43be3537a02153122a31eb7533a","6d173e78ee719a0713abdcf2110cd7f38d082ae7","ab657a056195325116f056cbfdca48a483453e3d"],"id":"3ab132929e2ea0da1c1cd77e5f22822dfe383a43","s2Url":"https://semanticscholar.org/paper/3ab132929e2ea0da1c1cd77e5f22822dfe383a43","authors":[{"name":"Wenfeng Luo","ids":["2674397"]},{"name":"Meng Yang","ids":["50367201"]}],"doi":"10.1109/ICPR.2018.8545571"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202141","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Policy search methods and, more broadly, reinforcement learning can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of guided policy search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We describe how both policy learning and data collection can be conducted in parallel across multiple robots, and present a detailed empirical evaluation of our system. Our results indicate that distributed learning significantly improves training time, and that parallelizing policy learning and data collection substantially improves utilization. We also demonstrate that we can achieve substantial generalization on a challenging real-world door opening task.","inCitations":["551c60bd9178a199c20723122cd26ddd9c0c93b6","b936f9d2a299f71ff75a903b4e6d3909b0bfa69e","851f6a2f0479a3ad11782e5b4697ff7fad1984c1","560ff280d54194c62ca6a942a38c8ba148dcc123","050a89a91b3e828c841972a81f17807f82c79713","c4d36ce32634c4ecef70ae3dfc75a0066bd5da02","336b6a803cd5b65a56354192bca22d854e77d655","09cd5d3634d84d528d16c4bcc89c6fd5b883c7cc","93621fa5bd706eea7cd20f00dec39dd3fccc43a3","27f9e60b26c5db497333f4563dfc990321e130dc","562383bfb605bb1f3b227c591d97add60169ac3d","fc4fcdba9c4ed15fafc32227ad7a6fde5a193e79","ded85a1068612563a1a5998f072290d63d8a0260","afbe59950a7d452ce0a3f412ee865f1e1d94d9ef","42526c4807ee9e40d94c9504c0799d7c48a90425","9d4dab15a6379338147e57d085e9107ad6664d95","aab037c6e8ea13f3d307fbf45605d9b6a23351b5","099d5e15338c80d7db616262c9b09f7af1781cce","ef5bc20cee874f257a74d45bf9c4585b1bbb7ea2","ad51140c8d5f56e2d205befa2f98c33566bd7b4f","0e311a73c4f29e748ac30e5b2b6f80c6a2779bc6","40cc7cdffa0a861cb557410518246d97d1678642","9e9103067efe3d7996fec048fab81236a12349c1","017b07fe36e8d43965b2125f6170a97c9d747fca","f2b9337c7d78a80717cf5c52afcf9dfb1945f1b9","c200d3862df97e119e11ef0031b62c5bacc0ca00","20589b37883fbae68d060cf4d4143d491677b096","410c7fdfeb2accf30e19a175069c1cef22622bf4","e7019d9786af58f56c5de7e7daa2ce9050ba60f0","eb37e7b76d26b75463df22b2a3aa32b6a765c672","160d6fe8921c6e66dd5fa925f88d40d0f6682d27"],"pmid":"","title":"Collective robot reinforcement learning with distributed asynchronous guided policy search","journalPages":"79-86","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1610.00673","https://doi.org/10.1109/IROS.2017.8202141","https://arxiv.org/pdf/1610.00673v1.pdf"],"entities":["Automatic parallelization","Reinforcement learning","Robot"],"journalVolume":"","outCitations":["3d4f6a526ea8230274416be94652b826f4aba396","681a6e7f8c168b75eeb55d61cc8d594165cf3625","d86c6b6addcd4fa67bebf38d24432a0566bc0acb","2d02cf53bc0f2d919b89bec8f9160b50916bb625","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","0122e063ca5f0f9fb9d144d44d41421503252010","25fb5a6abcd88ee52bdb3165b844c941e90eb9bf","427c24c75128412166326e2afda1e3cd5a35a16a","1407ae8725ccef768d634b2f6ec2baa2197b0bbb","4f50f82802f7df6f7590e4e297de65376b8e3195","5fa4a43bc0b1fc391e590eaa5cf61fbfff7755da","7f2d8be4305321e3690eab3a61857d4a80db664a","0144941d255dad89d3d90c2d131a15cc01df9829","abe1321251be816165f59d0c7a21bf6302ed3df8","67cc087f65d232829130911879141ce1ae8bf86e","2c04fb844c536df6030e9abd159347899d12521d","15cf63f8d44179423b4100531db4bb84245aa6f1","27db31b3e29cbdb19a91beedd3cdfdda46ce1fd1","1dffb9ea4b5eb7d31c18eec5de1fc4b96a444b49","197f137c0174141d65418502e8702281281cdf3a","1e6bcda276bb1681125eb18ca4d62a44e94c25c9","1554d6787b88b24cde34f398a330e1ad4d30b634","1085f966422299c9a54a7a8cecfe815ac9824efb","48230ed0c3fa53ef1d43d79e1f6b113f13e83b9b","48d893f022e3167fd6ec42d1475a82623e9d6456"],"id":"0334cf4857b465184f969c65f1ea0347d452ff18","s2Url":"https://semanticscholar.org/paper/0334cf4857b465184f969c65f1ea0347d452ff18","authors":[{"name":"Ali Yahya","ids":["5741968"]},{"name":"Adrian Li","ids":["2598515"]},{"name":"Mrinal Kalakrishnan","ids":["1729262"]},{"name":"Yevgen Chebotar","ids":["2527420"]},{"name":"Sergey Levine","ids":["1736651"]}],"doi":"10.1109/IROS.2017.8202141"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206071","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Flexible manipulator enables curvilinear accessibility through small incisions or natural orifices for minimally invasive surgery and diagnosis, which makes it a good choice for minimally invasive surgery. In order to control the robot precisely and safely, the real-time position and shape information of the robot need to be measured well. In this paper, we propose a magnetic tracking based tip pose and shape detection method for wire driven flexible robots. A permanent magnet is mounted at the distal end of the robot. Its magnetic field can be sensed with a sensor array. Therefore, position and orientation of the tip can be estimated utilizing the tracking method. A shape sensing algorithm is then carried out to estimate the real-time shape based on the tip pose. With the tip pose and shape display in the reconstructed visual environment, navigation can be achieved. This method provides the advantages that no sensors are needed to mount on the robot and has no line-of-sight problem. Experimental results verified the feasibility of the proposed method. A navigation error of 1.9mm is achieved.","inCitations":[],"pmid":"","title":"Preliminary study on magnetic tracking based navigation for wire-driven flexible robot","journalPages":"2517-2523","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206071"],"entities":["Accessibility","Algorithm","Experiment","Line-of-sight (missile)","Real-time clock","Real-time locating system","Robot","Sensor"],"journalVolume":"","outCitations":["e158fee7e248899cf09aa37b8e25b84c8c147e87","02ff3de0b831ab5045b27987439aa94503752d44","6de1617df91ea9c6b32b7e800fc5d7d55bdbafb4","d1472213895d920cede17b0d520b26301ca86f4f","92cfee675015a2e53d1cf19a0bf8d2efae45be78","c643048c6ab2ceaab1ced919a392cab17e8744d4","7a2a6b3d8825e8d9df1466aaa3c8bcc819b8d59d","ad825c3bdd19817ebf73d11d230a045343e7a53c","67c366fa0377271972765db9e3d90a4a46470498","13b1364045a8325071cd3a4da17506f115fa1bbb","39aaae91d7f4fb1cde03c417c4b26a3c588d9571","1f1b41392a01252f4b19bc24f2555f25f61f6881","2495972dfd6d988c3c5fbfc75f74b9bf4f8215ff","08f1b5ffda1dcc1f98ae3f9d3b1009a2a707b8f1","3fc7dbd009c93f9f0a163d9dfd087d7748ee7d34","0083a817251a5f4c1d625a94398d5d0255a9a00e"],"id":"d9117d891d8b9fa273f25d52d6085c0d212a82bc","s2Url":"https://semanticscholar.org/paper/d9117d891d8b9fa273f25d52d6085c0d212a82bc","authors":[{"name":"Changchun Zhang","ids":["2121446"]},{"name":"Yi Lu","ids":["47006112"]},{"name":"Xiaoxiao Qiu","ids":["2051105"]},{"name":"Shuang Song","ids":["6970155"]},{"name":"Li Liu","ids":["46457827"]},{"name":"Max Q.-H. Meng","ids":["1735045"]}],"doi":"10.1109/IROS.2017.8206071"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206302","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper we provide a method for identifying and temporally localizing tactile force actions from measured force signals. Our key idea is to use the continuous wavelet transform (CWT) with the Complex Morlet wavelet to transform force signals into feature vectors amenable to machine learning algorithms. Our method uses these feature vectors to train a classifier that recognizes different actions. We demonstrate our approach in a system that records human activities with an instrumented set of tongs. Our system successfully identifies a wide range of actions based on a small set of labeled examples.","inCitations":["5988de6202683bcb2b81ad1f50e8a8c489a61a8d","2e19e455327176d02f9c82e43bd9638d97a9908d"],"pmid":"","title":"Recognizing actions during tactile manipulations through force sensing","journalPages":"4386-4393","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206302","https://graphics.cs.wisc.edu/Papers/2017/SRWZG17/17-IROS-Wavelets%20preprint.pdf"],"entities":["Approximation algorithm","Complex wavelet transform","Continuous wavelet","Experiment","Feature vector","Internationalization and localization","Machine learning","Morlet wavelet","Online and offline","Real-time computing","Robotics","Statistical classification","Stylus (computing)","Supervised learning","Video post-processing"],"journalVolume":"","outCitations":["3c86dfdbdf37060d5adcff6c4d7d453ea5a8b08f","1be16d8c557b15cdf2db9e7eb4453f2274fd60af","7f5205f995284da3d2e5ec569755ea5ea0d3a1ef","1c91621232315fc8e3d58129a6bd4334983912b4","44c918bea209d1d77e4c0e9a06491d286cf1c2ea","563e821bb5ea825efb56b77484f5287f08cf3753","fc6802269336cf1e0abb529644d4c522bce45872","d984b580e02da76cd4d991953e6d430fadf3d578","90c9c29e58b17794feae791122cb9daba7444c9a","521a1fdd0b8bbfcb08296f452dce18b6df2faa60"],"id":"3ce918c06ac30ba3ca55baed97da44ed739af9cc","s2Url":"https://semanticscholar.org/paper/3ce918c06ac30ba3ca55baed97da44ed739af9cc","authors":[{"name":"Guru Subramani","ids":["2770394"]},{"name":"Daniel Rakita","ids":["34807781"]},{"name":"Hongyi Wang","ids":["3148801"]},{"name":"Jordan Black","ids":["34581044"]},{"name":"Michael R. Zinn","ids":["2233785"]},{"name":"Michael Gleicher","ids":["1776507"]}],"doi":"10.1109/IROS.2017.8206302"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594069","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The remote center of motion (RCM) mechanism is a prominent candidate to aid bone drilling. The surgeon can simply place a drill with the RCM mechanism near the entry point to provide drill alignment with the target. Using this assistive mechanism for bone drilling improves drilling accuracy and reduces the complexity of bone drilling robotic systems. However, because most RCM mechanisms have been developed for laparoscopic surgery or needle insertion into soft tissue, they lack rigidity and are unsuitable for bone drilling. One of the most difficult and important surgical procedures in bone drilling is maintaining as well as guiding the orientation of the drill with respect to the target. This paper proposes an improved RCM mechanism in which a pair of linear actuators and a gearless arc-guide are employed to achieve high rigidity and resolution, which enable bone drilling. A vision-guided navigation system is also integrated into the proposed system to automatically guide the orientation. To verify that the proposed RCM mechanism has sufficient rigidity and targeting accuracy, a series of experiments was performed. The results obtained confirm that the proposed mechanism can maintain its tilting angle under up to 50 N, with a targeting error of approximately 0.28mm.","inCitations":[],"pmid":"","title":"Trigonometric Ratio-Based Remote Center of Motion Mechanism for Bone Drilling","journalPages":"4958-4963","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594069"],"entities":[],"journalVolume":"","outCitations":[],"id":"7c7b2e69c9970ceb21d707a2d4c3edbbf571a82b","s2Url":"https://semanticscholar.org/paper/7c7b2e69c9970ceb21d707a2d4c3edbbf571a82b","authors":[{"name":"Seongbo Shim","ids":[]},{"name":"Seongpung Lee","ids":[]},{"name":"Daekeun Ji","ids":[]},{"name":"Hyunseok Choi","ids":[]},{"name":"Jaesung Hong","ids":[]}],"doi":"10.1109/IROS.2018.8594069"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00025","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"In this paper, we introduce a new 3D hand gesture recognition approach based on a deep learning model. We propose a new Convolutional Neural Network (CNN) where sequences of hand-skeletal joints' positions are processed by parallel convolutions; we then investigate the performance of this model on hand gesture sequence classification tasks. Our model only uses hand-skeletal data and no depth image. Experimental results show that our approach achieves a state-of-the-art performance on a challenging dataset (DHG dataset from the SHREC 2017 3D Shape Retrieval Contest), when compared to other published approaches. Our model achieves a 91.28% classification accuracy for the 14 gesture classes case and an 84.35% classification accuracy for the 28 gesture classes case.","inCitations":["9ee40245339ca8280ad62837782aa94ba3783fe8","d1f1428cef11b3cbbdf3886b9e76e65bd91cb91e","5858879f587cf61792c3ff6ccd353f8dc77da8da"],"pmid":"","title":"Deep Learning for Hand Gesture Recognition on Skeletal Data","journalPages":"106-113","s2PdfUrl":"","pdfUrls":["https://hal-mines-paristech.archives-ouvertes.fr/hal-01737771/document","http://doi.ieeecomputersociety.org/10.1109/FG.2018.00025"],"entities":[],"journalVolume":"","outCitations":["0d7fec7828da2b684dd9afe8ac7274a499e7211d","d6f030b95018bc1264d20626affc07281ba9db8e","b6052dc718c72f2506cfd9d29422642ecf3992ef","8387f38770e3c661c4ef466a8d9733aab58c068a","b94ccb595375bf57617575454b418fc6371b1d7c","460f858853c83391952ffd3c9d8cb920c126420d","def773093c721c5d0dcac3909255ec39efeca97b","7ed597e4f58ce4ea82610277523eb63a6302acde","073bcb3b1aed5cdf7bff4e9fe46a21175f42c877","80f67ec9d8005c70cb9e18cf58195c4abe32deef","f451d0212e65ab9970a58b584b3363a853c9811c","1551ab876f75b1d11eab4a7ebd21c95ac771da43","f6567671cc9d204c1dd1322e9c49d2053ed734c5","0b0cf7e00e7532e38238a9164f0a8db2574be2ea","7354c2a2955bae59303b7e099bd2de51095b3fca","3036a2c04ff7bcaf2c252dc94f209a40f6a2d95b","6c8b30f63f265c32e26d999aa1fef5286b8308ad","6f3b511729fe87658da0d22065688c42bdcd1f4c","acceed3fae3b1d0befd552804fc971a547fd74bc","17341a734128311fd363bd2fdb527f2fc735486d","070e9c201a99ed33e975d141514dd2b765e2a44a","3b2697d76f035304bfeb57f6a682224c87645065","a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","99b26202f350c8d06eb3fcbf3226045b16c34eb5","0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f","061356704ec86334dbbc073985375fe13cd39088","ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","52eec5b914f72c4cd3f03eaedf1d38bb9a4df6de","38bfdb0909acec6a86c0f764ea5d0723d289a72d","0b3cfbf79d50dae4a16584533227bb728e3522aa","75f8644c29c024961c90a6b9d3803470ddac8d59","272216c1f097706721096669d85b2843c23fa77d","014eeadc4d22dad50995b462c29540c5024f2c2d","2c03df8b48bf3fa39054345bafabfeff15bfd11d","162d958ff885f1462aeda91cd72582323fd6a1f4","dece43cbd0a7b21bdfe2878639f472fcc7e8054a","990e94324222e773a0323d5675abd889707fcb98","6da8e424200ed48c59e71d1fda59250cf518c9ed","50ca90bc847694a7a2d9a291f0d903a15e408481","0bf046038a555bc848030a28530f9836e5611b96","14fed18d838bf6b89d98837837ff314e61ab7c60","84f7b9d697ad786a6ca6e4170c072b5882803229","56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719","0b544dfe355a5070b60986319a3f51fb45d1348e","55c0113534c62b7f3f238210cf501b42d91cc33a","019475245d325f70fc3c930b9e96c0c48196ca21"],"id":"c307acefaf74743db78e563d840133b369429e19","s2Url":"https://semanticscholar.org/paper/c307acefaf74743db78e563d840133b369429e19","authors":[{"name":"Guillaume Devineau","ids":["46248275"]},{"name":"Fabien Moutarde","ids":["1748488"]},{"name":"Wang Xi","ids":["47207386"]},{"name":"Jie Yang","ids":["1688428"]}],"doi":"10.1109/FG.2018.00025"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546037","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Image classification with datasets that suffer from great imbalanced class distribution is a challenging task in computer vision field. In many real-world problems, the datasets are typically imbalanced and have a serious impact on the performance of classifiers. Although deep convolutional neural networks (DCNNs) have shown remarkable performance on image classification tasks in recent years, there are still few effective deep learning algorithms specifically for imbalanced image classification problems. To solve imbalanced image classification problem, in this paper, we explore a new deep learning algorithm called Squeeze-and-Excitation Deep Pyramidal Residual Network (SE-PyramidNet) combining with Generative Adversarial Network (GAN)-based curriculum learning. Firstly, we construct the refined Deep Pyramidal Residual Network by embedding the \u201cSqueeze-and-Excitation\u201d (SE) blocks. Secondly, towards the class imbalance problem, we adopt GAN to generate samples of minority classes. Finally, we draw lessons from the curriculum learning strategy by teaching our classifier training from original easy samples to generated complex samples, which improves the classification ability. Experimental results show that our method achieves around 0.5% gains for accuracy and 0.02 gains for F1 score respectively outperforming the state-of-the-art DCNNs.","inCitations":[],"pmid":"","title":"Teaching Squeeze-and-Excitation PyramidNet for Imbalanced Image Classification with GAN-based Curriculum Learning","journalPages":"2444-2449","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546037"],"entities":[],"journalVolume":"","outCitations":["4fced2483420a98f3b11ea4c53e3547011417130","8ac6a7f53c141206254951e7f4842f7e9dbbb23a","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","450e9676991b91e6b5eba3f77ac95dd0d3d6b655","d84c67ba4e40d838bd65d8988a4dbacbf09e850b","0421c3afe37ad35ff36b7a9b1d8da3ee68275be3","bfcf14ae04a9a326f9263dcdd30e475334a96d39","b9706be6af653bbcfe82d0898f7e237da3998e32","ecc0edd450ae7e52f65ddf61405b30ad6dbabdd7","2cbb4a2f8fd2ddac86f8804fd7ffacd830a66b58","7c2cb0272af5817cd58fb1707d6e5833c851d72b","8e9149ab00236d04db23394774e716c4f1d89231","3b2697d76f035304bfeb57f6a682224c87645065","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","2da966e3501600b8c7e3a2ce0e13bf640aedd54d","7c297b36f8ad53a1e387613336886a466e3f0d01","a538b05ebb01a40323997629e171c91aa28b8e2f","2a4744bf5acfd888f0dcda3057367b0e0eccaa29","26e0353a5b1fdff83dd2fdcbf190467e22044562","21d255246cd7ddba24a651fd716950f893ea8eb2","14b5e8ba23860f440ea83ed4770e662b2a111119","7693cafd6f29623f61d66f031cadd60b6ce827d7","877000fd60a5d2e54088b0a99123f7fa7867d4d9","185e12006c548dca57e5e11bc332725569727942","1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","14318685b5959b51d0f1e3db34643eb2855dc6d9","99282e5e30b7064403a900b1351c55d91c574d2e","0bfee28960fe3318600e3540ef8cdc6cbe9eeed3","5eb1b872bd1ded1a293935697eb7f0af37bf6635","a278ab2341f60402bdf4958e6c64a6c804669efd","061356704ec86334dbbc073985375fe13cd39088","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","2f84d432e46ed1253764e238e3038c9c791790e7","2c03df8b48bf3fa39054345bafabfeff15bfd11d","444eb20f2fed03cb50b58855c7b30bc33a5036da","cb9034572899d0575014e5b266603b72db10a83d","ae341ad66824e1f30a2675fd50742b97794c8f57"],"id":"0cbc904d2469eee619fd53345a52e94760966d8c","s2Url":"https://semanticscholar.org/paper/0cbc904d2469eee619fd53345a52e94760966d8c","authors":[{"name":"Jing Liu","ids":["46701354"]},{"name":"Angang Du","ids":["52153580"]},{"name":"Chao Wang","ids":["1722340"]},{"name":"Haiyong Zheng","ids":["49170762"]},{"name":"Nan Wang","ids":["38515715"]},{"name":"Bing Zheng","ids":["46275275"]}],"doi":"10.1109/ICPR.2018.8546037"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593964","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents the motion planning framework for a hexapod, based on advanced motions, for accessing challenging spaces, namely narrow pathways and large holes, both of which are surrounded by walls. The advanced motions, wall and chimney walking, utilise environment surfaces that are perpendicular to the ground plane to support the robot motion. Such techniques have not yet been studied in the literature. The hierarchical planning framework proposed here is an extension to existing approaches which have only considered ground walking where foothold contacts are confined to the ground plane. During the pre-processing phase of the 2.5D grid map, the motion primitives employed are assessed for each cell and stacked to the graph if valid. The A* algorithm is then used to find a path to the goal position. Following that, the path is post-processed to smoothen the motions and generate a continuous path. Footholds are then selected along the path. The framework has been evaluated in simulation on the custom-designed Corin hexapod. The resulting path enables access to areas that are previously thought to be inaccessible and reduces the travelling distance compared to previous studies.","inCitations":[],"pmid":"","title":"Grid-Based Motion Planning Using Advanced Motions for Hexapod Robots","journalPages":"3573-3578","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593964"],"entities":[],"journalVolume":"","outCitations":[],"id":"5996123155257b791048f738ebe6f8df5b388203","s2Url":"https://semanticscholar.org/paper/5996123155257b791048f738ebe6f8df5b388203","authors":[{"name":"Wei Cheah","ids":[]},{"name":"Hassan Hakim Khalili","ids":[]},{"name":"Simon Watson","ids":[]},{"name":"Peter Michael Green","ids":[]},{"name":"Barry Lennox","ids":[]}],"doi":"10.1109/IROS.2018.8593964"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594029","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Humans recognize perceived continuous information by dividing it into significant segments such as words and unit motions. We believe that such unsupervised segmentation is also an important ability that robots need to learn topics such as language and motions. Hence, in this paper, we propose a method for dividing continuous time-series data into segments in an unsupervised manner. To this end, we proposed a method based on a hidden semi-Markov model with Gaussian process (GP-HSMM). If Gaussian processes, which are nonparametric models, are used, unit motion patterns can be extracted from complicated continuous motion. However, this approach requires the number of classes of segments in the time-series data in advance. To overcome this problem, in this paper, we extend GP-HSMM to a nonparametric Bayesian model by introducing a hierarchical Dirichlet process (HDP) and propose the hierarchical Dirichlet processes-Gaussian process-hidden semi-Markov model (HDP-GP-HSMM). In the nonparametric Bayesian model, an infinite number of classes is assumed and it becomes difficult to estimate the parameters naively. Instead, the parameters of the proposed HDP-GP-HSMM are estimated by applying slice sampling. In the experiments, we use various synthetic and motion-capture data to show that our proposed model can estimate a more correct number of classes and achieve more accurate segmentation than baseline methods.","inCitations":[],"pmid":"","title":"Sequence Pattern Extraction by Segmenting Time Series Data Using GP-HSMM with Hierarchical Dirichlet Process","journalPages":"4067-4074","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594029"],"entities":[],"journalVolume":"","outCitations":[],"id":"602c3e5d67e0544d18cc95e6cacfa7d7362d3542","s2Url":"https://semanticscholar.org/paper/602c3e5d67e0544d18cc95e6cacfa7d7362d3542","authors":[{"name":"Masatoshi Nagano","ids":[]},{"name":"Tomoaki Nakamura","ids":[]},{"name":"Takayuki Nagai","ids":[]},{"name":"Daichi Mochihashi","ids":[]},{"name":"Ichiro Kobayashi","ids":[]},{"name":"Masahide Kaneko","ids":[]}],"doi":"10.1109/IROS.2018.8594029"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594062","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Conventional skid or wheel based helicopter landing gears severely limit off-field landing possibilities, which are crucial when operating in scenarios such as mountain rescue. In this context, slopes beyond 8° and small obstacles can already pose a substantial hazard. An adaptive landing gear is proposed to overcome these limitations. It consists of four legs with one degree of freedom each. The total weight was minimized to demonstrate economic practicability. This was achieved by an innovative actuation, composed of a parallel arrangement of motor and brake, which relieves the motor from large impact loads during hard landings. The loads are alleviated by a spring-damper system acting in series to the actuation. Each leg is individually force controlled for optimal load distribution on compliant ground and to avoid tipping. The operation of the legs is fully autonomous during the landing phase. A prototype was designed and successfully tested on an unmanned helicopter with a maximum take-off weight of 78 kg. Finally, the implementation of the landing gear concept on aircraft of various scales was discussed.","inCitations":[],"pmid":"","title":"An Adaptive Landing Gear for Extending the Operational Range of Helicopters","journalPages":"1757-1763","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594062"],"entities":[],"journalVolume":"","outCitations":[],"id":"f46514d10947abb05fcdbfba0aa309a64bb90c21","s2Url":"https://semanticscholar.org/paper/f46514d10947abb05fcdbfba0aa309a64bb90c21","authors":[{"name":"Boris Stolz","ids":[]},{"name":"Tim Brodermann","ids":[]},{"name":"Enea Castiello","ids":[]},{"name":"Gokula Englberger","ids":[]},{"name":"Daniel Erne","ids":[]},{"name":"Jan Gasser","ids":[]},{"name":"Eric Hayoz","ids":[]},{"name":"Stephan Muller","ids":[]},{"name":"Lorin Muhlebach","ids":[]},{"name":"Tobias Low","ids":[]},{"name":"Dominique Scheuer","ids":[]},{"name":"Luca Vandeventer","ids":[]},{"name":"Marko Bjelonic","ids":[]},{"name":"Fabian Günther","ids":[]},{"name":"Hendrik Kolvenbach","ids":[]},{"name":"Mark A. Höpflinger","ids":[]},{"name":"Marco Hutter","ids":[]}],"doi":"10.1109/IROS.2018.8594062"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593669","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents an autonomous perching concept for multirotor aerial vehicles. The Autonomous Grasping Robotic Aerial System for Perching (AGRASP)represents a novel integration of robotics perception, vision-based path planning, and biomimetically-inspired manipulation on a small, lightweight aerial robot with highly-constrained sensor and processing capacity. Computationally lightweight perception algorithms pull candidate perch structures out of a complex environment with no a priori knowledge of the operational space. The innovative manipulator design combines both active grasp and passive grip enabling it to maintain hold on the perch even with all power off. We experimentally demonstrate, for the first time, a quadrotor autonomously detecting and landing on a perch relying solely on onboard sensing and processing.","inCitations":[],"pmid":"","title":"Autonomous Grasping Robotic Aerial System for Perching (AGRASP)","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593669"],"entities":[],"journalVolume":"","outCitations":[],"id":"f992f30bdcc18ebf1ce3137ebb13668baab036ed","s2Url":"https://semanticscholar.org/paper/f992f30bdcc18ebf1ce3137ebb13668baab036ed","authors":[{"name":"Katie M. Popek","ids":[]},{"name":"Matthew S. Johannes","ids":[]},{"name":"Kevin C. Wolfe","ids":[]},{"name":"Rachel A. Hegeman","ids":[]},{"name":"Jessica M. Hatch","ids":[]},{"name":"Joseph L. Moore","ids":[]},{"name":"Kapil D. Katyal","ids":[]},{"name":"Bryanna Y. Yeh","ids":[]},{"name":"Robert J. Bamberger","ids":[]}],"doi":"10.1109/IROS.2018.8593669"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.141","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Human age estimation plays an important role inhuman facial image analysis. Aging feature representation isone of the widely studied problems in this topic. Convolutionalmap (bio-inspired features, or BIF) has been proven to be themost successful framework, but its manual crafted filters cannot easily capture the complicated facial aging pattern. In thispaper, we adopt this convolutional map framework but proposea novel feature learning approach based on convolutional sparsecoding (CSC) that can automatically learn to characterizeaging signatures. Compared to other popular feature learningapproaches like deep convolutional neural network (CNN), weverify that our learning approach can extract localized subtleaging features like CNN, and also significantly reduce themodel size. Moreover, we employ the standard deviation (STD)pooling to summarize the aging feature. Finally, the extractedfeatures are fed into a discriminative manifold learning modelto obtain more discriminative low-dimensional representationsand further improve the computational efficiency. We evaluateour approach over the standard benchmark datasets. Theexperimental results demonstrate that our approach impressivelyoutperforms the state-of-the-art results. The proposedage estimation scheme also performs well in the cross-databaseage estimation task.","inCitations":["97f3d35d3567cd3d973c4c435cdd6832461b7c3c"],"pmid":"","title":"A Study of Convolutional Sparse Feature Learning for Human Age Estimate","journalPages":"566-572","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.141"],"entities":["Antivirus software","Artificial neural network","Benchmark (computing)","British Informatics Olympiad","Convolutional neural network","Cross-validation (statistics)","Feature learning","Human-based computation","Image analysis","Map","Neural coding","Nonlinear dimensionality reduction","Sparse","Sparse matrix"],"journalVolume":"","outCitations":["7220090ec024ed61d2d8cfc022a5bc670b8e27ac","289919021222236377fee58a39276bd261c81b0f","0f5bf8eb4f4e34ef83fdb2867455d8650acac158","15fbb5fc3bdd692a6b2dd737cce7f39f7c89a25c","3edb0fa2d6b0f1984e8e2c523c558cb026b2a983","b47a3c909ee9b099854619054fd00e200b944aa9","2cbb4a2f8fd2ddac86f8804fd7ffacd830a66b58","944be76b1deba5c971bc60ecc50a7d46452578e5","97f3d35d3567cd3d973c4c435cdd6832461b7c3c","9c61f9f8089fc326a4f6cb13bd746483d380cb59","ab29f97e7180b284be7003fbc6725566ea0cd459","20aa8348cf4847b9f72fe8ddbca8a2594ea23856","42917e8bf8dc220ee6145cd13aed688f556f64ff","8000c4f278e9af4d087c0d0895fff7012c5e3d78","281e1000e0f9873f217a1d5895036ee72a54bf7a","1f88427d7aa8225e47f946ac41a0667d7b69ac52","67db50715cf08e06d6fe7588c205c85d5656714f","12439a6ff384e95ee2262ee982bc055534e30487","4516c90e553e2af25dc5f5e594409fb645d1abd8","1c5a5d58a92c161e9ba27e2dfe490e7caaee1ff5","af12a79892bd030c19dfea392f7a7ccb0e7ebb72","0c741fa0966ba3ee4fc326e919bf2f9456d0cd74","a53d13b9110cddb2a5f38b9d7ed69d328e3c6db9","af961f130fa89225848c96c28df60972059d398a","13e415ed39f406f1a7c687cb55b6129b1c40ddd5","5d2a01e3a445a92ecdce5f20656fd87e65982708","66edc1de8b1cc9a968270b9fb87de75ee710de57","294cf6f4bf6fbb2c028900e13d1c202224dbd384","6ab33fa51467595f18a7a22f1d356323876f8262","1f41a96589c5b5cee4a55fc7c2ce33e1854b09d6","27a3728e586e333a768f91eb56ded3bbf34fbe3d","a8ef957793793c22547aaab3677870c7336c25c1","9055b155cbabdce3b98e16e5ac9c0edf00f9552f","8cffe360a05085d4bcba111a3a3cd113d96c0369","2c3f31d058f268afbc29940a94207d8800d02ba9","dcde6dd8ca7f57630818b8ea1b750919d32e6b00","cd63759842a56bd2ede3999f6e11a74ccbec318b","cdfe859fbe15e40847399dbad0f0c5a258ab7854"],"id":"997c7ebf467c579b55859315c5a7f15c1df43432","s2Url":"https://semanticscholar.org/paper/997c7ebf467c579b55859315c5a7f15c1df43432","authors":[{"name":"Xiaolong Wang","ids":["1709719"]},{"name":"Robert L Li","ids":["37801261"]},{"name":"Yin Zhou","ids":["3030962"]},{"name":"Chandra Kambhamettu","ids":["1708413"]}],"doi":"10.1109/FG.2017.141"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206185","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a model for safe driving at blind intersections and its integration to a local planner based on a Frenet frame. The model predicts potential moving obstacles from blind intersections to proactively slow down to avoid potential collisions. The derivation of the model is described and its parameters are detailed. The local planner computes smooth trajectories with smooth velocity profiles so that the vehicle can follow the paths without jerk and sudden accelerations resulting in safe and comfortable navigation. Experimental results in simulation and in the real field with an autonomous car, show that the proposed predictive driving framework can reproduce human expert driver's trajectories and velocities when facing blind intersections.","inCitations":["8e7bfe1e6794300743a87f7738e53b362362eab3","f120a5e085190767dfbabe3d30c370e854b32bee","2f3230652b8a7189d297c1b8c6770a07d82a86a3","a9a85c2a809b7cf92c4b28a13f4df0e129bc6fe3","8a138c0dd010745b9665ac23f4790ff345f361d6"],"pmid":"","title":"Autonomous predictive driving for blind intersections","journalPages":"3452-3459","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206185"],"entities":["Autonomous car","Experiment","Real life","Requirement","Simulation","Velocity (software development)"],"journalVolume":"","outCitations":["04e83b3a4932315a277809817ad1e85bbbf0a011","b348042a91beb4fa0c60fd94f27cf0366d5f9630","b56dc8cc9217c8d8f7d3d134ced57fbe06454e7b","8768c7cb89c727680f87f6e068f290c86d607c53","3f77f8b0e1a3e3dcedb0953a42018b6948b60617","7fe680297a5521e9add69c2591e7b967f806acd8","59d1f559070457ea647858d5ea6b57c26222cd3b","217ca7f368d969942e12ae89eafbfa29c1925900","066b4f278f517eb7081a3abea578a755429262df","530237ab2a179995ac6e33213877bbbfcb1fe3d0","b7865fee4ecf4a34ff79eee55d117bdff6c9b441","a1b8cc54e55586cb8664caaf78e75a3aef35d84e","5adf34e44d51659edf4437d602aaf22ec0a50d46","907393cddb3a3c7c613a1610bd70c0b701a71a7e","0ad2637d9cd0f1f6307b0d0cb646beee2585b8ff","f95577df76d3a78de71ff0ced34140708bc3e2b4","31c7ec53e96e85f9691da0441d890cf06cd72529","bc088b082951d0f5274ad44688b266321654e75c","e241941dde0bbee962630d8202ea67e886d106ea"],"id":"3465c714831813244d5118a08df4b1af0f6433b6","s2Url":"https://semanticscholar.org/paper/3465c714831813244d5118a08df4b1af0f6433b6","authors":[{"name":"Yuki Yoshihara","ids":["1875768"]},{"name":"Luis Yoichi Morales Saiki","ids":["2060731"]},{"name":"Naoki Akai","ids":["1859648"]},{"name":"Eijiro Takeuchi","ids":["49567362"]},{"name":"Yoshiki Ninomiya","ids":["3242949"]}],"doi":"10.1109/IROS.2017.8206185"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594198","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"During percutaneous interventions in the brain, puncturing a vessel can cause life threatening complications. To avoid such a risk, current research has been directed towards the development of steerable needles. However, there is a risk that vessels of a size which is close to or smaller than the resolution of commonly used preoperative imaging modalities (0.59 × 0.59 × 1 mm) would not be detected during procedure planning, with a consequent increase in risk to the patient. In this work, we present a novel ensemble of forward looking sensors based on laser Doppler flowmetry, which are embedded within a biologically inspired steerable needle to enable vessel detection during the insertion process. Four Doppler signals are used to classify the pose of a vessel in front of the advancing needle with a high degree of accuracy (2° and 0.1 mm RMS errors), where relative measurements between sensors are used to correct for ambiguity. By using a robotic assisted needle insertion process, and thus a precisely controlled insertion speed, we also demonstrate how the setup can be used to discriminate between tissue bulk motion and vessel motion. In doing so, we describe a sensing apparatus applicable to a variety of needle steering systems, with the potential to eliminate the risk of hemorrhage during percutaneous procedures.","inCitations":[],"pmid":"","title":"Vessel Pose Estimation for Obstacle Avoidance in Needle Steering Surgery Using Multiple Forward Looking Sensors","journalPages":"3845-3852","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594198"],"entities":[],"journalVolume":"","outCitations":[],"id":"b37a7357b4139da68dcc88eb6f3a03b03a83216b","s2Url":"https://semanticscholar.org/paper/b37a7357b4139da68dcc88eb6f3a03b03a83216b","authors":[{"name":"Vani Virdyawan","ids":[]},{"name":"Ferdinando Rodriguez y Baena","ids":[]}],"doi":"10.1109/IROS.2018.8594198"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593761","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Visually recognising a traversed route - regardless of whether seen during the day or night, in clear or inclement conditions, or in summer or winter - is an important capability for navigating robots. Since SeqSLAM was introduced in 2012, a large body of work has followed exploring how robotic systems can use the algorithm to meet the challenges posed by navigation in changing environmental conditions. The following paper describes OpenSeqSLAM2.0, a fully open-source toolbox for visual place recognition under changing conditions. Beyond the benefits of open access to the source code, OpenSeqSLAM2.0 provides a number of tools to facilitate exploration of the visual place recognition problem and interactive parameter tuning. Using the new open source platform, it is shown for the first time how comprehensive parameter characterisations provide new insights into many of the system components previously presented in ad hoc ways and provide users with a guide to what system component options should be used under what circumstances and why.","inCitations":[],"pmid":"","title":"OpenSeqSLAM2.0: An Open Source Toolbox for Visual Place Recognition Under Changing Conditions","journalPages":"7758-7765","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1804.02156v2.pdf","http://arxiv.org/abs/1804.02156","https://doi.org/10.1109/IROS.2018.8593761","http://export.arxiv.org/pdf/1804.02156"],"entities":[],"journalVolume":"","outCitations":["7356df7847e4fee813ee2c0058c86ad33b7f96a3","cde4fbd607f84c4ea220150f5cdf7c2f4179b327","8582fd09981cb5fe9729b2014112674c2e991c1f","19e86a297581ffa2ee61f4ca6400b887bfa94954","27eb13ada9240ed713a06840096757f4a7c45003","80eb30e83a4f6c0a742f69a75c499e1d0a3acca2","88a00eb1eb6b1a7df54ed0ac668afde26122af1b","dde1b9976d2e9b38a4bb0f12036bd47a9c53bbdf","448babca3ceca4a4f2b2ce1d2e87cd4ff43e337a","a00c0928a03d58a365d086a8df76ae93caf7cc63","9096e3d187ca5e6de6e49bd91b6dcddd89e35989","4e2baa8db03104257c3016f4e9f948334ec251da","27b87bdee46964757b83b5afb4184e438cad6b1b","b95f5ad43cdc8deb51bd04eba6bbcf2a4eeb2031","1c6a063611cf80fd6dbca33e85919fb0b1fb3dab","b05d91344f5510a1b549876ebf404b0329967419","76dde58447cfd8d59d459f95a0af60a439ab9927","8ba288f8a574af3136ac93501284a91bbfc587f1","978e7f8557774fa1ebd228e181c4c154449423c9","07b623900ce574a65f8133696ebe56ff7779f2ff","ec5c4a3a1fb7b84c76bf09e8ca0115b161e9472a","ead7afd5ee175af8119fd63fd62c29a87c4afd21","a93d6e82cd500663a8eea02a8e3617632aafe913","613239a6c0188a60c9fac6772572861112a385a9","c67d4dbcb405e0aa52df4e152f2dbf9bd2772a3d","1cf8464425870c125b4f3a0560651c97c3911a8e","53b5d6d5b8bd3499d522eb929566bc77e54fa9a7","ce6d34010f04afa4cf3018f51bad8f480ebc759c","1bc5ea91875143e0526ac587b9c4aa6c64ceec98","f877373c0ec962ad488f48d2cc917a0c005d54e7","ba282518e054d16c085ebdd9b52dafd70260befc","144cffb99d850fc35bae237431b593520c2a35cd","230ad73e8bd1d3268d56c66a83442d24176b864d","6cfdb6c251ab7051a74bdfbad2882fd8c9fc31e3","1a998506899da3bc703455bde45114bd4c228947","c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2","041a09a9db9a318596b72d041a832b05100b0ddf","b1397c9085361f308bd70793fc2427a4416973d7"],"id":"f15e788ef97bc2bd7705969317e61422eba423a3","s2Url":"https://semanticscholar.org/paper/f15e788ef97bc2bd7705969317e61422eba423a3","authors":[{"name":"Ben Talbot","ids":["16823219"]},{"name":"Sourav Garg","ids":["1735947"]},{"name":"Michael Milford","ids":["1809144"]}],"doi":"10.1109/IROS.2018.8593761"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546065","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In real-world conditions, robust face alignment is challenging due to the large variability of occlusion and pose. Many methods aim to solve the problem, but can handle either images with occlusion only or with arbitrary poses only. In this paper, we propose a unified framework by ignoring the points which cannot be seen under occlusion and extreme poses, in which we get facial parts first by classification and then train regression models to get key points. It leads to higher accuracy when locating the truly existing points without considering the occluded and non-existent points. Besides, we observed that the drift and shape of face detection results affect face alignment. As far as we know, we are the first to explicitly raise the issue and solve it to some extent. Finally, our method outperforms the state-of-the-art methods on AFLW and COFW datasets. It is also comparable to other methods on LFPW dataset.","inCitations":[],"pmid":"","title":"A New Method for Face Alignment under Extreme Poses and Occlusion","journalPages":"1586-1591","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546065"],"entities":[],"journalVolume":"","outCitations":["8a3c5507237957d013a0fe0f082cab7f757af6ee","2529fa5dd51817a302b6a737025c37cdebae5f72","b33e8db8ccabdfc49211e46d78d09b14557d4cba","2d072cd43de8d17ce3198fae4469c498f97c6277","57ebeff9273dea933e2a75c306849baf43081a8c","1c1f957d85b59d23163583c421755869f248ceef","084bd02d171e36458f108f07265386f22b34a1ae","b2cd92d930ed9b8d3f9dfcfff733f8384aa93de8","8d4f12ed7b5a0eb3aa55c10154d9f1197a0d84f3","614a7c42aae8946c7ad4c36b53290860f6256441","3fb3c7dd12561e9443ac301f5527d539b1f4574e","ad64984bf514154147fd54e39fe367e18cba9a41","405526dfc79de98f5bf3c97bf4aa9a287700f15d","085ceda1c65caf11762b3452f87660703f914782","6754c98ba73651f69525c770fb0705a1fae78eb5","a643302a89805bb8d3d204660a3a60420fee36e2","88a323dc70de8788a9cdd415f7c65fc3d3a8c7cb","2724ba85ec4a66de18da33925e537f3902f21249","e2a16867d35eaf8d36e9730250cd8c73d2e9cc44","65126e0b1161fc8212643b8ff39c1d71d262fbc1","1824b1ccace464ba275ccc86619feaa89018c0ad","22e2066acfb795ac4db3f97d2ac176d6ca41836c","303a7099c01530fa0beb197eb1305b574168b653","0e986f51fe45b00633de9fd0c94d082d2be51406","03f98c175b4230960ac347b1100fbfc10c100d0c","2c17d36bab56083293456fe14ceff5497cc97d75","37ce1d3a6415d6fc1760964e2a04174c24208173","58970f1f51432a094faaeb3f4f70aa1778d61a42"],"id":"63c0bba6ac0937736ec4d8e147acda594cc1fb70","s2Url":"https://semanticscholar.org/paper/63c0bba6ac0937736ec4d8e147acda594cc1fb70","authors":[{"name":"Jun Li","ids":["47786808"]},{"name":"Qiongling Xiao","ids":["52205857"]},{"name":"Ruoyu Yang","ids":["8092869"]}],"doi":"10.1109/ICPR.2018.8546065"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202171","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"A new class of simple, adaptive, under-actuated and compliant robot hands has recently attracted the interest of the robotics community. The under-actuated mechanisms and the structural compliance used in these hands facilitate and robustify not only grasping but also the execution of dexterous, in-hand manipulation tasks. Another significant characteristic of the particular hands is that they are able to efficiently grasp a wide range of everyday life objects even under significant object pose uncertainties. However, these hands, are difficult to model due to kinematic constraints introduced by the underactuation and the use of complex flexure joints. Moreover, adaptive hands tend to reconfigure upon contact with the object surface, imposing certain parasitic object motions. In this paper, we propose a learning scheme that uses the contact force measurements collected from tactile sensors to estimate the post-contact reconfiguration of the hand-object system and the imposed parasitic object motion. The learning scheme's estimates are compared with \u201cground truth\u201d data that describe the actual motion of the object and that are collected using a vision based motion capture system. The proposed learning scheme can be used with any type of adaptive robot hand and its efficiency is experimentally validated using extensive paradigms involving different hand designs and various everyday life objects.","inCitations":[],"pmid":"","title":"Learning the post-contact reconfiguration of the hand object system for adaptive grasping mechanisms","journalPages":"293-299","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202171","https://www.eng.yale.edu/grablab/pubs/Liarokapis_IROS2017.pdf","https://minasliarokapis.com/2017_IROS_LearningPostContactReconfigurationAdaptiveHands.pdf"],"entities":["Constrained optimization","Experiment","Ground truth","Machine learning","Mathematical optimization","Motion capture","Random forest","Robot","Robotics","Robustification","Synergy","Tactile sensor","Underactuation"],"journalVolume":"","outCitations":["c52d2e62acca8f603b88fd4a70a22acf42de1462","bcefcbe4ece6dfed3cd172d941ccaf1d279bf2b8","b4f15cdae2cabaf2ae9c1bf4c9c53073ef9ba758","27e124d91b5a7c94f43a5637f47b72b1075d953f","aab3ef7e6cd102fab813739b9978d1feda675d46","0c935f3d33e39897d3311faaa0c8a5e0c83c5548","9c8279d5e85ae2adcb52d76efb37f234664373b3","1af13d4d2bccd1e17a1fcfef4bc5713e613d1e2e","aad63668f6622fb0bad31b84f2452af3e3661167","24c8e52919b284deb939e1fa51ca04741e106063","0c286e6867b9341e06571d03de7aadaf0225bc10","13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986","6159347706d25b51ae35e030e818625a1dbbc09d","3af10f8e2da64e89f8a1997661a662aed4c62182","dc22d08a65286949f2834618e83d88bbd9f303c0","2d90537b709ae92e07856b93aab70614ac5af561","a77372982b29dbea6e09c8d92093e8153960b1c9","8bc81aa91dad3658fa2b9359a47add148b423aac","39379e3793b8fe242d23bd55e33aa2c2917d4de2","4214ca9a5fa712ca0319ad731c882aa503cad6b8","8dd39a6cc232e69a91ff32d60c6c0df750a51c0e","7ad3d124088965aad9a8d815b728b2ecb86fdbb0","cfc6c3d68f36ec767cbb2fb2b0e7ad3c462dab46","9fb6ea4c66ca39f66a01986b939c7daf19dcae50","510cce09ac2553c6cafdbf10deccba894f6d8721","b214ab56da307365dfea6cc96f05dfcba0b07669","42714ca6c17a6759fae41b17e3b7798b14595664","9993aed9d282494f7e2ea7abca5896878ed61a9d","055f18bf5e906a66fa63154021bd4741b47fee53","44c915caf721b35dc436324aa98aef78ac878c8c","351e67194b9bd47321fb41cd31aaf330f72c7ea4","e25f8585c8fa52d78887794e48c639ecde31d5b6","b6f85a3ce8739e328510e5d0080b5c85bcf54995","0d39812dbf19de44a33f2caac7bec8c0f79074fb"],"id":"a62875052985c433708c715e06aa76b80c10bafa","s2Url":"https://semanticscholar.org/paper/a62875052985c433708c715e06aa76b80c10bafa","authors":[{"name":"Minas V. Liarokapis","ids":["2646612"]},{"name":"Aaron M. Dollar","ids":["1797110"]}],"doi":"10.1109/IROS.2017.8202171"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00105","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Facial micro-expression (ME) recognition has posed a huge challenge to researchers for its subtlety in motion and limited databases. Recently, handcrafted techniques have achieved superior performance in micro-expression recognition but at the cost of domain specificity and cumbersome parametric tunings. In this paper, we propose an Enriched Long-term Recurrent Convolutional Network (ELRCN) that first encodes each micro-expression frame into a feature vector through CNN module(s), then predicts the micro-expression by passing the feature vector through a Long Short-term Memory (LSTM) module. The framework contains 2 different network variants: (1) Channel-wise stacking of input data for spatial enrichment, (2) Feature-wise stacking of features for temporal enrichment. We demonstrate that the proposed approach is able to achieve reasonably good performance, without data augmentation. In addition, we also present ablation studies conducted on the framework and visualizations of what CNN \"sees\" when predicting the micro-expression classes.","inCitations":["c2148f81ffffeaff3fed49448fa5485f65917865","cb13e29fb8af6cfca568c6dc523da04d1db1fff5"],"pmid":"","title":"Enriched Long-Term Recurrent Convolutional Network for Facial Micro-Expression Recognition","journalPages":"667-674","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1805.08417v1.pdf","http://pesona.mmu.edu.my/~johnsee/research/papers/files/elrcn-megc-fg18.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2018.00105","http://arxiv.org/abs/1805.08417"],"entities":["Convolutional neural network","Database","Feature vector","Gene Ontology Term Enrichment","Long short-term memory","Optical flow","Pixel","Preprocessor","Sensitivity and specificity","Stacking","Test engineer","The Grid Analysis and Display System (GrADS)"],"journalVolume":"","outCitations":["2ea6a93199c9227fa0c1c7de13725f918c9be3a4","63e7e389efd0a26f6431624d16e82d290960dbb9","33fad977a6b317cfd6ecd43d978687e0df8a7338","5582bebed97947a41e3ddd9bd1f284b73f1648c2","a5a5df091332de6785bf2ab7c3b6fa05f4fb89f4","099053f2cbfa06c0141371b9f34e26970e316426","370b5757a5379b15e30d619e4d3fb9e8e13f3256","725d7cd107c69ebefbf53e5daba61f0601dc2f66","85093e191579fa1176c53b88ed651ab18f3dbf7c","31da84afa4e1d3d02ec334c989d2792d9719e5f3","3e8355b608099a0d5564b6a6d8ecd8b505007251","0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c","330a47b31c7cb16e0e582b2e1bd0bd5f0cb8d849","29cb1e90d1e08f0990925c8d4e8d00fa5fa49100","3e2b9ffeb708b4362ebfad95fa7bb0101db1579d","3e58d9800aa31e5db89d99dcd33e5786b7837bfd","2950ecd5b9320c254ae0647e672ea9f51ea16de1","71b6cfd6b65f88fc01539a77109d768a6c32b959","21dff32ff0f6b1244ce9c4aad2f468889748fbb2","24115d209e0733e319e39badc5411bbfd82c5133","8ba608cea5af13484f2ade6a482b6bca04ffb759","d418e9929a43ac6b4cde3adba0984213103d43e3","14318685b5959b51d0f1e3db34643eb2855dc6d9","0c54e9ac43d2d3bab1543c43ee137fc47b77276e","0e78af9bd0f9a0ce4ceb5f09f24bc4e4823bd698","11540131eae85b2e11d53df7f1360eeb6476e7f4","ebb1c29145d31c4afa3c9be7f023155832776cd3","272216c1f097706721096669d85b2843c23fa77d","3beb6b76acb0485d28e8e02f7cfbcc407560708e","162ea969d1929ed180cc6de9f0bf116993ff6e06","b52886610eda6265a2c1aaf04ce209c047432b6d","044fdb693a8d96a61a9b2622dd1737ce8e5ff4fa","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","2c03df8b48bf3fa39054345bafabfeff15bfd11d","fea282aa4f2437b8cc81322cf4e20e240bb7884b","318e7e6daa0a799c83a9fdf7dd6bc0b3e89ab24a","061356704ec86334dbbc073985375fe13cd39088"],"id":"69c2b7565e080740e2bdb664e6b00fd760609889","s2Url":"https://semanticscholar.org/paper/69c2b7565e080740e2bdb664e6b00fd760609889","authors":[{"name":"Huai-Qian Khor","ids":["30470673"]},{"name":"John See","ids":["2339975"]},{"name":"Raphael C.-W. Phan","ids":["6633183"]},{"name":"Weiyao Lin","ids":["8131625"]}],"doi":"10.1109/FG.2018.00105"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594163","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Fast and accurate visual search is an enabler for many applications of drones. Prior works use POMDPs to produce effective search strategies. As the observation models are from heuristics, the robustness of these approaches on the field is unclear. This work builds a testbed that combines latest developments in related areas, including mobile CNNs for inference on mobile platforms and policy search with point based methods, in a POMDP framework. A dataset for a simple but realistic application, search for a single basketball, is collected to train the perception modules, investigate their error characteristics and validate the control algorithm. From simulation using realistic parameters, we found the significant role persistent factors in the environment can play in designing a fast search strategy. Failure to taking these factors into account results in up to 60% longer search time at the same success rate. Our empirical tests using mobile CNN and real data reveals that prior assumptions on error rates as functions of heights are wrong. The errors grows non-linearly, and there is significant between false positive and false negative rates. Our findings shed new lights on what to consider in designing visual search strategies in a drone platform and is one step towards a fast and robust algorithm.","inCitations":[],"pmid":"","title":"Target Localization with Drones using Mobile CNNs","journalPages":"2566-2573","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594163"],"entities":[],"journalVolume":"","outCitations":[],"id":"15d42a13188664e833829b77f0bfdab617a80fd4","s2Url":"https://semanticscholar.org/paper/15d42a13188664e833829b77f0bfdab617a80fd4","authors":[{"name":"Yongxi Lu","ids":[]},{"name":"Zeyangyi Wang","ids":[]},{"name":"Ziyao Tang","ids":[]},{"name":"Tara Javidi","ids":[]}],"doi":"10.1109/IROS.2018.8594163"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206254","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The developments of robotics inform research across a broad range of disciplines. In this paper, we will study and compare two collision selective neuron models via a vision-based autonomous micro robot. In the locusts' visual brain, two Lobula Giant Movement Detectors (LGMDs), i.e. LGMD1 and LGMD2, have been identified as looming sensitive neurons responding to rapidly expanding objects, yet with different collision selectivity. Both neurons have been modeled and successfully applied in robotic vision system for perceiving potential collisions in an efficient and reliable manner. In this research, we conduct binocular neuronal models, for the first time combining the functionalities of LGMD1 and LGMD2 neurons, in the visual modality of a ground mobile robot. The results of systematic on-line experiments demonstrated three contributions of this research: (1) The arena tests involving multiple robots verified the effectiveness and robustness of a reactive motion control strategy via integrating a bilateral pair of LGMD1 and LGMD2 models for collision detection in dynamic scenarios. (2) We pinpointed the different collision selectivity between LGMD1 and LGMD2 neuron models, which fulfill corresponding biological research. (3) The utilized micro robot may also benefit researches on other embedded vision systems as well as swarm robotics.","inCitations":["1ce223d1e366f1c8323570796258bad523261235","7626985a73300f7fb55d3def02209b9e61f5f2c9"],"pmid":"","title":"Collision selective LGMDs neuron models research benefits from a vision-based autonomous micro robot","journalPages":"3996-4002","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206254"],"entities":["Autonomous robot","Bilateral filter","Binocular vision","Collision detection","Control theory","Embedded system","Experiment","Microbotics","Mobile robot","Modality (human\u2013computer interaction)","Neuron","Online and offline","Selectivity (electronic)","Sensor","Swarm robotics"],"journalVolume":"","outCitations":["2aa16a7506d2590933d66643769df6de6387feef","38394c885f179ba84eecf4c426fd523a0868eafb","7254ef1642e181ca974ba824155f711926c36197","bafd604070dc11b976aa8661ac28030137192101","e006db181f0b5a0417e1faf4966742486b98ad5d","7d4a7376ddd2b47dbb96a3d6aa44c7b255bdbc85","e09771803eb1a7aa9885679573a4ae850bab36e3","630c4854da799ea4caa6088e9fb45e84ce67764d","46e06c64a01b7f339ec34b54424890c3308abe2b","06385ff46014cd2160aba2435c089abed2207a7a","12613a630ed549de2295ee5f56a9da56acb49de4","f04c88dfeee75ab19909062749c40d9593a0af5d","105bfcc08fc6412476cc6e14192d3d1e5d9af40b","aa28ce9b620945596bf495ed27cccdf60011e25c","59835ae01c6ce26576f58ffd9719d228de07c339","df5d086193e79b86271dce7d04ae0109fa1e9008","a7abb57f37d8b385303d838dbd99141009873ecb","15a0b5ad64ef55d566213e9af57452651b6e5c6f","cea0cee0ed89b88a035dc3f7d052207b206857b1","8417e4f7bcc679c9fef58df167051cdee09494f2","31dd54805df07e34baffcbe13aae65a1b74be683","9e6a8157ee18cc7f58c787e872d4c965269eae38","4bf44cc1870850768ff790357df43fdefbd2a7fd"],"id":"0eb439f0ea2ec5e51246a521592cbaf9a5b5da35","s2Url":"https://semanticscholar.org/paper/0eb439f0ea2ec5e51246a521592cbaf9a5b5da35","authors":[{"name":"Qinbing Fu","ids":["2232829"]},{"name":"Cheng Hu","ids":["46622449"]},{"name":"Tian Liu","ids":["50021659"]},{"name":"Shigang Yue","ids":["1740378"]}],"doi":"10.1109/IROS.2017.8206254"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206602","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Mission planning and execution for autonomous vehicles is crucial for their effective and efficient operation during scientific exploration, or search and rescue missions, to mention a few. Automated Planning has shown to be a useful tool for \u201chigh level\u201d mission planning, that is, allocating tasks to vehicles while following given constraints (e.g., energy, collision avoidance). In this paper, we focus on making mission planning flexible and robust. That is, a human mission coordinator can modify tasks during the mission execution, so the tasks have to be dynamically reallocated during the process. Moreover, we assume that communication might not be reliable when vehicles are \u201coutside\u201d, i.e., performing the tasks, and thus we enforce vehicles to come back to their safe spots regularly. To address these requirements, we have developed two models, namely \u201call tasks\u201d and \u201cone round\u201d, and integrated them to the control software. We have evaluated our approach in a field experiment focused on a mine-hunting scenario.","inCitations":[],"pmid":"","title":"Mixed-initiative planning, replanning and execution: From concept to field testing using AUV fleets","journalPages":"6825-6830","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206602"],"entities":["Automated planning and scheduling","Autonomous robot","Control system","Experiment","High-level programming language","List of version control software","Requirement"],"journalVolume":"","outCitations":["4a8ae9029db8a351a35fca867809a70f7d1856ba","ee23b33287b4ca847ce20f56f77dc516cea7d579","423a2fc7ac0958a7d4533e20b3a1325ef9df02e6","06cbe2df989877a79eb5a51c82e561b41167da15","1a70708b4f03231bf8a795a03a207f487efcd40c"],"id":"4b0eee6e88e970694c7b2c640da04341840f3946","s2Url":"https://semanticscholar.org/paper/4b0eee6e88e970694c7b2c640da04341840f3946","authors":[{"name":"Lukás Chrpa","ids":["2467098"]},{"name":"José Pinto","ids":["14143548"]},{"name":"Tiago Sa Marques","ids":["37834126"]},{"name":"Manuel A. Ribeiro","ids":["34396902"]},{"name":"João Borges de Sousa","ids":["3072084"]}],"doi":"10.1109/IROS.2017.8206602"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546159","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Body height, weight, as well as the associated and composite body mass index (BMI) are human attributes of pertinence due to their use in a number of applications including surveillance, re-identification, image retrieval systems, as well as healthcare. Previous work on automated estimation of height, weight and BMI has predominantly focused on 2D and 3D full-body images and videos. Little attention has been given to the use of face for estimating such traits. Motivated by the above, we here explore the possibility of estimating height, weight and BMI from single-shot facial images by proposing a regression method based on the 50-layers ResNet-architecture. In addition, we present a novel dataset consisting of 1026 subjects and show results, which suggest that facial images contain discriminatory information pertaining to height, weight and BMI, comparable to that of body-images and videos. Finally, we perform a gender-based analysis of the prediction of height, weight and BMI.","inCitations":[],"pmid":"","title":"Show me your face and I will tell you your height, weight and body mass index","journalPages":"3555-3560","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546159"],"entities":[],"journalVolume":"","outCitations":["6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","1d226ef08fecc001d0d53504b896fcdf9565daca","0720e61771a298235806e139a7534108250895c6","cbf3b7cdd8e3d8517fb1ca10288d4b347c8c63c5","6ae120df3660fc0f33674859495245f7e7335819","89e1e3ef566b3be026c0af3df895c0ae988939c4","12e2934b9e5d5b2336e854bda717434b52e3f8dd","1795e4f14f8bd98120a3507c24aee131db40a40f","491d42f3df46dd4cb9e2bf3f71f39f5313520d82","a33fb9db60253e312196cfd4dff0dd6c0867be96","55bc7abcef8266d76667896bbc652d081d00f797","839ae42eea07d8e34957178622620c31a2211881","03264fc373634b916e089be8ed78b81cdd67d456","11dad1520eafbf524677e582a1bcc6b9c0071ea9","85ae6fa48e07857e17ac4bd48fb804785483e268","6e12ba518816cbc2d987200c461dc907fd19f533","be7e4224eda13659170657ac4ce4407013db9596","67546c8456d7f2a1c381d2aa5a1eb0b83358f631","3b2697d76f035304bfeb57f6a682224c87645065","8b13de4030c52bd84252fddc2a2d93c1751059fe","8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486","f14aeff5ee0ec017e02591f924b502c57b528160","a534f41608ee0157ff002a0d5e5d39deb0d34797","8b8728edc536020bc4871dc66b26a191f6658f7c","358ea4bf2c2cebb75358e11c723c91d6a18e6c14","359ae2261d4a478e5d79e95a01a36054badb0b1b","1a53ca294bbe5923c46a339955e8207907e9c8c6","722fcc35def20cfcca3ada76c8dd7a585d6de386","2c03df8b48bf3fa39054345bafabfeff15bfd11d","6020a3665c50ea1477c0a49a97ef9751282e1251","81bf1bb41debef7f62d2a78d31b539b2218f356e","7df4f96138a4e23492ea96cf921794fc5287ba72","3e3153d38210832d8201e96ff6a4d7133e004ea3","c5de2570ee8980669252ab19341f39463b6a8188"],"id":"b1413b4ff88f33f3d14e30f6e9e206bd3f5d8fd3","s2Url":"https://semanticscholar.org/paper/b1413b4ff88f33f3d14e30f6e9e206bd3f5d8fd3","authors":[{"name":"Antitza Dantcheva","ids":["3299530"]},{"name":"Francois Bremond","ids":["40267845"]},{"name":"Piotr Tadeusz Bilinski","ids":["31474374"]}],"doi":"10.1109/ICPR.2018.8546159"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594482","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Deployment of robotics and remote systems for tasks in unstructured nuclear environment has been impeded by the severe task requirements such as high radiation and dexterous and complex manipulation of heavy materials, which cannot be addressed by the current telerobotics technology. To address such practical challenges, this paper presents an enhanced teleoperator interface incorporating multi-modal augmented reality, and new method of telerobotic operation based on perceptual overlay - \u2018virtual fixtures\u2019. Rather than trying to devise complex robotic systems, innovation is directed to enhancement of teleoperator interface so as to draw more performance and intuition from the human operator. Particular enhancements were made over the current technology basis in 3D sensing and reconstruction, virtual fixture generation, and operator interface. The telerobotic system was developed using ROS (Robot Operating System) to streamline system integration and resource sharing. The presented innovation is expected to allow deployment of simple and rugged robots to perform dexterous manipulation of heavy objects.","inCitations":[],"pmid":"","title":"Implementation of Augmented Teleoperation System Based on Robot Operating System (ROS)","journalPages":"5497-5502","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594482"],"entities":[],"journalVolume":"","outCitations":[],"id":"87c42a207c328239761836f35675e3894b20563a","s2Url":"https://semanticscholar.org/paper/87c42a207c328239761836f35675e3894b20563a","authors":[{"name":"Donghyeon Lee","ids":[]},{"name":"Young Soo Park","ids":[]}],"doi":"10.1109/IROS.2018.8594482"}
{"doiUrl":"","venue":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","journalName":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","sources":[],"year":2017,"text":"In this paper, a new content-based image retrieval (CBIR) scheme is proposed in neutrosophic (NS) domain. For this task, RGB images are first transformed to three subsets in NS domain and then segmented. For each segment of an image, color features including dominant color discribtor (DCD), histogram and statistic components are extracted. Wavelet features are also extracted as texture features from the whole image. All extracted features from either segmented image or the whole image are combined to create a feature vector. Feature vectors are presented for ant colony optimization (ACO) feature selection which selects the most relevant features. Selected features are used for final retrieval process. Proposed CBIR scheme is evaluated on Corel image dataset. Experimental results show that the proposed method outperforms our prior method (with the same feature vector and feature selection method) by 2% and 1% with respect to precision and recall, respectively. Also, the proposed method achieves the improvement of 13% and 2% in precision and recall, respectively, in comparison with prior methods.","inCitations":["ce1e915597548eb4c84764af5fe72a9ef6d1744a","151828b8197b4c2ca856915e790cd3c8beff0bea","b60289b7a77d68dc00134bd2dd1b073d7167870f","6e18e88ba41d1cc7e047a4793614b82ff060f8fc"],"pmid":"","title":"Content-based image retrieval with color and texture features in neutrosophic domain","journalPages":"50-55","s2PdfUrl":"","pdfUrls":["http://vixra.org/pdf/1711.0097v1.pdf"],"entities":["Ant colony optimization algorithms","Categories","Content-based image retrieval","Domain model","Extraction","Feature selection","Feature vector","Greater","Mathematical optimization","Physical object","Precision and recall","Silo (dataset)","Wavelet","Weight"],"journalVolume":"","outCitations":["efadbda38d942ba43b72b17f4ac7de550ad785f7","544a3a8b60c02f403fe154ae38ed27358df45e73","517b80bb9ff089611572cc2d1929927858118421","04e2e60534316a3d00943c3289a757137917df63","d6125dfd3438c6931ed6ae8f6b3a512173a977c0","68e2f4012844fac4c75f51e18413e736dd53c1db","0ac84c76ef3915c0da58046f688a28c3850261df","32ed41af723c99602257bbca888bd5d522a3c461","172a5380e175d357fbee5d16a86c9b98ef3f54de","caaea0597bc987f2ef931f1cbf85e8e4a34b3a2b","3f0bc54da893207fb6114cc336658755aaed988a","a87944fdf3a0f32f3385d08ce10ccaa01b929f9d","5912dc5f84990032bb5fca6720ec228016e7983d","5dc7d0635822dfd43a041e2751b287f6dff5a987","ce921102cd002e1d85a4250101be79d0beb4725f","75d0f2a4a2dff5cce4eb4097c2e68e1ff75ae569","4471e94d1341c55aaaf55b6f9a91722be10003ab","894c1af3356b7f0e41b67832fd73d3e9ab7a00a2","532d391d522fe1533977f32775fc7e61a3bc313c","5d7538c8f02f91ec8293a94685bc3d2fb2be033a","0f0500b8e27037b1f6959a8749cf2f083eb950cc","0e4d18f396a9d68a9505a1fc7f7b70e1009fc491","6dc1616f6c3b721bb525e53a1305a3aa03fa841c","70e5556b72340708e82f139245e9cec76bca02a5","50144057cba3b1758462de85d6b556b9613dcf80","3dd76739c8387da1a4bea67f3997173bda7cb391"],"id":"1adb82895968186537de791b19e97e95cd5abb59","s2Url":"https://semanticscholar.org/paper/1adb82895968186537de791b19e97e95cd5abb59","authors":[{"name":"Abdolreza Rashno","ids":["3406858"]},{"name":"Saeed Sadri","ids":["8129026"]}],"doi":""}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206187","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Most Unmanned Aerial Vehicle (UAV) controllers require linear velocities as input. An effective method to obtain linear velocity is to place a downward facing camera and to estimate the velocity from the optical flow. However, this technique fails in outdoor environments when the ground is covered with grass or other objects which move due to winds such as those caused by the propellers. We present a novel method to estimate the linear velocities from stereo images even in the presence of disorderly motion of image features. We validate the approach using imagery obtained from a UAV flying through orchard rows.","inCitations":["1f5b0caec136778decf7689cb090cb1911c2263f"],"pmid":"","title":"Linear velocity from commotion motion","journalPages":"3467-3472","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206187"],"entities":["Aerial photography","Algorithm","Closed-loop transfer function","Commotion (animation)","Effective method","Ground truth","Motion estimation","Optical flow","Orchard","Stereo camera","Unmanned aerial vehicle","Velocity (software development)"],"journalVolume":"","outCitations":["6104128eb7c5e5097cce3baa2d183fa6c95c2a1a","51a8054b854f0f009014c599dc0ab4d05a6c54b4","36d6f343b5f375f547f216775673f55ac1a214a5","579e2e12c1e46e8636206eb5028ecf04bf5c205c","151c1fbfcc6609dd2f75b5e1eeba6ed3d2d72811","9be8547a9a84f1bb221c9ebbd81004579fdaa390","83d36ce6febe198018263c53f0239a3494d6f874","a56ed0bc2d69094e351a044ae8bc64ca0da691f8","1c8bbb289447958d690f0ee4bfe9a355e20fa3c1","0a202f1dfc6991a6a204eaa5e6b46d6223a4d98a"],"id":"688fcce7a38095b7ec8678f55e2bbfada62ed5c1","s2Url":"https://semanticscholar.org/paper/688fcce7a38095b7ec8678f55e2bbfada62ed5c1","authors":[{"name":"Wenbo Dong","ids":["50702793"]},{"name":"Volkan Isler","ids":["1698835"]}],"doi":"10.1109/IROS.2017.8206187"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545367","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Intrinsic Image Decomposition (IID) refers to recovering the albedo and shading from images, and it plays important roles in addressing computer vision tasks such as illumination-invariant object recognition and image recoloring. IID is an ill-posed problem and lacks of actual labelled samples for learning. This paper presents a deep neural network (DNN) based method to address this problem. To facilitate the training of DNN, we synthesize an intrinsic image dataset through rendering $\\pmb 3\\mathbf{D}$ models. To make the learnt model well generalize to realworld images with better visual results, we employ the perceptual loss in model learning. The perceptual loss is constructed upon the activations of a neural network pre-trained on real images, such as the VGG network. Such a loss function implicitly introduces knowledge from real-world images and has the ability of multi-level semantic understanding on the decomposed results. Experimental results show that our model trained on synthetic single-object dataset can produce well decomposition results not only on synthetic images but also on real-world scene-level images (containing multiple objects).","inCitations":[],"pmid":"","title":"Learning Intrinsic Image Decomposition by Deep Neural Network with Perceptual Loss","journalPages":"91-96","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545367"],"entities":["Artificial neural network","Biological Neural Networks","Body Dysmorphic Disorders","Computer vision","Deep learning","Loss function","Neural Network Simulation","Numerous","Outline of object recognition","Physical object","Sensorineural Hearing Loss (disorder)","Shading","Silo (dataset)","Synthetic intelligence","Well-posed problem","hearing impairment"],"journalVolume":"","outCitations":["92e48e15e43d7d96e7678e70171932e1a6a003d2","3fe41cfefb187e83ad0a5417ab01c212f318577f","07eb6f0bbc47008ff07dc2d8f2ff2aa21bea0ccb","0f84a81f431b18a78bd97f59ed4b9d8eda390970","8ca53d187f6beb3d1e4fb0d1b68544d578c86c53","77c512cbb832436e1a35ad434e6bb3d763799763","06feba1ffd596b41884cea6e8ef0da89b6dd2233","915c4bb289b3642489e904c65a47fa56efb60658","676dfbda07d006c46fa621e563f1c64370204c48","f98f2c1cc6e2450adf948aa04feb97da5f1d351b","fb528dcfa355779aff996cd1f0ef9d96f918080f","15d3d368a93392841a0a057b7571e7eeadad2061","3ba179bceb9692d4d21109d0b87b120195761148","b13b40d94e06723401963342dd67e104132fe489","2aa5aaddbb367e477ba3bed67ce780f30e055279","c6faf7f3f25857bcdc14fc6b7fc51874b94505eb","2327c389267019508bbb15788b5caa7bdda30998","061356704ec86334dbbc073985375fe13cd39088"],"id":"d6ae5ea0bbdafb5133f344ffc1c32987cf18a4f6","s2Url":"https://semanticscholar.org/paper/d6ae5ea0bbdafb5133f344ffc1c32987cf18a4f6","authors":[{"name":"Guangyun Han","ids":["40997198"]},{"name":"Xiaohua Xie","ids":["2002129"]},{"name":"Wei-Shi Zheng","ids":["3333315"]},{"name":"Jianhuang Lai","ids":["1750721"]}],"doi":"10.1109/ICPR.2018.8545367"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202246","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents findings from two deployments of an autonomous mobile robot in older adult low income Supportive Apartment Living (SAL) facilities. Design guidelines for the robot hardware and software were based on query of clinicians, caregivers and older adults through focus groups, member checks and surveys, to identify what each group believed to be the most important daily activities for older adults to accomplish physically, mentally and socially. After data analysis, hydration and walking encouragement were found to be critical daily activities, becoming the focus of our deployments. The aim of the deployments was to understand the efficacy of human-robot interaction and identify ways to enhance the robot design and programming. Through observation of older adults interacting with the robot and post-interaction surveys filled out by the older adults, conclusions were drawn for further advancement of the robot development to be tested in future deployments. Results overall indicated high perceived usefulness and growing acceptance of the robot by older adults with increased interactions.","inCitations":["15606c2ca01bf755f2f9130b1dc7313af11c496b"],"pmid":"","title":"Evaluating older adults' interaction with a mobile assistive robot","journalPages":"840-847","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202246","http://www.modlabupenn.org/wp-content/uploads/2017/08/Mucchiani_IROS2017.pdf"],"entities":["Assistive technology","Autonomous robot","Focus group","Human\u2013robot interaction","Mobile robot"],"journalVolume":"","outCitations":["0284df7abdd404e3da3ba16bb60314c738a69e5e","afe2c8927a2477986c3c2bdb6f369093afe76a9d","d3cd8037af465307e092ef7328b4e93068512f63","7c718e4280e51cfa2f4717f09e20091b43826ad9","0b359816f15e2d4d31de59a0fd23182c5ffd28c5","8de3299f91bc07d68c546d3a578b05a70a48153e","4a85b7fa5e81ae4cefc963c897ed6cf734a15fbc","f227a6379ec4da8922c8b014c87411660f00e3e7","2327027337447bbbb85388bbe9d29c5a39fc81f4","2a0f2076663bc10f2291a1326bf899ff000f638f","30f67b7275cec21a94be945dfe4beff08c7e004a","7a871d2b91cc6f54f2f6e8fcce6222b04fc222ea","3e9bd2ece97b74905a93de395df66c1c91a3405d","4b4cd2373b783f6312a8238c75e99752e3975b53","63cb280a841f1e105d48479025c196b726f83c28","2c52e0d84c595ff31c3bd93ef41711b7e58f3282","722233f423dcb61830f1b29e68422d8fe7ac2ce6","53e64baebb35fb632d6ceffe0330d79656186d52","5a9a17e26facb80a4126caba8a88c10ba2cd15c8","c6f6d0dc0f5c8ede86adf1ede38355c90606bffe","db1070139b61c65988f8247ed705fd19c3ddd6f5"],"id":"5c8363fec05253788750bebc6954c211c6216bbf","s2Url":"https://semanticscholar.org/paper/5c8363fec05253788750bebc6954c211c6216bbf","authors":[{"name":"Caio Mucchiani","ids":["25084298"]},{"name":"Suneet Sharma","ids":["47192273"]},{"name":"Megan Johnson","ids":["46878137"]},{"name":"Justine Sefcik","ids":["35002906"]},{"name":"Nicholas Vivio","ids":["7830877"]},{"name":"Justin Huang","ids":["31489711"]},{"name":"Pamela Z Cacchione","ids":["3941121"]},{"name":"Michelle J. Johnson","ids":["4292339"]},{"name":"Roshan Rai","ids":["34270329"]},{"name":"Adrian Canoso","ids":["32352789"]},{"name":"Tessa A. Lau","ids":["1800706"]},{"name":"Mark Yim","ids":["1715986"]}],"doi":"10.1109/IROS.2017.8202246"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594466","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper investigates the straight path following problem for a class of underactuated bio-inspired snake robots on ground with unknown and varied friction coefficients. Existing works usually design control input requiring the exact values of these friction coefficients, which however rely on the specific operating terrain and may not always be known a priori. By virtue of backstepping technique, we present a novel adaptive controller that can compensate for unknown and varied friction coefficients in real-time. Moreover, it is proved via LaSalle-Yoshizawa theorem that the path following errors converge to zero asymptotically and all the parameter estimates are bounded. Simulations and experiments on an 8-link snake robot are carried out to illustrate the effectiveness of the proposed controller.","inCitations":[],"pmid":"","title":"Adaptive Path Following of Snake Robot on Ground with Unknown and Varied Friction Coefficients","journalPages":"7583-7588","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594466"],"entities":[],"journalVolume":"","outCitations":[],"id":"1005438a045397b53b1c17650f626d0f5f3ba3f5","s2Url":"https://semanticscholar.org/paper/1005438a045397b53b1c17650f626d0f5f3ba3f5","authors":[{"name":"Gang Wang","ids":[]},{"name":"Weixin Yang","ids":[]},{"name":"Yantao Shen","ids":[]},{"name":"Haiyan Shao","ids":[]}],"doi":"10.1109/IROS.2018.8594466"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594399","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Many mobile robots rely on 2D laser scanners for localization, mapping, and navigation. However, those sensors are unable to correctly provide distance to obstacles such as glass panels and tables whose actual occupancy is invisible at the height the sensor is measuring. In this work, instead of estimating the distance to obstacles from richer sensor readings such as 3D lasers or RGBD sensors, we present a method to estimate the distance directly from raw 2D laser data. To learn a mapping from raw 2D laser distances to obstacle distances we frame the problem as a learning task and train a neural network formed as an autoencoder. A novel configuration of network hyperparameters is proposed for the task at hand and is quantitatively validated on a test set. Finally, we qualitatively demonstrate in real time on a Care-O-bot 4 that the trained network can successfully infer obstacle distances from partial 2D laser readings.","inCitations":["54d33780af477f718792f615c6ec3c94116c9fba"],"pmid":"","title":"Hallucinating Robots: Inferring Obstacle Distances from Partial Laser Measurements","journalPages":"4781-4787","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594399","https://arxiv.org/pdf/1805.12338v2.pdf","http://arxiv.org/abs/1805.12338","http://export.arxiv.org/pdf/1805.12338"],"entities":[],"journalVolume":"","outCitations":["7884e4595efd5e0df579c5a90be4c98833483512","d40b2e732f5e93e43e909fe43674e735302ce9b3","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","049fc0afdf8fa9ad747657b1aaf9d8c334786c3f","dcb9623a33c4dbcab80b64a97f9690d826afb425","18168aea48a22f6fe2fe407c0ff70083cba225a7","93769ed9b19f2f4e2560728d485fe2eff91885f5","5256fd103b53491184584a431f0a9995cb761e8c","f85f69c8ba0852897c4b037af0a37152de204e34","2ab131680c1d0d0b49e1ddb947ceb2d162dbffa9","15961dd6fcec17390eff133e11c83279654b5675","a5d93765b5a63328059a009b0d6dfbbb0c974469","0f2f4edb7599de34c97f680cf356943e57088345","3e298fddfd7700dcbeb168c7eccbc0242375991e","3ba179bceb9692d4d21109d0b87b120195761148","19c5205efc7409eafbe80d12ad9eed3be1c393d8","272216c1f097706721096669d85b2843c23fa77d","fbc6ab911a0c9563724922db0932a58fbc8710b5"],"id":"b5bbd0552ea9c8961dc05832897fe867d3c80bd9","s2Url":"https://semanticscholar.org/paper/b5bbd0552ea9c8961dc05832897fe867d3c80bd9","authors":[{"name":"Jens Lundell","ids":["34127709"]},{"name":"Francesco Verdoja","ids":["2014967"]},{"name":"Ville Kyrki","ids":["2717428"]}],"doi":"10.1109/IROS.2018.8594399"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594212","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Soft actuators with auxetic, or negative Poisson's ratio (NPR), behavior offer a way to create soft robots with novel kinematic behavior. This paper presents an original framework for reinforcement of a soft actuator using a generalized NPR element, called a Representative Auxetic Element (RAE), and an experimental validation of the kinematic behavior that it enables. We build a generalized kinematic model that enables the design of RAE-patterned actuators and reveal the distinct auxetic behavior of RAE actuators with comparable model accuracy to the legacy McKibben actuators. A simple, reproducible way of designing and fabricating RAE actuators is described and varied prototypes are shown. This RAE-based design scheme can be used to create actuators with specified kinematics like bending, extension, and radial expansion, which can also vary across the actuator's surface both circumferentially and axially in a tractable, scalable manner.","inCitations":[],"pmid":"","title":"Auxetic Sleeves for Soft Actuators with Kinematically Varied Surfaces","journalPages":"464-471","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594212"],"entities":[],"journalVolume":"","outCitations":[],"id":"e6bfe7c3840fe6eb6e3a4b424b0332a57f5d7c08","s2Url":"https://semanticscholar.org/paper/e6bfe7c3840fe6eb6e3a4b424b0332a57f5d7c08","authors":[{"name":"Audrey Sedal","ids":[]},{"name":"Michael W. Fisher","ids":[]},{"name":"Joshua Bishop-Moser","ids":[]},{"name":"Alan Wineman","ids":[]},{"name":"Sridhar Kota","ids":[]}],"doi":"10.1109/IROS.2018.8594212"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206107","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In human-robot collaboration the robot's behavior impacts the worker's safety, comfort and acceptance of the robotic system. In this paper we address the problem of how to improve the worker's posture during human-robot collaboration. Using postural assessment techniques, and a personalized human kinematic model, we optimize the model body posture to fulfill a task while avoiding uncomfortable or unsafe postures. We then derive a robotic behavior that leads the worker towards that improved posture. We validate our approach in an experiment involving a joint task with 39 human subjects and a Baxter torso-humanoid robot.","inCitations":["142687714cb17445f97c6e65b0d7183efcd2bad5","56584ef3363edf4620801d884756ce07f5541b48","236153260f3b1b5e67a7c701a2d3a1e5c5f97a7d"],"pmid":"","title":"Postural optimization for an ergonomic human-robot interaction","journalPages":"2778-2785","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206107","http://www.inesc-id.pt/publications/13850/pdf/","https://hal.archives-ouvertes.fr/hal-01629426/file/iros2017_final.pdf"],"entities":["Baxter (robot)","Graphical user interface","Human factors and ergonomics","Humanoid robot","Human\u2013robot interaction","Mathematical optimization","Mobile robot","Personalization","Poor posture","Tracking system","Usability testing","Workspace"],"journalVolume":"","outCitations":["e652a6ebe980c6efe9129aeee12b686a17993263","aa57e2c92e589378ff2d960b694efc3b571cf6a4","f159316fcec136204c267b5c4476834578a8491a","da54c7197f2855919fa8a5c5e4f680f5c2634df0","1579460ddd60a53a35b58a7af4070220427edaf5","521cd3ad4ecc8760c07867c2444af42431fb72da","7fc89366619fdbbf14d096a0e3e77a6193ad09ba","eb57ede54724b220e2f2517056c1088cbe8daa38","6ef00566ddf71cd073d79f10487b9e607cebab67","2bca54f33d50442c372e7ee841ab5171f9abcd93","7b93c0e32698075620eca8aecb320058604e8105","f938a9e2f5b3dd676aaadf1f6102ee10fc97631a","4d9c5e4c3526227a9c59799c157f298bfb7daf93","d44773f5d9bcecd4bef1ca1c309d1d542732298c","8f9d96ca56f5e1e01ae6afb2f0ebf8dd0ed50fcb","a24dc292751f81e2be1d0b9ca5f9a9c16fb2a36e","8ffaa850d4de94d344cf1fc1825984ba4a943a54","d2d9cc9385ad344ff55616e337aa5cceb59ffc05","1c168c198d8d1128e378c9febaaa471fab744b7c","e0fa1fb0a6976839ab5c704d254443851e9d14a4","0d1eec44f210a21a43008863b02b7abe0da9d085","2c757c2620f868f00088e5d13b84e140edbd2ac2","079afa32b43894dbf4e045a45153d0d60aece160","f582c42843cc2bfb51e3eb38d7372ef3da24dbd3"],"id":"dba69db668142e789237ea67cffd04953b663a40","s2Url":"https://semanticscholar.org/paper/dba69db668142e789237ea67cffd04953b663a40","authors":[{"name":"Baptiste Busch","ids":["2004158"]},{"name":"Guilherme Maeda","ids":["1854335"]},{"name":"Yoan Mollard","ids":["2842181"]},{"name":"Marie Demangeat","ids":["48906640"]},{"name":"Manuel Lopes","ids":["49335487"]}],"doi":"10.1109/IROS.2017.8206107"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593824","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Using an Unmanned Aerial Systems (UAS) to autonomously deploy soil sensors enables their installation in otherwise hard to access locations. In this paper, we present a system that integrates a UAS and a digging mechanism which can carry, secure, and install a small sensor into dirt effectively and efficiently. The integrated system includes 1) a low profile, light-weight, inexpensive auger mechanism, 2) a sensor carrying and deploying mechanism with low power consumption, and 3) sensors and software that control and evaluate the auger performance during digging. When tested on a suite of target soils and a target depth of 120mm, the system achieved a success rate of 100% for indoor tests and 92.5% for outdoors, verifying the potential of the approach.","inCitations":["d3f1839a1055774b868b1d6014045845fa22c3e9"],"pmid":"","title":"Unmanned Aerial Auger for Underground Sensor Installation","journalPages":"1374-1381","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593824","http://cse.unl.edu/~carrick/papers/SunPNETD-2018IROS.pdf"],"entities":[],"journalVolume":"","outCitations":["88e81cbefa742300d369a87c89996627d4e1c388","86a6e23e9514433ca538c1c0efaff706a172bdfa","9364c99f98a9f9380468f8496b118c90ae7c3424","51810377a8748d421e9d9808373bba4eb75b8083","303f94382555681ef4a8d68fa40984b004cc57b5","3060b429294ac07026d8cd156a6ca106e71097d7","7b430daa520d320955a7c18ad302a350e0771033","393165eec94688d69cd43d2c931b92ae8f290a87","03027cf82c50b40ac180e5d68f14c68c2721ace9","24de8cdd827720700f071c73688c1b94efb68cb8","051f02f5a558fbd93ac6e649e4d6dc36f5255904"],"id":"156ba5c15b9ff15cefafb31c50ab840c325ad1a5","s2Url":"https://semanticscholar.org/paper/156ba5c15b9ff15cefafb31c50ab840c325ad1a5","authors":[{"name":"Yue Sun","ids":["4958137"]},{"name":"Adam Plowcha","ids":[]},{"name":"Mark Nail","ids":[]},{"name":"S. Elbaum","ids":["48113494"]},{"name":"Benjamin S. Terry","ids":["34865747"]},{"name":"Carrick Detweiler","ids":["1694389"]}],"doi":"10.1109/IROS.2018.8593824"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545565","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Despite biometrics is deemed a more secure and user-friendly solution than password-based or token-based approach for identity management, biometric templates are vulnerable to adversary attacks that may lead to privacy invasion and irreversible identity theft. Cancelable biometrics is a template protection method that generates a noninvertible identifier from the original biometric template by means of a parameterized transformation function and user/application-specific parameters. However, the necessity to input parameter, either in possession (token) or in memory (password) form along with biometrics, hence two factors, jeopardizes usability of the biometrics. In this paper, we propose a one-factor cancellable biometric authentication scheme that empowered by Indexing First Order hashing, a tailor-made locality sensitive hashing function for template protection. We evaluate the proposed scheme with respect to four template protection design criteria, namely noninvertible, renewability, unlinkability and accuracy performance. We also analyze the threat model of the proposed scheme that enclosed five major security attacks. Despite the scheme can be applied to any binary biometric features, we adopt binary fingerprint vector as a case study for this paper. The evaluations have been carried out under six datasets taken from FVC 2002 and FVC 2004 benchmark databases.","inCitations":[],"pmid":"","title":"One-factor Cancellable Biometrics based on Indexing-First-Order Hashing for Fingerprint Authentication","journalPages":"3108-3113","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545565"],"entities":[],"journalVolume":"","outCitations":["f6cf1c5f6d9059928a513104b592cc40fc7a7a54","f726789bfa73883086bc6307ec22ff7a9f41af5b","034a10911e5467ec6a9f11e18ca6c6c26bc4e301","3a556a325ac5982de4bcf2f6b55ce37dd60edd0d","4276331cbbf541e70dcc1f87d63dbebf9ec5a87c","0e5c39d12b1d5d671ed892a12096d6f232e38d7e","b1d71796f31e5922c4b9abebee69f26571557646","c7013e0495db55e99b8ea83830f3742068af164a","34c79bec4bd785bc65e885a3a992e692cdc76126","dde73432dbce85e11e3d3eeae8850c89dd4cc473","11dcfab405f1802fb2849009282e8c0f70423a1a","29f0414c5d566716a229ab4c5794eaf9304d78b6","0bbc78f4845bce574b122385c515b50e2269c436","3d1af6c531ebcb4321607bcef8d9dc6aa9f0dc5a","c5b3f5072fb02d8d3d09a546ce2ed2b7478dd540","d966e340f7b0012db281012f92e46157764babbb","f97f55e7bd43740c17c0cfab7db61b12e6075ebf","0a55adc5741b0dc0d2727ca3f7aa88e11cedcd13","1c799eca7983c62f7815ac5f41787b3e552567b6","4f9ff591fefbe4a831f67848aee749c2b15903a9","f88d6de222dec4a584dd7f690f34cfb603038b8b","dbcfc2a2abd46ceb8d89bf0869141804a0b3df7a","3c0f7caec87e0a1c6dbee1d62c8d1f5e9a760d15"],"id":"be7ff2620c8e1c176a6e0735e5085874a6d9d1a4","s2Url":"https://semanticscholar.org/paper/be7ff2620c8e1c176a6e0735e5085874a6d9d1a4","authors":[{"name":"Jihyeon Kim","ids":["7950505"]},{"name":"Andrew Beng Jin Teoh","ids":["9162326"]}],"doi":"10.1109/ICPR.2018.8545565"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545808","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The emergence of user-operated media motivates the explosive growth of online videos. Browsing these large amounts of videos is time-consuming and tedious, which makes finding the moments of user major or special preference (i.e. highlights extraction) becomes an urgent problem. Moreover, the user subjectivity over a video makes no fixed extraction meets all user preferences. This paper addresses these problems by posing a query-related highlight extraction framework which optimizes selected frames to both semantically query-related and visually representative of the entire video. Under this framework, relevance between the query text and the video frames is first computed on a visual-semantic feature embedding space induced by a convolutional neural network (Query-Inception network). Then we enforce the diversity on the video frames with the determinantal point process (DPP), a recently introduced probabilistic model for diverse subset selection. The experimental results show that our query-related highlight extraction method is particularly useful for news videos content fetching, e.g. showing the abstraction of the entire video while playing focus on the parts that matches the user queries.","inCitations":[],"pmid":"","title":"Unsupervised Video Highlight Extraction via Query-related Deep Transfer","journalPages":"2971-2976","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545808"],"entities":[],"journalVolume":"","outCitations":["99cdc11621f728869f4bfe971752bf26d698abe4","620fe6c786d15efca7f553ad70f295e2b693b391","013164e7b314417d1b7451ce3410bd36e9406234","cb8b98d4baac08badf750f084294b7b09e891cb8","06265ee80b9008299a07a4e8ea567f439ebf466d","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","158452a25143013e4c406ee2d41a7399c34df3db","2eadb5f90f450fbe1012bf698ac6027aa07e3e30","6b59ca9fd875378bb40f1dee95856290947f591f","e53cdfdc476c5018cea21ca82005615a958e3a88","20df3eba00d2371c9d84eba67a77507ba43fca22","09eb5f50134b4665461c0c982071e65a5100eebd","5c7adde982efb24c3786fa2d1f65f40a64e2afbf","639fb648206b184c59d9ee4bfa29191f94890e3e","0696b633404183479bf57bc337de2e2121cf0a5e","0fa956029110bd82b34208cd18a77ca34d2c5eed","06905887596d1fdd559781d194d5614662430372","28f0e0d3783659bc9adb2cec56f19b1f90cdd2be","48031e454325487b5fa7972280d1a2400bdef1d4","33a8777f59cb888583855038be580a1318bd800c","3475255a4122954cd85bf892cf4c6c1ac8384c7d","0626908dd710b91aece1a81f4ca0635f23fc47f3","37e084cdf8b8704b497b2d8e7547380c09468c7b","c711a3db0945ee59b334c387d1bf32cfd1345dc3"],"id":"7071c41a024acb83cec869a2d9ae524965bf82c8","s2Url":"https://semanticscholar.org/paper/7071c41a024acb83cec869a2d9ae524965bf82c8","authors":[{"name":"Han Wang","ids":["49528629"]},{"name":"Huangyue Yu","ids":["52144028"]},{"name":"Pei Chen","ids":["47978662"]},{"name":"Rui Hua","ids":["49380728"]},{"name":"Chuyi Yan","ids":["52195887"]},{"name":"Ling Zou","ids":["49887120"]}],"doi":"10.1109/ICPR.2018.8545808"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202229","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This work presents a study of robot software using the Robot Operating System (ROS), focusing on detecting inconsistencies in physical unit manipulation. We discuss how dimensional analysis, the rules governing how physical quantities are combined, can be used to detect inconsistencies in robot software that are otherwise difficult to detect. Using a corpus of ROS software with 5.9M lines of code, we measure the frequency of these dimensional inconsistencies and find them in 6% (211 / 3,484) of repositories that use ROS. We find that the inconsistency type \u2018Assigning multiple units to a variable\u2019 accounts for 75% of inconsistencies in ROS code. We identify the ROS classes and physical units most likely to be involved with dimensional inconsistencies, and find that the ROS Message type geometry_msgs::Twist is involved in over half of all inconsistencies and is used by developers in ways contrary to Twist's intent. We further analyze the frequency of physical units used in ROS programs as a proxy for assessing how developers use ROS, and discuss the practical implications of our results including how to detect and avoid these inconsistencies.","inCitations":["c0eb27702d66644d5cad560035ca4d3d20ebade1","b4c41894472efcbf6e09af1420012810ab46a9f6"],"pmid":"","title":"Dimensional inconsistencies in code and ROS messages: A study of 5.9M lines of code","journalPages":"712-718","s2PdfUrl":"","pdfUrls":["https://roscon.ros.org/2017/presentations/ROSCon%202017%20Lightning%20101.pdf","http://cse.unl.edu/~carrick/papers/OreDEIROS2017.pdf","https://doi.org/10.1109/IROS.2017.8202229"],"entities":["Approximation","C++","Proxy server","Robot Operating System","Robot software","Sensor","Source lines of code"],"journalVolume":"","outCitations":["fdbf011b391ec862a3547267504fb5b0e552e4c0","feaa1484d33f8f2af606d1fb940792243ae26614","0734cd989a6f77cde952efa6f50073987a5fec57","d45eaee8b2e047306329e5dbfc954e6dd318ca1e","675292eae532a971728ff3986587c92f8389abd4","7b9958e3de5f4ebba8200db91b8f877e81db01d9","10652afefe9d6470e44f0d70cec80207eb243291","298be5d604c5d13c02eb8db1bd18654cb5f54573","5edbfc8e3510515e940baa35090bd4472af371be","4063ba3bcf90f49d1e4976b6a8bb6d261a564a45","4a0d36ad6e6c4f94ab0aa8ddbd2d8dd13d1181b4","7556527b3e05a8b349c7f21595d1a19ace59d5e5","32991f153e92010784446391a138104142cf283c"],"id":"d2c70073316ee264e393411cd49182a9be78deb4","s2Url":"https://semanticscholar.org/paper/d2c70073316ee264e393411cd49182a9be78deb4","authors":[{"name":"John-Paul Ore","ids":["2515935"]},{"name":"Sebastian G. Elbaum","ids":["1793503"]},{"name":"Carrick Detweiler","ids":["1694389"]}],"doi":"10.1109/IROS.2017.8202229"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545487","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Temporal action detection aims at not only recognizing action category but also detecting start time and end time for each action instance in an untrimmed video. The key challenge of this task is to accurately classify the actions and determine the temporal boundaries of each action instance. In temporal action detection benchmark: THUMOS 2014, large variations exist in the same action category while many similarities exist in different action categories, which always limit the performance of temporal action detection. To address this problem, we propose to use joint Identification-Verification network to reduce the intra-action variations and enlarge inter-action differences. The joint Identification-Verification network is a siamese network based on 3D ConvNets, which can simultaneously predict the action categories and the similarity scores for the input pairs of video proposal segments. Extensive experimental results on the challenging THUMOS 2014 dataset demonstrate the effectiveness of our proposed method compared to the existing state-of-art methods for temporal action detection in untrimmed videos. We further demonstrate that our model is a general framework by evaluating our approach on Charades dataset.","inCitations":[],"pmid":"","title":"Temporal Action Detection by Joint Identification-Verification","journalPages":"2026-2031","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1810.08375","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545487","https://arxiv.org/pdf/1810.08375v1.pdf","https://export.arxiv.org/pdf/1810.08375"],"entities":[],"journalVolume":"","outCitations":["36358eff7c34de64c0ce8aa42cf7c4da24bf8e93","2d83ba2d43306e3c0587ef16f327d59bf4888dc3","b5f2846a506fc417e7da43f6a7679146d99c5e96","009fba8df6bbca155d9e070a9bd8d0959bc693c2","20e9e36329ddabc30261ef5bee487f491d27f835","970b4d2ed1249af97cdf2fffdc7b4beae458db89","374a0df2aa63b26737ee89b6c7df01e59b4d8531","6123e52c1a560c88817d8720e05fbff8565271fb","53698b91709112e5bb71eeeae94607db2aefc57c","3c86dfdbdf37060d5adcff6c4d7d453ea5a8b08f","1c30bb689a40a895bd089e55e0cad746e343d1e2","fd33df02f970055d74fbe69b05d1a7a1b9b2219b","53f0cb5ac6b5e10f74ebb732493e2e786da788d6","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","64a6c30ca95e85427c56acb4c1c20f62c6ec0709","eb100638ed73b82e1cce8475bb8e180cb22a09a2","367008b91eb57c5ea64ef7520dfcabc0c5c85532","25d7da85858a4d89b7de84fd94f0c0a51a9fc67a","722fcc35def20cfcca3ada76c8dd7a585d6de386","193089d56758ab88391d846edd08d359b1f9a863","03c48d8376990cff9f541d542ef834728a2fcda2","283b3160f02db64759259b4eb39dd54c4969d6f8","1f05473c587e2a3b587f51eb808695a1c10bc153","41951953579a0e3620f0235e5fcb80b930e6eee3","20a0b23741824a17c577376fdd0cf40101af5880","d1cc37e57b4a98db7ac1358536121dd60fda8b66"],"id":"211b35befe51baf018e971d53d3c35cacbe6ad3e","s2Url":"https://semanticscholar.org/paper/211b35befe51baf018e971d53d3c35cacbe6ad3e","authors":[{"name":"Wen Wang","ids":["47824655"]},{"name":"Yongjian Wu","ids":["47096329"]},{"name":"Haijun Liu","ids":["8424682"]},{"name":"Shiguang Wang","ids":["2019481"]},{"name":"Jian Cheng","ids":["1709439"]}],"doi":"10.1109/ICPR.2018.8545487"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593774","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The use of sensor technologies for material characterization is rapidly growing and innovative advancement is observed. However, the use of sensor combinations for a raw material characterization in mining is very limited and automation of the material identification process using a combined sensor signal is not defined. Potential sensor technologies for raw material characterization were evaluated based on the applicability and technological maturity. To ensure a rapid implementation of the Real-time mining (RTM) project concept, mature technologies such as Red Green Blue (RGB) imaging, Visible Near Infrared (VNIR) hyperspectral imaging, Short Wave Infrared (SWIR) hyperspectral imaging, Fourier-Transform Infrared Spectroscopy (FTIR), Laser Induced Breakdown Spectroscopy (LIBS) and Raman were selected. Each selected technology was assessed for automation in sensing and applicability (for characterization of the test case materials). Based on the results the sensor data were further considered for data fusion. The proposed sensor combinations approach encompasses three levels of data fusion: low-level, mid-level and high-level. The data of the different sensors are fused together in order to acquire a wide range of mineral properties within each lithotype and an improved classification and predictive models. The preferred level of data fusion and preferred sensor data combinations will be used to develop a multi-variate statistical interpretation rule which relates combination of sensors signals with raw material properties. Thus a tool which integrates the combined sensor signal with materials properties will be developed and used to automate the material characterization process.","inCitations":[],"pmid":"","title":"Automation in sensing and raw material characterization - a conceptual framework","journalPages":"1501-1506","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593774"],"entities":[],"journalVolume":"","outCitations":[],"id":"b1f30de2b7066674933541e7bcc276e60b9c75a4","s2Url":"https://semanticscholar.org/paper/b1f30de2b7066674933541e7bcc276e60b9c75a4","authors":[{"name":"F. S. Desta","ids":[]},{"name":"M. W. N. Buxton","ids":[]}],"doi":"10.1109/IROS.2018.8593774"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206310","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Hybrid driving-stepping locomotion is an effective approach for navigating in a variety of environments. Long, sufficiently even distances can be quickly covered by driving while obstacles can be overcome by stepping. Our quadruped robot Momaro, with steerable pairs of wheels located at the end of each of its compliant legs, allows such locomotion. Planning respective paths attracted only little attention so far. We propose a navigation planning method which generates hybrid locomotion paths. The planner chooses driving mode whenever possible and takes into account the detailed robot footprint. If steps are required, the planner includes those. To accelerate planning, steps are planned first as abstract manoeuvres and are expanded afterwards into detailed motion sequences. Our method ensures at all times that the robot stays stable. Experiments show that the proposed planner is capable of providing paths in feasible time, even for challenging terrain.","inCitations":["f49b3e40d868b3f409a17719b5619040680b60d7","95196ed6444b79b9944de0453e1834d2827092af","e9a58473d2d5b431ecb958eb22a4bd35e4b80355","b4d795968eb42a9df3f6923b184d5182116e4b65","b3f1a42c781f5f435b63d00f8c8058f52e135fa6","7da3e27039df9ba5716c9b5cf4a51946971df9eb"],"pmid":"","title":"Anytime hybrid driving-stepping locomotion planning","journalPages":"4444-4451","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1809.07064","https://doi.org/10.1109/IROS.2017.8206310","https://arxiv.org/pdf/1809.07064v1.pdf","http://arxiv-export-lb.library.cornell.edu/pdf/1809.07064"],"entities":["Anytime algorithm","Driving simulator","Experiment","Heuristic","Mobile manipulator","Motion planning","Robot","Stepping level","Wheels"],"journalVolume":"","outCitations":["d46a3efd1b85c11cc9376fc8b48388eae5221d2b","4835c80de0426c78321e639e16d3a096a4d1da7d","dabdbb636f02d3cff3d546bd1bdae96a058ba4bc","9bc108b3d85765de9e0c4f14165b100f72307e66","d967d9550f831a8b3f5cb00f8835a4c866da60ad","0515b1803656762dcad0ad66097578e810d7d5dd","1a87557b29dd5bbe747e3af94f657727b42cd7fd","a54067fdfd135434fd2c5fe71ea19e6123f23b2b","030579e04b8af3325ffa08e9441abc7323ae1eb0","51311f54032dec2d7ebbdfd794b76ed3f4307765","4aa54b141717727aa4130cfc660c51d218c51823","74d2df53e97603f8a887ba3e923eac51d668ac56","9f8c56f0c6a9a78d35d783323c33a6a74926187b","0be69d1f510ed6df35717ea780746b52e0ac79c2","74cd42a4ed934a26b6e21b28d56c54dd3e66d2e0","837ff562b991d1979f2f91994200386943b51789","100857eb8b35bbc06443d20ad2e3e2362cb46fb4","33872551e151b1e9e607d42f5afd336859fa0a55","741e01a92553f52c7774950e32de4ebc90ac334b","9dfd9554e948b95cc92a64f4d16c3369cdde82de"],"id":"737e566c51d4a0a46a54e93a75aa43123dace323","s2Url":"https://semanticscholar.org/paper/737e566c51d4a0a46a54e93a75aa43123dace323","authors":[{"name":"Tobias Klamt","ids":["32356580"]},{"name":"Sven Behnke","ids":["1699019"]}],"doi":"10.1109/IROS.2017.8206310"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593621","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Experimental Advanced Superconducting Tokamak (EAST) is the world's first fully superconducting tokamak fusion device with non-circular cross-section which was built in China The EAST articulated maintenance arm (EAMA) system is developed for real-time detection and rapid repair operations to damaged internal components during plasma discharges without breaking the EAST ultra-high vacuum (UHV) condition. To achieve the desired objectives, the EAMA system design should guarantee that the robot can stably run in the harsh environments of high temperature (80\u2013120 °C) and high vacuum (~ 10−5Pa). Meanwhile, the errors caused by the deformation of long flexible robot arms should also be predicted and compensated in real-time to obtain high accuracy for maintenance operations. In this paper, the vacuum-available design scheme of the manipulator system was firstly introduced. Secondly, inverse kinematics and obstacle avoidance strategy of the highly redundant EAMA robot was built. Then, flexible errors were predicted utilizing a back-propagation neural network (BPNN) model which was established on the basis of real experimental data. Finally, an integrated control strategy for error prediction and compensation was developed.","inCitations":[],"pmid":"","title":"Development and Error Compensation of a Flexible Multi-Joint Manipulator Applied in Nuclear Fusion Environment","journalPages":"3587-3592","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593621"],"entities":[],"journalVolume":"","outCitations":[],"id":"e8d233a7ef1d257caa8e70ed23008887c0db7e57","s2Url":"https://semanticscholar.org/paper/e8d233a7ef1d257caa8e70ed23008887c0db7e57","authors":[{"name":"Shanshuang Shi","ids":[]},{"name":"Yong Cheng","ids":[]},{"name":"Hongtao Pan","ids":[]},{"name":"Wenlong Zhao","ids":[]},{"name":"Huapeng Wu","ids":[]}],"doi":"10.1109/IROS.2018.8593621"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593625","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"3D radiation source localization is a common task across applications such as decommissioning, disaster response, and security, but traditional count-based sensors struggle to efficiently disambiguate between symmetries in sensor, source, and environment configurations. Recent works have demonstrated successful passive source localization using a bearing sensor called the Compton gamma camera that can image radiation. This paper first presents an approach to mapping the spatial distribution of radiation with a gamma camera to estimate source locations. An active source localization framework is then developed that greedily selects new waypoints that maximize the Fisher Information provided by the camera's range and bearing observations for source localization. Finally the common assumption of a static step size in between waypoints is relaxed to allow step sizes to adapt online to the observed information. The proposed radiation mapping approach is evaluated in 5×4 m2 and 14×6 m2 laboratory environments, where multiple point sources were localized to within an average of 0.26 m or 0.6% of the environment dimensions. The active source localization approach is evaluated in simulation and an adaptive step size yields a 27% decrease in the localization time and a 16% decrease in the distance traveled to localize a source in a 15×15×15 m3 environment.","inCitations":[],"pmid":"","title":"Active Range and Bearing-based Radiation Source Localization","journalPages":"1389-1394","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593625"],"entities":[],"journalVolume":"","outCitations":[],"id":"4d6c2b8806f1df7929cd4742a8290678069eda24","s2Url":"https://semanticscholar.org/paper/4d6c2b8806f1df7929cd4742a8290678069eda24","authors":[{"name":"Michael S. Lee","ids":[]},{"name":"Daniel Shy","ids":[]},{"name":"William Whittaker","ids":[]},{"name":"Nathan Michael","ids":[]}],"doi":"10.1109/IROS.2018.8593625"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205987","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a method for trajectory generation with Bézier curves, by considering the wind field and dynamic constraints for small Unmanned Aerial Systems (UAS). Atmospheric phenomena affect UAS trajectories, so the wind presence should be considered in order efficiently perform a mission. These missions require precise area decomposition, efficient waypoint sequencing, and a smooth trajectory generation to fulfill the required level of safety and reliability. In this context, a path planning algorithm is presented which incorporates the use of an area decomposition scheme which considers complex shapes and restrictions, in order to provide high levels of coverage. A novel trajectory generation algorithm is proposed, which aims to harvest energy from the atmosphere by taking advantage of the previously estimated wind field and the identified wind features, such as shear wind and discrete gusts, by taking into account flight envelope restrictions. Different test cases and scenarios, including Software-In-The-Loop (SITL) simulations and real telemetry data analysis, are presented to assess the energy gain for the implemented algorithms. The results indicate promising energy gains of almost 15% in voltage saving if the mentioned factors are considered to re-plan the path with the energy-efficient trajectories.","inCitations":["38bc5d42b2292679c12d4dac53787bc22e0f4897"],"pmid":"","title":"Energy-efficient trajectory generation with spline curves considering environmental and dynamic constraints for small UAS","journalPages":"1739-1745","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205987"],"entities":["Aerial photography","Algorithm","Automated planning and scheduling","Bézier curve","Control point (mathematics)","Energy drift","Experiment","Interpolation","Motion planning","Real-time transcription","Requirement","Simulation","Spline (mathematics)","Test case","Tracking system","USB Attached SCSI","Unmanned aerial vehicle","Verification and validation","Waypoint"],"journalVolume":"","outCitations":["33781115e8f82de9cd81b6c5d7bc06aea344f2ef","9efe2d8d144553d0ccd082cf4028e4c984c50b4a","d28abd8b261f380ffba8631f8fdfd8e51617d7b9","83e52fe5f0c13a1dae02c3dd04aaf60929c3edde","955c3184c4db00ded8193f6b8a66559f7c28d380","0d79d2f07ea52477b90d2f1bf178fca93b1cfb92","3056e4596525f6c8d2db6eae62edce7a4623b2e2","acc10bb0f35d60a7854524b89280c42c81deba5a","c630a48c329b8c7a123b5558173ebf06d8079aa7","5f47298e5461a9cd29fc69c6393c5a3ad8bfd71a","ff0a185032dc98f7a06de4086f87d0957da57775","878e8e1f25f2d22633134c0f6011b0a96e2de75d","8d78b793f9625de8c37ac7d5c426dde3afc053b0","2309c5b95b9d7c51a7381e46bf62ed70e050cc68","d892380d6688dfe242356508eaa4bb99907fd467","d7e74c95682545ff7953e154c9afc1911f27d458","468c44c599195d587f8436da4aeaefe46623f935","32c488588ff81e1b53c11e230d249e203afb8268","abfc17bec366d040593153e113564530499b206c","5b756a8f5212f4878657f522e5745ba2e2164dfe"],"id":"53ee8b6d34fbb897b372d7c1c3afe6511d8d9cd6","s2Url":"https://semanticscholar.org/paper/53ee8b6d34fbb897b372d7c1c3afe6511d8d9cd6","authors":[{"name":"Lycinda Rodriguez","ids":["40428633"]},{"name":"Fotios Balampanis","ids":["2889142"]},{"name":"Jose A. Cobano","ids":["9433164"]},{"name":"Iván Maza","ids":["1767904"]},{"name":"Aníbal Ollero","ids":["1748298"]}],"doi":"10.1109/IROS.2017.8205987"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545326","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Hypertension is a common cardiovascular disease, which will lead to severe complications without timely treatment. Accurate hypertension identification is essential to preventing the condition deteriorated. However, the state of art hypertension identification methods only extract features from very few aspects, and hence have limited identification accuracy. Furthermore, they only can judge whether the subjects are hypertensive or not, more meaningful information (such as, why the subjects suffer from hypertension) that can help doctors to improve their diagnosis level are absent. In this paper, we propose a class association rules-based method to identify hypertension. Particularly, its key idea is to utilize the relationship existing in multi-dimensional features to characterize hypertension pattern more effectively, in order to improve the identification performance. In addition, it can also generate a set of class association rules (CARs), which can reflect the subjects' physiological status and are proved to be useful for doctors to analyze subject's condition deeply. Experiments based on 128 subjects (61 hypertension patients and 67 healthy subjects) shows that our method outperforms the baseline methods and the accuracy, precision and recall reach 85.2%, 85.0%, and 83.6%, respectively. Additionally, a user study based on five clinicians demonstrates the utility of the generated CARs.","inCitations":[],"pmid":"","title":"Identification of Hypertension by Mining Class Association Rules from Multi-dimensional Features","journalPages":"3114-3119","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545326"],"entities":[],"journalVolume":"","outCitations":["3e4291043c940d19961dbc7a188e8fee68dc6769","3ed7fde0f19c487529e9a0cd0e41692f4c846151","8b1bf43f48a84196c871114483411f9be9156e76","2a814b2c6230e7378a70281dea545a0913fce890","bf3bb5a09f1c21b846c880587112d747a97317e7","5861b3441666604b9b343c46eb0012ce5ca94dc2","3ceff6cea43d5e1e3a905c833a4eb4537ea2cfac","83f563ff79a5c5b18277244bad7b78ba1ab9fa46","0134626480301a5b2cbcf490b261a6c76d33ce34","d6a1fdb735e2f7088bb213d028c4e0e150472611","00e752adb3c2e3715c6f2c37756d75d1e9678877","7019b2728796fb1a32f580899d51ef2aa6e84f84","d12ff311aef69d3662acefc86d0d4f34d2f9b0b8","6bd76215f7a686fb6fc28c330515617379aa030f","b226da4056cb0da4d3cb625af178260f46d229f2","07974baf26ca7e81c3801bdc7cf84bbd6be9bd96","bd5190e7299e68940bd36f3f020e2b5571c09894","0fe3af7ede3ae553b5e35856e5f169ef4a4b1498"],"id":"2170bca91af5da4f3486970307d54489576c16fe","s2Url":"https://semanticscholar.org/paper/2170bca91af5da4f3486970307d54489576c16fe","authors":[{"name":"Fan Liu","ids":["38547074"]},{"name":"Xingshe Zhou","ids":["1743113"]},{"name":"Zhu Wang","ids":["48708884"]},{"name":"Tianben Wang","ids":["2976666"]},{"name":"Yanchun Zhang","ids":["46867272"]}],"doi":"10.1109/ICPR.2018.8545326"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.23","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Relatively small data sets available for expression recognition research make the training of deep networks very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redundant information from the pretrained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully-connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization results show that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu- CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.","inCitations":["3266b06bcbb7ab12fd8a595c3bdf3d244735a8d4","6ac1dc59e823d924e797afaf5c4a960ed7106f2a","30723ada764c6ec186927522d666eaa8eeae35b1","7f3da52c13c70fd5b93c2ccebdd4a4527fa597fa","92be73dffd3320fe7734258961fe5a5f2a43390e","734083b72b707dd2293ef2791f01506dec9f8a99","ab8ecf98f457e29b000c44d49f5bf49ec92e571c","df096e221cc92c481d34443f4788c96ee412b189","a54fcdcb02da0844d28b3191145bbc99675714df","47d4838087a7ac2b995f3c5eba02ecdd2c28ba14","561ae67de137e75e9642ab3512d3749b34484310","52af7625f7e7a0bd9f9d8eeafd631c4d431e67e7","9addd3eb94e8ef7b826f2ec38b84d28a1f0c5cad","2776e8cff4249a7775c040731495878135972e77","82d2af2ffa106160a183371946e466021876870d","08a7dcfaea52c7fc5e603273f0d5d465acfc5c64","f45c260db90378c4d6dcee623ecd444b001050a5","a1973e527059171e83f2ef2863a8fc60afedd7b1","7f03d628f3371c44a1248962dcb3740fb8573fd0","081fb4e97d6bb357506d1b125153111b673cc128","90e89d2251ea2de123c29912b07bb89efb9dd677","98c548a4be0d3b62971e75259d7514feab14f884","7334987d38cc09b02d291de188426eb593e8f53c","e568236493c3606ecd8c576a62540545aed1c170","2af733d1a779e69c62b0b2b4b40d8667702cff4d","405ffae3b7a7702e77418de5dce0337b0596acc4","9e297343da13cf9ba0ad8b5b75c07723136f4885","0f0cab9235bbf185acdd4f9713fd111ca50effca","29f49f12457e8f18ee97ee77b7f72de6f1529da0","2c93c8da5dfe5c50119949881f90ac5a0a4f39fe","61971f8e6fff5b35faed610d02ad14ccfc186c70","d9bad7c3c874169e3e0b66a031c8199ec0bc2c1f","edc37f68cc6a36ba51db77574788cce4b2719719","11aa92ecea8674138c375d2bfea3ddc4f5811b45","3827f1cab643a57e3cd22fbffbf19dd5e8a298a8","01fbc6089029844880d532d0060d0ad3c8a33763","123c3798779e1d85bc2a7864e1a133c4c2571080","2594a77a3f0dd5073f79ba620e2f287804cec630","b2b535118c5c4dfcc96f547274cdc05dde629976","8dcc95debd07ebab1721c53fa50d846fef265022","f879556115284946637992191563849e840789d1","6584c3c877400e1689a11ef70133daa86a238602","41781474d834c079e8fafea154d7916b77991b15","205f39189a7f696be8b4ee0018d071cb473b5006","fdf8e293a7618f560e76bd83e3c40a0788104547","18b344b5394988544c386783e7bb8e73e0466e0e","525da67fb524d46f2afa89478cd482a68be8a42b","7a16f37ecccca4f9703ce190dc596149b4ccc8d2","53d3738e867cae5f19f2bd9bb96d0d06eb0362b0"],"pmid":"","title":"FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition","journalPages":"118-126","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1609.06591","http://arxiv.org/pdf/1609.06591v1.pdf","https://arxiv.org/pdf/1609.06591v2.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2017.23","http://arxiv.org/pdf/1609.06591v2.pdf"],"entities":["Algorithm","Append","Database","Facial recognition system","High- and low-level","High-level programming language","Neuron","Teaching method","Vii"],"journalVolume":"","outCitations":["2dc9b005e936c9c303386caacc8d41cabdb1a0a1","42e0127a3fd6a96048e0bc7aab6d0ae88ba00fb0","3e34632cd69be865fc4639cdd1f43a8b5f2a3ea6","1d827e24143e5fdfe709d33b7b13a9a24d402efd","d785fcf71cb22f9c33473cba35f075c1f0f06ffc","33f9ec18e8c16f1d4b28e0d7209c29d1be3adbb3","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","635158d2da146e9de559d2742a2fa234e06b52db","887b7676a4efde616d13f38fcbfe322a791d1413","82b8cf061d53c32ab5a403543d1c440bc502416a","0d735e7552af0d1dcd856a8740401916e54b7eee","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","2f25bdbb72990c76c12565c4aa7a6d9ee3c9045a","4d9a02d080636e9666c4d1cc438b9893391ec6c7","56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719","f35a493afa78a671b9d2392c69642dcc3dd2cdc2","0568fc777081cbe6de95b653644fec7b766537b2","cd85a549add0c7c7def36aca29837efd24b24080","a57b37549edba625f5955759e259e52eb0af8773","74fc396d0b8ec548d600395182f12c9b06cc84e9","722fcc35def20cfcca3ada76c8dd7a585d6de386","162ea969d1929ed180cc6de9f0bf116993ff6e06","77e3c48aa10535276e7f570a3af594ba63de7d65","6c8b30f63f265c32e26d999aa1fef5286b8308ad","853bd61bc48a431b9b1c7cab10c603830c488e39","1768909f779869c0e83d53f6c91764f41c338ab5","73b90573d272887a6d835ace89bfaf717747c59b","ad0863aa16301d7c6617f8b965a64cf58f38594f","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","eac2a468de0c4f241fad7696ca930bb2e11545b3","8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486","061356704ec86334dbbc073985375fe13cd39088","32bfa75e9f6ed4d04b093adf19c92f0556f77623","bddc822cf20b31d8f714925bec192c39294184f7","96e0cfcd81cdeb8282e29ef9ec9962b125f379b0","2c285dadfa6c07d392ee411d0213648a8a1cf68f","8e461978359b056d1b4770508e7a567dbed49776","8fe5feeaa72eddc62e7e65665c98e5cb0acffa87","3661a34f302883c759b9fa2ce03de0c7173d2bb2","c0c0b8558b17aa20debc4611275a4c69edd1e2a7","1bcbf2a4500d27d036e0f9d36d7af71c72f8ab61","f9c431f58565f874f76a024add2aa80717ec5cf5","009fba8df6bbca155d9e070a9bd8d0959bc693c2","0359f7357ea8191206b9da45298902de9f054c92","b6c53891dff24caa1f2e690552a1a5921554f994"],"id":"0334a8862634988cc684dacd4279c5c0d03704da","s2Url":"https://semanticscholar.org/paper/0334a8862634988cc684dacd4279c5c0d03704da","authors":[{"name":"Hui Ding","ids":["46418923"]},{"name":"Shaohua Kevin Zhou","ids":["1682187"]},{"name":"Rama Chellappa","ids":["9215658"]}],"doi":"10.1109/FG.2017.23"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593519","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We use multiple spinning micro-rafts at the air-water interface as mobile microrobot collectives and present here their collective behaviors, including navigating around anchored obstacles, and trapping and transporting floating objects. The 3D-printed micro-rafts are circular disKS of 100 μm in diameter and have parametrically defined undulating edge profile. The study of their local interactions, manifested by the pairwise interactions between micro-rafts, reveals competing magnetic and capillary interactions that keep the collectives in their dynamic state. Using collectives of 7, 19, and 36 micro-rafts and micro-channels between millimeter-sized posts, we demonstrate the effects of the size of the collectives, the size of the obstacles, and maneuver strategies on the collective navigation. Employing methods from information theory, we show that the pairwise mutual information of the collectives increases significantly during the channel-crossing as a result of the additional constraints of the channel walls on the collectives. Finally, we demonstrate the trapping of 1-mm-diameter polystyrene bead and the trapping and transporting of 600~μm-wide pm","inCitations":[],"pmid":"","title":"Collectives of Spinning Mobile Microrobots for Navigation and Object Manipulation at the Air-Water Interface","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593519"],"entities":[],"journalVolume":"","outCitations":[],"id":"3bcf97c660f686511946b78269a6853cb647bfdc","s2Url":"https://semanticscholar.org/paper/3bcf97c660f686511946b78269a6853cb647bfdc","authors":[{"name":"Wendong Wang","ids":[]},{"name":"Vimal Kishore","ids":[]},{"name":"Lyndon Koens","ids":[]},{"name":"Eric Lauga","ids":[]},{"name":"Metin Sitti","ids":[]}],"doi":"10.1109/IROS.2018.8593519"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593973","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Spinal cord stimulation (SCS) has recently enabled humans with motor complete spinal cord injury (SCI) to independently stand and recover some lost autonomic function. However, the nature of the recovered motor activity and the interplay between SCS and motor training are not well understood. Understanding the effect of stand training and spinal stimulation on motor activity during bipedal standing is important for designing spinal rehabilitation therapies that seek to combine spinal stimulation and rehabilitative robots. In this study, we examined electromyography (EMG) data gathered from two SCI patients and six healthy subjects as they attempted standing. We analyzed the muscle activation patterns and EMG waveform shape to quantify both the changes in SCI patient motor activity with training, and the differences between healthy motor activity and SCI patient motor activity under stimulation. We also looked for correlations between the similarity in SCI patients' motor activity to healthy subjects and their overall standing ability. We found that good standing in SCI patients does not emulate healthy standing muscle activity. Furthermore, patient stand training heavily influenced motor activation patterns, but not in ways that improved standing ability. These results indicate that current training techniques do not optimally influence motor activity, and robotic rehabilitation strategies for SCI patients should target essential features of motor activity to optimize functional performance, rather than emulate healthy activity.","inCitations":[],"pmid":"","title":"On Muscle Activation for Improving Robotic Rehabilitation after Spinal Cord Injury","journalPages":"798-805","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593973"],"entities":[],"journalVolume":"","outCitations":[],"id":"5cd4d76053219204392b0cc7a13c14ad1143c3e0","s2Url":"https://semanticscholar.org/paper/5cd4d76053219204392b0cc7a13c14ad1143c3e0","authors":[{"name":"Richard Cheng","ids":[]},{"name":"Yanan Sui","ids":[]},{"name":"Dimitry Sayenko","ids":[]},{"name":"Joel W. Burdick","ids":[]}],"doi":"10.1109/IROS.2018.8593973"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594255","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Unmanned Aerial Vehicles (UAVs) can charge Wireless Rechargeable Sensor Networks (WRSNs) in remote or hard to access locations. However, the charging efficiency is heavily affected by the distance between the wireless transmitter and receiver. This efficiency impacts the possible power level increase of each charged node. Most charging algorithms require full knowledge of sensor nodes' power levels to identify the nodes to charge. Collecting this power information adds overhead to the network and limits scalability. We propose and implement Charging with Power Transfer Efficiency Compensation (CPTEC), an algorithm that charges a WRSN without the need for a priori knowledge of the nodes' power levels. We show that CPTEC compensates for efficiency drops, due to landing alignments, making it practical for real-world power transfer scenarios. Our results show that CPTEC is able to perform with a median at ≈ 72% of the optimal performance of a full knowledge algorithm that assumes maximum power transfer efficiency, while other work drops to ≈ 22%. Under constant maximum efficiency CPTEC performs ≈ 90% of the optimal full knowledge case.","inCitations":[],"pmid":"","title":"UAV Based Wireless Charging of Sensor Networks Without Prior Knowledge","journalPages":"3151-3158","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594255","http://cse.unl.edu/~carrick/papers/NajeebD-2018IROS.pdf"],"entities":[],"journalVolume":"","outCitations":["0bedd5f991cdd531dc78e9bd1ed212888f787323","5f68f0326c85d31190ac1f017cd715fce4468a7b","b78ec881ae8af03c0e2253c613a9ccd5c23dc7d3","0f3a2166fdc28f49ef6a7de8da9afa6cdd33a3ce","013ca25b12d0783120ad052cbbc44697b32a9130","0c7a3b07e52e208a14c34bdd98f7a25af49f3790","f7c9677e0f8bdf6701f71fe853bcc9cf2b083aa0","2114f00130901f0db235986eb139b7f964afee41","47688d1bb5df746eb94ef8bf1083e43e633aff77","bda6a178bab36c3f11ef8a1f9e2d5a9b5034db8f","4f7cf1afe1edd6c910c565752f40b24337f9d250","99ecf4d9060129acd92d9cd2e89457f366a11a01","cd492f66313ffd82efc1b4778cc4087f81364f6f","0578ac8620cf6b7305494113866eb64ff8439f8b","c889ec5dcba4ac05eada079ce7cdc57cae24d8ec","2abbe933282ba7be6d4fe0f6ffd5fc0e0f0615c4","a626e22de7433b8d160cc1ef81c2298199a7a060","41570d2cd290212a9bafa008c6d1a6121bfc6fc6","e5a574869c4b469ba26509c41df6727ad608a519","8df22f1d5073a05326b89247a8d0d63cbd191500","d1fecb5c2b22e28e4b11299aea307256a4a65d95","f935245c5628bfcdd16a7b8d5a7da7086bf94111"],"id":"8ef346ac5db7561058272a41616603ba5932b6df","s2Url":"https://semanticscholar.org/paper/8ef346ac5db7561058272a41616603ba5932b6df","authors":[{"name":"Najeeb W. Najeeb","ids":["26354277"]},{"name":"Carrick Detweiler","ids":["1694389"]}],"doi":"10.1109/IROS.2018.8594255"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202197","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Soft robots with inherent compliance have been recently investigated intensively for locomotion or manipulations. A critical problem for soft robots is the capability to estimate their shapes to enable closed-loop control for precise motion. In this paper, we propose a new low-cost sensor that can be leveraged for shape estimation of soft robots. This sensor, recently discovered as an artificial muscle, can be conveniently fabricated from low-cost conductive sewing threads. We recently found that the resistance will increase if the fabricated sensor is elongated due to an external force [1]. Since the sensor is inherently soft, it can be embedded into soft robots to estimate the shape. We establish a physics-based model to predict the external force and the displacement if the resistance is given and experimentally validate its correctness. Moreover, to demonstrate the sensing capability, we embed the proposed sensor into soft materials and successfully measure two curvatures of a two-segment soft robot. Therefore, the proposed sensor has the potential to estimate complicated shapes of soft robots to enable closed-loop control.","inCitations":["eaa3644f6e876617ca7cd2ad766e3d6cd3c366bd"],"pmid":"","title":"Twisted and coiled sensor for shape estimation of soft robots","journalPages":"482-487","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202197"],"entities":["Control theory","Correctness (computer science)","Displacement mapping","Embedded system","Experiment","Ground truth","Mathematical model","Robot locomotion","Sensor","Thread (computing)","Twisted"],"journalVolume":"","outCitations":["216a6a6c691efc76f432bf9998baa21a20803d78","45b8ce78c0e7a0a0947202eaab50c12657289f7b","13b1364045a8325071cd3a4da17506f115fa1bbb","ff9d79955484e0e1d8f1b01acec7d778191cfcca","ae4180a045ee4ddfeb2a6696a246a7f2b860404a","55a50b8e1c631252538380ef6bca2a27f54da020","d9789c8efab2763c133777e1cec6fb948199dbda","0cf388962e3214d3ece2320082eae483673b9d66","5e4666e34953f7a012ac7ed0b6b3e5449f803019","9c42648e1117ab1e40dbccd4d0d8aa302c613bff","e81df966e00263bf311cbc59483cd67496d73d0c","b01bf6d5ad742b5a2104efd13544ae7406ee2505","03a0fdbc6232df2de7cddb1b946f3ae1a75b22d9","a1db0c9928ca7406710a6f1371926ad0c147ef3a","70c44a842f20ff3c45d74d6e2e6653bcc40ef388","2944f6a08a8fbf31049a06bfa1f9644005456c56","5d04bf8682c73a7e5d3ab3ff3d88195781931365","6174f115b22aedaff844c8643b13a0cdca373b18","5f48dda32e85e356201fb913c0eab6993fd42fb9","65125f74fe4b5b45f678b987ad1b0f22745aa3ec","56e7d11fe0b2cd3ef26a395e8929aa5c74696a2c","d1601efa077a1b09b81de12bac32cff520c51ca5","03b63bca56ed392db05bbe1ed3ba0ae369f0ff3f","9f90ef6d260ad694e7505de12105659a615415aa","e263912a74403c14c22d66283a587b78708b87d1","3806a19a724302c9f9c40d582ba774a4d7400656","e7627e0ce8dd060691fffe1f63e53b3ca9cb94ae","40975d415b39697ab866ce6bd061050629c96fd0","bf2809e3e5f48e1805a3d38ef21572506d954ea5","a606b0e5ee2371e0781c65e7203a1543c7602199"],"id":"a370849342380b736412c8ab36a311f18cc68364","s2Url":"https://semanticscholar.org/paper/a370849342380b736412c8ab36a311f18cc68364","authors":[{"name":"Ali Abbas","ids":["3312978"]},{"name":"Jianguo Zhao","ids":["7818027"]}],"doi":"10.1109/IROS.2017.8202197"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594178","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Robot-assisted Fenestrated Endovascular Aortic Repair (FEVAR) is currently navigated by 2D fluoroscopy which is insufficiently informative. Previously, a semi-automatic 3D shape instantiation method was developed to instantiate the 3D shape of a main, deployed, and fenestrated stent graft from a single fluoroscopy projection in real-time, which could help 3D FEVAR navigation and robotic path planning. This proposed semi-automatic method was based on the Robust Perspective-S-Point (RP5P) method, graft gap interpolation and semiautomatic multiple-class marker center determination. In this paper, an automatic 3D shape instantiation could be achieved by automatic multiple-class marker segmentation and hence automatic multiple-class marker center determination. Firstly, the markers were designed into five different shapes. Then, Equally-weighted Focal U-Net was proposed to segment the fluoroscopy projections of customized markers into five classes and hence to determine the marker centers. The proposed Equally-weighted Focal U-Net utilized U-Net as the network architecture, equally-weighted loss function for initial marker segmentation, and then equally-weighted focal loss function for improving the initial marker segmentation. This proposed network outperformed traditional Weighted U-Net on the class-imbalance segmentation in this paper with reducing one hyperparameter - the weight. An overall mean Intersection over Union (mIoU) of 0.6943 was achieved on 78 testing images, where 81.01 % markers were segmented with a center position error < 1.6mm. Comparable accuracy of 3D shape instantiation was also achieved and stated. The data, trained models and TensorFlow codes are available on-line.","inCitations":["bb747a5567867973f62336a99aca199459e99ca6","c3ec7ac15f409127b2ecf61e8edc12d7500dbd6f","5ef2be1aadd2f666756b2ab66bc05d146ba0681b","0bf94917e1e71029b2345b9fb8f1d7ff030ad069"],"pmid":"","title":"Towards Automatic 3D Shape Instantiation for Deployed Stent Grafts: 2D Multiple-class and Class-imbalance Marker Segmentation with Equally-weighted Focal U-Net","journalPages":"1261-1267","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1711.01506v4.pdf","https://doi.org/10.1109/IROS.2018.8594178"],"entities":[],"journalVolume":"","outCitations":["d738fd2a737d45a89c76b646f166d84eb8033bb1","0690ba31424310a90028533218d0afd25a829c8d","45704b67a1b54c0e5851455c6de8718789e8c666","f226ec13e016943102eb7ebedab7cf3e9bef69b2","b8c913479cbf2ee279312720a46f9324bffbdda7","9201bf6f8222c2335913002e13fbac640fc0f4ec","f9463970705be8124ef2093c0302a7716bd9deb9","7b66c5c66d5d49c1be8d908b7db4df75552540bb","cab372bc3824780cce20d9dd1c22d4df39ed081a","503c16d9cb1560f13a7d6baedf8c9f889b22459d","79cfb51a51fc093f66aac8e858afe2e14d4a1f20","3ab5edee37e0626d25f05ae7a1509f6115fc437b","8c4bbf119bfed74a81ab209d55f71cca5c7c1dd3","24c98aa6c604ceae37a7329fddee21089a7a70bb","a1cb7dd7f3c605edfff0593292e8308a42115672","67cb2ed8216ca795fdaa1fb1b6e7bd42f7d4764b","59d5a686c82c9c73ee5f6ef42b8be0c245dc84b1","b13b40d94e06723401963342dd67e104132fe489","d0e20aa3d61b77d17f005a1d24d7cf47600836ef"],"id":"df1234b2b815c21c8eded647bdce8b2a03e27959","s2Url":"https://semanticscholar.org/paper/df1234b2b815c21c8eded647bdce8b2a03e27959","authors":[{"name":"Xiao-Yun Zhou Celia Riga","ids":["51284226"]},{"name":"Su-Lin Lee","ids":["1847967"]},{"name":"G. Yang","ids":["50146936"]}],"doi":"10.1109/IROS.2018.8594178"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206209","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This study proposes an actuator whose design relies on a coordinated approach to control and hardware design: impedance control is supplemented with the introduction of a spring-damper coupler between the actuator and reference (ground). The coupler between the actuator and reference has the effect of further reducing the impedance apparent to the environment. Unlike traditional series elastic actuation, where the deformation of a physical spring coupler introduced between an actuator and link is fed back for control, our coupler is located between actuator and reference and its deformation is not fed back for control. The proposed control further employs time-delay estimation to account for the system dynamics that include both the physical and virtual couplers and to realize accurate and robust control over end-effector position and contact forces. We present a simple method for estimating interaction forces and regulating the contact force without a priori knowledge of the environment. A numerical simulation demonstrates the efficacy of the proposed approach.","inCitations":[],"pmid":"","title":"Impedance control with structural compliance and a sensorless strategy for contact tasks","journalPages":"3623-3628","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206209"],"entities":["Acoustic coupler","Characteristic impedance","Control theory","Nominal impedance","Numerical weather prediction","Power dividers and directional couplers","Robot end effector","Robust control","Simulation","System dynamics","Term (logic)","Transparent Data Encryption"],"journalVolume":"","outCitations":["8e4e070626ff865a1adaf332993b8971c8b6f876","38200617446c050ec737f6a6b8cdc2ee14d04ad2","64a4c62b589506090d8b3eb2abd5c20d175d2941","31097025b94903fc3950f5f1906942f04f7e29b0","e5320d64d1e345137f93b5e2d34d3b954d622b3c","f9e37638ae1d470ac182a4d34684337c929c5ef3","21ad883112d6bddd7488301d729b016ee340b6f4","5203fd9e799e655c5c5daa1dd7e83411ab42a157","aeba7625992d8fba83a73653835ff35e622f5362","7ebe9eb32480b32a0b75fcc3ca5a0a1aa1602ecf","9a4b711e1783226f21004d8d219b7e167dab4236","1a560f187bc5186c26f17315aac08020eff3417c","03034a7ae93e9cda097b1626d6558fa14e8c7742","b6f9ecfb98d6c53057ceae8615ea0d6ae4d01167","5c9edddc8d05557384da44d4ed6b129fbc4bb343","cd368480a24312d26ddfd236115be7ad149b0fd4","350721ab6f02a092f974f1c6a140a16372bfe815","48e27b4d82b59e2f0cb01a5c64553239a07a1151","930109ef17ab704a0d4e307456d8cf8d46c3a1df","f420eb3e461a5eccefbdcae09d2df2b8729a2677","a6720977e0180889bb9641a8e2cd423e05d37012","74c86a36d0d86b770a7ce630e226c668d4367f8e","c8bea137780d17c2dad85fef6527b27a38b29a86","fbe882245c3ad8b7c66858aa52738dd861c18d18","fd2a709c4583010c78c5645aa60eb2bc8ceb750e","782ac3a1c17bf7c2d80e6301c1be29ae7d3a4db3","be52f96932e71632c5ccfc7e84c9483eefb66a28","4c3306fb7ddbe32b00c9652555516701fdfa6ed3"],"id":"9c3956e0b7cb835bc946f00d03e76a31eb52051f","s2Url":"https://semanticscholar.org/paper/9c3956e0b7cb835bc946f00d03e76a31eb52051f","authors":[{"name":"Dongwon Kim","ids":["2052852"]},{"name":"Sang-Hoon Kang","ids":["2066370"]},{"name":"Gwang Min Gu","ids":["3023713"]},{"name":"Maolin Jin","ids":["2565054"]}],"doi":"10.1109/IROS.2017.8206209"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594428","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"3D reconstruction and surgical tool segmentation are necessary for several advanced tasks in robot-assisted laparoscopic surgery. These tasks include vision-based force estimation, surgical guidance, and medical image registration where pre-operative data (CT or MRI scan image slices) are overlaid on patient anatomy in real-time during surgery [1] to name a few. In this work, two main strategies were considered: (1) initialize with surgical tool segmentation from 2D images, then proceed to local 3D reconstruction near the tool-tissue interaction region by projecting the segmented result into 3D space, and (2) initialize with 3D reconstruction of the entire surgical task space, followed by surgical tool segmentation from within the 3D reconstructed model. Both methods were implemented on the Raven II surgical robot system, and accuracy and time complexity for both methods were comparatively analyzed while considering various task parameters. Finally, based on the results of this work, guidelines for selecting reconstruction and segmentation strategies and procedure for particular situations are outlined in Section V.","inCitations":[],"pmid":"","title":"Comparison of 3D Surgical Tool Segmentation Procedures with Robot Kinematics Prior","journalPages":"4411-4418","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594428"],"entities":[],"journalVolume":"","outCitations":[],"id":"ec24b7b767d556c2a38508158ffbdccfab6a5766","s2Url":"https://semanticscholar.org/paper/ec24b7b767d556c2a38508158ffbdccfab6a5766","authors":[{"name":"Yun-Hsuan Su","ids":[]},{"name":"Issac Huang","ids":[]},{"name":"Kevin Huang","ids":[]},{"name":"Blake Hannaford","ids":[]}],"doi":"10.1109/IROS.2018.8594428"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593856","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This communication presents an application for the use of ontologies in the generation of robot structures. The ontology developed for this app relies on the IEEE Standard Ontologies for Robotics and Automation (ORA) and it incorporates a set of concepts, relations and axioms that link robotic skills with the structural parts needed for their realization. The user can select a base configuration and/or a set of desired skills that the robot should be able to perform. Then, the application evaluates the axioms and returns an abstract structure that can carry out the requested skills. The final implementation of the structure can be achieved with any modular robotic platform that could identify each structural part with a physical device.","inCitations":[],"pmid":"","title":"Skill-Oriented Designer of Conceptual Robotic Structures*This work was supported by CDTI under expedient IDI-20150289 (BOTBLOQ: Ecosistema integral para el diseño, fabricación y programación de robots DIY).","journalPages":"5679-5684","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593856"],"entities":[],"journalVolume":"","outCitations":[],"id":"fffb60ad671c0664170278805732943d7ab2087a","s2Url":"https://semanticscholar.org/paper/fffb60ad671c0664170278805732943d7ab2087a","authors":[{"name":"Francisco Ramos","ids":[]},{"name":"Cristian O. Scrob","ids":[]},{"name":"Andrés S. Vázquez","ids":[]},{"name":"Raul Fernandez","ids":[]},{"name":"Alberto Olivares Alarcos","ids":[]}],"doi":"10.1109/IROS.2018.8593856"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206277","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a generalized multi-stage Bayesian approach for an unmanned aerial vehicle to estimate the location of a mobile target. The major hardware components of the proposed approach are a camera with a fisheye lens and another camera with a normal lens and a pan/tilt unit. With wide angle of view (AOV), the fisheye lens camera first detects the bearing of the target, and the PT camera next captures the target in its AOV. The recursive Bayesian estimation steadily locates the target in a globally defined space. The paper also proposes a multi-stage detection method for the fisheye lens camera. The level of confidence is defined in association with the probability of detection (POD) for each detection technique, and the fisheye lens enables continuous detection by gradually increasing the POD. The observation likelihood is finally derived from the POD in a generalized manner. The proposed approach was applied to the detection of a mobile target by a multi-rotor helicopter, and results have demonstrated the effectiveness of both the proposed multi-stage Bayesian approach and multi-stage fisheye lens detection method.","inCitations":[],"pmid":"","title":"Multi-stage Bayesian target estimation by UAV using fisheye lens camera and pan/tilt camera","journalPages":"4167-4172","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206277"],"entities":["Altered level of consciousness","Autonomous robot","FishEye","R.O.T.O.R.","Recursion","Television antenna","Unmanned aerial vehicle"],"journalVolume":"","outCitations":["48fd5570fd81bfba35cd39f8301c0eabb66da96a","c494dc56968585dcdca2c164a586277ce89d0330","a020325745d2f9ff5ce04dea567cfa7fc7e2d600","2a30b168d7bbc6a6b1fc66778fd9be429fd70153","a73aedd055a09665bf5c9115e4b6f810caeed347","31e6e3e049a60f66f4510ad884850e116f9148e5","78e74823f6f68f65c4edcf4951db8a9030d1502b","92fe01ebee28474542d16843498af4626844ca97","0395e7dd0b8b2dff57be3321c7d115eb2d62b0a4","02dea0d105b8931b0edb4ace2da50bfdb1f4954a","39fba5d0f06f4d6ae8cf7a4f5221e3ba795252f8","d086c4c610521dc8fddab7b03a90d4cff4de5264","5cf1e36f0cffeec9ed6469908adbcea0b6e227af"],"id":"cd927bfac16dc40d4e1b9ef1a14ae5f8a945d0d3","s2Url":"https://semanticscholar.org/paper/cd927bfac16dc40d4e1b9ef1a14ae5f8a945d0d3","authors":[{"name":"Tomonari Furukawa","ids":["1768688"]},{"name":"Changkoo Kang","ids":["32769880"]},{"name":"Boren Li","ids":["2707131"]},{"name":"Gamini Dissanayake","ids":["1679214"]}],"doi":"10.1109/IROS.2017.8206277"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206051","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The ability to accurately identify human activities is essential for developing automatic rehabilitation and sports training systems. In this paper, large-scale exercise motion data obtained from a forearm-worn wearable sensor are classified with a convolutional neural network (CNN). Time-series data consisting of accelerometer and orientation measurements are formatted as images, allowing the CNN to automatically extract discriminative features. A comparative study on the effects of image formatting and different CNN architectures is also presented. The best performing configuration classifies 50 gym exercises with 92.1% accuracy.","inCitations":["29f2c7de056975e02b265c721086f54e2f36730f","1209381094c03742333306e7c417cbd6fb430c0c","0ad56c9bbf0a8c827a23e4ea4d5400b032ad853f","478c4131fcc8688147ec07b2226b3ef597c4ae29","354c572cbbbe40514d31d648a7865a082cb78665","b41de530f4e44926eb4d54f61403011a6d8b3e41","90294d4ecef9ad4e3129ef9c76b839808ef59958","4174589675fc00985e396e67f09fa39ba0edcc07","940dc751a48358d48a7be62e20730034ee8e3c3a","0e2e47d0ed01e51cc213221d57cfb11f4c719efc","fa1deb7a18a34bb795a0c88ce7875ac4607ff026"],"pmid":"","title":"Exercise motion classification from large-scale wearable sensor data using convolutional neural networks","journalPages":"2385-2390","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1610.07031v1.pdf","https://arxiv.org/pdf/1610.07031v2.pdf","http://arxiv.org/abs/1610.07031","https://doi.org/10.1109/IROS.2017.8206051","https://arxiv.org/pdf/1610.07031v3.pdf"],"entities":["Artificial neural network","Convolutional neural network","Wearable computer"],"journalVolume":"","outCitations":["24e555913192d8722f4a0240445bf73db71bd884","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","8adb8257a423f55b1f20ba62c8b20118d76a25c7","25ea8a7314fe47b459b688e35f8bcf12113c9797","162d958ff885f1462aeda91cd72582323fd6a1f4","b2aa686739bdf69c283597f0e5b6e73391db8b49","4e4813156056506a590aec735e1dd6fb4a7d7886","a538b05ebb01a40323997629e171c91aa28b8e2f","0b3cfbf79d50dae4a16584533227bb728e3522aa","0824049706fd13e6f7757af5e50ee6ca4e38506b","272216c1f097706721096669d85b2843c23fa77d","d984b580e02da76cd4d991953e6d430fadf3d578","1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba","6548fcc99e4da95793d39105e0e0865bbe9220e3","2c03df8b48bf3fa39054345bafabfeff15bfd11d","6c8b30f63f265c32e26d999aa1fef5286b8308ad","c28e21e4d28d7d8a2d90fde64b5ec422cd11d873","a8e8f3c8d4418c8d62e306538c9c1292635e9d27","96e1e2e165f33eab706c1ecdb48875251d64f7e1","74c487ca0d697a6903e54a25eb0ffd25367fd99e","1f78745806628110b3f36ee8dcffb3c719ec5b3b","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc"],"id":"4eca7aa4a96300caf8622d666ecf5635d8b72132","s2Url":"https://semanticscholar.org/paper/4eca7aa4a96300caf8622d666ecf5635d8b72132","authors":[{"name":"Terry Taewoong Um","ids":["2384501"]},{"name":"Vahid Babakeshizadeh","ids":["7688910"]},{"name":"Dana Kulic","ids":["1768765"]}],"doi":"10.1109/IROS.2017.8206051"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593897","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Research on legged robots has developed rapidly in the recent decades. One-legged robots, unlike multi-legged ones, have only one type of motion, called hopping. Hopping motion is generally divided into stance and flight phases. Switching between these two phases represents a hybrid dynamic model. Dynamic stabilization of hopping motion is a challenging control issue. Most of one-legged hopping robots studied in the past are able to hop with their one springy leg. In this paper, a novel one-legged robot is introduced and studied with two springs on the two sides. The one-legged somersaulting robot is able to hop with both springy sides. This ability causes lower energy consumption in passing obstacles and a longer step length in comparison with well-known SLIP robots with hopping motion stemming from the fact that it only has one rotary actuator. Fuzzy logic control is applied to achieve a stable limit cycle in the robot's somersaulting motion.","inCitations":[],"pmid":"","title":"Modeling and Fuzzy Control of One-legged Somersaulting Robot","journalPages":"2701-2706","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593897"],"entities":[],"journalVolume":"","outCitations":[],"id":"fdd295be12c1e35b6f2fa65d4268dc97262b9e17","s2Url":"https://semanticscholar.org/paper/fdd295be12c1e35b6f2fa65d4268dc97262b9e17","authors":[{"name":"Mehdi Zabihi","ids":[]},{"name":"Aria Alasty","ids":[]}],"doi":"10.1109/IROS.2018.8593897"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594449","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper strives to answer the following question: Is it possible to recognize an intersection when seen from different road segments that constitute the intersection? An intersection or a junction typically is a meeting point of three or four road segments. Its recognition from a road segment that is transverse to or 180 degrees apart from its previous sighting is an extremely challenging and yet a very relevant problem to be addressed from the point of view of both autonomous driving as well as loop detection. This paper formulates this as a problem of video recognition and proposes a novel LSTM based Siamese style deep network for video recognition. For what is indeed a challenging problem and the limited annotated dataset available we show competitive results of recognizing intersections when approached from diverse viewpoints or road segments. Specifically, we tabulate effective recognition accuracy even as the approaches to the intersection being compared are disparate both in terms of viewpoints and weather/illumination conditions. We show competitive results on both synthetic yet highly realistic data mined from the gaming platform GTA as well as on real world data made available through Mapillary.","inCitations":[],"pmid":"","title":"Towards View-Invariant Intersection Recognition from Videos using Deep Network Ensembles","journalPages":"1053-1060","s2PdfUrl":"","pdfUrls":["http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.bb45ae2eedb4ab19.49524f5331385f313039305f4d532e706466.pdf","https://doi.org/10.1109/IROS.2018.8594449"],"entities":[],"journalVolume":"","outCitations":["0b3cfbf79d50dae4a16584533227bb728e3522aa","448babca3ceca4a4f2b2ce1d2e87cd4ff43e337a","14318685b5959b51d0f1e3db34643eb2855dc6d9","2696a0565e21634b891fe3ae8ddc21399ec164f6","e66955e4a24b611c54f9e7f6b178e7cbaddd0fbb","12660f0defc6580e566c0fa2ac909971d6c6883b","bdc6acc8d11b9ef1e8f0fe2f0f41ce7b6f6a100a","7e5d66723839698190e0668aec18cbf9433f8378","4d9d25e67ebabbfc0acd63798f1a260cb2c8a9bd","536727ec6ecef383f1b5ddd7308835d48f45e046","013cd20c0eaffb9cab80875a43086e0c3224fe20","79828e6e9f137a583082b8b5a9dfce0c301989b8","127316fbe268c78c519ceb23d41100e86639418a","3cdb1364c3e66443e1c2182474d44b2fb01cd584","8a3b69f0d30d5f267701fb34bc6e49fe5e9a3954","1b7ef1a7df32d8d38efbb7aca9a9b2468d0da224","041a09a9db9a318596b72d041a832b05100b0ddf","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","ead7afd5ee175af8119fd63fd62c29a87c4afd21","39d7531b5077c01e1bb83b7d9551af617b8d5907","978e7f8557774fa1ebd228e181c4c154449423c9","9a45d32e3fa4c9fbfbe6fdcf4e209916fbdb6ebd","40be3888daa5c2e5af4d36ae22f690bcc8caf600","fd4413352062020f1f91e8904b5220d93a74493a","f25430bf769f43fbe8622309bc5a09b9dcc88ce3","4b8089bc9b49f84de43acc2eb8900035f7d492b2"],"id":"80d31c8a700afc8d42e94709ddde1b0474d3435f","s2Url":"https://semanticscholar.org/paper/80d31c8a700afc8d42e94709ddde1b0474d3435f","authors":[{"name":"Abhijeet Kumar","ids":[]},{"name":"Gunshi Gupta","ids":[]},{"name":"Avinash Sharma","ids":[]},{"name":"K. Madhava Krishna","ids":[]}],"doi":"10.1109/IROS.2018.8594449"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546194","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this work, we propose a new method for multi-person pose estimation which combines the traditional bottom-up and the top-down methods. Specifically, we perform the network feed-forwarding in a bottom-up manner, and then parse the poses with bounding box constraints in a top-down manner. In contrast to the previous top-down methods, our method is robust to bounding box shift and tightness. We extract features from an original image by a residual network and train the network to learn both the confidence maps of joints and the connection relationships between joints. During testing, the predicted confidence maps, the connection relationships and the bounding boxes are used to parse the poses of all persons. The experimental results showed that our method learns more accurate human poses especially in challenging situations and gains better time performance, compared with the bottom-up and the top-down methods.","inCitations":[],"pmid":"","title":"Bottom-up Pose Estimation of Multiple Person with Bounding Box Constraint","journalPages":"115-120","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546194","https://arxiv.org/pdf/1807.09972v1.pdf","http://arxiv.org/abs/1807.09972"],"entities":[],"journalVolume":"","outCitations":["56d3df5ce2ffb695728c091252087979be31f0c7","8a6ee59cda77eeb7e126e3bc3d82e742ae1b3e58","719da2a0ddd38e78151e1cb2db31703ea8b2e490","ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","cab372bc3824780cce20d9dd1c22d4df39ed081a","7d39d69b23424446f0400ef603b2e3e22d0309d6","3aea679168c72c6df7ead45d4f7f1fd7f3680a11","3ff40f0760bd8d3c46d72147b0f5b0d4aee2a24f","6801c8ea1fcb2f76799234a9a81c6199dd61b24c","25f5df29342a04936ba0d308b4d1b8245a7e8f5c","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","1550caab8d12c3f0ea19faaaa6bab3bdd092bafd","573c11e7e00389a033787984223ced536e15c904","ed173a39f4cd980eef319116b6ba39cec1b37c42","3f3a483402a3a2b800cf2c86506a37f6ef1a5332","272216c1f097706721096669d85b2843c23fa77d","5ad0e283c4c2aa7b9985012979835d0131fe73d8","722fcc35def20cfcca3ada76c8dd7a585d6de386","2c03df8b48bf3fa39054345bafabfeff15bfd11d","02a88a2f2765b17c9ea76fe13148b4b8a9050b95","0f2f4edb7599de34c97f680cf356943e57088345","061356704ec86334dbbc073985375fe13cd39088","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","87a66ccc68374ffb704ee6fb9fa7df369718095c","1cf53b650c4a3e212bd6f25e3c9fe8c757862a7d"],"id":"6950b44613b7a98e860d800fdbbe1693c6444729","s2Url":"https://semanticscholar.org/paper/6950b44613b7a98e860d800fdbbe1693c6444729","authors":[{"name":"Miaopeng Li","ids":["51152933"]},{"name":"Zimeng Zhou","ids":["2903484"]},{"name":"Jie Li","ids":["47786973"]},{"name":"Xinguo Liu","ids":["3227032"]}],"doi":"10.1109/ICPR.2018.8546194"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545519","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The problem of determining whether clusters are present in numerical data (tendency assessment) is an important first step of cluster analysis. One tool for cluster tendency assessment is the visual assessment of tendency (VAT) algorithm. VAT and improved VAT (iVAT) produce an image that provides visual evidence about the number of clusters to seek in the original dataset. These methods have been successful in determining potential cluster structure in various datasets, but they can be computationally expensive for datasets with a very large number of samples. A scalable version of iVAT called siVAT approximates iVAT images, but siVAT can be computationally expensive for big datasets. In this article, we introduce a modification of siVAT called siVAT+ which approximates cluster heat maps for large volumes of high dimensional data much more rapidly than siVAT. We compare siVAT+ with siVAT on six large, high dimensional datasets. Experimental results confirm that siVAT+ obtains images similar to siVAT images in a few seconds, and is 8 - 55 times faster than siVAT.","inCitations":[],"pmid":"","title":"Approximate Cluster Heat Maps of Large High-Dimensional Data","journalPages":"195-200","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545519"],"entities":[],"journalVolume":"","outCitations":["ad106f633b730b9f4c687235a1e02ef83d813840","0286a1efab096db135715ee99e6ce122174b9f3e","927e90295da4164645ed2d2fde682e4f86e37745","37508298d90cdf4d8a0ed71b500d3c764a4f0b97","e0792b3c5897d98b4bdb09943225029a82b326c2","08cc179ea02830c4438a2963613e0d7cbb77d140","169cc6caba38c8fe576a02ed2443c1987b12cecd","c5faa674df8fe81d17ec2537b865045c20d79990","e1afd3a0ead72c2fe76dcc5ce2df26c07650ebbf","143fd493b45f560d7be3d14db2318700cd23ee4e","1aae9dc04013b7561f97db9e14f410e1d02ee303","f2ed007f374fe8a6148cb52130a5dd2e8ec186c0","2730199df41370009c2967528982bb20db56968d","3d77a8f6f8ab5779e771512d52ebaa822555ee44","ef865d15a223899b5d094ec9c99356e3fab0d479","4955a2e2a9c13490341ba8dd373658065cea40b8"],"id":"61459d38e304c68415d0506db1237753a220b1fa","s2Url":"https://semanticscholar.org/paper/61459d38e304c68415d0506db1237753a220b1fa","authors":[{"name":"Punit Rathore","ids":["39540587"]},{"name":"James C. Bezdek","ids":["4929024"]},{"name":"Dheeraj Kumar","ids":["38992942"]},{"name":"Sutharshan Rajasegarar","ids":["1687334"]},{"name":"Marimuthu Palaniswami","ids":["26384381"]}],"doi":"10.1109/ICPR.2018.8545519"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545847","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The Automated Facial Expression Recognition (FER) in the wild is still a challenge problem. Currently, most of Deep Convolutional Neural Networks(DCNNs) based FER methods adopt softmax cross-entropy loss to encourage the separability of inter-class features. Many deep embedding approaches (e.g. contrastive loss, triplet loss, center loss) have been extended to the field of FER to enhance the discriminative ability of deep expression features and obtain the predictive effect. In this work, we present a novel deep embedding approach explicitly designed to respect the huge intra-class variation of expression features while learning discriminative expression features. We aim at forming a locally compact representation space structure through minimizing the distance between samples and their nearest subclass center. We demonstrate the effectiveness of this idea on RAF(Real-world Affective Faces) database. The experiment results show that our approaches can not only improve the classification performance but also adaptively learn a locally compact and expression intensity-aware feature space structure. We further extend our models to Static Facial Expressions in the Wild (SFEW) dataset and the results show the generalized ability of our approaches.","inCitations":[],"pmid":"","title":"Local Subclass Constraint for Facial Expression Recognition in the Wild","journalPages":"3132-3137","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545847"],"entities":[],"journalVolume":"","outCitations":["9ebfce7a20c19ca5233b83a28a5743354fb1ecda","081fb4e97d6bb357506d1b125153111b673cc128","e9c008d31da38d9eef67a28d2c77cb7daec941fb","daa7e0cafee740ba91fdba581dea4d273153802c","e18c9afdd56bd5a12666b60cbec7148378cffd0c","0f16f6f478b5c788dce466eb50e36c612273c36e","ad29afdbce160d3749a3685da75bc35b16d64b95","4cfd770ccecae1c0b4248bc800d7fd35c817bbbd","b5f3b0f45cf7f462a9c463a941e34e102a029506","2679e4f84c5e773cae31cef158eb358af475e22f","df3a9e4a3570704af677cdec25d13271fb9920c2","d36a1e4637618304c2093f72702dcdcc4dcd41d1","92527ace7f75188b5ec209ff7d59f431343075e4","d57982dc55dbed3d0f89589e319dc2d2bd598532","2c285dadfa6c07d392ee411d0213648a8a1cf68f","0d781b943bff6a3b62a79e2c8daf7f4d4d6431ad","ad0863aa16301d7c6617f8b965a64cf58f38594f","0fca9a022f4910dda7f8bdc92bbbe8a9c6e35303","0334a8862634988cc684dacd4279c5c0d03704da","eac2a468de0c4f241fad7696ca930bb2e11545b3","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","59b83666c1031c3f509f063b9963c7ad9781ca23","0359f7357ea8191206b9da45298902de9f054c92","8fe5feeaa72eddc62e7e65665c98e5cb0acffa87","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","19d583bf8c5533d1261ccdc068fdc3ef53b9ffb9","061356704ec86334dbbc073985375fe13cd39088","3661a34f302883c759b9fa2ce03de0c7173d2bb2"],"id":"7334987d38cc09b02d291de188426eb593e8f53c","s2Url":"https://semanticscholar.org/paper/7334987d38cc09b02d291de188426eb593e8f53c","authors":[{"name":"Zimeng Luo","ids":["46177846"]},{"name":"Jiani Hu","ids":["50779065"]},{"name":"Weihong Deng","ids":["1774956"]}],"doi":"10.1109/ICPR.2018.8545847"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593907","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Developing analytical models of efficient locomotion in biology is one of the most interesting goals in bio- inspired robotics. This paper presents a mathematical framework in order to model one of the most energy efficient locomotion types in flying animals; i.e., thermal soaring. Unlike the legged locomotion, in flying, modeling the environmental effects on animals' behaviors is very important. In doing so, we develop our model by assuming thermals as bubbles of rising air. According to pieces of real evidence, this kind of modeling is more compatible with the nature of thermal soaring. Moreover, we present a simple hybrid control strategy for obtaining the optimal path in order to maximize benefit from the updraft of air-flow. By using this control strategy, the flying robot can plan a path for traveling between thermals without flapping; i.e., energy efficient flying. So as to investigate the compatibility of presented model and controller with reality, we set their parameters based on the biological evidences. As a result, in simulations, it is observed that the generated flying behavior is comparable with the thermal soaring behavior of real birds. This observation provides a confirmation for generality and applicability of the presented approach.","inCitations":[],"pmid":"","title":"Analytical Model of Thermal Soaring: Towards Energy Efficient Path Planning for Flying Robots","journalPages":"7589-7594","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593907"],"entities":[],"journalVolume":"","outCitations":[],"id":"a56325fe81a2c75eee83901c2a8aa55ccfa40905","s2Url":"https://semanticscholar.org/paper/a56325fe81a2c75eee83901c2a8aa55ccfa40905","authors":[{"name":"Javad Khaghani","ids":[]},{"name":"Mahdiar Nekoui","ids":[]},{"name":"Rezvan Nasiri","ids":[]},{"name":"Majid Nili Ahmadabadi","ids":[]}],"doi":"10.1109/IROS.2018.8593907"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594135","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Skin impedance should be minimized to obtain reliable and precise surface electromyography (sEMG) signals. High skin impedance decreases sensitivity to muscular activation and makes sEMG signals vulnerable to external noise. Microneedle-based electrodes have been proposed to achieve low skin impedance and high spatial resolution. However, unstable skin contact can occur during dynamic motion due to the electrodes small contact area, and the signal is easily influenced by motion artifacts. In this study, a pneumatic microneedle-based high-density sEMG sleeve is proposed that guarantees stable sEMG signal measurement by pressing the electrodes to the skin. Pneumatic air control, sEMG signal processing, and wireless signal transmission are processed in a single processor. The proposed interface automatically controls the air volume according to sEMG signal quality and is comfortable for users. The usability of the proposed interface was compared to conventional Velcro armbands by examining acquired sEMG signals. The results indicated that the proposed pneumatic sleeve guarantees reliable sEMG signal measurement during dynamic motion. Additionally, optimal pneumatic pressure and needle length were investigated.","inCitations":[],"pmid":"","title":"Pneumatic Microneedle-Based High-Density sEMG Sleeve for Stable and Comfortable Skin Contact During Dynamic Motion","journalPages":"5477-5482","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594135"],"entities":[],"journalVolume":"","outCitations":[],"id":"595c2942f4e8bc6437c0df4952eb500ead405da3","s2Url":"https://semanticscholar.org/paper/595c2942f4e8bc6437c0df4952eb500ead405da3","authors":[{"name":"Minjae Kim","ids":[]},{"name":"Gangyong Gu","ids":[]},{"name":"Wan Kyun Chung","ids":[]}],"doi":"10.1109/IROS.2018.8594135"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202294","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"To leverage soft hands to their full potential for grasping, we propose to design their morphology and control signals together. Considering both parameter domains makes it easier and faster to find solutions compared to fixing parameters of either domain. Additionally, the approach scales well to high-dimensional parameter spaces, which is a precondition to make automated co-design useful for soft hands. We further present an efficient simulator for simulating grasps with soft hands which is based on the SOFA framework and enables us to simulate more than a million grasps per day. These two complementary improvements promise a boost in the development of competent soft hands and their control in the future.","inCitations":["62f9d259e00f1ace4c9cef4dcbb323314bd27ac9"],"pmid":"","title":"Automated co-design of soft hand morphology and control strategy for grasping","journalPages":"1213-1218","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202294","http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen_pdf/Deimel-Irmisch-Wall-17-IROS_final.pdf"],"entities":["Computation","Control theory","Galaxy morphological classification","General-purpose modeling","Mathematical optimization","Precondition","SOFA","Simulation","Time complexity","Virtual fixture"],"journalVolume":"","outCitations":["0739d8f1c1c9200ed7baf373143d3408c4bbbf1c","058771e6016c27309344069482c8b00e693cf3c3","62cb859b18e264aa752b0b49d06d883573419a19","aad63668f6622fb0bad31b84f2452af3e3661167","40975d415b39697ab866ce6bd061050629c96fd0","24fbef243eea168e36bfe1d5cdc91f37af0585f5","509d6cdec122bbbdca565006fc8b0059d946331f","d4cbb08752810e275c0007015b0ef474536dfa3b","3be7fe3f13cfd568330abb02e2842ff4bab809d7","5eac420c4e617961b23bbbf14efe109e4b47a9d1","85e1dde9b27335a353208dc193b5c1382edd0cfa","03a283ca72dd7fb9f544e3a049efba6bf4203520","257c8f64686893a5d368312d992309f0093f6003","995a91b1064c6fa901ee3e20356cdcc2f9d53a3c","0d39812dbf19de44a33f2caac7bec8c0f79074fb","2df913118b898f13de4f5da4d78274489b7e8142","41f2eb4d6f661e0ce9c5061c62484b68afd6b1c6","bae62d61a6af1075686c46e21dfb149b5873a48a","0772ffbd1958f0a51fdac21ff5a4a48a274e140f","1fe16f51e74180988f9ed1a9f0a03000bf839d8e","5ef3555a83a7b0ef00e1e2a47a2d41ae2a900517","69e61752710afb972dc22a7385657d2f35397ed5","183357d25d6bad84e16bdb8db8f57ad616f3a9ce"],"id":"8c0215be1a1fddb50cff356ec685f432f9d85a93","s2Url":"https://semanticscholar.org/paper/8c0215be1a1fddb50cff356ec685f432f9d85a93","authors":[{"name":"Raphael Deimel","ids":["1833463"]},{"name":"Patrick Irmisch","ids":["31726657"]},{"name":"Vincent Wall","ids":["12036080"]},{"name":"Oliver Brock","ids":["1717724"]}],"doi":"10.1109/IROS.2017.8202294"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594115","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a method and associate platform to couple model-based system engineering and safety analysis at the early phases of robotic system (RS) life-cycle. The method is compatible with IEC12100 and ISO13482. The platform is based on Papyrus UML modeler and supports RobotML, a domain specific language for RSs, as well as tools for safety analysis and risk assessment, Sophia and Safety Architect. It includes an ability (a) to model architecture of RSs; (b) to automatically run safety analysis (e.g. failure mode and effects analysis, fault tree analysis, etc.); (c) to save and reuse safety artefacts; (d) to represent safety analysis results in the modeling environment. We illustrate the proposed method by considering a humanoid personal care robot from SoftBank Robotics developed in the scope of the ROMEO2 project.","inCitations":[],"pmid":"","title":"Model-Based Engineering, Safety Analysis and Risk Assessment for Personal Care Robots","journalPages":"6136-6141","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594115"],"entities":[],"journalVolume":"","outCitations":[],"id":"a8a9a32439621dd4192a06187d2071d7454cd4ed","s2Url":"https://semanticscholar.org/paper/a8a9a32439621dd4192a06187d2071d7454cd4ed","authors":[{"name":"Nataliya Yakymets","ids":[]},{"name":"M. Sango","ids":[]},{"name":"S. Dhouib","ids":[]},{"name":"R. Gelin","ids":[]}],"doi":"10.1109/IROS.2018.8594115"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594417","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We study the labeled multi-robot path planning problem in continuous 2D and 3D domains in the absence of obstacles where robots must not collide with each other. For an arbitrary number of robots in arbitrary initial and goal arrangements, we derive a polynomial time, complete algorithm that produces solutions with constant-factor optimality guarantees on both makespan and distance optimality, in expectation, under the assumption that the robot labels are uniformly randomly distributed. Our algorithm only requires a small constant-factor expansion of the initial and goal configuration footprints for solving the problem, i.e., the problem can be solved in a fairly small bounded region. Beside theoretical guarantees, we present a thorough computational evaluation of the proposed solution. In addition to the baseline implementation, adapting an effective (but non-polynomial time) routing subroutine, we also provide a highly efficient implementation that quickly computes near-optimal solutions. Hardware experiments on the microMVP platform composed of non-holonomic robots confirms the practical applicability of our algorithmic pipeline.","inCitations":[],"pmid":"","title":"SEAR: A Polynomial- Time Multi-Robot Path Planning Algorithm with Expected Constant-Factor Optimality Guarantee","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1709.08215v2.pdf","https://doi.org/10.1109/IROS.2018.8594417"],"entities":[],"journalVolume":"","outCitations":["3b22ce4c3243a0fc2ff68d91c7e1ae085f276452","5cc6b72cacb0eaeb7d8195986a48ee4f0e483fa1","57b471e6e53d778b84de5e0716793b97f8055599","d46a3efd1b85c11cc9376fc8b48388eae5221d2b","b515b83e2309962c4b77a3ac6de112d642f8389e","38f89f55196dbcfdc4f8aef3d19bc6271b190744","1f68714344e542a245beea9b97265fd7a3c07f82","c07238579a95c424707dbe855efba189cce68650","8f4cb9bd34ced84c14889740ec4c7fa0c6f5d3e8","ac32830c1bde98dcf160c06d7c33843601542925","146061be1affd4af17b8996f1d0316ad147368f5","f65a611e6c83df9c378fa96f75d98f2c70e50a05","70a0e514cb21d3229f0ec5f55d237efbc4ddf8cd","7f5d47f5b2a83dc281fe28149c47855368f13726","6ea7553e9b3f428f9b6286d65aaa6f8a807f25ea","7d4b2043018dd9f25bdfc7e4eba4449127a8341b","9e82d358ebd659fa6eb267f5a4ee1864967d5615","1830358582e969220158dae2df3ead17239a10db","546809d5586c495f6e8787804c003bbde8dd14ca","0acdf6cf0d09b6886bf40bf7ceda4c8e140023f6","9c1781be228e56b64d347bd32ad378b633321deb","be5b06fc4e97fe25112b1407102ae594fae5ed38","3654bcfc6bdd7055a68a08675149d5aee4038f8f","e3c51035cf2305845f6e8a763540fcb07c9e9e0a","ec9dc40d4533ac842b7d50d4549cf21461bdc1f8","ef1b44dce67b0c5d5b839cabd83a6f532dfa6bb6","34fc3f39898778147f8c30997edd545afa6454df","a4ab36dbb8ecf8db18998afa8b4f7943a97e24ba","aa37b47a30e3d24f15a111af0e8a5f2d66e6b3c1","5a70a7f2df2dc80b9f18f4b9d3aae431576bdcbc","2c59ab06207383ac5cab3e98eb23f5271cf3663c","9bd4c082be3bb194aea8157f2ba647fb46ba5aa9","a77e043b5c8f39091ca450db267dbf982016dd3d","19e64814c5ab27a471e45377469903f4d3c42b12","4e5111a72568b5274a926ff23520a30620ccc0a2","45b1f6b683fdc5f8d5a340a7ba203c574c1d93a5","27db3dd0c56fc5664651b5c2eeacc4bf5f525f17","699e1ae106fc54b97fe16a8edf9d99fbf3bb65f0","09fd0a3fe56988bec5ed9cd7099a9f0e3d6a9609","4ab3e778f78229366107d408357e9ff8ed4b140b","516dc9b41150445746f8223f6c4fe36b59bdd94a","5c64f9199855c8a07eb52edf22dd5ea9b35d9d97","21f66f0794efc8ff2a2096560cafdb3425962095","c18417566c95eb8edce83cfffea02860e44f41e5","3227556d2ab6177022c2887f8889fc8c3b83550e","1da1d0238aae185af623a06e52e2ff293a97dccb"],"id":"297ca128d96161d482a0958d32dfe61b0e8bf3c1","s2Url":"https://semanticscholar.org/paper/297ca128d96161d482a0958d32dfe61b0e8bf3c1","authors":[{"name":"Shuai D. Han","ids":["36066651"]},{"name":"Edgar J. Rodriguez","ids":["48424692"]},{"name":"Jingjin Yu","ids":["35855037"]}],"doi":"10.1109/IROS.2018.8594417"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205983","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We investigate tethered flight of a small quadrotor robot in the context of creating stippled prints. At a low level, we use motion capture to measure the position of the robot and the canvas, and a robust control algorithm to command the robot to fly to different stipple positions to make contact with the canvas using an ink soaked sponge. With the objective of fully autonomous flight, we power our quadrotor using a wired tether. We compensate for the tether in our control of the robot by assuming a static catenary curve of fixed length between the robot and the power source, and model the forces and torques produced. We evaluate accuracy of hovering and flight on simple paths, and compare the results to untethered flight.","inCitations":["55ef6ba6c2f6222471407c1536edd520b0d908a2"],"pmid":"","title":"Tethered flight control of a small quadrotor robot for stippling","journalPages":"1713-1718","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205983","http://www.cs.mcgill.ca/~kry/pubs/iros17/iros17.pdf"],"entities":["Algorithm","Autonomous robot","Experiment","Motion capture","Printing","Robotics","Robust control"],"journalVolume":"","outCitations":["2555bfa76346d8fa7a223fa138e833c2e9efedd4","75ea8cce2b90247168f910890cf7a08d8ac01238","bb8eac73fdd5e94eb62c84a0355382f1c75e9b6d","803c2f0380ff1af76d92731927173fac657b48ee","3b94d7407db24e630e5cc4e886f16755a76bd583","c35be0c4d48164f7f238c3cb39aab8224b6f2ddc","15928cb593865a647586719a6539b29692fe2e4c","5118a15588e670031f40b72f418e21a8f47d8727","ecf58e0c956c3da371dbf5da40fd421e4274d446","2b5a51619a8293c2a6fa6574d555922235e980e2","80c3f33f08c56789f9aa9050e5b8ddadec52a2e6","158a08fe813ac13b75a1b8b538989a3abbfe23c7","146fee1f7c4fa3233be28efcb6da50683b37adba","dbd561d5218fc7e41d955467dd0aea7a0ff23734","18c0b0e3071bf0029383ef8e34ef75edc7c3a606"],"id":"450b5152294e38e39dcb2ecb3b896901978ab23a","s2Url":"https://semanticscholar.org/paper/450b5152294e38e39dcb2ecb3b896901978ab23a","authors":[{"name":"Brendan Galea","ids":["16075262"]},{"name":"Paul G. Kry","ids":["1970147"]}],"doi":"10.1109/IROS.2017.8205983"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206377","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The human forearm is composed of two long, thin bones called the radius and the ulna, and rotates using two axle joints. We aimed to develop a forearm based on the body proportion, weight ratio, muscle arrangement, and joint performance of the human body in order to bring out its benefits. For this, we need to miniaturize the muscle modules. To approach this task, we arranged two muscle motors inside one muscle module, and used the space effectively by utilizing common parts. In addition, we enabled the muscle module to also be used as the bone structure. Moreover, we used miniature motors and developed a way to dissipate the motor heat to the bone structure. Through these approaches, we succeeded in developing a forearm with a radioulnar joint based on the body proportion, weight ratio, muscle arrangement, and joint performance of the human body, while keeping maintainability and reliability. Also, we performed some motions such as soldering, opening a book, turning a screw, and badminton swinging using the benefits of the radioulnar structure, which have not been discussed before, and verified that Kengoro can realize skillful motions using the radioulnar joint like a human.","inCitations":[],"pmid":"","title":"Human mimetic forearm design with radioulnar joint using miniature bone-muscle modules and its applications","journalPages":"4956-4962","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206377"],"entities":["Correctness (computer science)","Experiment","Soldering","Tendon-driven robot"],"journalVolume":"","outCitations":["2d34c1d33a3c293934bd4e85344c947a96bf8e61","8409bab208799e5945bffd88e3a9e43e65bc49f9","3162c0bb2b7bbedd2c3db0f57ac833e8b834ace4","afba83157286c9892a8715511b6b9d0ce99ac506","2602e66bf56ef5cf5337d59850ceb41bab11b9e3","40c9827032408670f99c6d1096f3219e02079c39","f7d140b9d95f6544180710f7396ab8b603c1943a","8d70b9a319b2d0bbc31e952b2b83e5b9a831e452","d5e21f87dd6d9422fa4c6ebd81539d524f212cec","d4319e6c668b97bcd3d2698ce1bf91a5a8d3e340","5a7569406fe2b2683d3f9060dd2a24a0abe7f857","716ad7fa261ba4af9bf98f5a586efeb4bf57b2e8","91e00abaee60fd39bf9317f94338864fc9701666","04bc1a7ce8ea0941c5dd43cb9d9b84b9f1ecaaf2"],"id":"4a1bf0929b0e1f5258834693893c754d9d2c2e3f","s2Url":"https://semanticscholar.org/paper/4a1bf0929b0e1f5258834693893c754d9d2c2e3f","authors":[{"name":"Kento Kawaharazuka","ids":["8308607"]},{"name":"Shogo Makino","ids":["49641819"]},{"name":"Masaya Kawamura","ids":["7431435"]},{"name":"Yuki Asano","ids":["1739828"]},{"name":"Youhei Kakiuchi","ids":["2271736"]},{"name":"Kei Okada","ids":["1683608"]},{"name":"Masayuki Inaba","ids":["1749935"]}],"doi":"10.1109/IROS.2017.8206377"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594222","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Legged-wheeled robots combine the advantages of efficient wheeled mobility with the adaptability to real-world terrains through the legged locomotion. Due to this hybrid mobility skill, they can excel in many application scenarios where other mobile platforms are not suitable for. However, their versatile mobility increases the number of constraints in their motion control where both the properties of legged and wheeled systems need to be considered. Relevant schemes for legged-wheeled platforms so far have been developed exploiting separate motion control of the wheeled and legged functionalities. This paper discusses the legged-wheeled motion kinematics without constraining the camber angles of the wheels, and it proposes a first-order inverse kinematics scheme that stabilizes the legged-wheeled system in the wheeled motion. Furthermore, the work adopts a floating base model that allows to easily incorporate the legged motion to the scheme. The developed controller is tested in simulation and experiments on a legged-wheeled centaur-like robot - CENTAURO.","inCitations":[],"pmid":"","title":"On the Kinematics of Wheeled Motion Control of a Hybrid Wheeled-Legged CENTAURO robot","journalPages":"2426-2433","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594222"],"entities":[],"journalVolume":"","outCitations":[],"id":"278de92eef8a59fd2619376cc8c3e565d6e890f4","s2Url":"https://semanticscholar.org/paper/278de92eef8a59fd2619376cc8c3e565d6e890f4","authors":[{"name":"Malgorzata Kamedula","ids":[]},{"name":"Navvab Kashiri","ids":[]},{"name":"Nikolaos G. Tsagarakis","ids":[]}],"doi":"10.1109/IROS.2018.8594222"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594168","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a novel architecture and a self-supervised policy gradient algorithm, which employs unsupervised auxiliary tasks to enable a mobile robot to learn how to navigate to a given goal. The dependency on the global information is eliminated by providing only sparse range-finder measurements to the robot. The partially observable planning problem is addressed by splitting it into a hierarchical process. We use convolutional networks to plan locally, and a differentiable memory to provide information about past time steps in the trajectory. These modules, combined in our network architecture, produce globally consistent plans. The sparse reward problem is mitigated by our modified policy gradient algorithm. We model the robots uncertainty with unsupervised tasks to force exploration. The novel architecture we propose with the modified version of the policy gradient algorithm allows our robot to reach the goal in a sample efficient manner, which is orders of magnitude faster than the current state of the art policy gradient algorithm. Simulation and experimental results are provided to validate the proposed approach.","inCitations":[],"pmid":"","title":"Learning Sample-Efficient Target Reaching for Mobile Robots","journalPages":"3080-3087","s2PdfUrl":"","pdfUrls":["https://export.arxiv.org/pdf/1803.01846","http://arxiv.org/abs/1803.01846","https://arxiv.org/pdf/1803.01846v1.pdf","https://doi.org/10.1109/IROS.2018.8594168"],"entities":[],"journalVolume":"","outCitations":["620412c443e85a1fc2f9ab950ddfada8d18d63b4","0233f4e12db890dac1d06b5593c3ae7205d721a6","0b34bced44cefeadd595d72c2a4c3ba4112d92fa","6285c6317d8de5caae96ac59d1204367ac5d6e40","0223de3e9a23f574d188baa1101a82df352d428c","024f01f390ba94cbee81e82e979a65151d20a6fd","74a8fccef12927b80ea3146798044cefaaf91be0","1407ae8725ccef768d634b2f6ec2baa2197b0bbb","dd90dee12840f4e700d8146fb111dbc863a938ad","7bc9ba75c75c190c3ccb4f6899b0efd31cb0266c","272216c1f097706721096669d85b2843c23fa77d","81d059cd86fb6dd3ced0cf2be8e63c1c07cb83d0","3ee01ec27e4e66e089b72a9989724be611c2ad90","784ee73d5363c711118f784428d1ab89f019daa5","667fb84bfca10bec165f9e2cca3e21f5e4829ca7","964e452cb1e9c0e97c47d37e82c5211854f4acc4","5129a9cbb6de3c6579f6a7d974394d392ac29829","6078a0ef43084bde3b2a9e57578e99bc06d7ebf3"],"id":"4d694fd2d82c05eabdc5d0e3cd963e5a83fc11b8","s2Url":"https://semanticscholar.org/paper/4d694fd2d82c05eabdc5d0e3cd963e5a83fc11b8","authors":[{"name":"Arbaaz Khan","ids":["3405021"]},{"name":"Vijay Kumar","ids":["39806219"]},{"name":"Alejandro R. Ribeiro","ids":["1710399"]}],"doi":"10.1109/IROS.2018.8594168"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594374","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we present the system design and initial testing of SUPERball v2, a completely re-designed 2-meter spherical six-bar tensegrity robot designed to survive high-speed landings as well as locomote to desired locations. SUPERball v2 was designed to enable a host of new actuation and experimentation. The prototype features a fully actuated six-bar design (24 actuators), compliant nylon cables (up to 15% stretch), torque-control enabled motors, and a robust mechanical structure capable of surviving impact velocities upwards of 8 m/s.","inCitations":["ee62fa9cb93c86e9797b9d48e22675dd33e0d769"],"pmid":"","title":"Design of SUPERball v2, a Compliant Tensegrity Robot for Absorbing Large Impacts","journalPages":"2865-2871","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594374"],"entities":[],"journalVolume":"","outCitations":[],"id":"2b939acdbd4b54e876e1eb2e70a9bced75506e8a","s2Url":"https://semanticscholar.org/paper/2b939acdbd4b54e876e1eb2e70a9bced75506e8a","authors":[{"name":"Massimo Vespignani","ids":[]},{"name":"Jeffrey M. Friesen","ids":[]},{"name":"Vytas SunSpiral","ids":[]},{"name":"Jonathan Bruce","ids":[]}],"doi":"10.1109/IROS.2018.8594374"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594502","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Many scenarios require a robot to be able to explore its 3D environment online without human supervision. This is especially relevant for inspection tasks and search and rescue missions. To solve this high-dimensional path planning problem, sampling-based exploration algorithms have proven successful. However, these do not necessarily scale well to larger environments or spaces with narrow openings. This paper presents a 3D exploration planner based on the principles of Next-Best Views (NBVs). In this approach, a Micro-Aerial Vehicle (MAV)equipped with a limited field-of-view depth sensor randomly samples its configuration space to find promising future viewpoints. In order to obtain high sampling efficiency, our planner maintains and uses a history of visited places, and locally optimizes the robot's orientation with respect to unobserved space. We evaluate our method in several simulated scenarios, and compare it against a state-of-the-art exploration algorithm. The experiments show substantial improvements in exploration time (2 ⨯ faster), computation time, and path length, and advantages in handling difficult situations such as escaping dead-ends (up to 20 ⨯ faster). Finally, we validate the on-line capability of our algorithm on a computational constrained real world MAV.","inCitations":[],"pmid":"","title":"History-Aware Autonomous Exploration in Confined Environments Using MAVs","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1803.10558v1.pdf","http://arxiv.org/abs/1803.10558","https://doi.org/10.1109/IROS.2018.8594502"],"entities":[],"journalVolume":"","outCitations":["a9715f91079dc18c9207c388a70198691f2094ff","3728d80356be3278236743e9481d31f612836579","c3a1864dbb342c6f247daa54c51e06d7dcf29132","0b2d197f90f3bce607a4465ea7312995f024a1a0","2cc85de6c80639d3204ae0d4f247296cf6d9999d","54584fa817611e9b24492eff04b81a0e834258e1","37dd210c4f6b43940338ff401848f43ae0018af7","14e47f3ba1890b4589794048594c200d82a74991","ec298a4e558c7f25e5ec2ba382eb776e01f68dc7","f7b27db11e1043c7e2a3f5a9a79f239fc1da53c1","a9bdaae018bb29a7a838947ba0015553b7dd3ea1","5e3b383ec7970262425dcc2d1e7d710c52fa7b9c","0da29d4783bd5ac4170bd329cb084782159da805","32b768f0c6fe28a4072df6d6340c88c4aad6b02a","2376078d13761387cabb933798b93a706c2ea7ef","7216ed68677549dca19164b142d8570c19c2d5df","e28c8a6c397a48775cc10aa53adc003309af2fd8","67ac0bc9689cfc73b7c3f1b93669f41fc288ad01","7aa52c13d0090b8f3001837e39c8da59272b8fcc","0d0aacd45db148d4e37b9c58b9f20518bd238aa3","2d2a2683ca69a81ab1c4d1a271c8a54195f494bc","3108106f67863914aacbb74b75208f63f0e0dfdb","4f23eb7390a729e2b48dfd505c05ce2625ae7ac9","b9bc9a32791dba1fc85bb9d4bfb9c52e6f052d2e","424e843a92db6731f49df216d2bc9c4fa917186c","05c9dc44595d35163b50109e2b1e14bb863124cf","5a1791a0a31f2ae051926ebcabd324286dec7105","c9a00047bec86649658df1e114ffcdc2875dbd0a","22bc5485272f5300fb0e8064139232608e0ec2ab","d591682c056fe9d082a180c14324ee46e5a618f0","9f85c8f83c835b997f4c613525836f6b53cfdb7f"],"id":"29e247d8251fdb575d249fa50b8c10b51fd7af8d","s2Url":"https://semanticscholar.org/paper/29e247d8251fdb575d249fa50b8c10b51fd7af8d","authors":[{"name":"Christian Witting","ids":["47846381"]},{"name":"Marius Fehr","ids":["3414373"]},{"name":"Rik Bähnemann","ids":["7908479"]},{"name":"Helen Oleynikova","ids":["1880747"]},{"name":"Roland Siegwart","ids":["1720483"]}],"doi":"10.1109/IROS.2018.8594502"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546162","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Intrusion detection systems (IDS) play an important role in the protection of network operations and services. In this paper, we propose an effective network intrusion detection scheme based on deep learning techniques. The proposed scheme employs a denoising autoencoder (DAE) with a weighted loss function for feature selection, which determines a limited number of important features for intrusion detection to reduce feature dimensionality. The selected data is then classified by a compact multilayer perceptron (MLP) for intrusion identification. Extensive experiments are conducted on the UNSW-NB dataset to demonstrate the effectiveness of the proposed scheme. With a small feature selection ratio of 5.9%, the proposed scheme is still able to achieve a superior performance in terms of different evaluation criteria. The strategic selection of a reduced set of features yields satisfactory detection performance with low memory and computing power requirements, making the proposed scheme a promising solution to intrusion detection in high-speed networks.","inCitations":[],"pmid":"","title":"An Effective Deep Learning Based Scheme for Network Intrusion Detection","journalPages":"682-687","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546162"],"entities":[],"journalVolume":"","outCitations":["a9d7904a78d6c4df31e003c9df55c78a9ca485de","2831f4c732ae55b87685a6acb7ebd1188f82b4c2","7c2cb0272af5817cd58fb1707d6e5833c851d72b","480205cdc0b039764478a27b8accdcabe1e1e108","02cd1e1a82a1da73f7302193ce19b0e19f3f7501","85dacb2fa8093e2d8a792c6f53c3429cbbdf5606","e145346143b237d49ba7ecc61b4b7f252ce1b7f5","0ef9ae1ce8c91ce671a211bdda792bf3752d1522","652537f9d1c0f41b185fecd0f218a0e37729474f","2a56ca15bea4f9a911835bfd08a2f4526091d785","776b94ead40a97d984fd1d5ae93b0e9b57c3bd13","9f78e6e81a20d96e8c07cbf3d1cc111b410ff14a","1605d856f4502c2ddd234b8c61bbbe1fc22af3ef","19e079593df4a2958574313063917f6b9ef72bae","24aa99e0be30dd6c2cb79351efe06a6264816310","25deaf0adeb9ea455a2f2d211ee86890eb64b69b","ff1e7c85682e1bba92b44eb82c2dd40571b8b56e","f7df2f143a707020819eaf40e4b2083ac1b83d59","443b6e8fdb315d577b4f09f560b5a792d9745ad3","985d0a5fddfcc23edcc3c332387afbe834bdd270","4db2f23a5ad2259b90aa9b0545d27cd44337515a","09671d4c8205b765335e8a9a47a4e2917f9667a1","5b94feaed7f4eeb18283e657410aeb12faa35f94","2dbdb256c1c8df1dba9451b65b2586dfab9cb9dd","521120c3907677e17708c17c5b6bab9087e61c5b","0e7af8e91b8cb2cea1164be5ac5d280b0d12c153","33987e58b45755a6120905ce582fc9cbec434763","2c9f461af0159f65ac642109d9cbd8d673e0acd7","9d921586592ac8ede9251da705ffffeb3f506723"],"id":"976c3afebc60be9bf112ec626220eb5b9878f361","s2Url":"https://semanticscholar.org/paper/976c3afebc60be9bf112ec626220eb5b9878f361","authors":[{"name":"Hongpo Zhang","ids":["40940523"]},{"name":"Chase Qishi Wu","ids":["2069412"]},{"name":"Shan Gao","ids":["47357428"]},{"name":"Zongmin Wang","ids":["3109824"]},{"name":"Yuxiao Xu","ids":["3432163"]},{"name":"Yongpeng Liu","ids":["2968263"]}],"doi":"10.1109/ICPR.2018.8546162"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00053","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Facial attribute recognition is conventionally computed from a single image. In practice, each subject may have multiple face images. Taking the eye size as an example, it should not change, but it may have different estimation in multiple images, which would make a negative impact on face recognition. Thus, how to compute these attributes corresponding to each subject rather than each single image is a profound work. To address this question, we deploy deep training for facial attributes prediction, and we explore the inconsistency issue among the attributes computed from each single image. Then, we develop two approaches to address the inconsistency issue. Experimental results show that the proposed methods can handle facial attribute estimation on either multiple still images or video frames, and can correct the incorrectly annotated labels. The experiments are conducted on two large public databases with annotations of facial attributes.","inCitations":[],"pmid":"","title":"Attributes in Multiple Facial Images","journalPages":"318-324","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00053","http://arxiv.org/abs/1805.09203","https://arxiv.org/pdf/1805.09203v1.pdf","http://export.arxiv.org/pdf/1805.09203"],"entities":[],"journalVolume":"","outCitations":["7eb8a3b55bb02867b959461e3577a2d6f22fda49","d8a99802f0606063a7b55be4e898f2a0ab8f5264","14ce7635ff18318e7094417d0f92acbec6669f1c","b8084d5e193633462e56f897f3d81b2832b72dff","346c9100b2fab35b162d7779002c974da5f069ee","4ed6c7740ba93d75345397ef043f35c0562fb0fd","0ad4a814b30e096ad0e027e458981f812c835aa0","a34d75da87525d1192bda240b7675349ee85c123","6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","8a3eaaef13bdaee26142fd2784de07e1d24926ca","3312eb79e025b885afe986be8189446ba356a507","759a3b3821d9f0e08e0b0a62c8b693230afc3f8d","217de4ff802d4904d3f90d2e24a29371307942fe","90f07d2d5c7164d46b70fc0edeacd77ab172787b","8b8728edc536020bc4871dc66b26a191f6658f7c","931282732f0be57f7fb895238e94bdda00a52cad","19d583bf8c5533d1261ccdc068fdc3ef53b9ffb9","061356704ec86334dbbc073985375fe13cd39088","0e986f51fe45b00633de9fd0c94d082d2be51406","0626908dd710b91aece1a81f4ca0635f23fc47f3","2c03df8b48bf3fa39054345bafabfeff15bfd11d","14318685b5959b51d0f1e3db34643eb2855dc6d9","9458c518a6e2d40fb1d6ca1066d6a0c73e1d6b73","70f189798c8b9f2b31c8b5566a5cf3107050b349"],"id":"e8ab6ed88fecfdc936c44bc7a73e00a1eaf17f63","s2Url":"https://semanticscholar.org/paper/e8ab6ed88fecfdc936c44bc7a73e00a1eaf17f63","authors":[{"name":"Xudong Liu","ids":["1767347"]},{"name":"Guodong Guo","ids":["1822413"]}],"doi":"10.1109/FG.2018.00053"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593607","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In Minimally Invasive Robotic Surgery (MIRS) a robot is interposed between the surgeon and the surgical site to increase the precision, dexterity, and to reduce surgeon's effort and cognitive load with respect to the standard laparoscopic interventions. However, the modern robotic systems for MIRS are still based on the traditional telemanipulation paradigm, e.g. the robot behaviour is fully under surgeon's control, and no autonomy or assistance is implemented. In this work, supervised and shared controllers have been developed in a vision-free, human-in-the-Ioop, control framework to help surgeon during a surgical suturing procedure. Experiments conducted on the da Vinci Research Kit robot proves the effectiveness of the method indicating also the guidelines for improving results.","inCitations":[],"pmid":"","title":"A Comparison of Assistive Methods for Suturing in MIRS","journalPages":"4389-4395","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593607"],"entities":[],"journalVolume":"","outCitations":[],"id":"354efa12f13fb4e78916e1519e4cd7276d7238cc","s2Url":"https://semanticscholar.org/paper/354efa12f13fb4e78916e1519e4cd7276d7238cc","authors":[{"name":"Giuseppe Andrea Fontanelli","ids":[]},{"name":"Guang-Zhong Yang","ids":[]},{"name":"Bruno Siciliano","ids":[]}],"doi":"10.1109/IROS.2018.8593607"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594071","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Autonomous elevator operation is considered an intelligent solution in handling the inter-floor navigation problem of service robots. As one of the most fundamental steps, elevator button recognition starts to receive more and more attention. However, due to the challenging image conditions and severe class imbalance problem, the performance of existing results is unsatisfying. In this paper, we propose to combine an optical character recognition (OCR) network and the Faster RCNN architecture into a single neural network, called OCR-RCNN to facilitate an end-to-end training and elevator button recognition procedure. To verify our method, we collect a large dataset of elevator panels and carry out extensive comparative experiments. The experiment results show that our method can greatly outperform the traditional recognition pipelines, yielding an accurate and robust performance on recognizing untrained elevator buttons.","inCitations":[],"pmid":"","title":"A Novel OCR-RCNN for Elevator Button Recognition","journalPages":"3626-3631","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594071"],"entities":[],"journalVolume":"","outCitations":[],"id":"187a4ff379638ac8c2d35ac4ed9b9fe64e58af1f","s2Url":"https://semanticscholar.org/paper/187a4ff379638ac8c2d35ac4ed9b9fe64e58af1f","authors":[{"name":"Delong Zhu","ids":[]},{"name":"Tingguang Li","ids":[]},{"name":"Danny Ho","ids":[]},{"name":"Tong Zhou","ids":[]},{"name":"Max Q.-H. Meng","ids":[]}],"doi":"10.1109/IROS.2018.8594071"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594430","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The robustness of the positioning and posturing of robot hands relative to target object and support surface is an important issue for autonomous grasping. For example, to perform a grasping action such as picking up thin objects from a table top, the position and posture of the hand must be controlled to keep adequate relative posture and distance to the support surface besides those between the hand and the target object. Because slight errors in the posture and position are enough to cause grasping failure, the positioning and posturing of the hand must be precise enough, specially when the hand is close to the target object and support surface. To improve the robustness of robotic grasping, in this paper we present a method by grasping control based on the relative posture and position between hand and support surface besides those between hand and target object, using proximity sensors. Proximity sensors are newly installed on fingernails besides on the fingertips. As the fingernail sensor, an integration of Time-of-Flight (TOF) sensor and photo-reflector is designed to realize long range detection, as well as with precise and high-speed detection regardless of the reflectance of support surfaces when approaching the support surface. By the sensors, the hand can approach the object and support surface coarsely first, and then can be controlled fast and precisely to realize adequate grasping motion along the support surface but without contact with the support face. The method has been implemented to a manipulator system, and successful grasping experiments have demonstrated the effectiveness of the proposed method.","inCitations":[],"pmid":"","title":"Robotic Grasping Using Proximity Sensors for Detecting both Target Object and Support Surface","journalPages":"2925-2932","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594430"],"entities":[],"journalVolume":"","outCitations":[],"id":"0d1d48e2622ba81948fcac44288878e4c88d7ae4","s2Url":"https://semanticscholar.org/paper/0d1d48e2622ba81948fcac44288878e4c88d7ae4","authors":[{"name":"Koichi Sasaki","ids":[]},{"name":"Keisuke Koyama","ids":[]},{"name":"Aiguo Ming","ids":[]},{"name":"Makoto Shimojo","ids":[]},{"name":"Régis Plateaux","ids":[]},{"name":"Jean-Yves Choley","ids":[]}],"doi":"10.1109/IROS.2018.8594430"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206280","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present a robust multi-robot convoying approach that relies on visual detection of the leading agent, thus enabling target following in unstructured 3-D environments. Our method is based on the idea of tracking-by-detection, which interleaves efficient model-based object detection with temporal filtering of image-based bounding box estimation. This approach has the important advantage of mitigating tracking drift (i.e. drifting away from the target object), which is a common symptom of model-free trackers and is detrimental to sustained convoying in practice. To illustrate our solution, we collected extensive footage of an underwater robot in ocean settings, and hand-annotated its location in each frame. Based on this dataset, we present an empirical comparison of multiple tracker variants, including the use of several convolutional neural networks, both with and without recurrent connections, as well as frequency-based model-free trackers. We also demonstrate the practicality of this tracking-by-detection strategy in real-world scenarios by successfully controlling a legged underwater robot in five degrees of freedom to follow another robot's independent motion.","inCitations":["3db42d084fef010b00e0bc61e7820b34596cda0a","bc8e53c1ef837531126886520ce155e14140beb4","441602586cef0471397866dfe58b6aa90b7318e7","b6326613899077be8309d545149e46ce9fde5a55","c533e33190b8183c7fe1a4c6b4180f2e179812ef","8230cc44edc9c197db1f7485f95fd2b97877674d","475de283dad61a8a9ed231dce0d8d62a54f4d062","99ae00a21c26e59f565bac8afbed0ac4bc1b813c","9a39cf74e284c7a944117df3e2f928f728a74b85"],"pmid":"","title":"Underwater multi-robot convoying using visual tracking by detection","journalPages":"4189-4196","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1709.08292v1.pdf","http://www.cim.mcgill.ca/~florian/pdfs/visual_convoying.pdf","http://export.arxiv.org/pdf/1709.08292","https://doi.org/10.1109/IROS.2017.8206280","http://arxiv.org/abs/1709.08292"],"entities":[],"journalVolume":"","outCitations":["6d7f3d5db64718e6100ca78034f717b22cea35cc","54dfe5d85d06613a06b7cdc4f8e60c8bd21b2afa","bbd04c1de5eac645ac70d04d0ea17055480c1fb3","2ce63d77eecc35faef85a3b752a314c93a077ac9","0919b3ab82ff8a052490389f217c480c273f2b92","6ac3acd8c0cd3d6b234cb2c3f9ca747056c794e0","5fc8f3ffad38c23529ba2f964cf802c24c8d9ccb","22a879d96f95640feebce359cbc057e94a01a80c","eebc4a70509aca8491c2302c8a4241fd82c9007f","03cb50b26c72ffcec4c4d985c854ccba77efb9a2","ff3745d6650b44d864c0d5a66c80a968650e4ea5","21a1654b856cf0c64e60e58258669b374cb05539","29a3a5ed926bf667a3520dfdcfe48d61d151367c","48eccca5f29735ebc4ed63b18f9291ed3bb6b7d9","2ac63e298fd3454099cf7d4851756da1de36c41c","947eb3736a9600f5acab52a553cf937759b37df8","54ad1036dbfb1efbb90417383b2d0de282560222","5df067183af829068154a83438e96e4b82dc194e","2e6fa2bec6f1f3f3f91362910008b76df6a1047f","dfa358918f7e4c6108aed483cb62231113e43358","3a186019b67871548eb00bda0d3032826a12ab1f","7d39d69b23424446f0400ef603b2e3e22d0309d6","89d22f17c63ef1b84d8de20e4b93cf591b8002f9","06bae254319f8d39e80c7254c841787b45baf820","c9c58dcb4ee35abdffd6e9c9463ec5cba4c253d8","061356704ec86334dbbc073985375fe13cd39088","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","4e47663416e0b45fbcfb9d67304f30409a8ddfa9","c43b91b60e568aab30032f40ebaae5d89d177724"],"id":"3a09820d23bf469462e4f8cca52f2e6ddc88f4c2","s2Url":"https://semanticscholar.org/paper/3a09820d23bf469462e4f8cca52f2e6ddc88f4c2","authors":[{"name":"Florian Shkurti","ids":["2162768"]},{"name":"Wei-Di Chang","ids":["24064144"]},{"name":"Peter Henderson","ids":["40068904"]},{"name":"Md Jahidul Islam","ids":["3393395"]},{"name":"Juan Camilo Gamboa Higuera","ids":["2550467"]},{"name":"Jimmy Li","ids":["2666951"]},{"name":"Travis Manderson","ids":["2106736"]},{"name":"Anqi Xu","ids":["1681292"]},{"name":"Gregory Dudek","ids":["1798721"]},{"name":"Junaed Sattar","ids":["1765604"]}],"doi":"10.1109/IROS.2017.8206280"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593765","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"For autonomous multi-robot teams, the individual team members are tasked with completing their assigned tasks as defined by a team plan provided by a centralized team planner. However in complex dynamic domains, the team plans are generated by the team planner with assumptions due to the complexity of modeling the domain. Failures in execution are therefore inevitable for the team members, and as such, replanning will occur for the team. In this paper, we introduce a rationale-driven team plan representation that provides rationales on why actions were chosen by the team planner. During a failure, the individual team members autonomously use our described intra-robot replanning algorithm to select all applicable replan policies for a given rationale. We then describe a method to learn the predicted cost of each replan policy, given a state of the environment, in order for the individual robots to select the lowest costing replan policy to improve team performance.","inCitations":[],"pmid":"","title":"A Rationale-Driven Team Plan Representation for Autonomous Intra-Robot Replanning*","journalPages":"2389-2394","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593765","http://www.cs.cmu.edu/afs/cs/user/mmv/www/papers/18iros-CookseyVeloso.pdf"],"entities":[],"journalVolume":"","outCitations":["057e84167d1e6d60c289ec661243610e2fdd6cd2","51e46c5bc9a8af9686371a68936635d40964df83","284d6bc26f47305d119a937ead6e1a6d54af9f63","05d753748fdf5e12630b65d1cf677bd64e68fcb7","e517ff44312cb17aa85fcab3bd5d4230d7361109","7ec8f5d7f553ad56f867041065519e7fda01ae2e","c1cef5071516a066cc9466b3982bd6a70aa824f3","14e6ab68417358f91799df40aeceee03fe20f02f","5510f9f5f7327a8bb58de586033382cfd78162e8","149a748f024abbe0024cb3631ddd7e813b453be0","a9193987f145ff64430ed3586273583999156254","8174acb86af51d485825a0e4e9cda83ec03d12a9"],"id":"524df39db33c554d65d513ec870593584b9460f1","s2Url":"https://semanticscholar.org/paper/524df39db33c554d65d513ec870593584b9460f1","authors":[{"name":"Philip Cooksey","ids":["2248744"]},{"name":"Manuela M. Veloso","ids":["1703940"]}],"doi":"10.1109/IROS.2018.8593765"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593677","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We present the first fully spatial hopping gait of a 12 DoF tailed biped driven by only 4 actuators. The control of this physical machine is built up from parallel compositions of controllers for progressively higher DoF extensions of a simple 2 DoF, 1 actuator template. These template dynamics are still not themselves integrable, but a new hybrid averaging analysis yields a conjectured closed form representation of the approximate hopping limit cycle as a function of its physical and control parameters. The resulting insight into the role of the machine's kinematic and dynamical design choices affords a redesign leading to the newly achieved behavior.","inCitations":[],"pmid":"","title":"Analytically-Guided Design of a Tailed Bipedal Hopping Robot","journalPages":"2237-2244","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593677"],"entities":[],"journalVolume":"","outCitations":[],"id":"95bc9c8ce74797060036c178a570e25fd2ffb278","s2Url":"https://semanticscholar.org/paper/95bc9c8ce74797060036c178a570e25fd2ffb278","authors":[{"name":"Abdulaziz Shamsah","ids":[]},{"name":"Avik De","ids":[]},{"name":"Daniel E. Koditschek","ids":[]}],"doi":"10.1109/IROS.2018.8593677"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545096","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Convolutional Neural Networks (CNNs) have shown outstanding performance in visual object tracking. However, most of classification-based tracking methods using CNNs are time-consuming due to expensive computation of complex online fine-tuning and massive feature extractions. Besides, these methods suffer from the problem of over-fitting since the training and testing stages of CNN models are based on the videos from the same domain. Recently, matching-based tracking methods (such as Siamese networks) have shown remarkable speed superiority, while they cannot well address target appearance variations and complex scenes for inherent lack of online adaptability and background information. In this paper, we propose a novel object-adaptive LSTM network, which can effectively exploit sequence dependencies and dynamically adapt to the temporal object variations via constructing an intrinsic model for object appearance and motion. In addition, we develop an efficient strategy for proposal selection, where the densely sampled proposals are firstly pre-evaluated using the fast matching-based method and then the well-selected high-quality proposals are fed to the sequence-specific learning LSTM network. This strategy enables our method to adaptively track an arbitrary object and operate faster than conventional CNN-based classification tracking methods. To the best of our knowledge, this is the first work to apply an LSTM network for classification in visual object tracking. Experimental results on OTB and TC-128 benchmarks show that the proposed method achieves state-of-the-art performance, which exhibits great potentials of recurrent structures for visual object tracking.","inCitations":[],"pmid":"","title":"Object-Adaptive LSTM Network for Visual Tracking","journalPages":"1719-1724","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545096","https://pure.qub.ac.uk/portal/files/153824535/Object_Adaptive_LSTM_Network_icpr2018.pdf"],"entities":[],"journalVolume":"","outCitations":["1af794673d64112edf60f071ca24066e6a227d2b","a9a414604cff39f1a03c5547385dc421e6c8452e","2ce63d77eecc35faef85a3b752a314c93a077ac9","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","189ff58190c6ef1ff49b46f5c68a7b124b9b0059","15e024d8f5625ec03c8ac592fbc093687cfb5f02","1c42b5543c315556c8a961b1a4ee8bc027f70b22","ca00dafc19e9e8b279d2af6c7ed362ae18bf47ee","1dd4282b26211074eb099c80d12fa5c02f73e0f3","9ad8c207d66553d0fa7a7cb57c5e1be12896d1d9","47b5e4d564f36bf322c14893b51ae4ecf782b53b","7b69739682b8edbd3d9ae5af141800fb2a89e751","87283935f0eec5ddc0e5ad3062568df8eb89e7e0","f1d80d6adef260500ebb4eb6347fd9a8adf8961e","549910b8450e56f3001321f6db0c0352f34592db","5684d284310582ae0f69c5b7a4d6b791a13fcf49","9cf3c67529085d31c646091b97be1a1e3dc191f2","13b99ce00f12b02b28dd5d4b6b1ac699750a1749","1812c98ad2a9aed7ef3e662449297d9b8807a3d9","b7d540cd0de72e984cdec44afa4a4d039cfd5eea","dda27eb7ddc4510f94cac0e5134b5d56aa77b075","65c9b4b1d49f46b3f8f64a5f617acfc14f85d031","1d5d68bee741d81771e9224fe53806e85ed469aa","08f646c369e6f7880ec74c1cde869347f6223a3e","a2e0807a43b4fe1443488cf1e2c0f007b706ecaf","0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5","b2180fc4f5cb46b5b5394487842399c501381d67","3b2697d76f035304bfeb57f6a682224c87645065"],"id":"691519ad5ed60b82f43d534b50609b7b68cb4c9b","s2Url":"https://semanticscholar.org/paper/691519ad5ed60b82f43d534b50609b7b68cb4c9b","authors":[{"name":"Yihan Du","ids":["52216251"]},{"name":"Yan Yan","ids":["47971190"]},{"name":"Si Chen","ids":["47336404"]},{"name":"Yang Hua","ids":["48541586"]},{"name":"Hanzi Wang","ids":["37414077"]}],"doi":"10.1109/ICPR.2018.8545096"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594484","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Wheel-legged hybrid robot with multi-modal locomotion can efficiently adapt to different terrain environments, as well as realize rapid maneuver on flat ground. We have developed a novel hydraulic wheel-legged robot (WLR) combined with a humanoid structural design. This robot can assist to emergency scenarios where the high mobility, adaptability and robustness are required. The paper introduces the details of the WLR, highlighting the innovative design and optimization of physical construction which is considered to maximize the mobile abilities, enhance the environmental adaptability and improve the reliability of hydraulic system. Firstly, maximizing the mobile abilities includes optimizing the configuration of each actuator and integrating them with the structure, so as to achieve a large range of movement and also reduce the mass and inertia of the legs. Secondly, the environmental adaptability can be ensured with a magnetorheological (MR) fluid-based damper and direct-drive wheels. Thirdly, improving the reliability of hydraulic system involves using the selective laser melting (SLM) technology to integrate hydraulic system and reducing the number of exposed tubes. The maneuverability of the WLR is demonstrated with a series of experiments. At present, the WLR can perform the following operations, including moving on the flat ground, squatting, and picking up a heavy load.","inCitations":[],"pmid":"","title":"Design and Experiments of a Novel Hydraulic Wheel-Legged Robot (WLR)","journalPages":"3292-3297","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594484"],"entities":[],"journalVolume":"","outCitations":[],"id":"77c3285f8a0b6666e7734299ac3af7e8eb89a9b7","s2Url":"https://semanticscholar.org/paper/77c3285f8a0b6666e7734299ac3af7e8eb89a9b7","authors":[{"name":"Xu Li","ids":[]},{"name":"Haitao Zhou","ids":[]},{"name":"Haibo Feng","ids":[]},{"name":"Songyuan Zhang","ids":[]},{"name":"Yili Fu","ids":[]}],"doi":"10.1109/IROS.2018.8594484"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546070","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"We introduce a high-resolution equirectangular panorama (aka 360-degree, virtual reality, VR) dataset for object detection and propose a multi-projection variant of the YOLO detector. The main challenges with equirectangular panorama images are i) the lack of annotated training data, ii) high-resolution imagery and iii) severe geometric distortions of objects near the panorama projection poles. In this work, we solve the challenges by I) using training examples available in the \u201cconventional datasets\u201d (ImageNet and COCO), II) employing only low resolution images that require only moderate GPU computing power and memory, and III) our multi-projection YOLO handles projection distortions by making multiple stereographic sub-projections. In our experiments, YOLO outperforms the other state-of-the-art detector, Faster R-CNN, and our multi-projection YOLO achieves the best accuracy with low-resolution input.","inCitations":["7c971b4185da7a6c1a0b998087d1e5185f7b05b2"],"pmid":"","title":"Object Detection in Equirectangular Panorama","journalPages":"2190-2195","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546070","https://arxiv.org/pdf/1805.08009v1.pdf","http://arxiv.org/abs/1805.08009","http://export.arxiv.org/pdf/1805.08009","http://vision.cs.tut.fi/data/publications/icpr2018_yolo_360.pdf"],"entities":[],"journalVolume":"","outCitations":["ba227bb94ea9414bad8846673c904a10d813e443","503036c4a96fa9a1d5c5be3fd27d41d1c9edbf65","e441d7524a247eaaea5998c9492062cf0e2fd79c","7d39d69b23424446f0400ef603b2e3e22d0309d6","11755a33910e48c6bb1e7a9bbff7c74fe1aa3585","790503249cc1feeccb854b591b091769eb1cd898","3b2697d76f035304bfeb57f6a682224c87645065","98e5981e7a465da92f551422891515ecd7edeb0f","9d8e1e1e7511ed17db4dec4ee60ed284e5516e60","ab0715642330502d5efca948e4753651cb004d84","9100bb05210dcb4539c1e5d92f4401d5a2ec9c10","df38c330833fe2e03258fba9229d656aa88a0a47","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","7c89cc9bf1f2598d552585ca4d79a82ef9b5a5e6","061356704ec86334dbbc073985375fe13cd39088","424561d8585ff8ebce7d5d07de8dbf7aae5e7270"],"id":"7a70afd9ff6028052f4f28aeb0aff439c657bc65","s2Url":"https://semanticscholar.org/paper/7a70afd9ff6028052f4f28aeb0aff439c657bc65","authors":[{"name":"Wenyan Yang","ids":["49230589"]},{"name":"Yanlin Qian","ids":["35347896"]},{"name":"Francesco Cricri","ids":["2176784"]},{"name":"Lixin Fan","ids":["2034793"]},{"name":"Joni-Kristian Kämäräinen","ids":["1706467"]}],"doi":"10.1109/ICPR.2018.8546070"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593662","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this work, we introduce pose interpreter networks for 6-DoF object pose estimation. In contrast to other CNN-based approaches to pose estimation that require expensively annotated object pose data, our pose interpreter network is trained entirely on synthetic pose data. We use object masks as an intermediate representation to bridge real and synthetic. We show that when combined with a segmentation model trained on RGB images, our synthetically trained pose interpreter network is able to generalize to real data. Our end-to-end system for object pose estimation runs in real-time (20 Hz) on live RGB data, without using depth information or ICP refinement.","inCitations":[],"pmid":"","title":"Real-Time Object Pose Estimation with Pose Interpreter Networks","journalPages":"6798-6805","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1808.01099","https://doi.org/10.1109/IROS.2018.8593662","https://arxiv.org/pdf/1808.01099v1.pdf","http://people.csail.mit.edu/bzhou/publication/iros18-pose.pdf"],"entities":[],"journalVolume":"","outCitations":["f50ea302912dd7535b3eda6831cd474e3d6da9fe","48bca606ecab7f5bfc37b93df4ca51a564c3027c","dcfcc635b299184bb96a020ac6501e4b14fa0674","1a48f5d26ed348311a84c001c906b5807200bdc4","3554853f23339fcefec624e420dfa8728ab64398","2c03df8b48bf3fa39054345bafabfeff15bfd11d","a325138d5c4a6e21c1e6c5aced579a60bd3d9196","1e0f00de4902f33d48612f8fb5384366ce7654d4","56676f712637057e230b2b842b9cdef2630c9af2","cab372bc3824780cce20d9dd1c22d4df39ed081a","1a2e9a56e5f71bf95a2f68b6e67e2aaa1c6bf91e","0674c1e2fd78925a1baa6a28216ee05ed7b48ba0","45fa189f2696c9ea6896f749622da5969d6ac44f","f226ec13e016943102eb7ebedab7cf3e9bef69b2","ed2563f8db8791fd0b032c989ee18b1599b9be5d","3ca3993b1f3536b15112f759067f62e999c5d38f","00319cd17cebae5e1095a248260bd7be15781362","8cab3bd872a631d2552754be340018e435bd358b","d1d73bde8d8fe24ab4065731539198f57f8dc23f","3b2697d76f035304bfeb57f6a682224c87645065","e23222907f95c1fcdc87dc3d3cd93edeaa56fa66","009fba8df6bbca155d9e070a9bd8d0959bc693c2","9201bf6f8222c2335913002e13fbac640fc0f4ec","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","25c740bb2a94ad3942e3a6ee329724cb3905e619","055fec9810fe1e98944a229303b0000afbb47b31","b8e9ef9059d25bab93cecfdb829ae1d8e50b63e6","2b5f66d487e49244b1b146401a3010c6f7d7d622","72fb8234a338f8e723e7adf399dae0313244c70d","74c4b1057f5b8a9bda0906806de5ee7a19614928","224e48ca432e207d242832055396f6a3dc7dd749","1c7e078611c9df412e6eb3a356f31a0da0c1f99c","c76611e2fee7ad8e17f5a2a426c93353d7e8ac7d","71d8fae870ea78a89e231247afb3259267e09799","3fed2126c6b09519aa66fefa360f15e7f926e317","06ca4088d5dbadb1b5ec27f4aa0da0db8621b765","db299ad09f629a0fcd45b74fa567da476d83a4f3","307d322d6a296305c6a0896c5566217a0d448d21","a825680aeb853fc34c65b5844c4c4391148f18c3","5f0d86c9c5b7d37b4408843aa95119bf7771533a","2742a33946e20dd33140b8d6e80d5fd04fced1b2","82eb267b8e86be0b444e841b4b4ed4814b6f1942","3cdb1364c3e66443e1c2182474d44b2fb01cd584","59d31ac428301f033d01210db2f9f8c1bb6f27f7","0c931bb17d9e5d4dc12e2662a3571535841ab534","218b4c5f1c14bd8fe57a849490f64a0832434a0c","1102ee43f33a2949c6efbb6a98e2b4ec477b5c71","7957fa25b25b8631c1d59e656a6d1173d6d2ddc8","45e7d537c289ad70b125f959b8c971d14c7ead07","20a78d3145279dcd799cd7a856ae2714f4863a16","0b554b5195197bb61248ba737a2b42956f0c0cc6"],"id":"89266d1a9aa16b53b2c96d1a3960a29ce47ba6d6","s2Url":"https://semanticscholar.org/paper/89266d1a9aa16b53b2c96d1a3960a29ce47ba6d6","authors":[{"name":"Jimmy Wu","ids":["47876500"]},{"name":"Bolei Zhou","ids":["1804424"]},{"name":"Rebecca Russell","ids":["26791830"]},{"name":"Vincent Kee","ids":["9127266"]},{"name":"Syler Wagner","ids":["32207483"]},{"name":"Mitchell Hebert","ids":["31435167"]},{"name":"Antonio Torralba","ids":["1690178"]},{"name":"David M. S. Johnson","ids":["47134093"]}],"doi":"10.1109/IROS.2018.8593662"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593595","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Finding the parameters of a vignetting function for a camera currently involves the acquisition of several images in a given scene under very controlled lighting conditions, a cumbersome and error-prone task where the end result can only be confirmed visually. Many computer vision algorithms assume photoconsistency, constant intensity between scene points in different images, and tend to perform poorly if this assumption is violated. We present a real-time online vignetting and response calibration with additional exposure estimation for global-shutter color cameras. Our method does not require uniformly illuminated surfaces, known texture or specific geometry. The only assumptions are that the camera is moving, the illumination is static and reflections are Lambertian. Our method estimates the camera view poses by sparse visual SLAM and models the vignetting function by a small number of thin plate splines (TPS) together with a sixth-order polynomial to provide a dense estimation of attenuation from sparsely sampled scene points. The camera response function (CRF) is jointly modeled by a TPS and a Gamma curve. We evaluate our approach on synthetic datasets and in real-world scenarios with reference data from a Structure-from-Motion (SfM) system. We show clear visual improvement on textured meshes without the need for extensive meshing algorithms. A useful calibration is obtained from a few keyframes which makes an on-the-fly deployment conceivable.","inCitations":[],"pmid":"","title":"Keyframe-Based Photometric Online Calibration and Color Correction","journalPages":"1-8","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593595","http://www.ais.uni-bonn.de/papers/IROS_2018_Quenzel.pdf"],"entities":[],"journalVolume":"","outCitations":["1dcae5e5e9fc1b4c0315b6e68f19b9bb2d9dba05","a56ed0bc2d69094e351a044ae8bc64ca0da691f8","4e05f81a41cc3e501c1cec85296a5ea2445c043c","e58400a0e7635bd72a42684e57533f6a7350f430","edcfc5dcefd791ca8d6dfb35132b782932a1b535","2131a1b39f4a1a700a2cd068886de62961e81a02","74953f3ec4fdcc1d8a0a45f3f0f84eaa003ae4c3","03d4d496d53407f15354b8dfb8ac0fb220b3380e","8e7e5944102fb361d104303a3e9bdb1607c6de51","40235b65e210656445b24b93e301543ba3594d04","b8f1ea118487d8500d45e5fbf95ab80eedd7fa92","20035f36d8d21207170ca2a6c15623ca056a8641","4466dcf725e5383b10a6c5172c860f397e006b48","4bcb2e7a0b0c0c6fb0ea3303cff9aca50d99aa45","667ff68d1a91da9cb57a0ab79d11a732310e480c","6fa0299df78e95833857d567f489d9cfe8708ec1","02677b913f430f419ee8a8f2a8860d1af5e86b63","282cf28ac9508bd66a6ddf0709c9db9dc0fdf162","0e1f7d160e1d01afe2d57bbceb9260e6c31db877","70c1886fed58671cb3417dae6b88926b852e4992"],"id":"5023a5a9a5c2bb9cb64645aa9cc82aa644775880","s2Url":"https://semanticscholar.org/paper/5023a5a9a5c2bb9cb64645aa9cc82aa644775880","authors":[{"name":"Jan Quenzel","ids":["7547605"]},{"name":"Jannis Horn","ids":["49273321"]},{"name":"Sebastian Houben","ids":["41136635"]},{"name":"Sven Behnke","ids":["1699019"]}],"doi":"10.1109/IROS.2018.8593595"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594387","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Flapping-wing micro air vehicles (FWMAVs) become promising research platforms due to their advantages such as various maneuverability, and concealment. However, unsteady flow at low Reynolds number around the wings makes their dynamics time-varying and highly non-linear. It makes autonomous flight of FWMAV as a big challenge. In this paper, we suggest a model-based control strategy for FWMAV using learning architecture. For this task, we construct a ground station for logging flight data and control inputs, and train dynamics with a neural network. Then, we apply model predictive control (MPC) to the trained model. We validate our method by hardware experiments.","inCitations":[],"pmid":"","title":"Learning-based Path Tracking Control of a Flapping-wing Micro Air Vehicle","journalPages":"7096-7102","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594387"],"entities":[],"journalVolume":"","outCitations":[],"id":"c4ed08f2a7324a8d98ce7cf02ff19b432f7b5f79","s2Url":"https://semanticscholar.org/paper/c4ed08f2a7324a8d98ce7cf02ff19b432f7b5f79","authors":[{"name":"Jonggu Lee","ids":[]},{"name":"Seungwan Ryu","ids":[]},{"name":"Taewan Kim","ids":[]},{"name":"Wonchul Kim","ids":[]},{"name":"H. Jin Kim","ids":[]}],"doi":"10.1109/IROS.2018.8594387"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593630","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Applications of safety, security, and rescue in robotics, such as multi-robot target tracking, involve the execution of information acquisition tasks by teams of mobile robots. However, in failure-prone or adversarial environments, robots get attacked, their communication channels get jammed, and their sensors may fail, resulting in the withdrawal of robots from the collective task, and consequently the inability of the remaining active robots to coordinate with each other. As a result, traditional design paradigms become insufficient and, in contrast, resilient designs against system-wide failures and attacks become important. In general, resilient design problems are hard, and even though they often involve objective functions that are monotone or submodular, scalable approximation algorithms for their solution have been hitherto unknown. In this paper, we provide the first algorithm, enabling the following capabilities: minimal communication, i.e., the algorithm is executed by the robots based only on minimal communication between them; system-wide resiliency, i.e., the algorithm is valid for any number of denial-of-service attacks and failures; and provable approximation performance, i.e., the algorithm ensures for all monotone (and not necessarily submodular) objective functions a solution that is finitely close to the optimal. We quantify our algorithms approximation performance using a notion of curvature for monotone set functions. We support our theoretical analyses with simulated and real-world experiments, by considering an active information gathering scenario, namely, multi-robot target tracking.","inCitations":["b2672c47c16ccac3a5038bb0d02c02219f36faf5","ccb6886b5d14b980211864fea746b536ba89aada","9caafe195bd47dda0a7eee83e54aca432edb8739"],"pmid":"","title":"Resilient Active Information Gathering with Mobile Robots","journalPages":"4309-4316","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1803.09730","https://arxiv.org/pdf/1803.09730v3.pdf","https://doi.org/10.1109/IROS.2018.8593630","http://export.arxiv.org/pdf/1803.09730"],"entities":[],"journalVolume":"","outCitations":["dd817925ff0749cca85fed947e90899244166995","a83cb80dd4ed5160c66dbd31c7782b01e7a97170","39f68fa16716d2bb2da5a95e919396abac7baff1","07d6453e84ae1d059ae345c89a57cc972f7e9266","c019b183bc499ea38ff4260cb006141761851da9","2c97deb923a6e269e74685c57714198d887e9c69","529e521f6af7c62b3bd0b33b8b9f5a4c64d45088","b1ea82ecdd729992d8e81979dcc3dc6d0b4eef99","b4692d9f8a91ca447fd9e93aa24a97c9c9b24e21","57736c69ceda8bd1035702f0f192e9fbe52dd3cf","1553c77d882137f6314848df2166f0ab9f65a600","2e268b70c7dcae58de2c8ff7bed1e58a5e58109a","2518f108a056eeda4ff8070e5e53e66b0a4ba6b2","d3040a93bdcc6a9990ca4cb6d5b000aac0777fbb","d6f2a526207eb4808f59f2a6f15d6880c6491fbf","1e28d851768a79c6f3de9a344c1f2354813789da","1ffe327ae702cf3135efb2fef588d7517b0e10d3","a9bdaae018bb29a7a838947ba0015553b7dd3ea1","79e70b7255996bbe19e39d73b80c0fc10eb20ee7","b9e43395663f74c581982e9ca97a0d7057a0008c","2aa362740ac9a2b304a74122da820e3829689842","5f3b63c957cc45a8cfca84d58610b71a4ed78189","50e54f9a42cf04aa5b131c60123c96364c3d56c8","cb033767207151c3ec2a29fae89a65ebae641acf","dad08bd7a88f6589588a8c2bbc410272fff03ecd","27ca8b8775bd5f24be5fd1af3b84f0b53fd714ad","52831acda2d5cdaa25e80c64c73f54010ef837ed"],"id":"48c34278f39326f14bb616223ff9288fc925e47e","s2Url":"https://semanticscholar.org/paper/48c34278f39326f14bb616223ff9288fc925e47e","authors":[{"name":"Brent Schlotfeldt","ids":["2366739"]},{"name":"Vasileios Tzoumas","ids":["1886606"]},{"name":"Dinesh Thakur","ids":["3866165"]},{"name":"George J. Pappas","ids":["1777216"]}],"doi":"10.1109/IROS.2018.8593630"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594401","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This work presents a model-based approach for creating robust control policies for rolling locomotion with a spherical tensegrity topology. Utilizing the structured dynamics of Class-1 tensegrity systems, we turn to model predictive control (MPC)to generate optimal multi-cable actuation trajectories for dynamic rolling. Although the resulting multi-cable state-action trajectories successfully outperform the benchmark single-cable policy performance in speed, computational constraints prevent MPC from being applied in real-time. To address this, we demonstrate that a contextual policy trained using supervised deep learning on the generated optimal MPC trajectories can be used as an end-to-end feedback policy for real-time directed rolling locomotion.","inCitations":[],"pmid":"","title":"Multi-Cable Rolling Locomotion with Spherical Tensegrities Using Model Predictive Control and Deep Learning","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594401"],"entities":[],"journalVolume":"","outCitations":[],"id":"57159e90d5da6582de950932805de6d078ce1b8f","s2Url":"https://semanticscholar.org/paper/57159e90d5da6582de950932805de6d078ce1b8f","authors":[{"name":"Brian Cera","ids":[]},{"name":"Alice M. Agogino","ids":[]}],"doi":"10.1109/IROS.2018.8594401"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545185","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In spite of various observation tools, tongue shapes are still scarce resource in reality. Autoencoder, a kind of deep neural networks (DNN), performs well on data reduction and pattern discovery. However, since autoencoder usually needs large scale data in training, challenges exist for traditional autoencoder to obtain tongues' motion patterns only from tens or hundreds of available tongue shapes. To overcome this problem, we propose a two-steps autoencoder, where we first construct a stacked denoising autoencoder (dAE) to learn the essential presentation of the tongue shapes from their possible deformations; then an additional autoencoder with small number of hidden units is added upon the previous stacked autoencoder, and used for dimensionality reduction. Experiments run on 240 vowels' tongue shapes obtained from Chinese speakers' pronunciation X-ray films, and the proposed model is compared with traditional dAE and the classical principal component analysis (PCA) on dimensionality reduction and reconstruction in details. Results validate the performance of the proposed tongue model.","inCitations":[],"pmid":"","title":"Reducing Tongue Shape Dimensionality from Hundreds of Available Resources Using Autoencoder","journalPages":"2875-2880","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545185"],"entities":[],"journalVolume":"","outCitations":["31361806d61a038d2d3f43b6cfcf3210660080e4","a35cb104301e002f6692529ea344c35cbb99f760","d2c16e1973d832e507bcda4c3a731385b8aa0d2c","9f3a89275376e4355532b287233949b07c3e5536","3744b5007cc098e0fac031365c835f018e219991","77fb206a428a7849e65cc5ab8939c83afadda30c","9895b42be1f3357f153d1f07859f3adeada14190","44c977c18752d8913746efc7ea8635b0e4be4e47","76323ebf7d5e1241418f630f8419db76d6569a52","2bef88411b148573296a4f224bd806b645610736","3b2bf65ebee91249d1045709200a51d157b0176e","320ae855834a34abf7ada6440549fe7e2855723a","c336fbc34a99b5a44d56fd220a054000f4f56621","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","9f6a5a3746f126434bbdcaf3f559f441d49e9d72","1e6c6b4d5942ebf37e4e1d404b2192bb264755c2","c5b28cae82b14417f1250e58bb241367248e827d","5d79d86d4501b4a5381a7a8cc3562bf66cf975e6","b47bfc632220bdb2b7da09c792199dbb572d588d","55b81991fbb025038d98e8c71acf7dc2b78ee5e9","213d7af7107fa4921eb0adea82c9f711fd105232","16c6fd43c6f1120e5994df07e48ad6e46e6d3703","55f9af415c67ec7671f82ee6f2fabf8c8abcfb2a","6ac8d8c14fff4cb893982280f451f7c1e7dcf6c3","d5b021ecf25929ac1a0103b9c1e601083b1f69da","08751772c88872b742a11e9c64c20286befefe28","373f76633cc1f6c7a421e31c989842021a52fca4","d35786884fb235f05d3da35c7b6cf5bfb2543fa1"],"id":"a8096eb986f4914f401301ea7fc0c0ce408bb7de","s2Url":"https://semanticscholar.org/paper/a8096eb986f4914f401301ea7fc0c0ce408bb7de","authors":[{"name":"Minghao Yang","ids":["2740129"]},{"name":"Dawei Zhang","ids":["46334895"]},{"name":"Jianhua Tao","ids":["37670752"]}],"doi":"10.1109/ICPR.2018.8545185"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594002","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Robotic wheelchairs with built-in assistive features, such as shared control, are an emerging means of providing independent mobility to severely disabled individuals. However, patients often struggle to build a mental model of their wheelchair's behaviour under different environmental conditions. Motivated by the desire to help users bridge this gap in perception, we propose a novel augmented reality system using a Microsoft Hololens as a head-mounted aid for wheelchair navigation. The system displays visual feedback to the wearer as a way of explaining the underlying dynamics of the wheelchair's shared controller and its predicted future states. To investigate the influence of different interface design options, a pilot study was also conducted. We evaluated the acceptance rate and learning curve of an immersive wheelchair training regime, revealing preliminary insights into the potential beneficial and adverse nature of different augmented reality cues for assistive navigation. In particular, we demonstrate that care should be taken in the presentation of information, with effort-reducing cues for augmented information acquisition (for example, a rear-view display) being the most appreciated.","inCitations":[],"pmid":"","title":"Head-Mounted Augmented Reality for Explainable Robotic Wheelchair Assistance","journalPages":"1823-1829","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594002"],"entities":[],"journalVolume":"","outCitations":[],"id":"2162e684d671263a8bea9cb3c4fc61d8d129e272","s2Url":"https://semanticscholar.org/paper/2162e684d671263a8bea9cb3c4fc61d8d129e272","authors":[{"name":"Mark Zolotas","ids":[]},{"name":"Joshua Elsdon","ids":[]},{"name":"Yiannis Demiris","ids":[]}],"doi":"10.1109/IROS.2018.8594002"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546199","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The denoising and deblurring of images are the two essential restoration tasks in the document image processing task. As the preprocessing stages of the processing pipeline, the quality of denoising and deblurring heavily influences the result of subsequent tasks, such as character detection and recognition. In this paper, we propose a novel neural method for restoring document images. We named our network Skip-Connected Deep Convolutional Autoencoder (SCDCA), which is composed of multiple layers of convolution followed by a batch normalization layer and the leaky rectified linear unit (Leaky ReLU) activation function. Inspired by the idea of residual learning, we use two types of skip connections in the network. One is identity mapping between convolution layers and the other is used to connect the input and output. Through these connections, the network learns the residual between the noisy and clean images instead of learning an ordinary transformation function. We empirically evaluate our algorithm on an open and challenging document images dataset. We also assess our restoring results using the optical character recognition (OCR) test. Experimental results have demonstrated the effectiveness and efficiency of our proposed algorithm by comparing with several state-of-the-art methods.","inCitations":[],"pmid":"","title":"Skip-Connected Deep Convolutional Autoencoder for Restoration of Document Images","journalPages":"2935-2940","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546199"],"entities":[],"journalVolume":"","outCitations":["9612fce47fdc4fb8fd1c565737d2bc300404eb0c","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","2bf8c3a1dea0e191af53fe0b0d6df3993c1c987c","265f52bd3e8fb8b4d0a793ccf8ccb1e192ab35ee","0c03ddd72323362bbe45ac99f6b868e482d51829","fc684e2e880df46018407f44d45c38d1fee7ec67","546b3592f59b3445ef12fba506b729c832198c33","3ba179bceb9692d4d21109d0b87b120195761148","367f2c63a6f6a10b3b64b8729d601e69337ee3cc","f0f3e83965f00ca4f5216f63558a65078f00f882","4bad6722ad3d9b395ff6e76b427f999168929697","4766c4283d002bc91bdb4a1b45abdd6331b67b16","c54762c6956fd2135e7a3be7ebcb571f72dae590","0ea9ed4b344fd8546f20c938266e07d819221bc0","39c3ba33f6ec8f9de36f0a8461c5f78e4f911d98","5b606354cc102f981616ee14d7ed710294870815","13313124277b42dc61096edd170e74b87592ddab","3243c835617e9f02e8e7cfb891d835891d8fe1c3","dc8861b4ab6799be542829ae1ace13f23cf807cd","5763c2c62463c61926c7e192dcc340c4691ee3aa","53290c029ecf9bb16d35a02198fef03a30ca0dff","18168aea48a22f6fe2fe407c0ff70083cba225a7","bba624ceffea983211d44fd76f6eb6a3334d7b14","d003c167e9e3c8ee9f6ba6f96e298a5646e39b41","2c03df8b48bf3fa39054345bafabfeff15bfd11d","e2b7f37cd97a7907b1b8a41138721ed06a0b76cd","a55970013b984f344dfbbbba677d89dce0ba5f81","1262307c2b17313c0522383d669705e9954eac49","10f4236459c59e846f287785c983304b40efefb5","5620050230ba1b45689cfc5e3efa064219a66a2e","eef29a4fef85c7ed8acde9ca42f8f09d944f361d","a6574d111bfb12d6a9988bdbbf24639d3c4534ec"],"id":"69aafc572c01b2419cbf5fed94dd462a52e8ac23","s2Url":"https://semanticscholar.org/paper/69aafc572c01b2419cbf5fed94dd462a52e8ac23","authors":[{"name":"Guoping Zhao","ids":["50175792"]},{"name":"Jiajun Liu","ids":["49721729"]},{"name":"Jiacheng Jiang","ids":["3269436"]},{"name":"Hua Guan","ids":["47550441"]},{"name":"Ji-Rong Wen","ids":["2259699"]}],"doi":"10.1109/ICPR.2018.8546199"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202157","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The use of concentric tube robots has recognized advantages for accessing target lesions while conforming to certain anatomical constraints. However, their complex kinematics makes their safe telemanipulation in convoluted anatomy a challenging task. Collaborative control schemes, which guide the operator through haptic and visual feedback, can simplify this task and reduce the cognitive burden of the operator. Guaranteeing stable, collision-free robot configurations during manipulation, however, is computationally demanding and, until now, either required long periods of pre-computation time or distributed computing clusters. Furthermore, the operator is often presented with guidance paths which have to be followed approximately. This paper presents a heterogeneous (CPU/GPU) computing approach to enable rapid workspace analysis on a single computer. The method is used in a new navigation scheme that guides the robot operator towards locations of high dexterity or manipulability of the robot. Under this guidance scheme, the user can make informed decisions and maintain full control of the path planning and manipulation processes, with intuitive visual feedback on when the robot's limitations are being reached.","inCitations":[],"pmid":"","title":"Implicit active constraints for concentric tube robots based on analysis of the safe and dexterous workspace","journalPages":"193-200","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202157"],"entities":["Active set method","Artificial general intelligence","Central processing unit","Computation","Computer architecture","Distributed computing","Graphics processing unit","Haptic technology","Heterogeneous computing","Motion planning","Precomputation","Remote manipulator","Robot","Time complexity","User experience","Vii","Workspace"],"journalVolume":"","outCitations":["529c1230e969fc216ddca64606295df449ca352f","2aa0c511020575718253fc7ce58f9da20037b165","3dfe379678e3cc404bd6a26e11f2b5d27dcc615c","4397189a548127847c511302d49b6c9cf0d242f1","89c44bb1af32a27e020a703e02bbe738158081cd","d227b37a2739b28405d1c00d2e7b8c1486009a3b","c5257844c31b3582aed2625f657e867f50c3fbf6","3d87a0b35c1256f72516309b1d52861dc01cf616","57d11fee24e646a2f4c79db91d7f87bd9672d209","34e401456c63e27c8ee1312483d75c5cdb789c58","2b02f066b33a2ecb8764e132c075cfb1d16c9d10","aec46f5d5dd5ac28a4bc7e5e10159056f6e72648","16ff301a1212d2db3e2cc3c17c0c397f0db66624","9c0c450f7b3f5453f2dc32d205a3184424b53715","de6a47bf3272d16993f4dde1dd1af5fc89a2cc93","9ecdcddb670d1b120c7bbf68aaa239b74627716b","370c41b720a7bfd13cf607778c87407ff88a235f","cbb04d9e5edb1b81b905e99ce060b0a5ff789272","3e9faf8b1feff0d54eddb11fb0b092ae40e09a59","146bf961892e1155391e5827729fbfa1d76a978f","7c5b97d4f20e97766304e8e0fae1fc1f0f165ae5","8ee96fe2ab10b59c81fcf20d18d5aa9f52ba28da","dbdaad4cba4c3bf82225317317aef9a7338e2f09","1f1b41392a01252f4b19bc24f2555f25f61f6881","8686547a16bb48e19d337bafcb53353fcce52311","81eb03c70a6daf0e062b6c4dedcfaf08059dc6f4","2518d6b39387cc6c2ac64b3f921d5e60d544072c","6f5c60963160162747d54a8fd8c5a63cd6059755"],"id":"e0fbac596cba887f82f0dbf17c86df64c7b499aa","s2Url":"https://semanticscholar.org/paper/e0fbac596cba887f82f0dbf17c86df64c7b499aa","authors":[{"name":"Konrad Leibrandt","ids":["2694759"]},{"name":"Christos Bergeles","ids":["2208039"]},{"name":"Guang-Zhong Yang","ids":["1743111"]}],"doi":"10.1109/IROS.2017.8202157"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546273","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper we show the similarities and differences of two deep neural networks by comparing the manifolds composed of activation vectors in each fully connected layer of them. The main contribution of this paper includes 1) a new data generating algorithm which is crucial for determining the dimension of manifolds; 2) a systematic strategy to compare manifolds. Especially, we take Riemann curvature and sectional curvature as part of criterion, which can reflect the intrinsic geometric properties of manifolds. Some interesting results and phenomenon are given, which help in specifying the similarities and differences between the features extracted by two networks and demystifying the intrinsic mechanism of deep neural networks.","inCitations":[],"pmid":"","title":"Curvature-based Comparison of Two Neural Networks","journalPages":"441-447","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546273","https://arxiv.org/pdf/1801.06801v1.pdf","http://arxiv.org/abs/1801.06801"],"entities":["Activation function","Algorithm","Deep learning","Embedded system","Euclidean distance","Experiment","Neural Networks","Pixel","Visual descriptor"],"journalVolume":"","outCitations":["2e905896cf513e54a19b3bab78234ed6c9d89e23","5dd4e5fb722c6abd46271580cc427ea6a2e3cddb","692760a7a68e3b6e53fdab25f6d6a44626f2b701","35fc894c544dd0cbfbc3602b46287911f78edbd0","cb977a75abf25e1993c4b2a86b49b8a9ee13186e","a3f7a30abe44424e5ef8348a02cc103237ac5210","8a9a10170ee907acb3e582742bec5fa09116f302","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","cbb73bb39d435cd39cd35ec27e5a549b937a97a0"],"id":"fa265236195b7514a56f2de6b4bf0f734de22435","s2Url":"https://semanticscholar.org/paper/fa265236195b7514a56f2de6b4bf0f734de22435","authors":[{"name":"Tao Yu","ids":["47369443"]},{"name":"Huan Long","ids":["39007850"]},{"name":"John E. Hopcroft","ids":["1706504"]}],"doi":"10.1109/ICPR.2018.8546273"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593693","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a geometrically supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet","inCitations":[],"pmid":"","title":"CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks","journalPages":"1110-1117","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593693"],"entities":[],"journalVolume":"","outCitations":[],"id":"a67e66e437331cf7dbb02d2a7296e9c32dba67d5","s2Url":"https://semanticscholar.org/paper/a67e66e437331cf7dbb02d2a7296e9c32dba67d5","authors":[{"name":"Ganesh Iyer","ids":[]},{"name":"R. KarnikRam","ids":[]},{"name":"J. Krishna Murthy","ids":[]},{"name":"K. Madhava Krishna","ids":[]}],"doi":"10.1109/IROS.2018.8593693"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546184","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Classes in natural images tend to follow long tail distributions. This is problematic when there are insufficient training examples for rare classes. This effect is emphasized in compound classes, involving the conjunction of several concepts, such as those appearing in action-recognition datasets. In this paper, we propose to address this issue by learning how to utilize common visual concepts which are readily available. We detect the presence of prominent concepts in images and use them to infer the target labels instead of using visual features directly, combining tools from vision and natural-language processing. We validate our method on the recently introduced HICO dataset reaching a mAP of 31.54% and on the Stanford-40 Actions dataset, where the proposed method outperforms that obtained by direct visual features, obtaining an accuracy 83.12%. Moreover, the method provides for each class a semantically meaningful list of keywords and relevant image regions relating it to its constituent concepts.","inCitations":[],"pmid":"","title":"Action Classification via Concepts and Attributes","journalPages":"1499-1505","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546184","http://arxiv.org/abs/1605.07824","https://arxiv.org/pdf/1605.07824v2.pdf","http://arxiv.org/pdf/1605.07824v1.pdf"],"entities":["Classful network","LVM","Long tail","Natural language processing","Statistical classification","Type class","Weight function"],"journalVolume":"","outCitations":["343381065bbad97662e393293320d0556badba5d","0825788b9b5a18e3dfea5b0af123b5e939a4f564","0566bf06a0368b518b8b474166f7b1dfef3f9283","2f5e057e35a97278a9d824545d7196c301072ebf","a01d22166ed62f5ad485ae32827c70d583a88564","ce668a88e34dc8df1a95c7292688413274b8ed0b","30aac3becead355545b5ab7f0c3158040360021e","0ae74fabc585cfd1cf60ea3f9e218c59a4539091","1c0e8c3fb143eb5eb5af3026eae7257255fcf814","3d4c4f975cf047c2be6d5c90f4488f63fd3bb5be","4b3c5b49fa099a77c98bad4b5299c6b4eb0a8f2e","47c2a4e4c7a6d7d75840bb0a8c4404b87a5a7466","50e983fd06143cad9d4ac75bffc2ef67024584f2","856c09ab10efbc8c61a84a951746654d947370f3","12692fbe915e6bb1c80733519371bbb90ae07539","b7d41d5f18d8902c091f70a5fbc7831598854ff0","0b4d3e59a0107f0dad22e74054bab1cf1ad9c32e","70e9debebc4ec5604b9ed295238e40340a1a4661","061356704ec86334dbbc073985375fe13cd39088","3b2697d76f035304bfeb57f6a682224c87645065","0fbdd4b8eb9e4c4cfbe5b76ab29ab8b0219fbdc0","289d833a35c2156b7e332e67d1cb099fd0683025","28992e3ae9d81939125e87d7197f7d15abdae2ce","2c03df8b48bf3fa39054345bafabfeff15bfd11d","0fd1bffb171699a968c700f206665b2f8837d953","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a","6891fc343ed8b7e530b041eace4c21d227901e90"],"id":"11d256be1eb3da7789c0c9672f467079917baada","s2Url":"https://semanticscholar.org/paper/11d256be1eb3da7789c0c9672f467079917baada","authors":[{"name":"Amir Rosenfeld","ids":["32928116"]},{"name":"Shimon Ullman","ids":["1743045"]}],"doi":"10.1109/ICPR.2018.8546184"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594447","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Sampling based methods resulted in feasible and effective motion planning algorithms for high dimensional configuration spaces and complex environments. A vast majority of such algorithms as well as their application rely on generating a set of open-loop trajectories first, which are then tracked by feedback control policies. However, controlling a dynamic robot to follow the planned path, while respecting the spatial constraints originating from the obstacles is still a challenging problem. There are some studies which combine statistical sampling techniques and feedback control methods which address this challenge using different approaches. From the feedback control theory perspective, Reference Governors proved to be a useful framework for constraint enforcement. Very recently, Arslan and Koditschek (2017) introduced a feedback motion planner that utilizes Reference Governors that provably solves the motion planning problem in simplified spherical worlds. In this context, here we propose a \u201ctrajectory-free\u201d novel feedback motion planning algorithm which combines the two ideas: random trees and reference governors. Random tree part of the algorithm generates a collision-free region as a set of connected simple polygonal regions. Then, reference governor part navigates the dynamic robot from one region to the adjacent region in the tree structure, ensuring it stays inside the current region and asymptotically reaches to the connected region. Eventually, our algorithm robustly routes the robot from the start location to the goal location without collision. We demonstrate the validity and feasibility of the algorithm on simulation studies.","inCitations":[],"pmid":"","title":"RG-Trees: Trajectory-Free Feedback Motion Planning Using Sparse Random Reference Governor Trees","journalPages":"6506-6511","s2PdfUrl":"","pdfUrls":["http://etd.lib.metu.edu.tr/upload/12622569/index.pdf","https://doi.org/10.1109/IROS.2018.8594447"],"entities":[],"journalVolume":"","outCitations":["25aa4276baf44c47c87dce8ad409b63119b730ff","559f828820d194cce6693aa6f1ec5bb2cdcf1271","088e21d6931729e005e6a622c6a92695e3c9dce8","0dfb88c75aec3fb55a4fdc6dc05543f7fdd2c564","36256596180fe57c345210057bc360a7024bfc9d","62278010df9000e5496ea2523d46e98bb2206c56","7ff0a265d072472d78175a7fd1db25382e70f84b","d967d9550f831a8b3f5cb00f8835a4c866da60ad","0f996c0915e3e0a2e9ab5c685e603b02d979971d","dff013cb8db8b5d9cff741601832965c83ac402f","a1a8dad0b9e7c7676228b5b85ae07cacce05ee2e","d1c13131636db174911db9eb7edf666b92598143","788a7f0abfe43bf27636a6fc3676afe2c9332a29","8727d351895fbf2f98ac1a68791e7c746a4ce105","00d4552070590eb929550b1107331dd4624b2bbf","9e48e1bdd959fa169b584824af6ce30ff45a1d04","76f0b406491b1c6c21c66f772c7b6921e5a5356f","8218c6c807efc9e273858a4c49011a80ec6ea32f","01657e43a0a7b4a4dac47ca88d97437e32e96cea","2d5879be664cb753d1fbaea0ed6495505008b780","a3ee5adc6e5ac991b6a5aae221c3c1e9a4e953b4","67aaf4e84f11066d0db26a62723ade94625aac0a","19138ca2dddeec87cc658a0109b7491bd80b3a76","d8bb567103352bf5746c61a445d7c3700430b793","70cafe35a46c755bdf29c2518ef83d79a3f4e97f"],"id":"229a44b578adda653aa4ffe553504a6df9f602ab","s2Url":"https://semanticscholar.org/paper/229a44b578adda653aa4ffe553504a6df9f602ab","authors":[{"name":"Ferhat Golbol","ids":[]},{"name":"Mustafa Mert Ankarali","ids":[]},{"name":"Afsar Saranli","ids":[]}],"doi":"10.1109/IROS.2018.8594447"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206141","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, a comprehensive friction model for collaborative industrial robot joints is proposed which takes into account the velocity, temperature and load torque effects. The model indicates that the velocity and temperature have a strong influence on viscous friction nonlinearly, whereas load torque significantly influences the Coulomb friction linearly and causes a slight Stribeck effect. The friction characteristics were investigated on the collaborative robot joints which are equipped with temperature sensor and harmonic reducer. The proposed model has been verified successfully under wide velocity, temperature and load range. In addition, the permillage of the rated current of the motor is used to scale the value of joint torque, so that the model can be applied in common industrial robot.","inCitations":[],"pmid":"","title":"A friction model with velocity, temperature and load torque effects for collaborative industrial robot joints","journalPages":"3027-3032","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206141"],"entities":["Algorithm","Cobot","Dynamic simulation","Estimation theory","Industrial robot","Mobile robot","Nonlinear system","Online and offline","Steady state","Velocity (software development)"],"journalVolume":"","outCitations":["41a80324bd696d06b5ae74702d2de02e94e9da78","f73627c87c80770d889b661133952511e8cded91","fd81e835036fd58b7007033b502536e04db10b5a","711ccee96b672bbfd66b8e27014bbb9f7a630558","a388f2b65dfbaf64369fea63fc15d92d64d3f7ea","cfa078941e4ea371d99e545247f568e902465963","d711db31e5c7b808e5d3a818a01e858e77ab2e3d","4699abb2c8b5fa180f3e85c37b600d062946b14d","11bf12e381d37f72fee6933f22f38fb5fff65274","d72e8df3f7d20155afc8e6109124b4b9ed7e38c4","b6ce4120713a99805e47d27c295d59b3cb436375","4860fe5b5533316bc2adaf6b73dc9ff951397210","8959025e7cac6f33b8e556701eaa5fa6109df691","2877a8d804b92277e4305919390047b3966075e1","a796d1547ec1ef2edf74f81aad5fe5ec4cedaea0"],"id":"7aec79e957d2a907055be54968b203befa9ecd4b","s2Url":"https://semanticscholar.org/paper/7aec79e957d2a907055be54968b203befa9ecd4b","authors":[{"name":"Liming Gao","ids":["15108535"]},{"name":"Jianjun Yuan","ids":["40034801"]},{"name":"Zhedong Han","ids":["49780573"]},{"name":"Shuai Wang","ids":["1717480"]},{"name":"Ning Wang","ids":["49758192"]}],"doi":"10.1109/IROS.2017.8206141"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205986","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, an autonomous vision-based tracking system is presented to track a maneuvering target for a rotorcraft unmanned aerial vehicle (UAV) with an onboard gimbal camera. To handle target occlusions or loss for real-time tracking, a robust and computationally efficient visual tracking scheme is considered using the Kernelized Correlation Filter (KCF) tracker and the redetection algorithm. The states of the target are estimated from the visual information. Moreover, feedback control laws of the gimbal and the UAV using the estimated states are proposed for the UAV to track the moving target autonomously. The algorithms are implemented on an onboard TK1 computer, and extensive outdoor flight experiments have been performed. Experimental results show that the proposed computationally efficient visual tracking scenario can stably track a maneuvering target and is robust to target occlusions and loss.","inCitations":["633108be7ae94e575de1bcc3344fc870459a305c","e7a7498b53d31d5b7004bce6d44f5f5e60ec597e"],"pmid":"","title":"An autonomous vision-based target tracking system for rotorcraft unmanned aerial vehicles","journalPages":"1732-1738","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205986"],"entities":["Aerial photography","Algorithm","Algorithmic efficiency","Autonomous robot","Computation","Experiment","Extended Kalman filter","Feedback","Kernel method","Lyapunov fractal","Nonlinear system","Obstacle avoidance","Real-time clock","Tracking system","Unmanned aerial vehicle","Velocity obstacle","Video tracking"],"journalVolume":"","outCitations":["2f37c1b58ed2819ad88e8bb774bc9916c3172106","b7d540cd0de72e984cdec44afa4a4d039cfd5eea","b4a0b03986b59c4989b4a7d5fd056964caf6ad8e","8f667e55631e2437015dad63bafd5f9d49a9a122","0298cc05b54f2befe6694b739eb258177b7514d6","283df50be1d1a5fde310a9252ead5af2189f2720","bd9f3a5e970604ba4444c09c3c310c0bb5f7c5e5","2e25922f3d6b44b42cc988d4221798be524da62a","6216e24a44aef0908dafac35d9eabe388250eea7","bcb1514b9e502b37a36d52b313751410ca93c3c7","0b15c987dba1db98fd16c56997c1513db2fde306","b1e5907533b7bf87f0900e7ed9c0955568c07977","02dea0d105b8931b0edb4ace2da50bfdb1f4954a","18dca4171f01d594a9853fc074c65b549dd5be5c","65c9b4b1d49f46b3f8f64a5f617acfc14f85d031","9f64e7855c756d661bd910ebe9c5b9c293d16fbb","e62ed87694a8720c8be343e892f77fd397422ae5","2a2af16483c0264c2afe1ed00fa8c9a44501a5b2","e917675358c5382b13df9cebc084f983fc873fdb","a1bcdb820f6f0dde108996a18eefb239ef0e9566"],"id":"64a0640ae9c406f6367ca422c0030b8737c730bd","s2Url":"https://semanticscholar.org/paper/64a0640ae9c406f6367ca422c0030b8737c730bd","authors":[{"name":"Hui Cheng","ids":["47413456"]},{"name":"Lishan Lin","ids":["47965958"]},{"name":"Zhuoqi Zheng","ids":["32400353"]},{"name":"Yuwei Guan","ids":["48953180"]},{"name":"Zhongchang Liu","ids":["2591871"]}],"doi":"10.1109/IROS.2017.8205986"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593385","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Reliable object perception is a vital requirement for automated driving. Despite the availability of precise contour measurements, most state-of-the-art tracking systems still represent object geometry as bounding boxes. However, there are objects operating in public traffic for which the box assumption is highly inappropriate. We therefore propose to represent object contours using 2D polylines. Taking into account the mutual dependence of object poses and shape, our tracking framework targets at a simultaneous estimation of both states. Moreover, we propose to augment scan segments with free-space information at their boundaries and show how this knowledge can be incorporated into the tracking framework and beyond. Evaluation with real scan data shows that our method produces accurate dynamic estimates and consistent shape reconstructions.","inCitations":[],"pmid":"","title":"LiDAR-Based Object Tracking and Shape Estimation Using Polylines and Free-Space Information","journalPages":"4515-4522","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593385"],"entities":[],"journalVolume":"","outCitations":[],"id":"73966f821e3304504eac9fe7049dc66335c9b08e","s2Url":"https://semanticscholar.org/paper/73966f821e3304504eac9fe7049dc66335c9b08e","authors":[{"name":"Stefan Kraemer","ids":[]},{"name":"Christoph Stiller","ids":[]},{"name":"Mohamed Essayed Bouzouraa","ids":[]}],"doi":"10.1109/IROS.2018.8593385"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594129","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"A Spanish fan is a hand held traditional fan which is used as an accessory and also by Flamenco dancers. The manipulation of the fan is quite difficult as it involves dynamic motion which includes opening, flapping and closing the fan along a pivotal point. The key points include the motion to be quick and the fan to be opened to the maximum degree possible without human intervention. A robotic arm with 7 Degrees of Freedom (DOF) is used to manipulate the autonomous motion. The fan placed on the table is localized and detected using a camera by background subtraction, masking and filtering; post which the contour of the fan is detected. The pixels obtained is then transformed into real life coordinates. The Dynamixel motors then traverses to the coordinates of the fan's position to grasp, open, flap, close and put the fan down.","inCitations":[],"pmid":"","title":"Flamen − 7 DOF Robotic Arm to Manipulate a Spanish Fan","journalPages":"4152-4157","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594129"],"entities":[],"journalVolume":"","outCitations":[],"id":"3c8a0c24ebeb33f72a897fa7946d56db76b51dc5","s2Url":"https://semanticscholar.org/paper/3c8a0c24ebeb33f72a897fa7946d56db76b51dc5","authors":[{"name":"Manu Harikrishnan Nair","ids":[]},{"name":"Tarush Ghanshsyam Singh","ids":[]},{"name":"Gunjan Chourasia","ids":[]},{"name":"Amrit Das","ids":[]},{"name":"Akarsh Shrivastava","ids":[]},{"name":"Zeel Shaileshkumar Bhatt","ids":[]}],"doi":"10.1109/IROS.2018.8594129"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593786","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Sensor fusion is a fundamental process in robotic systems as it extends the perceptual range and increases robustness in real-world operations. Current multi-sensor deep learning based semantic segmentation approaches do not provide robustness to under-performing classes in one modality, or require a specific architecture with access to the full aligned multi-sensor training data. In this work, we analyze statistical fusion approaches for semantic segmentation that overcome these drawbacks while keeping a competitive performance. The studied approaches are modular by construction, allowing to have different training sets per modality and only a much smaller subset is needed to calibrate the statistical models. We evaluate a range of statistical fusion approaches and report their performance against state-of-the-art baselines on both realworld and simulated data. In our experiments, the approach improves performance in IoU over the best single modality segmentation results by up to 5%. We make all implementations and configurations publicly available.","inCitations":[],"pmid":"","title":"Modular Sensor Fusion for Semantic Segmentation","journalPages":"3670-3677","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593786","https://arxiv.org/pdf/1807.11249v1.pdf","http://arxiv.org/abs/1807.11249","http://export.arxiv.org/pdf/1807.11249"],"entities":[],"journalVolume":"","outCitations":["80dfa2753b6825351038106732c0ab940dd42537","8283cc4fd5218e2a818324214c79682bc684626d","9201bf6f8222c2335913002e13fbac640fc0f4ec","9360ce51ec055c05fd0384343792c58363383952","3f310c55503374a5097e02ea672960537776f411","d1ee87290fa827f1217b8fa2bccb3485da1a300e","0e6cb161465d3305142b440937116894f8a91671","3cdb1364c3e66443e1c2182474d44b2fb01cd584","8b70366c60eae942a39040a85493c0d6a3203d86","12660f0defc6580e566c0fa2ac909971d6c6883b","3605b9befd5f1b53019b8edb3b3d227901e76c89","18d5f0747a23706a344f1d15b032ea22795324fa","866bc958bfb67dd8477f1dda4328fb1b4a4595a5","f7103090b0bc74106025ac96b21b6ce5d1fb569c","56142d781c2c1231ffbda59efb6f96fc7b5b5b52","f3dab33e5f00e80ed4ce97d3443e96eb0ee96301","ad90445eb2b6615174981e3c36c9aa25754276b8","c0f17f99c44807762f2a386ac6579c364330e082","2153fb8ba5b10961da93ba5ece69114862e0fe56","91035e3dd523919fe0c818f53e06224112189a8d","b45d972d99b722fb2fbb077d9d1d698abcc70974","974fa8ac07387f9719c05a2ad60de3cd0c979bd2","0ee1916a0cb2dc7d3add086b5f1092c3d4beb38a","ec25da04ef7f09396ca00da3f9b5f2d9670cb6fc","32cde90437ab5a70cf003ea36f66f2de0e24b3ab","2c03df8b48bf3fa39054345bafabfeff15bfd11d","061356704ec86334dbbc073985375fe13cd39088"],"id":"e6d3ac59bf7ffffd713743f85f1458e81dbba008","s2Url":"https://semanticscholar.org/paper/e6d3ac59bf7ffffd713743f85f1458e81dbba008","authors":[{"name":"Hermann Blum","ids":["37471778"]},{"name":"Abel Gawel","ids":["3414446"]},{"name":"Roland Siegwart","ids":["1720483"]},{"name":"Cesar Cadena","ids":["2586813"]}],"doi":"10.1109/IROS.2018.8593786"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593442","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a novel sliding mode controller for trajectory tracking of the piezo-driven stage. The tracking performance of piezoelectric actuator is mainly affected by the hysteresis nonlinearity. Sliding mode control is a possible solution to achieve better tracking performance. However, conventional sliding mode control generates discontinuous control signal which results in chattering. Hence, the hysteresis nonlinearity is first compensated with a hysteresis model, and an uncertainty and disturbance estimator is designed and included to devise a smooth control action. The stability of the proposed method is demonstrated via Lyapunov analysis. Both simulation and experiment are also conducted to verify the effectiveness of the proposed approach. The results are compared with a conventional sliding mode controller and a proportional-integral control with notch filter (PIC-NF).","inCitations":[],"pmid":"","title":"Motion Control of Piezo-Driven Stage via a Chattering-Free Sliding Mode Controller with Hysteresis Compensation","journalPages":"6424-6430","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593442"],"entities":[],"journalVolume":"","outCitations":[],"id":"82c417c5523fa2cc79434030ba5cbf5eecc0de04","s2Url":"https://semanticscholar.org/paper/82c417c5523fa2cc79434030ba5cbf5eecc0de04","authors":[{"name":"Yunfeng Fan","ids":[]},{"name":"Yichang He","ids":[]},{"name":"Dingguo Zhang","ids":[]},{"name":"U-Xuan Tan","ids":[]}],"doi":"10.1109/IROS.2018.8593442"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545321","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Thermal images are essential to deal with situations in dark environments, as they capture the objects' temperature. While the objects can still be seen in thermal images, the texture is extremely blur or even not observable at all. We propose to extract different features from images that capture various characteristics of the images. As one feature emphasizes one distinguishing aspect differing from the others, we can grasp multiple pieces of evidence from the images and take advantage of each to improve the thermal image classification accuracy. In particular, in additional to corner features usually used in color images, we also extract features from the edges and the shapes of the objects that emphasize the integral image appearance, as well as the temperature characteristics obtained from the image intensity. In this way, even if one feature is not evident in an image, the others can still play a critical role towards the correct classification result. By optimizing the objective function, we maximize the fusion performance of multiple features. By doing so, we can to the largest extent make use of the information exhibited in the thermal images to classify the query image into the correct group. Experiments demonstrate promising thermal image classification result.","inCitations":[],"pmid":"","title":"Getting Rid of Night: Thermal Image Classification Based on Feature Fusion","journalPages":"2827-2832","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545321"],"entities":[],"journalVolume":"","outCitations":["b69be1416f374815ff1d6573f976d996aaf125bc","5a1d8aa620fc3f36a79c2bd43ecee2c1cc8ce1b3","730c2978b31e4302a5d1752b0b88dbf46aba3eb6","3a3e615fbd2063f57ddb216a1836b5a65b7f8085","1a998506899da3bc703455bde45114bd4c228947","29d5c0e598dd678abbdef557beb89704adcac87a","25e7412e12b922e6e3aab377caa85a7b51730f8f","7efdc6b00ec4b162ceb4127965ed486115b78d7c","45c7a4b20ad32ef6fa7029cb15c09b3e83a1a54b","488e60f22cf555727aac154df5bc38f6af4d486e","4ff7739f78b315b7d024658076bff6b0c44075f4","b9a60078cbce2a183f4f91e5390315d3e3537413","0fbef124ad7eb3a16b5c0dec070200b6c730a618","700950bcaf924ac0b7d3dea680c22836477d82e2","5bea3c2f5849d2b733e3f0a661a0f09c646830d5","bb52e0ece9afda068aaa8bf52cf524e9869efdab","727f3f30527f1726d13cde66e9f92083894784f1","9da15b932df57a8f959471ebc977d620efb18cc1","80014f206c166e85ff028f85eee81422c544aedc","3037897d2fd1cc72dfc5a5b79cf9c0e8cdae083e","b022b9c7319c19217fdf17bd99606dcf68942d60","9af2dbc9622eb8b26d10cc8e270846c8d1d72aa7","c14fa910dc69a5a2646f0335dad5c4a95f7f3f49","016bae9f81f091386af39605a0da89b7de1c683c","0acf48dab5a00be945a4aa46fb1ddb235fea669e","2555746d8f33c91b62f91099426cae8882578d3d","4e8793c4625f8989ef77a55dc43b8a72c542fb6d","ba5ee783f11e47663b3d56458650735437b73ee2"],"id":"57c000b218175b7b908f83a062988e8e13766bc3","s2Url":"https://semanticscholar.org/paper/57c000b218175b7b908f83a062988e8e13766bc3","authors":[{"name":"Guoyu Lu","ids":["3073046"]},{"name":"Huili Yu","ids":["1782297"]},{"name":"Chun Yuan","ids":["1794683"]}],"doi":"10.1109/ICPR.2018.8545321"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202297","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The use of flexure hinges offers significant benefits compared to conventional revolute joints, such as the use for compact monolithic designs of flexural structures, no friction losses and reduced assembly and manufacturing costs. The combination of this design concept with additive manufacturing (AM) techniques provides huge potential for kinematics design, because of the ability to manufacture complex kinematic structures in one part with integrated adjustable functional elements, i.e. flexure hinges. Considering that there is sparse knowledge of thin-walled structures fabricated using additive manufacturing in current literature, it is obvious that the design of flexure hinge mechanisms for soft robotic applications is challenging today. Therefore, this paper describes software for simulation of plane flexure hinge structures based on investigations on the material characteristics of laser sintered polyamide, especially fatigue strength. Furthermore, the software enables automated design of these flexural structures (e.g. manipulator structures) as STL files, representing the standard interface to all additive manufacturing technologies.","inCitations":["d8716da3d76cac95460b868834038b6dd69d4898"],"pmid":"","title":"Fatigue strength of laser sintered flexure hinge structures for soft robotic applications","journalPages":"1230-1235","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202297"],"entities":["3D printing","Design flow (EDA)","Resultant","Robot","STL (file format)","Simulation","Sparse matrix","Thin-film transistor","Utility functions on indivisible goods"],"journalVolume":"","outCitations":["9930d4d8d6589f7c916c07fc2d6356db830ab050","e3258697389c4b89429ca7746163f5848a786e15","f71cdfbd62f086f98716ea8026f78745313e4044","ffee8cd2b7630c6bec091ca7dceae1f5dcf4bfca","b75eb92d86d036a60ab98b390c8e1b00fad203b2","cdb52e25b27af1667f7c9ec0916078831d085b03","63b89b6b0b026176253264559d20d0e64e231f44"],"id":"8c32f9fbe74d46d0230ea01aaa335cbf90c92ef7","s2Url":"https://semanticscholar.org/paper/8c32f9fbe74d46d0230ea01aaa335cbf90c92ef7","authors":[{"name":"Yannick S. Krieger","ids":["3216115"]},{"name":"Clara-Maria Kuball","ids":["32142712"]},{"name":"Dominik Rumschoettel","ids":["2807669"]},{"name":"Christian Dietz","ids":["49536742"]},{"name":"Jonas H. Pfeiffer","ids":["1866442"]},{"name":"Daniel B. Roppenecker","ids":["2681287"]},{"name":"Tim Lüth","ids":["1709270"]}],"doi":"10.1109/IROS.2017.8202297"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545105","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we present a novel framework for interactive segmentation of glioblastoma in contrast-enhanced Tl-weighted magnetic resonance images. U-net based-fully convolutional network is combined with an interactive refinement technique. Initial segmentation of brain tumor is performed using U-net, and the result is further improved by including complex foreground regions or removing background regions in an iterative manner. The method is evaluated on a research database containing post-operative glioblastoma of 15 patients. Radiologists can refine initial segmentation results in about 90 seconds, which is well below the time of interactive segmentation from scratch using state-of-the-art interactive segmentation tools. The experiments revealed that the segmentation results (Dice score) before and after the interaction step (performed by expert users) are similar. This is most likely due to the limited information in the contrast-enhanced Tl-weighted magnetic resonance images used for evaluation. The proposed method is computationally fast and efficient, and could be useful for post-surgical treatment follow-up.","inCitations":[],"pmid":"","title":"Interactive Segmentation of Glioblastoma for Post-surgical Treatment Follow-up","journalPages":"1199-1204","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545105"],"entities":[],"journalVolume":"","outCitations":["5451322aec50b2697d8052f9609b00cb82d04698","39f6fcb4384e59ed07b2c7dc79ae2142366d4c0a","11dab2374426b1c57366f00fefd339084ea2738a","02e85d62fbd8249a046d00ac10e39546511b2a51","1366991fca32ed0f957f929cd893bd3e024bc611","687c10bc3fac364ff063ad19d259efda1726852d","88ab7165d780711ba12c477fef31b7bacf5ce95a","82af450121c9119ee75cc8258ab95b640fb5ae9e","0fcf6920134252995c916eb6e012baddb4b9e255","0cc4baae4f621e64826534c575d3870d6f49d37d","013cd20c0eaffb9cab80875a43086e0c3224fe20","5868c2be68b0fd4da9701fecfdef0f34dac5091e","e6cf98a006d27af35b86ed9eed3c1c2ca81183d8","f86d791013ae5454a75ae63eaa9f2e7031c5ab49","0b46771d9b51ad6dac9e5b8e514876e258c3794c","3ddfa0e7da788180f27ed76a22ec38372ff0ef0c","4e607b44337fe7160cb4f4ce153332c7e53964a7","4e9498322979ee4aa286b7aed222240d789efd02","6404fef3804ff16bdcea260ad04a6ea7078ca9e6","a8cfdd8da19aa68afb1dbe51c7af80414da4d2e6","7a953aaf29ef67ee094943d4be50d753b3744573","d08cf42a68eff88814cdff81afa7f7b16dafd05b","995b94cb4246172843153414c1e6dc4cb004dbf4","272216c1f097706721096669d85b2843c23fa77d","2116b2eaaece4af9c28c32af2728f3d49b792cf9","6642d4d6ae62d8c9fefd1752e5a4224ffdbe920a","19243d3fd6c42a96fb662cda8447c6c12226abff","b2f2aaad7206a91d84d70b2f905b4ee3a2be96b3","15cf63f8d44179423b4100531db4bb84245aa6f1","b13b40d94e06723401963342dd67e104132fe489","d7726d32457c37f62f4ce11211d8a8656fa6c52f"],"id":"be87ab56cad6f18b33a49372301c714be87aad00","s2Url":"https://semanticscholar.org/paper/be87ab56cad6f18b33a49372301c714be87aad00","authors":[{"name":"Ashis Kumar Dhara","ids":["2089923"]},{"name":"Erik Arvids","ids":["52203180"]},{"name":"Markus Fahlstrom","ids":["52182663"]},{"name":"Johan Wikström","ids":["46647901"]},{"name":"Elna-Marie Larsson","ids":["18127423"]},{"name":"Robin Strand","ids":["2299190"]}],"doi":"10.1109/ICPR.2018.8545105"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206408","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Selective weeding is one of the key challenges in the field of agriculture robotics. To accomplish this task, a farm robot should be able to accurately detect plants and to distinguish them between crop and weeds. Most of the promising state-of-the-art approaches make use of appearance-based models trained on large annotated datasets. Unfortunately, creating large agricultural datasets with pixel-level annotations is an extremely time consuming task, actually penalizing the usage of data-driven techniques. In this paper, we face this problem by proposing a novel and effective approach that aims to dramatically minimize the human intervention needed to train the detection and classification algorithms. The idea is to procedurally generate large synthetic training datasets randomizing the key features of the target environment (i.e., crop and weed species, type of soil, light conditions). More specifically, by tuning these model parameters, and exploiting a few real-world textures, it is possible to render a large amount of realistic views of an artificial agricultural scenario with no effort. The generated data can be directly used to train the model or to supplement real-world images. We validate the proposed methodology by using as testbed a modern deep learning based image segmentation architecture. We compare the classification results obtained using both real and synthetic images as training data. The reported results confirm the effectiveness and the potentiality of our approach.","inCitations":["b648d73edd1a533decd22eec2e7722b96746ceae","27a918a368e971a15b545abf63353e269a8ce8a2","73b320dac2e9288da15431228d96b168608e4428","149186775bdf9bc7a369ac498968581c154a367f","d00d799bb902b5737e9c1aa922ada2b596ebff85","7300d1b4ed4974c02bcc99ec5164d15c2138e624","4abb3db338fb5ced964f92059ddebfeb2d21a532","0ed360773188436b297250bcdc1eb28d5f00709f"],"pmid":"","title":"Automatic model based dataset generation for fast and accurate crop and weeds detection","journalPages":"5188-5195","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1612.03019v2.pdf","https://arxiv.org/pdf/1612.03019v3.pdf","http://arxiv.org/abs/1612.03019","https://doi.org/10.1109/IROS.2017.8206408","https://arxiv.org/pdf/1612.03019v1.pdf","http://flourish-project.eu/fileadmin/user_upload/publications/2017-iros-cicco.pdf"],"entities":["Algorithm","Deep learning","Explicit modeling","Image segmentation","Pixel","Robotics","Synthetic data","Synthetic intelligence","Testbed","Visual modeling","X86 memory segmentation"],"journalVolume":"","outCitations":["e04d7772b91a83a901408eb0876bbb7814b1d4b5","848343ff72682995da16e18b56d54823ab177474","344e99659aa1a705b17dc82cad69ac36f8d6b843","396aacab076a3607429f58ce442d5d57b5aaa794","d1e8598fef2e5dbdc388090c8a3262ae64448801","0863a20f900e61952ea3a16d603257089b7b27a7","4d9d25e67ebabbfc0acd63798f1a260cb2c8a9bd","0a5d6a04deae65337e8d16cd2806868e965feba0","f59d8504d7c6e209e6f8bcb62346140214b244b7","745714fa6f8118ecfd91853fa1341ae7acbfd45c","6f4a671537c9e60f042808451ff0fc06032d1221","03ae36b2ed0215b15c5bc7d42fbe20b1491e551a","6789d9b9a20da1de8d7f6547771fcb332434cedc","84b50ebe85f7a1721800125e7882fce8c45b5c5a","081651b38ff7533550a3adfc1c00da333a8fe86c","d421d1fac18f0883276547f49497c8669ad19aac","3cdb1364c3e66443e1c2182474d44b2fb01cd584","8e7954fcc4af9bbe07189bc3ddf8d145f1a9328b","a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","871603a2ed446e713d63ef5137d3c46b6ee52b28"],"id":"3471a0fa6e5b697b605b919b58eee380edf1a9cc","s2Url":"https://semanticscholar.org/paper/3471a0fa6e5b697b605b919b58eee380edf1a9cc","authors":[{"name":"Maurilio Di Cicco","ids":["2284017"]},{"name":"Ciro Potena","ids":["3235263"]},{"name":"Giorgio Grisetti","ids":["1737531"]},{"name":"Alberto Pretto","ids":["1680186"]}],"doi":"10.1109/IROS.2017.8206408"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202289","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We demonstrate a flexible, electrostatic adhesive gripper designed to controllably grasp and manipulate soft goods in space. The 8-fingered gripper has 50 cm2 of active electrodes operating at 3 kV. It generates electrostatic adhesion forces up to 3.5 N (0.70 kPa) on Ge-coated polyimide film and 1.2 N on MLI blanket, a film composite used for satellite thermal insulation. Extremely low-force gripper engagement (0.08 N) and release (0.04 N) of films is ideal for micro-gravity. Individual fingers generate shear adhesion forces up to 4.76 N (5.04 kPa) using electrostatic adhesive and 45.0 N (47.6 kPa) with a hybrid electrostatic / gecko adhesive. To simulate a satellite servicing task, the gripper was mounted on a 7-DoF robot arm and performed a supervised grasp, manipulate, and release sequence on a hanging, Al-coated PET film.","inCitations":["91a1e52587fc4a3c8f459b0d4085019458e76fbf","455aac9e473095310a0417df99e473a76e3a10af"],"pmid":"","title":"An electrostatic gripper for flexible objects","journalPages":"1172-1179","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202289"],"entities":["Autonomous robot","Buckling","Experiment","Gecko","Known-plaintext attack","Markov blanket","Polyethylene terephthalate","Record sealing","Robot end effector","Robotic arm","Sensor","Simulation"],"journalVolume":"","outCitations":["3e8fc8c8f78ec2a3027f086d8147c1ea09bddccb","125859e245d4ed391ba9d05420f0ff4a0a1eb025","542904ebede75dbae4d8abec4f2a72d0f4015de8","0ba08a6c2dd9f80e7b078164824dd88103dca196","f355b45a6047b917778f985a35a30cf949aac396","332648a09d6ded93926829dbd81ac9dddf31d5b9","c00a911d955bd81d88ea055ec675a3617232313f","0ebbb44ee09326bb3f7203632c93775873cb569b","61a57ae0bbed75cb826a7e53a124938c6bb8ea09","2b115bc798ad19fd418a042a5ead555464e006b8","916e5bbccbef8fa45782cac1ec9eee058b0122c7","c79dcbd2d833b95c3d24687ce4225986854c951f","bd1141001e479ee89904dcdcd830f5cfbd578c3b","0a6c3943b6110fbd24ef03686bbf2676b4e8b9c8","643cb7c4fb6d2db00a542b731e346e401b70ab7f","55bf92431eb81f02ee931692cba19e80e41d48a4","8ea4333131aaac285885a8273be8746c2cfc3b52","e6c95e390c23ef6105cc285779580c9afefcfbc5","7f0687923b4e86b1917979b4e017c91d768247f5"],"id":"84ae50626d5315fe5b8bf93548fda4c3a152d885","s2Url":"https://semanticscholar.org/paper/84ae50626d5315fe5b8bf93548fda4c3a152d885","authors":[{"name":"Ethan W. Schaler","ids":["1812403"]},{"name":"Donald Ruffatto","ids":["1865992"]},{"name":"Paul E. Glick","ids":["40583935"]},{"name":"Victor White","ids":["35439418"]},{"name":"Aaron Parness","ids":["1787355"]}],"doi":"10.1109/IROS.2017.8202289"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593766","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Effective human-robot collaboration in shared control requires reasoning about the intentions of the human user. In this work, we present a mathematical formulation for human intent recognition during assistive teleoperation under shared autonomy. Our recursive Bayesian filtering approach models and fuses multiple non-verbal observations to probabilistically reason about the intended goal of the user. In addition to contextual observations, we model and incorporate the human agent's behavior as goal-directed actions with adjustable rationality to inform the underlying intent. We examine human inference on robot motion and furthermore validate our approach with a human subjects study that evaluates autonomy intent inference performance under a variety of goal scenarios and tasks, by novice subjects. Results show that our approach outperforms existing solutions and demonstrates that the probabilistic fusion of multiple observations improves intent inference and performance for shared-control operation.","inCitations":[],"pmid":"","title":"Recursive Bayesian Human Intent Recognition in Shared-Control Robotics","journalPages":"3905-3912","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593766"],"entities":[],"journalVolume":"","outCitations":[],"id":"4f5b50d481fb845b2f633bdb6f587465998060ee","s2Url":"https://semanticscholar.org/paper/4f5b50d481fb845b2f633bdb6f587465998060ee","authors":[{"name":"Siddarth Jain","ids":[]},{"name":"Brenna Argall","ids":[]}],"doi":"10.1109/IROS.2018.8593766"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206529","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Manta ray's pectoral fins have been a great source of inspiration for propulsive mechanism of autonomous underwater vehicles, due to their propulsive capability. The geometry (shape) and flexibility factors of these fins have been hypothesized to be determinants of the propulsive capability of the fins in terms of thrust generation. In particular, the sweep angle factor has been omitted from previous studies, where it has been commonly set to about 30 degrees. This paper investigates the effects of sweep angle on thrust generation of oscillatory pectoral fins. Forty different fins were designed and fabricated to be experimented in a water channel, which involved measurement of thrust generated by the fins. The experiment was conducted under free stream (0.5 m/s) and still water conditions. Five different sweep angles (0, 10, 20, 30, 40 degrees) were incorporated into eight base designs of different flexibility characteristics to make up the 40 fins. To consider only sweep angle, other geometrical factors were not varied. Within the range of the sweep angle considered, the experimental results showed that sweep angle has no significant influence on the fins' thrust generation, under both free stream and still water conditions. Overall, it can be concluded that sweep angle may not be a determinant of oscillatory pectoral fins' thrust generation.","inCitations":["662ca071f2d20bbd51acb5563035e9bfbc2e5709"],"pmid":"","title":"Study of sweep angle effect on thrust generation of oscillatory pectoral fins","journalPages":"6271-6276","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206529"],"entities":["Autonomous robot","Experiment","Thrust","Uridium"],"journalVolume":"","outCitations":["001d439b369d19567a191c5bcf8deb90449d584f","d32be166fd1c018e4106a75d86c056d483c434ad","693b656143a085af8079f4e611c8f8e4af157067","a47ec4039eebf20bf10aa60bbfcc8a02dc261842","0cb800b591089aaf7c79deae45d905563101d2cc","8b1f9b6c528e1e368a3e119a53af0eb2b660e353","e84814c7a9af6239f8525e65dfd9acafe258eef2","eec578415df3858e7c9eb83539bbda4a17918412","4b253bd38b59ebfbaae88234c105e734485e5057","d781a6b139d58ae92d295b53a91d41945b171fa7","31fda38674899934149e7c942d11a3ba238c6809"],"id":"05c16c03ab423fb399232287e248b035c525efda","s2Url":"https://semanticscholar.org/paper/05c16c03ab423fb399232287e248b035c525efda","authors":[{"name":"Chee-Meng Chew","ids":["1784228"]},{"name":"Soheil Arastehfar","ids":["2631047"]},{"name":"Gunawan","ids":["2374738"]},{"name":"Khoon Seng Yeo","ids":["32607753"]}],"doi":"10.1109/IROS.2017.8206529"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545327","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) has demonstrated, in recent years, a great potential as a complementary diagnostic method for early detection and diagnosis of breast cancer. However, due to the large amount of data, DCE-MRI manual inspection is error prone and can hardly be handled without the use of a Computer Aided Diagnosis (CAD) system. In a typical CAD processing, the segmentation of the breast parenchyma is a crucial stage aimed to reduce computational effort and to increase reliability. In the last years, deep convolutional networks have outperformed the state-of-the-art in many visual tasks, such as image classification and object recognition. However, very few proposals based on a deep learning approach have been applied so far for segmentation tasks in the biomedical field. The aim of this work is to apply a suitably modified convolutional neural network for fully-automating the non-trivial breast tissues segmentation task in 3D MR data, in order to accurately segment breast parenchyma from the air and other tissues (such as chest-wall). The proposed approach has been validated over 42 DCE-MRI studies. The median segmentation accuracy and Dice similarity index were 98.93 (±0, 15) and 95.90 (±0, 74) respectively with p< 0.05, and 100% of neoplastic lesion coverage.","inCitations":[],"pmid":"","title":"Breast Segmentation in MRI via U-Net Deep Convolutional Neural Networks","journalPages":"3917-3922","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545327"],"entities":[],"journalVolume":"","outCitations":["c56bd53089f13424eb7b6b117d516dbe24c15722","b583746b878a9fb30bf65dbfac1d98e61a5358d0","080efc3f664cb31f95a8e7a255c5802f0d91031d","14318685b5959b51d0f1e3db34643eb2855dc6d9","52338edc1f2cfe2a1d50728073173ac1caefc7df","66de7b3cdd37fc080327fd6bd4a5d3d97d3cc29f","4b38421a5ea28ce82de8ec1d386a5382b58d13f7","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","061356704ec86334dbbc073985375fe13cd39088","409b145298af26cb9154ec7024324955fb071b2e","f76d5a02d00ebb86447e68199993f80dc9bc62fd","33da83b54410af11d0cd18fd07c74e1a99f67e84","9201bf6f8222c2335913002e13fbac640fc0f4ec","6a694e3134263b789823c486467d628a2e83bc98","9d62654b8493bd00e1cb1b342f67f0ea6dc84520","272216c1f097706721096669d85b2843c23fa77d","47a4d91c7249a7ddbeeee2a2d28e24c953a72d6b","ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","5c983f1ff747d11581a5f42465f96a6d34d077bc","2b40545a6af971b370932b854150e74c623f7357","6a801eb8fff8ae45c36198a7d01cf62391de0fb8","ff669b73fa3a1bc6fe4bdee7b4987e759f5dd848","fa234d48a1b9239291db7fb16ea505f786af0819","580cb7697b5d9dbb45bae70c4710d89e5b70bcb7","9356a0862ad0dd269e24f09924d2d1f99ea7fda9","fabfffd7aae0db57366584e57e24ca05c29cce57","63b635a5c303382c31ad6baf96512ca872821775","d722819edaa4b3aecfc86fffef12bee752bd79e4","5e129d09d09595dee89dd47d3a426f4fd8d12e75","f2ae8a32d9bc16b0fad2d382e7ec136b43697ffc","a5d89eabc03afef0a4aa2b2cd87dee3a4659f26d","0012de6bec1f25599e4f02517637e531a71909b9","09193e19b59fc8f05bee9d6efbfb1607ca5b6501","b13b40d94e06723401963342dd67e104132fe489"],"id":"9ee301de6c289cd2d46c870f5d053ff2355288af","s2Url":"https://semanticscholar.org/paper/9ee301de6c289cd2d46c870f5d053ff2355288af","authors":[{"name":"Gabriele Piantadosi","ids":["2658078"]},{"name":"Mario Sansone","ids":["36023013"]},{"name":"Carlo Sansone","ids":["6112715"]}],"doi":"10.1109/ICPR.2018.8545327"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594230","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper describes a cooperative control approach that combines the use of a powered knee joint orthosis along with Functional Electrical Stimulation (FES) for knee joint flexion-extension movement restoration. A closed-loop adaptive control and an open-loop FES of the quadriceps muscle group are combined together to track a desired knee joint angle trajectory of flexion/extension movements. A nonlinear disturbance observer is used to estimate the torque provided by the subject's muscles through the FES. Simulations and experiments with a healthy subject show the feasibility of the proposed approach. Experiments show the repeatability of motion and the complementarity between the torque provided by the quadriceps muscle through FES and the one delivered by the orthosis actuator to ensure satisfactory tracking of the desired trajectory.","inCitations":[],"pmid":"","title":"Cooperative Control for Knee Joint Flexion-Extension Movement Restoration","journalPages":"5175-5180","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594230"],"entities":[],"journalVolume":"","outCitations":[],"id":"5bb973bde8d6f5ea909853070eef3b9c1ed5fa7b","s2Url":"https://semanticscholar.org/paper/5bb973bde8d6f5ea909853070eef3b9c1ed5fa7b","authors":[{"name":"Mohamed Amine Alouane","ids":[]},{"name":"Hala Rifai","ids":[]},{"name":"Yacine Amirat","ids":[]},{"name":"Samer Mohammed","ids":[]}],"doi":"10.1109/IROS.2018.8594230"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593696","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper is a preliminary exploration of how to solve the dual-manipulator path-planning problem from an energy perspective. A virtual spring is set up between the two manipulator bodies and becomes compressed as they move into the area of danger, thus producing elastic potential energy. The initial paths of the manipulators are modelled as two ends-fixed elastic ropes. The elastic potential energy stored in the virtual spring is distributed between the two elastic ropes in a certain proportion so as to deform them. In this way, the original paths of the two manipulators will deviate toward the direction of their respective bases thereby avoiding any collision crisis that may potentially occur. When the dual-manipulator moves away from the danger area, the elastic potential energy caused by the deviation of the elastic ropes will be converted back into energy stored by the virtual spring, such that the elastic ropes revert to their original state and drive the manipulators back to their initial paths. Simple but representative simulations are established and results of the simulations show the reliability of our proposed method.","inCitations":[],"pmid":"","title":"Collision-Free Path Planning of Dual-Manipulator System Based on Energy Conversion","journalPages":"8360-8366","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593696"],"entities":[],"journalVolume":"","outCitations":[],"id":"f35e8c7e5e48a883d5bf11ae518a0120a1e363d4","s2Url":"https://semanticscholar.org/paper/f35e8c7e5e48a883d5bf11ae518a0120a1e363d4","authors":[{"name":"Chang Su","ids":[]},{"name":"Ruixin Wei","ids":[]},{"name":"Mingliang Zhang","ids":[]},{"name":"Hannes Rose","ids":[]},{"name":"Jianfeng Xu","ids":[]}],"doi":"10.1109/IROS.2018.8593696"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202176","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a kinetostatic framework to analyze the grasping and in-hand object manipulation abilities of two-finger underactuated hands. The framework includes a procedure to compute the Grasp Matrix and the Hand Jacobian for objects and fingertips of arbitrary shape considering rolling contacts without slipping. The usefulness of the proposed approach is illustrated in a case study of a pair of underactuated fingers driven by a tendon-pulley differential transmission mechanism and capable of performing in-hand object manipulation. The manipulability region for different object and fingertip shapes is computed and the results are discussed.","inCitations":[],"pmid":"","title":"Influence of fingertip and object shape on the manipulation ability of underactuated hands","journalPages":"329-334","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202176"],"entities":["Jacobian matrix and determinant","Underactuation"],"journalVolume":"","outCitations":["ca442cc2eef5d9bd5cd5cb4c516cd15991569d93","8a3673ac6db5072f46914263b7ab29f6c73f9375","998b22bdbeef8677babf56344ae7c323cb328746","88c83b3a5f78e71060d0f88235f0a5a7019c961f","21644ba3c5262442e1f6dbeb2b574200eea3cb41","ec92b3794e8b1906bfe1ffd030378ea40b05f983","3469c3b847cae209ffa6d2931d5c23759764c08c","9f26aa38e5ac5d6f4ce07c2a76aba84270593159","16ba6cbe2f6ca731c831be2aa3ca288a9618d3d4"],"id":"7c0c54d168b6e00dcb1641516a543221e0c2a025","s2Url":"https://semanticscholar.org/paper/7c0c54d168b6e00dcb1641516a543221e0c2a025","authors":[{"name":"Diego Ospina","ids":["47974288"]},{"name":"Alejandro Ramirez-Serrano","ids":["1682358"]}],"doi":"10.1109/IROS.2017.8202176"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593794","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Robotic systems for object handling and manipulation are hugely important for modern engineering and industry, with their efficiency, agility and robustness often depending on gripper design and performance. In this work, we investigate a gripper design that, when driven by a twisted string actuator, exhibits nearly-constant transmission ratio throughout its motion range. This allows for design of a highly-compact, modular and efficient robotic gripper driven by a low-power motor. We investigate kinematics of the device, experimentally verify developed models with a practical gripper testbed, and analyze transmission ratio and efficiency of the designed device. The resulting system has a nearly-constant transmission ratio of 550, with the constancy coefficient of 0.985.","inCitations":[],"pmid":"","title":"Design of Robotic Gripper with Constant Transmission Ratio Based on Twisted String Actuator: Concept and Evaluation","journalPages":"967-972","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593794"],"entities":[],"journalVolume":"","outCitations":[],"id":"7bd4934bba2f380fe964fb32a145a855f02b984f","s2Url":"https://semanticscholar.org/paper/7bd4934bba2f380fe964fb32a145a855f02b984f","authors":[{"name":"Simeon Nedelchev","ids":[]},{"name":"Igor Gaponov","ids":[]},{"name":"Jee-Hwan Ryu","ids":[]}],"doi":"10.1109/IROS.2018.8593794"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593903","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents the first two members of the new generation of CLASH hands, which exploit low cost actuation and rapid prototyping to create antagonistic modular and lightweight hands and grippers. The hands approach the robustness of the DLR Awiwi hand with a much lower complexity and cost. To reduce the number of required actuators, a differential coupling mechanism for underactuated fingers was developed, along with a new mechanism that uses variable stiffness actuation in order to increase the workspace of underactuated fingers. The hands provide a research platform for both hand-in-hand and robotic grasping. Design aspects are discussed, and an initial experimental validation verifies the hands' performance.","inCitations":[],"pmid":"","title":"CLASH: Compliant Low Cost Antagonistic Servo Hands","journalPages":"6469-6476","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593903"],"entities":[],"journalVolume":"","outCitations":[],"id":"2c361e44b00751cf6f40bdeb175809917d9d5cbb","s2Url":"https://semanticscholar.org/paper/2c361e44b00751cf6f40bdeb175809917d9d5cbb","authors":[{"name":"Werner Friedl","ids":[]},{"name":"Hannes Höppner","ids":[]},{"name":"Florian Schmidt","ids":[]},{"name":"Máximo A. Roa","ids":[]},{"name":"Markus Grebenstein","ids":[]}],"doi":"10.1109/IROS.2018.8593903"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546308","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Over the last decade, a large number of methods have been proposed for human fall detection. Most existing methods were evaluated based on trimmed datasets. More importantly, these datasets lack variety of falls, subjects, views and modalities. This paper makes two contributions in the topic of automatic human fall detection. Firstly, to address the above issues, we introduce a large continuous multimodal multivew dataset of human fall, namely CMDFALL. Our CMDFALL dataset was built by capturing activities from 50 subjects, with seven overlapped Kinect sensors and two wearable accelerometers. Each subject performs 20 activities including 8 falls of different styles and 12 daily activities. All multi-modal multi-view data (RGB, depth, skeleton, acceleration) are time-synchronized and annotated for evaluating performance of recognition algorithms of human activities or human fall in indoor environment. Secondly, based on the multimodal property of the dataset, we investigate the role of each modality to get the best results in the context of human activity recognition. To this end, we adopt existing baseline techniques which have been shown to be very efficient for each data modality such as C3D convnet on RGB; DMM-KDES on depth; Res-TCN on skeleton and 2D convnet on acceleration data. We analyze to show which modalities and their combination give the best performance.","inCitations":["f5f62615cf10ef9d6dcdcfdcc075e17f790279d7"],"pmid":"","title":"A multi-modal multi-view dataset for human fall analysis and preliminary investigation on modality","journalPages":"1947-1952","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546308"],"entities":[],"journalVolume":"","outCitations":["e05d01867f2d06685312d1f4a2193ef0ac11165c","721bc0c7399381eb4993741ebab5811437e9a242","99a78d3d5bc567e8f56fdb6a727727f940fab968","f75448f3951812886459090f9a0351c148981c2b","1920264d4ff7cae838ed37c74f39c6a37bd65332","7b3048f3a1b720798079e92b8a42c8daba328ec0","09b5b5890ff5fe68a11a7a2826333ac9eb6b3742","9ce4d5a8e2e83850552af2fed61dc76f7fcd98bc","542abc69c81c4ffdaa81631fb756b37b44ec18d0","1c30bb689a40a895bd089e55e0cad746e343d1e2","537bf5cefa79531990cdb3757cf4e4a4224bef7b","8ab709dcffad8abdc73f264993a88f4ca4f5299b","6f15a093497b60f65038dc399b39ee973f713459","be219bc2501b623300caee7f1bcfb14452da6811","92e9f31c6bd4ba7eff3d02446d2d25a5be014205","c03e06efa01fcfa24c866ffd519a9fdbf5a6cff7","5f12a421a7be30614b7dac1386b4a8c464d8cb5d","cc0d17d072c21de5886492c6a23642f15b7f7a25","911d6fc85399df59b574f42199b000404e407744","b829b6a25e3f6d75b5a1379d09860298730ecdb1","2428b0a7ac41085af816858d7b71d684c51f3d03","b4ea4bfa67321d3bbf4c778ce4c1c6b0a33cf7c4"],"id":"1b24dd96dd8593e11a3d45a32dbec9472dbf0341","s2Url":"https://semanticscholar.org/paper/1b24dd96dd8593e11a3d45a32dbec9472dbf0341","authors":[{"name":"Thanh-Hai Tran","ids":["2494823"]},{"name":"Thi-Lan Le","ids":["10128454"]},{"name":"Dinh-Tan Pham","ids":["35887972"]},{"name":"Van-Nam Hoang","ids":["2437194"]},{"name":"Van-Minh Khong","ids":["40956142"]},{"name":"Q Tran","ids":["12287940"]},{"name":"Thai-Son Nguyen","ids":["3060854"]},{"name":"Cuong Pham","ids":["4693097"]}],"doi":"10.1109/ICPR.2018.8546308"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206030","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Exploring and mapping previously unknown environments while avoiding collisions with obstacles is a fundamental task for autonomous robots. In scenarios where this needs to be done rapidly, multi-rotors are a good choice for the task, as they can cover ground at potentially very high velocities. Flying at high velocities, however, implies the ability to rapidly plan trajectories and to react to new information quickly. In this paper, we propose an extension to classical frontier-based exploration that facilitates exploration at high speeds. The extension consists of a reactive mode in which the multi-rotor rapidly selects a goal frontier from its field of view. The goal frontier is selected in a way that minimizes the change in velocity necessary to reach it. While this approach can increase the total path length, it significantly reduces the exploration time, since the multi-rotor can fly at consistently higher speeds.","inCitations":["2688364d5662b1e43b0f9b6057e034e9be6610d2","48c34278f39326f14bb616223ff9288fc925e47e","c1c4e8734564b91cd2636d693285f30729f67259","e4073aa4467bf7d75303fb97638fbf0365894939","05f22b465cbb96f7b5047b4abc7d9ddf85f3ab8f","29e247d8251fdb575d249fa50b8c10b51fd7af8d"],"pmid":"","title":"Rapid exploration with multi-rotors: A frontier selection method for high speed flight","journalPages":"2135-2142","s2PdfUrl":"","pdfUrls":["http://rpg.ifi.uzh.ch/docs/IROS17_Cieslewski.pdf","https://doi.org/10.1109/IROS.2017.8206030"],"entities":["Algorithm","Autonomous robot","Experiment","Instruction path length","R.O.T.O.R.","Simulation","Velocity (software development)"],"journalVolume":"","outCitations":["933282df1b96a7b0829dc71029a19dfd8f60de06","54584fa817611e9b24492eff04b81a0e834258e1","e28c8a6c397a48775cc10aa53adc003309af2fd8","088e21d6931729e005e6a622c6a92695e3c9dce8","2cc85de6c80639d3204ae0d4f247296cf6d9999d","5219bf0e5c957b18c8241e2b4ab38e490d6eac5c","01db820cbbbf763f34ea424f604f2a3ced8cb366","a56ed0bc2d69094e351a044ae8bc64ca0da691f8","53ead98595bc6d2e2c3570595bafd2e7eeee0841","6d14909167d1804da07b5f0937170f54d00fd893","0500a238f17e59535e8f6d8483bc1621880a7fe4","7216ed68677549dca19164b142d8570c19c2d5df","44ec5ddf1119c0e0f56a275cd4cc6c09c49bbece","5e3b383ec7970262425dcc2d1e7d710c52fa7b9c","5a1791a0a31f2ae051926ebcabd324286dec7105","0da29d4783bd5ac4170bd329cb084782159da805","9f85c8f83c835b997f4c613525836f6b53cfdb7f","ce81a0b905dcfc206d5122dd93785be3d1f40bc7","92f66d26dc52d4b9bde8684520a3dddfb3bd8d36","05c9dc44595d35163b50109e2b1e14bb863124cf"],"id":"a9bdaae018bb29a7a838947ba0015553b7dd3ea1","s2Url":"https://semanticscholar.org/paper/a9bdaae018bb29a7a838947ba0015553b7dd3ea1","authors":[{"name":"Titus Cieslewski","ids":["2017998"]},{"name":"Elia Kaufmann","ids":["32887979"]},{"name":"Davide Scaramuzza","ids":["2075371"]}],"doi":"10.1109/IROS.2017.8206030"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593832","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The transfer of a robot skill between different geometric environments is non-trivial since a wide variety of environments exists, sensor observations as well as robot motions are high-dimensional, and the environment might only be partially observed. We consider the problem of extracting a low-dimensional description of the manipulated environment in form of a kinematic model. This allows us to transfer a skill by defining a policy on a prototype model and morphing the observed environment to this prototype. A deep neural network is used to map depth image observations of the environment to morphing parameter, which include transformations and configurations of the prototype model. Using the concatenation property of affine transformations and the ability to convert point clouds to depth images allows to apply the network in an iterative manner. The network is trained on data generated in a simulator and on augmented data that is created with its own predictions. The algorithm is evaluated on different tasks, where it is shown that iterative predictions lead to a higher accuracy than one-step predictions.","inCitations":[],"pmid":"","title":"Kinematic Morphing Networks for Manipulation Skill Transfer","journalPages":"2517-2523","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593832","http://arxiv.org/abs/1803.01777","https://arxiv.org/pdf/1803.01777v1.pdf"],"entities":[],"journalVolume":"","outCitations":[],"id":"4734b0b22d044f007da4aaa108b834c4a9c598ca","s2Url":"https://semanticscholar.org/paper/4734b0b22d044f007da4aaa108b834c4a9c598ca","authors":[{"name":"Peter Englert","ids":["3060230"]},{"name":"Marc Toussaint","ids":["1731096"]}],"doi":"10.1109/IROS.2018.8593832"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206440","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Modern robotic systems often consist of a growing set of information-producing components that need to be appropriately connected for the system to function properly. This is commonly done manually or through relatively simple scripts by specifying explicitly which components to connect. However, this process is cumbersome and error-prone, does not scale well as more components are introduced, and lacks flexibility and robustness at run-time. This paper presents an algorithm for setting up and maintaining implicit subscriptions to information through its semantics rather than its source, which we call semantic subscriptions. The proposed algorithm automatically reconfigures the system when necessary in response to changes at run-time, making the semantic subscriptions adaptive to changing circumstances. To illustrate the effectiveness of adaptive semantic subscriptions, we present a case study with two SoftBank Robotics NAO robots for handling the cases when a component stops working and when new components, in this case a second robot, become available. The solution has been implemented as part of a stream reasoning framework integrated with the Robot Operating System (ROS).","inCitations":["75746f6316253e996ff200b1f40b2098643187c9"],"pmid":"","title":"Towards adaptive semantic subscriptions for stream reasoning in the robot operating system","journalPages":"5445-5452","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206440","http://www.ida.liu.se/divisions/aiics/publications/IROS-2017-Towards-Adaptive-Semantic.pdf"],"entities":["Algorithm","Canonical account","Central processing unit","Cognitive dimensions of notations","Computation","Middleware","Nao (robot)","Norm (social)","Robot Operating System","Run time (program lifecycle phase)","Scalability","Semantic Web","Semantic matching","Smart city","Substitution (logic)","Universal instantiation"],"journalVolume":"","outCitations":["3edf2cf8736c3b0c32adae354b9f159467cddef5","2a87f2d6dc109bbf749b27598458c6f52bf1911c","216580fd351f00fc67027b82042306a384453c74","958fd1a1eeea1a0da952f7e1585c898784159036","136f005da4f62d089140208ebd15fb3273616665","39fa98d17b06c6a56b2d98ed1e41f701ef56cd86","2cebd3c1700ba9a617f1091084f6505d1d39383a","a3e4a5f6055ab6d1a3f61f495826d5ba2258993e","4b8fbe5e18af87ce47b728bf7b4e644c9de0c95e","fe3abc47ea9cff302018ec7840a2463e661987b1","67ad38815cf8bc7e7810f9b3a57fdc3ad9926bf0","4b21f4a3ea3722a2cae26d4a6f61968253c1eec3","31f687621abfb19bcbe8c60c477b209e17e33bbf","158463840d4beed75e5a821e218526cd4d4d6801","7258820bc049bac680847a65d660477838ac060e","19448474c22eaa846a083acc5ce2f85aa7159392","0f45d04f7ae7a4e7ee1c327640e97b22c669f368","ee459f8a457ec69f20cd1b2b46c281141404c6a8","0c7ab71f963e1f329a029d34f234ae85d2f1c2b6","57820e6f974d198bf4bbdf26ae7e1063bac190c3"],"id":"1ffcab3bab7b5af677ca60ca758e981f6d3c637a","s2Url":"https://semanticscholar.org/paper/1ffcab3bab7b5af677ca60ca758e981f6d3c637a","authors":[{"name":"Daniel de Leng","ids":["3081699"]},{"name":"Fredrik Heintz","ids":["1711918"]}],"doi":"10.1109/IROS.2017.8206440"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594219","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This work presents the design and development of a semi-active hybrid orthotic system for support and facilitation of unilateral pathological human walking. The system is based on a novel lower limb orthosis with mechanical control and its combination with non-invasive muscle electrostimulation. The paper presents the concept design and realization of a novel Stance Control Knee Ankle Foot Orthoses (SCKAFO) and the design of Functional Electrical Stimulation (FES) strategies for artificial activation of ankle joint muscles in this hybrid scheme for gait support. In particular, we present the investigation of the effects of electrical stimulation patterns of ankle muscles synchronized in real-time with gait events., on the possible biomechanical alterations to gait in able-bodied individuals (n=8). Bilateral 3D ground reaction forces (GRF) were analyzed between barefoot overground walking with and without FES of ankle muscles. The observed effects of the tested FES strategy are coherent with a physiological gait strategy and compatible with the proposed SCKAFO.","inCitations":[],"pmid":"","title":"Design and Implementation of a Novel Semi-Active Hybrid Unilateral Stance Control Knee Ankle Foot Orthosis","journalPages":"5163-5168","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594219"],"entities":[],"journalVolume":"","outCitations":[],"id":"651a47c0d7dc6acfb17897424fc1217dbb566b1a","s2Url":"https://semanticscholar.org/paper/651a47c0d7dc6acfb17897424fc1217dbb566b1a","authors":[{"name":"Juan J. Gil","ids":[]},{"name":"Maria Carmen Sanchez-Villamanan","ids":[]},{"name":"J. Gomez","ids":[]},{"name":"A. Ortiz","ids":[]},{"name":"José Luis Pons Rovira","ids":[]},{"name":"Juan C. Moreno","ids":[]},{"name":"Antonio J. del Ama","ids":[]}],"doi":"10.1109/IROS.2018.8594219"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545325","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"The fast and accurate classification of polarimetric synthetic aperture radar (PolSAR) data in dynamically changing environments is an important and challenging task. In this paper, we propose an Incremental Multi-view Passive-Aggressive Active learning algorithm, named IMPAA, for PolSAR data classification. This algorithm can deal with online two-view multi-class categorization problem by exploiting the relationship between the polarimetric-color and texture feature sets of PolSAR data. In addition, the IMPAA algorithm can handle the dynamic large-scale datasets where not only the amount of data but also the number of classes gradually increases. Moreover, this algorithm only queries the class labels of some informative incoming samples to update the classifier based on the disagreement of different views' predictors and a randomized rule. Experiments on real PolSAR data demonstrate that the proposed method can use a smaller fraction of queried labels to achieve low online classification errors compared with previously known methods.","inCitations":[],"pmid":"","title":"An Incremental Multi-view Active Learning Algorithm for PolSAR Data Classification","journalPages":"2251-2255","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545325"],"entities":["Algorithmic learning theory","Class","Increment","Information","Multiclass classification","Name","Numerous","Polarimetry","Randomized algorithm","Small","Synthetic data"],"journalVolume":"","outCitations":["1d9ed45593efa254a482011fd6581a9a926342af","c20399b94b8da0fc20eb90a48be86c9c2db5a188","2780a78d5e4b97dde3da21455533289bd8739fac","cdbb793a7265868024ea739e8b8a2eac3a10bb52","1d54f8f409e3bd91a6470d3024415f00dbb6f9fa","d7a1ae2a9c6ca6cc49fc9612330cd7c27358b0f7","1277ae45da74654a38295370643eec6dc3a27a44","3741cb23745a0f37f7b4c1e02e6eef376face498","946985b1f15792c3f3f3bd08b8f8f66cd4d69b72","4e0f261e36ffeb8fb91da7b184c07fb5cfce3737","855f8935237dbb648b7982e0a0ddab0305346c44","c9ddd1b442d61c1dc66f0eeb1d019f3970707cf5","a1b608de62b03f0392c6142d32f3860568093882","1fbfa8b590ce4679367d73cb8e4f2d169ae5c624","08743d09f3ec33ab1f188d4c5f8f5550c312ace0","59003bbc3fcbe849b8a3432e7d3688144ecce47a","9117eff9c709f1fd2983f96f86bfee9bf454f29d","e7496108da7a11a0b59976a29999827e38de5b91"],"id":"d2293dc776131a0b4de74b03355ac53d03c754f5","s2Url":"https://semanticscholar.org/paper/d2293dc776131a0b4de74b03355ac53d03c754f5","authors":[{"name":"Xiangli Nie","ids":["2276170"]},{"name":"Yongkang Luo","ids":["2686849"]},{"name":"Hong Qiao","ids":["50173859"]},{"name":"Bo Zhang","ids":["39506391"]},{"name":"Zhong-Ping Jiang","ids":["1726851"]}],"doi":"10.1109/ICPR.2018.8545325"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594302","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In multi-robot systems, although the idea of behaviors allows for an efficient solution to low-level tasks, high-level missions can rarely be achieved by the execution of a single behavior. In contrast to this, a sequence of behaviors would provide the requisite expressiveness, but there are no a priori guarantees that the sequence is composable in the sense that the robots can actually execute it. In order to guarantee a provably correct composition of behaviors, Finite-Time Convergence Control Barrier Functions are introduced in this paper to guarantee the terminal configuration of one behavior is a valid initial configuration for the following one. Nominal control inputs prescribed by the behaviors are modified in a minimally invasive fashion, in order to establish the information-exchange network required by the following behavior. The effectiveness of the proposed composition strategy is validated on a team of mobile robots.","inCitations":["a7fb2d3d5c3d0245903df9893acb48ced6ca3ec8","3ab780599c0179c60b1d8b81d593a9bbc8f02737"],"pmid":"","title":"Formally Correct Composition of Coordinated Behaviors Using Control Barrier Certificates","journalPages":"3723-3729","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594302"],"entities":[],"journalVolume":"","outCitations":[],"id":"22f3fd03c904a07b8a93d4b3d4396d4bd3135145","s2Url":"https://semanticscholar.org/paper/22f3fd03c904a07b8a93d4b3d4396d4bd3135145","authors":[{"name":"Anqi Li","ids":[]},{"name":"Li Wang","ids":[]},{"name":"Pietro Pierpaoli","ids":[]},{"name":"Magnus Egerstedt","ids":[]}],"doi":"10.1109/IROS.2018.8594302"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594036","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Crossmodal conflict resolution is crucial for robot sensorimotor coupling through the interaction with the environment, yielding swift and robust behaviour also in noisy conditions. In this paper, we propose a neurorobotic experiment in which an iCub robot exhibits human-like responses in a complex crossmodal environment. To better understand how humans deal with multisensory conflicts, we conducted a behavioural study exposing 33 subjects to congruent and incongruent dynamic audio-visual cues. In contrast to previous studies using simplified stimuli, we designed a scenario with four animated avatars and observed that the magnitude and extension of the visual bias are related to the semantics embedded in the scene, i.e., visual cues that are congruent with environmental statistics (moving lips and vocalization) induce the strongest bias. We implement a deep learning model that processes stereophonic sound, facial features, and body motion to trigger a discrete behavioural response. After training the model, we exposed the iCub to the same experimental conditions as the human subjects, showing that the robot can replicate similar responses in real time. Our interdisciplinary work provides important insights into how crossmodal conflict resolution can be modelled in robots and introduces future research directions for the efficient combination of sensory observations with internally generated knowledge and expectations.","inCitations":["6b5aebfd0c9d36d7df2b19d51c5c22d486336482","2e110a46e3ebf03c90a2b4007b502e6caa744e04","2cc17f166d84b8a5b9ed0cb1254b98ee97e398eb"],"pmid":"","title":"A Neurorobotic Experiment for Crossmodal Conflict Resolution in Complex Environments *","journalPages":"2330-2335","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594036","https://arxiv.org/pdf/1802.10408v3.pdf","http://export.arxiv.org/pdf/1802.10408","http://arxiv.org/abs/1802.10408"],"entities":[],"journalVolume":"","outCitations":["7b88ab16449af88f3591207eb23bc234b802c08a","e50172e0aaf6527ad9a4b52bbf4b75dc6fd0c7fa","ca72745855417b733b2dbc6caed231b0454767d4","85a1fbe89aed6117eeb3a2ba4317bffb8c73cfd3","552bc5162ba0d3ff06dd25c8a5a5adefc3735910","114f86183e96689f3e83fd4efa87870fe302c82b","8194eb2b3a472c3884f712b9776232878d47a9ff","925cabc5143814410a1a02ad853a6e73fed3ce6e","00018f34a982eac1cb43025fae6fef6ce72d07aa","60f9c921d61f7b415b085f7f6cb46a9d9219db90","1f1fcfb7537812902eefbb6ebeb0f20cf29cdeb5","9f619bda530eb8969dbd00c7a190e5d8efd8269a","06f156ba3c50fd85ed355a9a0d111f2fcf749e2d","7e437dc0e0265195b92bb2b7dc39f0cc4d5c5fae","694b15f6822631959923ab8b1d69552b100d46a3","ea0ab3828cb750ece7aff5aaf609b4ee0b30a81f","37f94d78c1c2c12467709044d9da457a71d6402d","27ed7cec1bb9c92dc388c0f28a01f02ef48432bf","eef41ae597a20ea377461d522fd5100da6a7a9b7","825198ad69b2203997341fa903ce56ea28451644"],"id":"c5c434c1577b3d5117367603ce67a84435347195","s2Url":"https://semanticscholar.org/paper/c5c434c1577b3d5117367603ce67a84435347195","authors":[{"name":"German Ignacio Parisi","ids":["2988592"]},{"name":"Pablo V. A. Barros","ids":["1797394"]},{"name":"Di Fu","ids":["40360280"]},{"name":"Sven Magg","ids":["2632932"]},{"name":"Haiyan Wu","ids":["1748494"]},{"name":"Xun Liu","ids":["2201018"]},{"name":"Stefan Wermter","ids":["1736513"]}],"doi":"10.1109/IROS.2018.8594036"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206286","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a vision-based control strategy for tracking a ground target using a novel vision sensor featuring a processor for each pixel element. This enables computer vision tasks to be carried out directly on the focal plane in a highly efficient manner rather than using a separate general purpose computer. The strategy enables a small, agile quadrotor Unmanned Air Vehicle (UAV) to track the target from close range using minimal computational effort and with low power consumption. To evaluate the system we target a vehicle driven by chaotic dual-pendulum trajectories. Target proximity and the large, unpredictable accelerations of the vehicle cause challenges for the UAV in keeping it within the downward facing camera's field of view (FoV). A state observer is used to smooth out predictions of the target's location and, importantly, estimate velocity. Experimental results also demonstrate that it is possible to continue to re-acquire and follow the target during short periods of loss in target visibility. The tracking algorithm exploits the parallel nature of the visual sensor, enabling high rate image processing ahead of any communication bottleneck with the UAV controller. With the vision chip carrying out the most intense visual information processing, it is computationally trivial to compute all of the controls for tracking onboard. This work is directed toward visual agile robots that are power efficient and that ferry only useful data around the information and control pathways.","inCitations":["7dfad251cdcc854435962cacfcc6ef14c240d015"],"pmid":"","title":"Tracking control of a UAV with a parallel visual processor","journalPages":"4248-4254","s2PdfUrl":"","pdfUrls":["https://research-information.bristol.ac.uk/files/131410552/iros2017_final_greatwood_et_al.pdf","https://doi.org/10.1109/IROS.2017.8206286"],"entities":["Agile software development","Algorithm","Computer vision","Control theory","FOCAL (programming language)","Field of view in video games","Floating Point Systems","High-level programming language","Image processing","Information and Computation","Information processing","Offset binary","Parallel computing","Pixel","Robot","Sensor","Unmanned aerial vehicle","Velocity (software development)"],"journalVolume":"","outCitations":["5f9bcf879b009c4962e91b19a5812212b031e5d7","4ecb0317274838936c7f72a953fe8c8b13b57755","237102f615840ca692c6caa9e86103fb5c343606","67b239ddd1d2293a0f919f4a543ccc66267abef2","6a6daaa00ad51095e54a8782029d98b3722bd5e0","0a7962f03e21aeeff152349734965c4da975a3ab","17d922b9e23e9b52a649041063e53080ec464915","3c6293d1ea9da93831a6beefc491abdaa6271ffc","8f1308b6c58ee7ac35bda96dd64c4f21c1fe73f3","da73d84e48130bbae55d8aa5f6cf80fd4ae288df","0d09ac0a7093dea429e52205cf98bb986c223066","30f46fdfe1fdab60bdecaa27aaa94526dfd87ac1","20d62abc98ed651a267ea0176afe97bd34370529","b73a0c6d04ce062058385837e72552ec617b0d8f","17900e2c6a68b37ecb955cba67f97559f71dd15e","73b47570bcd4be891381dfea20c623ed6b0d957c","2e25922f3d6b44b42cc988d4221798be524da62a","c9bde4cac35afc70379a02adc1d977ec1aadd07f","1bbba5efed18e2d3ad048dd7f477047c4955088d"],"id":"8bf7d9ef244d1e8e6936e6ccd7bd099abe41f31e","s2Url":"https://semanticscholar.org/paper/8bf7d9ef244d1e8e6936e6ccd7bd099abe41f31e","authors":[{"name":"Colin Greatwood","ids":["21234013"]},{"name":"Laurie Bose","ids":["3414289"]},{"name":"Thomas S. Richardson","ids":["10726446"]},{"name":"Walterio W. Mayol-Cuevas","ids":["1731214"]},{"name":"Jianing Chen","ids":["34655873"]},{"name":"Stephen J. Carey","ids":["2691859"]},{"name":"Piotr Dudek","ids":["1900897"]}],"doi":"10.1109/IROS.2017.8206286"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206393","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present a novel method for classifying 3D objects that is particularly tailored for the requirements in robotic applications. The major challenges here are the comparably small amount of available training data and the fact that often data is perceived in streams and not in fixed-size pools. Traditional state-of-the-art learning methods, however, require a large amount of training data, and their online learning capabilities are usually limited. Therefore, we propose a modality-specific selection of convolutional neural networks (CNN), pre-trained or fine-tuned, in combination with a classifier that is designed particularly for online learning from data streams, namely the Mondrian Forest (MF). We show that this combination of trained features obtained from a CNN can be improved further if a feature selection algorithm is applied. In our experiments, we use the resulting features both with a MF and a linear Support Vector Machine (SVM). With SVM we beat the state of the art on an RGB-D dataset, while with MF a strong result for active learning is achieved.","inCitations":[],"pmid":"","title":"Selecting CNN features for online learning of 3D objects","journalPages":"5086-5091","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206393"],"entities":["Artificial neural network","Baseline (configuration management)","Computation","Convolutional neural network","Experiment","Feature extraction","Feature model","Feature selection","Firefox","Mobile robot","Modality (human\u2013computer interaction)","Mondrian OLAP Server","Online machine learning","Random forest","Requirement","Scalability","Selection algorithm","Statistical classification","Support vector machine"],"journalVolume":"","outCitations":["5b4d206e7116f9e57e3b3d0753db1f9d037486cf","370e1d1cc53d79ff47b3dbc299f6ced19f8d7252","319b28236e841917d4ee66a7fd3e0d99b98ade73","0c91d5305ad34814b631d4a642bb0535a2e066ea","8e52238300458dbacc81d3396efea7eb219f12f8","8db92d5cade0343eb922dec93f01be4335dc0a82","68ee0ca68e558eee13c208bb0c10f96a4730a248","127316fbe268c78c519ceb23d41100e86639418a","45fccf00ed3d06d0d3ca88b851f54382ecd890d3","614a0a3d7ac0d95e83fe6331c59007e56f8f2261","33c5d62fec8e75e540341613073f86c12259477f","055fec9810fe1e98944a229303b0000afbb47b31","9a27d256f00f6e282b215ef29db6474c523f4827","7da78191cc171cb99d4a9cc6dd7200b8ba725e97","0b15b4fec6e98aa94bebe37d001cd006c4138c47","d458e46d01edf346e223f55d675b5781415f1265","6f596196ad7093d84f9622cec2b56ef9ec06b15f","722fcc35def20cfcca3ada76c8dd7a585d6de386","6e69315f44dbca429c779d0a8e2936a0e38eeb6f","1708205b2c0e0e34051d2b68926534245b7868ea","ef83df34e693a28603eeed498090425a4357b267","efa3e8826aab1a79d05b1f3ab55b277c0120a092","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","061356704ec86334dbbc073985375fe13cd39088","2c1fdc1df2a64c96aa6fbf59a5d6056238cf072d","c030bb34eb4a4f1ebec10aafaa65f65f757a2769","5ab21cd570b563a667874d324f7752a0b7869b12","04903c5e874fb490d6d6e950d3a80460ed0ab0ea","88e63dbbc1b3372a38e2c3993702876acd5bbc20","9cb9a72a1b353a624f5b1b751459ffabc612002e"],"id":"34eea738c76f1eee33847ae83db51c8883630d14","s2Url":"https://semanticscholar.org/paper/34eea738c76f1eee33847ae83db51c8883630d14","authors":[{"name":"Monika Ullrich","ids":["32236833"]},{"name":"Haider Ali","ids":["9962536"]},{"name":"Maximilian Durner","ids":["39558462"]},{"name":"Zoltan-Csaba Marton","ids":["1709715"]},{"name":"Rudolph Triebel","ids":["2750689"]}],"doi":"10.1109/IROS.2017.8206393"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593865","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"When a human supervisor collaborates with a team of robots, the human's attention is divided, and cognitive resources are at a premium. We aim to optimize the distribution of these resources and the flow of attention. To this end, we propose the model of an idealized supervisor to describe human behavior. Such a supervisor employs a potentially inaccurate internal model of the the robots' dynamics to judge safety. We represent these safety judgements by constructing a safe set from this internal model using reachability theory. When a robot leaves this safe set, the idealized supervisor will intervene to assist, regardless of whether or not the robot remains objectively safe. False positives, where a human supervisor incorrectly judges a robot to be in danger, needlessly consume supervisor attention. In this work, we propose a method that decreases false positives by learning the supervisor's safe set and using that information to govern robot behavior. We prove that robots behaving according to our approach will reduce the occurrence of false positives for our idealized supervisor model. Furthermore, we empirically validate our approach with a user study that demonstrates a significant (p == 0.0328) reduction in false positives for our method compared to a baseline safety controller.","inCitations":[],"pmid":"","title":"Modeling Supervisor Safe Sets for Improving Collaboration in Human-Robot Teams","journalPages":"861-868","s2PdfUrl":"","pdfUrls":["http://arxiv.org/abs/1805.03328","https://export.arxiv.org/pdf/1805.03328","https://robotics.eecs.berkeley.edu/~sastry/pubs/Pdfs%20of%202018/McPhersonModeling2018.pdf","https://doi.org/10.1109/IROS.2018.8593865","https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-55.pdf","https://arxiv.org/pdf/1805.03328v1.pdf"],"entities":[],"journalVolume":"","outCitations":["518fd110bbf86df5259fb99126173d626a2ff744","286e3bc65c2bd4a59ec6b93d9bf42778fa1e87dc","20e6f09387b39e19e5e586743ef7540ff132a4c8","721ce250122dacc5202d6199a03815a9bc9b9aef","cd696f434ad5537ee462b2f23005cab849cc1774","220bdd265e6721e1d7ec1c4252aa41825147e61b","33304efdb2be0f4ff9c3306726f7fee767ffb19f","11b6bdfe36c48b11367b27187da11d95892f0361","26f7d738df74539b8de1e158e1844e5c5cece47f","169f1b737020a0f758ddd28cb7f3828faf7bce1e","3375f5e091a5084a72eaca9cc62ce27ca18896bd","410e4d021cfff01bf76f40e089a4f699145ebac2","032934b96c4cbb95b9c3e705f585a2617f1a7596","94c5b38b6446e7dd8832f65c16bc7fad99031256","c24d818819be966a6b3771c44195d680702b0b06","75a0211476ddc21363cfb3262c04d18794ad06ef","09da047a6103d47f23c285e53065da0e869c4edd"],"id":"8d7770ee0d92862121a22e6c39a4b133bb85af14","s2Url":"https://semanticscholar.org/paper/8d7770ee0d92862121a22e6c39a4b133bb85af14","authors":[{"name":"David L. McPherson","ids":["48500893"]},{"name":"Dexter R. R. Scobee","ids":["48342614"]},{"name":"Joseph Menke","ids":["40345427"]},{"name":"Allen Y. Yang","ids":["2784161"]},{"name":"S. Shankar Sastry","ids":["1717598"]}],"doi":"10.1109/IROS.2018.8593865"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545329","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Exploiting hierarchical label structure for multi-label classification can significantly improve classification performance and also benefit the labeling process. Existing work either can not make use of such structure or assume the hierarchy is given as a prior. In practice, such hierarchy is not always available beforehand and it is desirable to learn it from data. Moreover, the labels in the training data may be incomplete due to inconsistent labeling process, which raises another learning challenge. This paper studies multi-label learning with a latent label hierarchy and incomplete label assignments. Our goal is to simultaneously learn the hierarchy as well as a multi-label classifier given the input features and incomplete label assignments. We propose a probabilistic model that captures the hierarchical structure and the incompleteness of the labels and introduce an Expectation-Maximization (EM) procedure for maximum likelihood estimation.","inCitations":[],"pmid":"","title":"Learning with Latent Label Hierarchy from Incomplete Multi-Label Data","journalPages":"2075-2080","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545329"],"entities":[],"journalVolume":"","outCitations":["5820f6062c97063bbd9a955f681600077c9eab9b","5cd1c62dc99b6c3ecb2e678aa6fb2bffe3853c28","0351cb5a62ef4d250853ee90a763f6aea83e9c7e","4d3fb523b7892e0b7a7990366e02fae46d63dcb4","cdce1a0d88c3163545bca851d307196fb8eac315","418b15fc6a9b4f1e89724f2fe9008fd639b095a1","0a665d99541ed9a5c509fa87db5581f30bb364f6","57d774b8592b4b3f83f1304be43701ad8517e79a","1510cf4b8abea80b9f352325ca4c132887de21a0","e73a511644152bb44bc833464d275f14c35e10db","d7c586280488934ee95f4eec2912103098e57fc1","6853ac3b9a4d5fe940356e44e3cb99d84490a484","d8e8a36cd6e1d053eec711d97229e6b895b58522","c1cb5746b3bd9d1c80c02a95a64dcb6ba86e30ff","0604e26096bf166afabf20a1717357017927ab49","4745baf6c4ae7a088f03340fcc05ad7d18a0aca2","408a95068433e0373243e0f02c6b9aa832d6d735","930da10664a98d774fde0950194896d17ffe2406","c406f8248babaa5df843abcbb05d3f9521545f28","181b0aabdfd99dbf6bb9609cb960dbaff681902f","031eabfde410fd4b9f13aea9c68e9140c9712c6a","161cf8e46758e3bd06edd89d44ff5b4e8bc76795","53f83c8fabe98aace1429a7eb38006d935be49fd"],"id":"4f20f5e9ede42358dd42d18a4a5d31e7f32d3ab2","s2Url":"https://semanticscholar.org/paper/4f20f5e9ede42358dd42d18a4a5d31e7f32d3ab2","authors":[{"name":"Yuanli Pei","ids":["34102673"]},{"name":"Xiaoli Z. Fern","ids":["1694273"]},{"name":"Raviv Raich","ids":["1911921"]}],"doi":"10.1109/ICPR.2018.8545329"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594112","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper proposes the implementation of an impedance controller on the ankle level of COMAN+, a robot with parallel kinematics ankles actuated by a dual four-bar mechanism. The main contribution of the work is a realization of said control scheme that grants a less abrupt and safer robot response in case of system failures, that would cause the local joint torque controllers to lose their torque reference inputs. In particular, we propose a semi-centralized impedance control implementation which eliminates the instability of the pure joint torque control schemes used in the classical fully centralized methods when torque reference interruptions occur. Finally, we present experimental results, proving the effectiveness of our method and demonstrating how it ensures a safer behaviour compared to a fully centralized impedance control implementation when the communication to the ankle joints is interrupted. This paper is a follow-up work of [1], which presented and analyzed the parallel kinematics ankles.","inCitations":[],"pmid":"","title":"A Fail-Safe Semi-Centralized Impedance Controller: Validation on a Parallel Kinematics Ankle","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594112"],"entities":[],"journalVolume":"","outCitations":[],"id":"4ceb2a8c26b7f2af8395727bf839449f2cc29359","s2Url":"https://semanticscholar.org/paper/4ceb2a8c26b7f2af8395727bf839449f2cc29359","authors":[{"name":"Francesco Ruscelli","ids":[]},{"name":"Arturo Laurenzi","ids":[]},{"name":"Enrico Mingo Hoffman","ids":[]},{"name":"Nikolaos G. Tsagarakis","ids":[]}],"doi":"10.1109/IROS.2018.8594112"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545490","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Epipolar Plane Image (EPI) implies some important depth cues for light field depth estimation. Intuitively, the EPI patches with different spatial scales and orientations may exhibit different features and result in different estimation precision. In this paper, we discuss this issue and present a scale and orientation aware EPI-Patch learning model for depth estimation. We take the multi-orientation EPI patches of each pixel as input, and design two types of network structures for adaptive scale selection and orientation fusion. One type is a scale-aware structure, which feeds one orientation patch into a multi-layer feed-forward network with long and short skip connections. The other type is a shared-weight network for fusing the multi-orientation features. We demonstrate the effectiveness of our model by experiments on 4D Light Field Benchmark.","inCitations":[],"pmid":"","title":"Scale and Orientation Aware EPI-Patch Learning for Light Field Depth Estimation","journalPages":"2362-2367","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545490"],"entities":[],"journalVolume":"","outCitations":["6be503c3f652abdeba978c5891443452f40e3b8b","e4798c9fbe160382940d3591f6364c3eea15c9f8","ccedfda809966cd597bc1833ad843bf1fa66d20a","4d891502bfa2e6a8e854812d6d42b3f4d4dd5eea","55337eadc45e645f6533ebac2244bf1b71a670ea","1cc714f3c3a654712db16eb775ca185032d17107","a30f1e969868fa8897646fab834aca94509e863a","df9f083956c30a2a16a009bb958b9f3666983208","f5ad3316688225afcf358391c45781412230e445","501d99e392783e4acafb220136d32ea68a921282","2ad19e10f86db3a98e41d812b39b6bd3e8f239f2","1e2b8778cfe44de4bbe4a099ee7cdff5c2ca5f38","1a86e03c229adb5b94e1f43f8508f033f74e94ae","8e7baaf1b5769d36ed736a0b4857a4ceb590c226","3bc4913d9fbbd2527fd3e582c729bba96b6467f2","4db107e5d099f914658a6d3447b30496ecba231c","243e681e23e7d1744defd2ee0c83643b05f003d3","cce07a4d06b003e1a739dc6b4092eae00d0871de","37682985120dd10e40808d8e62ee2de5ed500d64","6e971828b50465e73e1819879baea494cdf6a831","2c03df8b48bf3fa39054345bafabfeff15bfd11d","57a86ef1c6fd16fbf554432c2950a509d9599e64","433ed83aead12cf87269b73e185bc36b6765dc32","6ba022b5ea6d5922ee8cfe6ce716e6729a7d4507","7f167d10e6fb57f4aa95fb7099a76ba9b5671025","7355d5840ec1f0dd032fb3876c48a6d1ad5f7803","e49ce518ce6603964913af9ed3cf693cd68344d7","0c5023cd0a5472e032215cb34555a3b5bdbdaed7","385d862c69b1db21a83e99c0bf38da27b3f0addb"],"id":"5ba72e915734605b4f6c5b3cb8a245f12051de58","s2Url":"https://semanticscholar.org/paper/5ba72e915734605b4f6c5b3cb8a245f12051de58","authors":[{"name":"Wenhui Zhou","ids":["46352268"]},{"name":"Linkai Liang","ids":["29075405"]},{"name":"Hua Zhang","ids":["46702279"]},{"name":"Andrew Lumsdaine","ids":["2556809"]},{"name":"Lili Lin","ids":["2553451"]}],"doi":"10.1109/ICPR.2018.8545490"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594495","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Enter the RobotriX, an extremely photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline using UnrealCV to generate raw data and ground truth labels. By taking this approach, we were able to generate a dataset of 38 semantic classes across 512 sequences totaling 8M stills recorded at +60 frames per second with full HD resolution. For each frame, RGB-D and 3D information is provided with full annotations in both spaces. Thanks to the high quality and quantity of both raw information and annotations, the RobotriX will serve as a new milestone for investigating 2D and 3D robotic vision tasks with large-scale data-driven techniques.","inCitations":[],"pmid":"","title":"The RobotriX: An Extremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions","journalPages":"6790-6797","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1901.06514v1.pdf","https://doi.org/10.1109/IROS.2018.8594495"],"entities":[],"journalVolume":"","outCitations":[],"id":"66842559a848f51f90cee3703e2ba1d69817c124","s2Url":"https://semanticscholar.org/paper/66842559a848f51f90cee3703e2ba1d69817c124","authors":[{"name":"Alberto Garcia-Garcia","ids":[]},{"name":"Pablo Martinez-Gonzalez","ids":[]},{"name":"Sergiu Oprea","ids":[]},{"name":"John Alejandro Castro-Vargas","ids":[]},{"name":"Sergio Orts","ids":[]},{"name":"José García Rodríguez","ids":[]},{"name":"Alvaro Jover-Alvarez","ids":[]}],"doi":"10.1109/IROS.2018.8594495"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206427","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We propose a probabilistic model that generates nod motions based on utterance categories estimated from the speech input. The model comprises two main blocks. In the first block, dialogue act-related categories are estimated from the input speech. Considering the correlations between dialogue acts and head motions, the utterances are classified into three categories having distinct nod distributions. Linguistic information extracted from the input speech is fed to a cluster of classifiers which are combined to estimate the utterance categories. In the second block, nod motion parameters are generated based on the categories estimated by the classifiers. The nod motion parameters are represented as probability distribution functions (PDFs) inferred from human motion data. By using speech energy features, the parameters are sampled from the PDFs belonging to the estimated categories. Subjective experiment results indicate that the motions generated by our proposed approach are considered more natural than those of a previous model using fixed nod shapes and hand-labeled utterance categories.","inCitations":[],"pmid":"","title":"Probabilistic nod generation model based on estimated utterance categories","journalPages":"5333-5339","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206427"],"entities":["Android (robot)","Approximation algorithm","Backchannel","Categorization","Experiment","Feature vector","Hard coding","Humanoid robot","Kinesiology","Portable Document Format","Rejection sampling","Sampling (signal processing)","Statistical model"],"journalVolume":"","outCitations":["69abe8cdbae88b9232303e731f30f7b6429982ee","ca6a5e3c1bed2588b6bfdbec7b048d44d63ed10d","22c7db4a4bfc25281bf8fad79e3c60ab1030d049","e475a4101fd8a42ecaf96f79e2c8d591f32784f9","28c29a364feaf18ab5eadca54fcda1cdd257e390","3b878094b4c63a33f38253cfac1fa227d0f2daa2","157aef34d39c85d6576028f29df1ea4c6480a979","0a7d9caf18519a24510131ceb826988f9ab7930a","35b42b1d244ef20af1e9e2b1883360bbb8871db5","4288cbc1fbc4f620045ca193086c1d476145dfae","9c494861dd2b89faf87ab8dfc1599d153315f740","62d8219e6be879d54b5602c90fdbeaff7a78ff74","176ab32d8b803fa4397864cdbc41461b1d5d95eb","bb5e7643c46cd5adcb4b411d6b3b3f03f1f554e1","7fe34e3822664be3c1d0793e3bab3cab31642bfa"],"id":"36c765908073529215ca0062462dbadefb883e67","s2Url":"https://semanticscholar.org/paper/36c765908073529215ca0062462dbadefb883e67","authors":[{"name":"Chaoran Liu","ids":["1765784"]},{"name":"Carlos Toshinori Ishi","ids":["1798428"]},{"name":"Hiroshi Ishiguro","ids":["1687808"]}],"doi":"10.1109/IROS.2017.8206427"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594276","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Social robots able to continually learn facial expressions could progressively improve their emotion recognition capability towards people interacting with them. Semi-supervised learning through ensemble predictions is an efficient strategy to leverage the high exposure of unlabelled facial expressions during human-robot interactions. Traditional ensemble-based systems, however, are composed of several independent classifiers leading to a high degree of redundancy, and unnecessary allocation of computational resources. In this paper, we proposed an ensemble based on convolutional networks where the early layers are strong low-level feature extractors, and their representations shared with an ensemble of convolutional branches. This results in a significant drop in redundancy of low-level features processing. Training in a semi-supervised setting, we show that our approach is able to continually learn facial expressions through ensemble predictions using unlabelled samples from different data distributions.","inCitations":[],"pmid":"","title":"An Ensemble with Shared Representations Based on Convolutional Networks for Continually Learning Facial Expressions","journalPages":"1563-1568","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594276"],"entities":[],"journalVolume":"","outCitations":[],"id":"83371cf672af3f9aa75d921db536ee82e51a7823","s2Url":"https://semanticscholar.org/paper/83371cf672af3f9aa75d921db536ee82e51a7823","authors":[{"name":"Henrique Siqueira","ids":[]},{"name":"Pablo V. A. Barros","ids":[]},{"name":"Sven Magg","ids":[]},{"name":"Stefan Wermter","ids":[]}],"doi":"10.1109/IROS.2018.8594276"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545446","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"We mainly propose a robust Embedded Locality-Constrained Label Consistent Dictionary Learning (ELC2DL) framework for discriminative classification. ELC2DL improves the representation and classification performance by performing DL in the noise-removed sparse embedding space, since most real data often contains noise and performing DL over noisy data for reconstruction may decrease performance potentially. To reduce the noise in data, our model computes a sparse projection jointly for noise reduction and then uses the noise-removed data for DL. By incorporating a noise-reduction term with a discriminative locality-constrained label consistent term that associates the label information with each dictionary atom to preserve local structure of training data, a noise-reduction projection, an over-complete dictionary and discriminative sparse codes are obtained jointly. Simulations on several image databases show that our algorithm can deliver enhanced performance over other state-of-the-arts.","inCitations":[],"pmid":"","title":"Robust Locality-Constrained Label Consistent K-SVD by Joint Sparse Embedding","journalPages":"1664-1669","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545446"],"entities":[],"journalVolume":"","outCitations":["98720e3b5b36160a73c09086afc88405f7ed517f","18c84e6b5f1d6da3c670454db3f0fa61266ab1e3","5aadbc0e624e23cc4b9627ee87fcc1e79fc1bb64","24662ce3f3499ec8c5ecc546dac69dbffad578c6","1827aff2dc345680de29a937f002da41accb2fe0","520d586b50ecaa9753f714c6e76e6b819663d1a4","96e626c8d03ff83b10adb45901e2e7fe247f2cd4","f735188dbcb276cd1da248110712fde0d1b2aec7","075bc988728788aa033b04dee1753ded711180ee","0b7c1bcd0289058b5dfc0d3ff114972712bc7f1a","ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2","7692613c83bed22f8d1bca7c722c878a3237c901","f2eab39cf68de880ee7264b454044a55098e8163","77c5f7da7710cd63ec193eb85f52b715b1ac6528","2e5b6fb6d6fe250e99a2af47949932eba01f5b8d","bab88235a30e179a6804f506004468aa8c28ce4f","e6ecb30060c5f607cdb99e36506f26b65c8c012a","260f08143a755aab0630c82b5d90733c63f5dd82","fa1d0fae9814c1ea349c723f1751dc21e81d312b","f88ce52c5042f9f200405f58dbe94b4e82cf0d34","d36b4a09ace65e816377bc7aee41ed1a2aae94b6","266bf8847801ff302c6f91f899f36269807317ee","0f22251fa9c4bb120f00767053430fbab141fac3","df8684817ada3d604c98f12b6b22512b2e29a6cd","9067db319ecb338312070a92b081341e6f03a6c6","09f2af091f6bf5dfe25700c5a8c82f220fac5631","8818ceb7bf41b07037a7396058b69a6da6dc06a6"],"id":"3e84c4cfb874d6f6407110359f1b70f71eca1f98","s2Url":"https://semanticscholar.org/paper/3e84c4cfb874d6f6407110359f1b70f71eca1f98","authors":[{"name":"Zhao Zhang","ids":["40630838"]},{"name":"Weiming Jiang","ids":["2390624"]},{"name":"Sheng Li","ids":["39541577"]},{"name":"Jie Qin","ids":["46390740"]},{"name":"Guangcan Liu","ids":["50084588"]},{"name":"Shuicheng Yan","ids":["1698982"]}],"doi":"10.1109/ICPR.2018.8545446"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545406","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Speech recognition has established a strong bond with various technological boons for the day to day life of the rustics across the continents. Such advances have not yet propagated to the grassroot level of India, one of the reasons for it being the multilingual nature of our country. We are habituated in using multiple languages while talking, which makes the task of speech recognition challenging thereby making Language Identification an important task. The technique of automatically identifying language from spoken phrases is termed as Automatic Language Identification. The problem of multilingual speech further elevates for South Indian languages which at times become very difficult to distinguish with negligible prior knowledge. In this paper, an Automatic Language Identification System is proposed to distinguish the 4 Dravidian languages which are also known as South Indian languages due to their pre dominant use in the South Indian subcontinent. Dataset size ranged up to the size of 12224 clips and a highest accuracy of 96.46% was obtained by using a newly proposed Line Spectral Pair-Grade (LSP-G) feature along with FURIA based classification technique.","inCitations":[],"pmid":"","title":"A Dravidian Language Identification System","journalPages":"2654-2657","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545406"],"entities":[],"journalVolume":"","outCitations":["b686a6c4d23e8c4471c4c67e80331ae8969d816c","ddabf896d387a4d943e2026da081f7be5efb7de2","90660c55f5e5feca5451a4a9f4e8aac83f6523a0","6d3f08c31a8e27884d0dbeaf6a01bea796863151","05d755a312ae8b8777a37b702489c7f9ce4ffea5","2c58f05f524da7259e7278fb06f8359960388794","8b0e0cb197dc626c14a45b03f0564e64cb30667c","b87f6a62482aaefdc07ac3c387b1ce11d8766433","6d8c9fcce8177d6f8d122d653c7d32d7624d6714","f9448b7a3e1b3de53449114bfae454898b4f1a4d","02554e78d108263cdeaa098ef4ee47cf0f357ef9","1ba23ed75d760de6c414a29fc0aa41fb3318c2f7","07389d6d64d21fc8fd4a621ff27f97645ecb3d71","909fa138add396ed30482fb3375f7b22f2ed49ab","2c222c3c4cda9b2c11f2dfd4690c9aa4b3382777"],"id":"26a7a67216bf9dc7039a8471d54fb04290a562f9","s2Url":"https://semanticscholar.org/paper/26a7a67216bf9dc7039a8471d54fb04290a562f9","authors":[{"name":"Himadri Mukherjee","ids":["39411146"]},{"name":"Sk Md Obaidullah","ids":["9377086"]},{"name":"Santanu Phadikar","ids":["3223443"]},{"name":"Kaushik Roy","ids":["49587323"]}],"doi":"10.1109/ICPR.2018.8545406"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206613","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a mono-camera based simultaneous obstacle recognition and distance estimation method for power transmission lines(PTLs) inspection robot to avoid obstacles on or around them. The proposed robot inspects the PTLs while moving along them between power transmission towers. For autonomous navigation, our robot recognizes obstacles and avoids a collision with them. In addition, it can stop at the ends of the PTLs by recognizing its structures, such as insulators, installed at the ends of them. In order to recognize obstacles or insulators efficiently, initially, a robust PTLs detection method based on vanishing point estimation is proposed since they are installed on or around the PTLs. Then, obstacle models with various scales are built, and multiple regions-of-interest(ROIs) according to the scales of the model are constructed along the detected PTLs. Finally, obstacles are recognized within the multiple ROIs. Because each ROI represents a corresponding scale of the obstacle model to be matched, the proposed approach can efficiently deal with scale changes and also estimate distance between the robot and the obstacle simultaneously with obstacle recognition. Experimental results not only show that the proposed method correctly recognizes obstacles but also that the distance between the robot and the obstacles is estimated efficiently.","inCitations":[],"pmid":"","title":"Mono-camera based simultaneous obstacle recognition and distance estimation for obstacle avoidance of power transmission lines inspection robot","journalPages":"6902-6907","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206613"],"entities":["Autonomous robot","Consistency model","Hough transform","Obstacle avoidance","Region of interest","Topological insulator","Transmission line","Vanishing point"],"journalVolume":"","outCitations":["17044824735c2908a7b225ed968134a32049ecd3","644ff0fa96b281bb98c55925b57a671adc9af4e6","7731b844925c551bd5f89764adfa97f0b164306c","83a76fe625bd1e88f69d6c0b0836de14305fa30d","89a4618d9af12b98af656f9d620256e7cfcd4a1c","56ca893694e7a36f913d508105ce6391f7f6f7f0","ef639689a21f2b0b4ca8ba23cb38be071f22242d","55efd9f164d245040caf8286669e86792e47968b","13b6a06c907648a5a8dc12b169e4da6f2f338be8","0fb06b6dbb2bfa5574885dca09445190565182ff","d7f016f6ceb87092bc5cff84fafd94bfe3fa4adf","772bc0e13e26bec85e4985858b5ec76a70dd77f5","2c37dde5fad32628a281c2aa8a1e562100ecdc39","000dc0252aee8569e4194a88e36d9e91fb92bda0","0d38869a8c0ae218ccc0f20065da27b9cf405ace","885798509e30fb4221149784363b07c46fa00642"],"id":"ab9adc22991e6b28ffd86fb9f52dccaf2eebecae","s2Url":"https://semanticscholar.org/paper/ab9adc22991e6b28ffd86fb9f52dccaf2eebecae","authors":[{"name":"Ju Han Yoo","ids":["3098504"]},{"name":"Changhwan Kim","ids":["40413453"]},{"name":"Dong Hwan Kim","ids":["50204016"]}],"doi":"10.1109/IROS.2017.8206613"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545289","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Different enhanced convolutional neural network (CNN) architectures have been proposed to surpass very deep layer bottleneck by using shortcut connections. In this paper, we present an effective deep CNN architecture modified on the typical Residual Network (ResNet), named as Cascade Network (CascadeNet), by repeating cascade building blocks. Each cascade block contains independent convolution paths to pass information in the previous layer and the middle one. This strategy exposes a concept of \u201ccross-passing\u201d which differs from the ResNet that stacks simple building blocks with residual connections. Traditional residual building block do not fully utilizes the middle layer information, but the designed cascade block catches cross-passing information for more complete features. There are several characteristics with CascadeNet: enhance feature propagation and reuse feature after each layer instead of each block. In order to verify the performance in CascadeNet, the proposed architecture is evaluated in different ways on two data sets (i.e., CIFAR-10 and HistoPhenotypes dataset), showing better results than its ResNet counterpart.","inCitations":[],"pmid":"","title":"CascadeNet: Modified ResNet with Cascade Blocks","journalPages":"483-488","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545289"],"entities":[],"journalVolume":"","outCitations":["4dc1641582a60abdc66a9d818c313a9d783a74be","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","04640006606ddfb9d6aa4ce8f55855b1f23ec7ed","3b2697d76f035304bfeb57f6a682224c87645065","14318685b5959b51d0f1e3db34643eb2855dc6d9","46a8b4a6079618f59f1ea58b9aeadbc9e0927337","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","cb4c88c00fdf9c521463c9191024391566f128be","60dc90046fbbfd1cbad7c3b9759843780c8bacea","dad7630b79e60f49424a63feea4e671e6d2fbb9a","1f76b7b071f3e65c97d09720f88d6b0ad9f07e8f","061356704ec86334dbbc073985375fe13cd39088","2c03df8b48bf3fa39054345bafabfeff15bfd11d","44760ff739ebc7feb8b62da7d621ea5814638f21","5d90f06bb70a0a3dced62413346235c02b1aa086"],"id":"41cd14af7c790b175227f4c2a2711c54517e7e83","s2Url":"https://semanticscholar.org/paper/41cd14af7c790b175227f4c2a2711c54517e7e83","authors":[{"name":"Xiang Li","ids":["1737850"]},{"name":"Wei Li","ids":["39093312"]},{"name":"Xiaodong Xu","ids":["48670051"]},{"name":"Qian Du","ids":["1686102"]}],"doi":"10.1109/ICPR.2018.8545289"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593722","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Bipedal locomotion skills are challenging to develop. Control strategies often use local linearization of the dynamics in conjunction with reduced-order abstractions to yield tractable solutions. In these model-based control strategies, the controller is often not fully aware of many details, including torque limits, joint limits, and other non-linearities that are necessarily excluded from the control computations for simplicity. Deep reinforcement learning (DRL) offers a promising model-free approach for controlling bipedal locomotion which can more fully exploit the dynamics. However, current results in the machine learning literature are often based on ad-hoc simulation models that are not based on corresponding hardware. Thus it remains unclear how well DRL will succeed on realizable bipedal robots. In this paper, we demonstrate the effectiveness of DRL using a realistic model of Cassie, a bipedal robot. By formulating a feedback control problem as finding the optimal policy for a Markov Decision Process, we are able to learn robust walking controllers that imitate a reference motion with DRL. Controllers for different walking speeds are learned by imitating simple time-scaled versions of the original reference motion. Controller robustness is demonstrated through several challenging tests, including sensory delay, walking blindly on irregular terrain and unexpected pushes at the pelvis. We also show we can interpolate between individual policies and that robustness can be improved with an interpolated policy.","inCitations":["793a21e6d586f092e3f41b11fb55c6cb876efc22","ded9e65c5bed9b9a0c09348b13c3a588e5102498","dc55206e9800ba5a20b09512c742ad452f488627","b9effa7f0d628ab825f07bee059a3997257348ce"],"pmid":"","title":"Feedback Control For Cassie With Deep Reinforcement Learning","journalPages":"1241-1246","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1803.05580v2.pdf","https://doi.org/10.1109/IROS.2018.8593722","https://export.arxiv.org/pdf/1803.05580","https://mime.oregonstate.edu/research/drl/publications/_documents/xie_2018.pdf","http://arxiv.org/abs/1803.05580"],"entities":[],"journalVolume":"","outCitations":["160315c07d18fe785aff07f50c9e44319a0af0cb","b36a5bb1707bb9c70025294b3a310138aae8327a","e6e01f580c973d91f6445d839389f9f2d5efc78e","16c248bec30bf7125b86527aded29a2fae7b9618","dce6f9d4017b1785979e7520fd0834ef8cf02f4b","eb02ed5e1f7eabb6e4920d11f8d43f3e72a894a5","9917363277c783a01bff32af1c27fc9b373ad55d","cddb1f7f9f004396a2efef285caf29d7780a8e21","591898dabb9237accc4adb3fbe94d2ce1fc5fe07","d18c84f5af0f705c71dce3df1cdd8a7a68512ab8","387e0d200604c3f05306b28dcd59b4d9b46ec645","921525b5ed9993541e53fb40ad4c833097ff1ee1","cec6a1c7532904ee1d4702f723c311523542c983","272216c1f097706721096669d85b2843c23fa77d","4775b7126345958450007faca0dc3aca1d6f4363","b354ee518bfc1ac0d8ac447eece9edb69e92eae1","0af8cdb71ce9e5bf37ad2a11f05af293cfe62172","6c5a3691643abbdc063fd2505951ac3573d431c9","494e2d5b40dcebde349f9872c7317e5003f9c5d2"],"id":"de79441986f363dca0124f2acb8cbaf483528aa3","s2Url":"https://semanticscholar.org/paper/de79441986f363dca0124f2acb8cbaf483528aa3","authors":[{"name":"Zhaoming Xie","ids":["31708890"]},{"name":"Glen Berseth","ids":["2994035"]},{"name":"Patrick Clary","ids":["3748005"]},{"name":"Jonathan W. Hurst","ids":["15624725"]},{"name":"Michiel van de Panne","ids":["1745029"]}],"doi":"10.1109/IROS.2018.8593722"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00016","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Most common approaches to one-shot gesture recognition have leveraged mainly conventional machine learning solutions and image based data augmentation techniques, ignoring the mechanisms that are used by humans to perceive and execute gestures, a key contextual component in this process. The novelty of this work consists on modeling the process that leads to the creation of gestures, rather than observing the gesture alone. In this approach, the context considered involves the way in which humans produce the gestures \u2013 the kinematic and biomechanical characteristics associated with gesture production and execution. By understanding the main \"modes\" of variation we can replicate the single observation many times. Consequently, the main strategy proposed in this paper includes generating a data set of human-like examples based on \"naturalistic\" features extracted from a single gesture sample while preserving fundamentally human characteristics like visual saliency, smooth transitions and economy of motion. The availability of a large data set of realistic samples allows the use state-of-the-art classifiers for further recognition. Several classifiers were trained and their recognition accuracies were assessed and compared to previous one-shot learning approaches. An average recognition accuracy of 95% among all classifiers highlights the relevance of keeping the human \"in the loop\" to effectively achieve one-shot gesture recognition.","inCitations":[],"pmid":"","title":"Biomechanical-Based Approach to Data Augmentation for One-Shot Gesture Recognition","journalPages":"38-44","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00016"],"entities":["Convolutional neural network","Gesture recognition","Kinesiology","Machine learning","Microsoft Research","One-shot learning","Relevance","Self-replication"],"journalVolume":"","outCitations":["18a1305d884f9e5f385fbffb2210c00f25afa001","016335ce7e0a073623e1deac7138b28913dbf594","448fd815a3b87bd6217dac498e6db7949ca1e81f","2691124c61258acf8203e94b94542c4922ab4f5c","7bcdecfb240604d1dbef285f6be2a0d2c20f2b95","4d423acc78273b75134e2afd1777ba6d3a398973","6e6ee29e0b1ae6b116994d843cc3c26d6701b1e3","9c8279d5e85ae2adcb52d76efb37f234664373b3","f68ed9aeeaae56270b420e0767342c49424dc62b","19df226147c7cb885a5a8c0e4eac9982321f6cfa","0504c46c80aba498434a81c8133c23cc458d42d3","08acb0e868cf1f84acb4bd525bde189e4fbe3690","28a6d1b25ffba3aa2cbb29b060032a2e66084f30","69c29aaf83d32e3eafe2c6360ee386e6770a79f8","de9de9609db4ff023d8765f4479983227ecb8b47","f86c3a57ab79116c5034965e5f83d2f83c1c6238","1af58221baebb2c9143a0fcf80eea744a5f30b66","8da6a5e454b568537e23a655502b08454a2c41f9","bbd0e204f48a45735e1065c8b90b298077b73192","46f74231b9afeb0c290d6d550043c55045284e5f","c8a89fffe5cf1a0659167dbcf521d67161b93718","d2caacf62920f45e110e8faa3369ea1187ad2402","e16b5afbe2acc57530359c065227dbb634b2109b","9dbcb9d6b9c5cf9585f7b1aba90ea5fc41edcf0c","3dfc91963ec74dec953914d6db99b44ad4b90ace","0ec9cf7c5cf69d79bdb3332220bc0c5049cc3b9b","a6b553a00e60cd1d33f91dc726fa0216728c20e9","d93751de29db7b34431842757cf5044cf9811b70","e34dbefd7d53fe58cdbdd2f2de85708e298b6605","7ae89ecab0dfec576e1239cfe458df0078987216","e23f807c2ddfdd389bc7f02924049ee98c03b6bc","dce6c5a2bc9b551c564cf5bd6dc85c098d0d8670","dbc7c45f86f11125fc4689e3f019485354f8e505","41111d6ea82840f772f8129c96e301679208d065","660947f0a61c4a415b1e650c2f8de0bf1c60a751","10fd174fefd5e36a523805e4c2d2fbf1d12a3ae8"],"id":"06ab24721d7117974a6039eb2e57d1545eee5e46","s2Url":"https://semanticscholar.org/paper/06ab24721d7117974a6039eb2e57d1545eee5e46","authors":[{"name":"Maria Eugenia Cabrera","ids":["40553105"]},{"name":"Juan Pablo Wachs","ids":["1768610"]}],"doi":"10.1109/FG.2018.00016"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206328","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We introduce a robust control architecture for the whole-body motion control of torque controlled robots with arms and legs. The method is based on the robust control of contact forces in order to track a planned Center of Mass trajectory. Its appeal lies in the ability to guarantee robust stability and performance despite rigid body model mismatch, actuator dynamics, delays, contact surface stiffness, and unobserved ground profiles. Furthermore, we introduce a task space decomposition approach which removes the coupling effects between contact force controller and the other non-contact controllers. Finally, we verify our control performance on a quadruped robot and compare its performance to a standard inverse dynamics approach on hardware.","inCitations":["514a54361e5baec0ca9693e045076a5fb4c4c68a","936f0dfa1de2b4d7c5cb2b69661eda3e08cf098b"],"pmid":"","title":"Robust whole-body motion control of legged robots","journalPages":"4589-4596","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1703.02326v1.pdf","https://doi.org/10.1109/IROS.2017.8206328","http://arxiv.org/abs/1703.02326"],"entities":["Coat of arms","Control flow","Dynamic programming","Interference (communication)","Inverse dynamics","Memory controller","Microsoft Outlook for Mac","Motion controller","Robot","Robot end effector","Robust control","Simulation","System dynamics"],"journalVolume":"","outCitations":["5a5d094794891c917b7c75b59087792c1f47b5e3","7cbb4c6654763d6b96bbf0cd75aac42af27b3353","f3e41789707be6ece79db4fd534ccae39b405021","733e557c28345592f2afb96ef160a9b1b0f6d64f","33523bf54366a86b6fbc58263dd2e366aff86fa6","0fa7b8e09238cf1cff4aa8312ffa78a88fa3b164","76d173977f315338bbfdd93d2657fd172b888efd","51ae30e1fe6c5e5dd076ef2b3aa23d6b92dbfe0a","4d562dec1d6fd6ee08604924923a010287c2f733","b344bb74e0844c2d47272f7d69616df4870bc5ca","23e3cd7bcb62a96cd268ab39d919b09693058ea5","835aacceea65d26fe0dd3d95da1043b1f18ad5d5","6e20ecd6cf5ebb5b220f42884494067466ee3254","204da6a56eb9746e174d6e2108d9c11e6d4f673d","33576c0fc316c45c3672523114b20a5bb996e1f4","56689aae995ccb4722d9261fafda11725ddd5570","09c92bf7d75ca4394fa4d70dbeab6aab06ba45ed"],"id":"9f272f55e48327f1884e5b84c03d60e071ddaf0d","s2Url":"https://semanticscholar.org/paper/9f272f55e48327f1884e5b84c03d60e071ddaf0d","authors":[{"name":"Farbod Farshidian","ids":["2583867"]},{"name":"Edo Jelavic","ids":["9927281"]},{"name":"Alexander W. Winkler","ids":["40238512"]},{"name":"Jonas Buchli","ids":["1741293"]}],"doi":"10.1109/IROS.2017.8206328"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545386","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"One of recent trends in network architecture design confirms that the inception-block convolutional group is efficient, since it can aggregate spatial context information in lower dimensions without causing significant loss in representative capabilities. We believe that not only the strong correlation between adjacent cells, multi-scale feature extraction also plays a vital role in this novel module. In this paper, we extend the profits of the block to a top-down donut convolutional network for semantic segmentation task. Our network automatically learns rich convolution kernels to capture more structure prior. In the inception-block design, it overcomes the limitations in larger kernel size and adaptively captures different object-scales contexts without chain sampling. Our experiments demonstrate that the proposed inception-block donut convolutional network is orthogonal and can further improve the performance of most off-the-shelf bottom-up based methods.","inCitations":[],"pmid":"","title":"Inception Donut Convolution for Top-down Semantic Segmentation","journalPages":"2492-2497","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545386"],"entities":[],"journalVolume":"","outCitations":["9201bf6f8222c2335913002e13fbac640fc0f4ec","6f0bfc7a894190cbb9c0bf4a8047b8e43d7beba4","000f90380d768a85e2316225854fc377c079b5c4","14725e03c93088c071f51c68137b5b8fcfe2129e","9c2e5e2ba7c5b3a555c6c72f518e3631aab23c19","cab372bc3824780cce20d9dd1c22d4df39ed081a","cdfe4e46d949b68c682e554c402d16679fd7247c","420c46d7cafcb841309f02ad04cf51cb1f190a48","72791b3d74d14777c91982cd547e88ca9004f9a5","6a1b76f1ef876061ec479ab9bc13fcd517eb4188","39978ba7c83333475d6825d0ff897692933895fc","1550caab8d12c3f0ea19faaaa6bab3bdd092bafd","322a7dad274f440a92548faa8f2b2be666b2d01f","361b19d2c00d086fa8ef860374f5e1d862fd2f30","06a23ffbd9752ce204197df59812b2ebd1a097ff","e87ea5fa265eadb7f68f8f06beabfb056e06579a","1abf6491d1b0f6e8af137869a01843931996a562","3cdb1364c3e66443e1c2182474d44b2fb01cd584","0626908dd710b91aece1a81f4ca0635f23fc47f3","722fcc35def20cfcca3ada76c8dd7a585d6de386","fa23d1528f8e6808c29a18b22b57c7da8789f933","d6ab38cba831b9a67bd9408da167c400581604e5"],"id":"7d3e9a562bdfa90119e78416bfec1c38d649f65e","s2Url":"https://semanticscholar.org/paper/7d3e9a562bdfa90119e78416bfec1c38d649f65e","authors":[{"name":"He Guan","ids":["32561502"]},{"name":"Zhaoxiang Zhang","ids":["48805516"]},{"name":"Tieniu Tan","ids":["1688870"]}],"doi":"10.1109/ICPR.2018.8545386"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206604","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In recent years, applications of visual tracking algorithms has seen a substantial growth with deployments in intelligent robots such as drones for human tracking. The algorithms for such tasks has to be efficient in terms of computational cost while been robust, accurate and fast. Object tracking algorithms based on handcrafted heuristics and constraints are widely used in uav applications. The handcrafted heuristics are mostly implemented for task-oriented applications which limits the extensions in uav's capability beyond the predefined functions. This paper considers the challenges of tracking and landing an autonomous uav on a speed high moving target, and presents a visual tracking algorithm that integrates correlation filters with deep comparison network for real-time tracking with state-of-the-art accuracy. The method first tracks the target upto translation using an online learnt model via local search technique. The changes in scale is estimated by a deep comparison network (DCN) instead of the commonly used pyramidal approach. In a single network evaluation, DCN can estimate the changes in scale as well as compensate the drifting of the tracker by refining the object region estimated by the correlation filters. The network is end-to-end trained which attempts to learn a powerful matching function for object localization using a known template. Generally, the integrated framework can be viewed as coarse-to-fine level motion estimation. Moreover, the framework can redetect the lost target without a need for a separate detector.","inCitations":["01b907765e6cb4a6950e6b3017d84d4101068c60","887061eb081bead2c2f0fd8a2ca49d391eed0eea"],"pmid":"","title":"Robust real-time visual tracking using dual-frame deep comparison network integrated with correlation filters","journalPages":"6837-6842","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206604"],"entities":["Algorithm","Algorithmic efficiency","Autonomous robot","Computation","Dynamic circuit network","End-to-end encryption","Heuristic (computer science)","Local search (optimization)","Motion estimation","Real-time clock","Sorting network","Video tracking"],"journalVolume":"","outCitations":["273b973092a4491974d173cc5258c74aede692cc","2fc65a000a7331cfe3eea818dffddad2c36b53b4","149e5e5eeea5a9015ab5ae755f62c45ef70fa79b","a9a414604cff39f1a03c5547385dc421e6c8452e","7d2b828b3791bdd6226534969008abc99e0ff666","fb39b1a73b37170d22d2588fd5af460c17156ce2","2e80ce889fa47bae8583f89d501a41e283c1551b","9ad8c207d66553d0fa7a7cb57c5e1be12896d1d9","413bd84c4dbc3d5d1481d58a03e5890e1474935e","1c42b5543c315556c8a961b1a4ee8bc027f70b22","e3c433ab9608d7329f944552ba1721e277a42d74","8f667e55631e2437015dad63bafd5f9d49a9a122","14318685b5959b51d0f1e3db34643eb2855dc6d9","7069a994c150b0228c4e471ca48ed55d7646bc62","5684d284310582ae0f69c5b7a4d6b791a13fcf49","0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","65c9b4b1d49f46b3f8f64a5f617acfc14f85d031","a1bcdb820f6f0dde108996a18eefb239ef0e9566"],"id":"bb2c1b63bb943072e07ef55fd00f79006506b85f","s2Url":"https://semanticscholar.org/paper/bb2c1b63bb943072e07ef55fd00f79006506b85f","authors":[{"name":"Krishneel Chaudhary","ids":["39731965"]},{"name":"Moju Zhao","ids":["3264557"]},{"name":"Fan Shi","ids":["48862090"]},{"name":"Xiangyu Chen","ids":["2572474"]},{"name":"Kei Okada","ids":["1683608"]},{"name":"Masayuki Inaba","ids":["1749935"]}],"doi":"10.1109/IROS.2017.8206604"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594459","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"A fundamental problem of non-linear state estimation in robotics is the violation of assumptions about the sensors' error distribution. State of the art approaches reduce the impact of these violations with robust cost functions or predefined non-Gaussian error models. Both require extensive parameter tuning and fail if the sensors' error characteristic changes over time, due to environmental changes, ageing or sensor malfunctions. We demonstrate how the error distribution itself can be part of the state estimation process. Based on an efficient approximation of a Gaussian mixture, we optimize the sensor model simultaneously during the standard state estimation. Due to an implicit expectation-maximization approach, we achieve a fast convergence without prior knowledge of the true distribution parameters. We implement this self-tuning algorithm in a least-squares optimization framework and demonstrate its real time capability on a real world dataset for satellite localization of a driving vehicle. The resulting estimation quality is superior to previous robust algorithms.","inCitations":["2b9db67601fd62521e7db154d6232a4dc2b0aea2"],"pmid":"","title":"Robust Sensor Fusion with Self-Tuning Mixture Models","journalPages":"3678-3685","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594459"],"entities":[],"journalVolume":"","outCitations":[],"id":"bb9fae2155e07da278572e725338fbf833d2f62f","s2Url":"https://semanticscholar.org/paper/bb9fae2155e07da278572e725338fbf833d2f62f","authors":[{"name":"Tim Pfeifer","ids":[]},{"name":"Peter Protzel","ids":[]}],"doi":"10.1109/IROS.2018.8594459"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206532","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper, we present a control strategy that enables intuitive physical human-robot collaboration with mobile manipulators equipped with an omnidirectional base. When interacting with a human operator, intuitiveness of operation is a major concern. To this end, we propose a redundancy solution that allows the mobile base to be fixed when working locally and moves it only when the robot approaches a set of constraints. These constraints include distance to singular poses, minimum of manipulability and distance to objects and angular deviation. Experimental results with a Kuka LWR4 arm mounted on a Neobotix MPO700 mobile base validate the proposed approach.","inCitations":["a579a40cf48e6db26d34fc5ea7290a940f631d0a"],"pmid":"","title":"A framework for intuitive collaboration with a mobile manipulator","journalPages":"6293-6298","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206532","https://hal.archives-ouvertes.fr/hal-01489029v2/document"],"entities":[],"journalVolume":"","outCitations":["58e514e8fa96fe2bd5426aabed28d3f21afb473b","b6c281af4c44fbe58176b5f9cee313f9fe9030cb","7e4ebf64c2885456e07d48884266530b87b70fcd","9c0c450f7b3f5453f2dc32d205a3184424b53715","02185fefbb714994d2a3ff7489e32bde35a8c81a","46600c09d0e4bc99c94a56d8f736f9b1437da22c","87f3c686d3e53738b32e7adfc3739ccffa2364de","bc18dccba4311fdc2a37dc4bad5d9d0fdeba0e40","9d753e54fe635d50828ec4a09e1380d27a30098a","f5f8a6aa4adc13070224c2bd43a255c4e0844c15","085c416cee44ae8df8d4415b5cd4ce3befe6711a","64e62d33471ac2a14cfd3272af1efa52aeb23d1b","1e2914a43adbd3f8efcda2231ac0d9643652d370","022dca2d4a82787e87b45dd5ac9fb7123b101af1","55d7551c54edacef55902209d7fc0fddcf7f0c5c"],"id":"ed6d7146c4e9f27ba042d01c91e8527fb283d206","s2Url":"https://semanticscholar.org/paper/ed6d7146c4e9f27ba042d01c91e8527fb283d206","authors":[{"name":"Benjamin Navarro","ids":["31249872"]},{"name":"Andrea Cherubini","ids":["48747847"]},{"name":"Aïcha Fonte","ids":["3305173"]},{"name":"Gérard Poisson","ids":["3067743"]},{"name":"Philippe Fraisse","ids":["1742523"]}],"doi":"10.1109/IROS.2017.8206532"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593913","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Motion trajectory planning is one crucial aspect for automated vehicles, as it governs the own future behavior in a dynamically changing environment. A good utilization of a vehicle's characteristics requires the consideration of the nonlinear system dynamics within the optimization problem to be solved. In particular, real-time feasibility is essential for automated driving, in order to account for the fast changing surrounding, e.g. for moving objects. The key contributions of this paper are the presentation of a fast optimization algorithm for trajectory planning including the nonlinear system model. Further, a new concurrent operation scheme for two optimization algorithms is derived and investigated. The proposed algorithm operates in the submillisecond range on a standard PC. As an exemplary scenario, the task of driving along a challenging reference course is demonstrated.","inCitations":[],"pmid":"","title":"Fast Trajectory Planning for Automated Vehicles Using Gradient-Based Nonlinear Model Predictive Control","journalPages":"7369-7374","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593913"],"entities":[],"journalVolume":"","outCitations":[],"id":"99862323c02fa341078693e21cd1d98e4e47759a","s2Url":"https://semanticscholar.org/paper/99862323c02fa341078693e21cd1d98e4e47759a","authors":[{"name":"Franz Gritschneder","ids":[]},{"name":"Knut Graichen","ids":[]},{"name":"Klaus C. J. Dietmayer","ids":[]}],"doi":"10.1109/IROS.2018.8593913"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593951","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this work we explore a new approach for robots to teach themselves about the world simply by observing it. In particular we investigate the effectiveness of learning task-agnostic representations for continuous control tasks. We extend Time-Contrastive Networks (TCN) that learn from visual observations by embedding multiple frames jointly in the embedding space as opposed to a single frame. We show that by doing so, we are now able to encode both position and velocity attributes significantly more accurately. We test the usefulness of this self-supervised approach in a reinforcement learning setting. We show that the representations learned by agents observing themselves take random actions, or other agents perform tasks successfully, can enable the learning of continuous control policies using algorithms like Proximal Policy Optimization (PPO) using only the learned embeddings as input. We also demonstrate significant improvements on the real-world Pouring dataset with a relative error reduction of 39.4% for motion attributes and 11.1% for static attributes compared to the single-frame baseline. Video results are available at https://sites.google.com/view/actionablerepresentations","inCitations":[],"pmid":"","title":"Learning Actionable Representations from Visual Observations","journalPages":"1577-1584","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593951","http://arxiv.org/abs/1808.00928","https://arxiv.org/pdf/1808.00928v2.pdf"],"entities":[],"journalVolume":"","outCitations":["21c9dd68b908825e2830b206659ae6dd5c5bfc02","2adae2da173b9dd720c8bcac0250a90a7f1ec697","0233f4e12db890dac1d06b5593c3ae7205d721a6","8f3f222127904d0e3568f4c8c807b1bf26b77f93","1c30bb689a40a895bd089e55e0cad746e343d1e2","8a7acaf6469c06ae5876d92f013184db5897bb13","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","b354ee518bfc1ac0d8ac447eece9edb69e92eae1","6285c6317d8de5caae96ac59d1204367ac5d6e40","17467609cbc1ccd1db683a851d4952825f223a8a","78a11b7d2d7e1b19d92d2afd51bd3624eca86c3c","a9a3ed69c94a3e1c08ef1f833d9199f57736238b","1554d6787b88b24cde34f398a330e1ad4d30b634","2ede6a685ad9b58f2090b01ce1e3f86e42aeda7e","e44be2dabcca29e01e9af51131142a1e28801737","6d4e3616d0b27957c4107ae877dc0dd4504b69ab","5ce3ceea40516453bce81e377f5c0481f14050de","6b1114df6a4c3d872ba675db91aba19309aa4af3","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","340f48901f72278f6bf78a04ee5b01df208cc508","3083fe8a8157fbe53176d4f4105e5fc8dd10cb32","19d583bf8c5533d1261ccdc068fdc3ef53b9ffb9","980cf8e3b59dd0923f7e7cf66d2bec4102d7035f","bdc783986e80dbfc64a2af702ecc673086bfce43","303d4aacde836904c53c07b366311cc654097251","59d86da5c5936e7a236678bf5eaaa7753c226fb1","24115d209e0733e319e39badc5411bbfd82c5133","b61a3f8b80bbd44f24544dc915f52fd30bbdf485","dce6f9d4017b1785979e7520fd0834ef8cf02f4b","b62f0f4fdac3ecf429c43b5cf0c4cc74c512b22c","8ccd751a4d5797a1b3ad06c0d4eba3e651b2c95a","0626908dd710b91aece1a81f4ca0635f23fc47f3","7b2d0720396b01dcfe112832d08a505c77a42c30","dc93f6d1b704abf12bbbb296f4ec250467bcb882","6cb26f4d6c7726abab06f4815185a8483e30ac08","2b10281297ee001a9f3f4ea1aa9bea6b638c27df","3d2a266f62a5e3184976623be7ef8124c5df57b5","0462fb777fd817cf3722e211843c8219988383a5","5333db28382b58af1d209dd06ea9d987baac6e78"],"id":"d70627510ec8131f96065cf927dd6d330d397853","s2Url":"https://semanticscholar.org/paper/d70627510ec8131f96065cf927dd6d330d397853","authors":[{"name":"Debidatta Dwibedi","ids":["2420123"]},{"name":"Jonathan Tompson","ids":["2704494"]},{"name":"Corey Lynch","ids":["32245472"]},{"name":"Pierre Sermanet","ids":["3142556"]}],"doi":"10.1109/IROS.2018.8593951"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594047","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Small-scale robots with soft joints and hinges have recently attracted interest because these components allow for more sophisticated locomotion mechanisms. Here, we investigate two different types of nanoscale swimmers as depicted in Figure 1. One consists of a rigid magnetic head linked to a semi-soft tail (1-link swimmer). Another consists of a rigid magnetic head and tail connected by a soft hinge (2-link swimmer). Both swimmers exhibit undulatory locomotion under an applied oscillating magnetic field. The speeds of the swimmers are assessed as a function of the oscillating magnetic field frequency and the sweeping angle. We find that a resonance-like frequency increases as the length decreases, and, in general, the speed increases as the sweeping angle increases. Last, we show that 2-link swimmers can also swim in a corkscrew-like pattern under rotating magnetic fields.","inCitations":[],"pmid":"","title":"Fabrication and Locomotion of Flexible Nanoswimmers","journalPages":"6193-6198","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594047"],"entities":[],"journalVolume":"","outCitations":[],"id":"032bf914035cd493e2e400213e1258005d577fc7","s2Url":"https://semanticscholar.org/paper/032bf914035cd493e2e400213e1258005d577fc7","authors":[{"name":"Bumjin Jang","ids":[]},{"name":"Amanda Aho","ids":[]},{"name":"Bradley J. Nelson","ids":[]},{"name":"Salvador Pané","ids":[]}],"doi":"10.1109/IROS.2018.8594047"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545850","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In fine-grained recognition, object misalignment and background noise are two long-standing factors that influence the robustness of deep learning models. This paper mainly focuses on person re-identification (re-ID) and introduces a feature alignment layer (FAL) which alleviates the target misalignment and the background noise simultaneously. Through attention mechanism, FAL informs the underlying importance of each pixel on feature maps, i.e., whether the pixel is beneficial towards discriminating different persons. Then the discriminative regions relocate to the center and are stretched to fill the feature maps. Such an \u201cattend and align\u201d mechanism is specified into two steps: target position prediction and value assignment. In the first step, a pixel on feature maps learns to find a target position which is ID-discriminative. In the second step, the pixel is assigned with a new value using the context of the predicted position. Moreover, FAL can be easily plugged into a canonical Convolutional Neural Network (CNN) and learned in an end-to-end manner. In experiment, our method yields competitive results compared with the state-of-the-art approaches on three person re-ID datasets, Market-1501, DukeMTMC-reID and CUHK03. We also demonstrate that our method improves a competitive fine-grained recognition baseline on CUB-200-2011.","inCitations":[],"pmid":"","title":"Attend and Align: Improving Deep Representations with Feature Alignment Layer for Person Retrieval","journalPages":"2148-2153","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545850"],"entities":[],"journalVolume":"","outCitations":["2dd2c7602d7f4a0b78494ac23ee1e28ff489be88","54dd77bd7b904a6a69609c9f3af11b42f654ab5d","2170636d5d31eb461618b5da10f4473c67e74e73","50bf4f77d8b66ec838ad59a869630eace7e0e4a7","3f45d73a7b8d10a59a68688c11950e003f4852fc","118e87ee5a8e0faa71b6ca5af6ff38f875132464","de0cfd94d16468cdaaa0fe725e214930587ed8ce","1d0dcb458aa4d30b51f7c74b159be687f39120a0","27a2fad58dd8727e280f97036e0d2bc55ef5424c","0c769c19d894e0dbd6eb314781dc1db3c626df57","92166eb883b0505040c2d61c758985e5ec051f83","744cc8c69255cbe9d992315e456b9efb06f42e20","13ea9a2ed134a9e238d33024fba34d3dd6a010e0","7559df35d786dad7577824153720795f7da762c8","34cf90fcbf83025666c5c86ec30ac58b632b27b0","367008b91eb57c5ea64ef7520dfcabc0c5c85532","6bd36e9fd0ef20a3074e1430a6cc601e6d407fc3","2c03df8b48bf3fa39054345bafabfeff15bfd11d","9fede3a43d6b63a8d5988cb7f52d74bba8d164cd","76ad6daa899a8657c9c17480e5fc440fda53acec","fc26fc2340a863d6da0b427cd924fb4cb101051b","f41c7bb02fc97d5fb9cadd7a49c3e558a1c58a44","4308bd8c28e37e2ed9a3fcfe74d5436cce34b410","bdd3e33cb78965bcd15d69b7902c813b2ac864ff","2788f382e4396290acfc8b21df45cc811586e66e","c069629a51f6c1c301eb20ed77bc6b586c24ce32","4cb36aea73a328da8ffcdc616407bae3c908aa07","fd0e1fecf7e72318a4c53463fd5650720df40281","9812542cae5a470ea601e7c3a871331694105093"],"id":"be79ad118d0524d9b493f4a14a662c8184e6405a","s2Url":"https://semanticscholar.org/paper/be79ad118d0524d9b493f4a14a662c8184e6405a","authors":[{"name":"Qin Xu","ids":["50537003"]},{"name":"Yifan Sun","ids":["2213925"]},{"name":"Yali Li","ids":["47002057"]},{"name":"Shengjin Wang","ids":["1678689"]}],"doi":"10.1109/ICPR.2018.8545850"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594338","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a methodology to design mechanosensor feedback to oscillator-based controller for worm-like soft-bodied robots. A reinforcement learning technique, i.e., PEPG, is employed to embed appropriate mechanosensor feedback to harness global entrainment among the controller, the body dynamics, and the environment without explicitly designing the interaction between the oscillators. Another reinforcement learning, actor-critic, was applied to train the controller for the simulation models to analyze the effectiveness of PEPG in the system. Furthermore, the gait controller was trained under different body dynamics, i.e., the physical model of a caterpillar and an earthworm. We found that PEPG is suitable for the system probably because it does not add exploration noise to actions and it conducts episode based parameter updates. The simulation results show the proposed method can acquire distinct behavior, i.e., caterpillars' crawling, inching and earthworms' crawling, under different body dynamics. The outcome implies, that by utilizing appropriate learning method, desired functionality can be achieved in soft-bodied robots without explicitly designing their behavior.","inCitations":[],"pmid":"","title":"Learning Oscillator-Based Gait Controller for String-Form Soft Robots Using Parameter-Exploring Policy Gradients","journalPages":"6445-6452","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594338"],"entities":[],"journalVolume":"","outCitations":[],"id":"cceb41d247f5efe5512c8b67e2fa3a1bf1415d50","s2Url":"https://semanticscholar.org/paper/cceb41d247f5efe5512c8b67e2fa3a1bf1415d50","authors":[{"name":"Matthew Ishige","ids":[]},{"name":"Takuya Umedachi","ids":[]},{"name":"Tadahrio Taniguchi","ids":[]},{"name":"Yoshihiro Kawahara","ids":[]}],"doi":"10.1109/IROS.2018.8594338"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205975","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The expanding 3C (Computer, Communication, and Consumer electronics) manufacturing industry leads to a high demand on the soldering of flexible PCBs. Current manual soldering has the disadvantages of low output, low speed, and low efficiency, and upgrading soldering operations with robotic technologies is mainly limited by the property of deformation of flexible PCBs. In this paper, a novel robotic manipulation system is developed for automatic soldering of flexible PCBs, consisting of the hardware of a dual-arm configuration and the software of a cooperative control scheme. The proposed system works in a sequential manner, in the sense that a Cartesian-space region reaching controller drives an assistive arm to actively contact the PCB first, and a vision-based tracking controller activates a soldering arm after the deformation is stabilized. The proposed formulation eliminates uncertain deformation of flexible PCBs and thus guarantees the feasibility of robotic soldering.","inCitations":["e985fb57d89294f5f902fb4022574d44dc1d9a5b","4b015007c49852161091fd790eb2342fde9d4500"],"pmid":"","title":"Cooperative robotic soldering of flexible PCBs","journalPages":"1651-1656","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205975"],"entities":["3D printing","ARM architecture","Assistive technology","Coat of arms","Consensus dynamics","Flexible electronics","Lyapunov fractal","Printed circuit board","Robot","Soldering"],"journalVolume":"","outCitations":["b3668dfda004b487d64d286d21de1170e0545458","0082ebe6b7af7b11a5c5b70e9c8ab51c9065d6e8","23d4acbd427488dc1f1cdd8d5decc5881e44d7df","706dd5f09d7c2ecc20404b5c9b3521bfa067af72","3c1b95360d3747a28f883915d3acd2e06c0c0c7c","c1cc1b928c70cdbe9560c1213ac6c6e64af22ad0","97415ba6783cd150d1f034dadfe0ff70adb23ba2","6bf3aca4b0f1f34bc43d0df6ccf371d86c10079b","b192af22c47794229d0459ce7781c5529a5c9774","e5cbc059ce5ade55ea41ceb1a51d02df8cf87e8f","94322142a8be81abc111b8f063cdf12e068ab952","1476d2d01ec4b02424e8977924038a9569800147","ba1bc931030807c5bd3c1a52b22ae2e7358280f1","da7d25b533f985d027b5c93fa44e4a1c1404ecf5","4b941a85fb4045e0361200b189073ba9da9d5426","f92240ae699798649eb94b758f07430ad2bb6f29","65fe76ee8a48d184c106c2c808cc780e1dfc1a71","4676d81d6a2477f6ea1de5723fc81e431fbaa96f","5dfd695c905db838d3fc3b7cf87bf0c3dcc7a0c3"],"id":"23ac0852375b83353721060973dd67c4b04599ad","s2Url":"https://semanticscholar.org/paper/23ac0852375b83353721060973dd67c4b04599ad","authors":[{"name":"Xiang Li","ids":["1737850"]},{"name":"Xing Su","ids":["50140777"]},{"name":"Yunhui Liu","ids":["40013571"]}],"doi":"10.1109/IROS.2017.8205975"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202259","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Snake like continuum robots are increasingly used for minimally invasive surgery. Most robotic devices of this sort that have been reported to date are controlled in an open loop manner. Using shape sensing to provide closed loop feedback would allow for more accurate control of the robot's position and, hence, more precise surgery. Fiber Bragg Gratings, magnetic sensors and optical reflectance sensors have all been reported for this purpose but are often limited by their cost, size, stiffness or complexity of fabrication. To address this issue, we designed, manufactured and tested a prototype two-link robot with a built-in fiber-optic shape sensor that can deliver and control the position of a CO2-laser fiber for soft tissue ablation. The shape sensing is based on optical reflectance, and the device (which has a 4 mm outer diameter) is fabricated using 3D printing. Here we present proof-of-concept results demonstrating successful shape sensing \u2014 i.e. measurement of the angular displacement of the upper link of the robot relative to the lower link \u2014 in real time with a mean measurement error of only 0.7°.","inCitations":[],"pmid":"","title":"Shape sensing of miniature snake-like robots using optical fibers","journalPages":"947-952","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202259"],"entities":["3D printing","AngularJS","Artificial neural network","Closed-loop transfer function","Displacement mapping","Optical fiber","Prototype","Real-time clock","Robot","Sensor","Triune continuum paradigm"],"journalVolume":"","outCitations":["435ccc3bceb6b82bc73502cb487577373fa2f765","63ffa4d0d956475efec9463b4ec93d4fbaa18d46","91c7cafeceaea55195f0fd4357260efcd23a9bf9","13b1364045a8325071cd3a4da17506f115fa1bbb","7e456b3a7efeb3fc5fc7c91fd35b9448a10e7cfb","4f488c259be9d772ddb8ffbcdbf16b76d012aee7","4d38c171e912d56069d183d716556cf30cc824a4","153caf649facd6dcd8a27fe3e4b6f080fab5361d","6d35fa6a33d17d00d23eb355e7786838a7da798c"],"id":"0662a92ae85c9948776410a35bd3608495d1753e","s2Url":"https://semanticscholar.org/paper/0662a92ae85c9948776410a35bd3608495d1753e","authors":[{"name":"Andreas Schmitz","ids":["49029658"]},{"name":"Alexander J. Thompson","ids":["50045046"]},{"name":"Pierre Berthet-Rayne","ids":["2244381"]},{"name":"Carlo Seneci","ids":["2786904"]},{"name":"Piyamate Wisanuvej","ids":["2296812"]},{"name":"Guang-Zhong Yang","ids":["1743111"]}],"doi":"10.1109/IROS.2017.8202259"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00064","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Estimating the 3D facial landmarks from a 2D image remains a challenging problem. Even though state-of-the-art 2D alignment methods are able to predict accurate landmarks for semi-frontal faces, the majority of them fail to provide semantically consistent landmarks for profile faces. A de facto solution to this problem is through 3D face alignment that preserves correspondence across different poses. In this paper, we proposed a Cascade Multi-view Hourglass Model for 3D face alignment, where the first Hourglass model is explored to jointly predict semi-frontal and profile 2D facial landmarks, after removing spatial transformations, another Hourglass model is employed to estimate the 3D facial shapes. To improve the capacity without sacrificing the computational complexity, the original residual bottleneck block in the Hourglass model is replaced by a parallel, multi-scale inception-resnet block. Extensive experiments on two challenging 3D face alignment datasets, AFLW2000-3D and Menpo-3D, show the robustness of the proposed method under continuous pose changes.","inCitations":["ca537e1726a8d8c371a71bbd6d9098774ab51955","2302933bb19072f2b3381c5049fb15c45e5d52a8","1929863fff917ee7f6dc428fc1ce732777668eca","9409753efbdbf82e1c01a717ff98aa5d4c6d980d"],"pmid":"","title":"Cascade Multi-View Hourglass Model for Robust 3D Face Alignment","journalPages":"399-403","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00064","https://ibug.doc.ic.ac.uk/media/uploads/documents/fg2018_3dalignment.pdf"],"entities":["Computational complexity theory","Expectation propagation","Experiment","Facial recognition system","Graphics processing unit","Semiconductor industry"],"journalVolume":"","outCitations":["94aa8a3787385b13ee7c4fdd2b2b2a574ffcbd81","88e2efab01e883e037a416c63a03075d66625c26","93420d9212dd15b3ef37f566e4d57e76bb2fab2f","57ebeff9273dea933e2a75c306849baf43081a8c","02e628e99f9a1b295458cb453c09863ea1641b67","ad5a35a251e07628dd035c68e44a64c53652be6b","0e3fcfe63b7b6620e3c47e9751fe3456e85cc52f","0561bed18b6278434deae562d646e8adad72e75d","30cd39388b5c1aae7d8153c0ab9d54b61b474ffe","5b0bf1063b694e4b1575bb428edb4f3451d9bf04","2724ba85ec4a66de18da33925e537f3902f21249","614a7c42aae8946c7ad4c36b53290860f6256441","1a8ccc23ed73db64748e31c61c69fe23c48a2bb1","14318685b5959b51d0f1e3db34643eb2855dc6d9","336488746cc76e7f13b0ec68ccfe4df6d76cdc8f","88a323dc70de8788a9cdd415f7c65fc3d3a8c7cb","2a4153655ad1169d482e22c468d67f3bc2c49f12","500b92578e4deff98ce20e6017124e6d2053b451","1da1487ec40b6e1e4ad672928558f5ceded14b8d","4cd0da974af9356027a31b8485a34a24b57b8b90","375435fb0da220a65ac9e82275a880e1b9f0a557","37ce1d3a6415d6fc1760964e2a04174c24208173","22e2066acfb795ac4db3f97d2ac176d6ca41836c","085ceda1c65caf11762b3452f87660703f914782","e2a16867d35eaf8d36e9730250cd8c73d2e9cc44","3352426a67eabe3516812cb66a77aeb8b4df4d1b","03f98c175b4230960ac347b1100fbfc10c100d0c","59d8fa6fd91cdb72cd0fa74c04016d79ef5a752b","75da1df4ed319926c544eefe17ec8d720feef8c0","a74251efa970b92925b89eeef50a5e37d9281ad0","4c078c2919c7bdc26ca2238fa1a79e0331898b56","6932baa348943507d992aba75402cfe8545a1a9b","ebc2a3e8a510c625353637e8e8f07bd34410228f","0ea96fe9ce2912e96a1d431962b3796576f9fad7","0f2f4edb7599de34c97f680cf356943e57088345","721e5ba3383b05a78ef1dfe85bf38efa7e2d611d","6d66c98009018ac1512047e6bdfb525c35683b16","7492c611b1df6bce895bee6ba33737e7fc7f60a6","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","061356704ec86334dbbc073985375fe13cd39088","e5533c70706109ee8d0b2a4360fbe73fd3b0f35d","e9b8f2ee742b32ae272c950cc6fa2d5a2d05f028","2c03df8b48bf3fa39054345bafabfeff15bfd11d","303065c44cf847849d04da16b8b1d9a120cef73a"],"id":"66490b5869822b31d32af7108eaff193fbdb37b0","s2Url":"https://semanticscholar.org/paper/66490b5869822b31d32af7108eaff193fbdb37b0","authors":[{"name":"Jiankang Deng","ids":["3234063"]},{"name":"Yuxiang Zhou","ids":["47943220"]},{"name":"Shiyang Cheng","ids":["1902288"]},{"name":"Stefanos P. Zafeiriou","ids":["1776444"]}],"doi":"10.1109/FG.2018.00064"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00032","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Depression is a serious mental disorder that affects millions of people all over the world. Traditional clinical diagnosis methods are subjective, complicated and need extensive participation of experts. Audio-visual automatic depression analysis systems predominantly base their predictions on very brief sequential segments, sometimes as little as one frame. Such data contains much redundant information, causes a high computational load, and negatively affects the detection accuracy. Final decision making at the sequence level is then based on the fusion of frame or segment level predictions. However, this approach loses longer term behavioural correlations, as the behaviours themselves are abstracted away by the frame-level predictions. We propose to on the one hand use automatically detected human behaviour primitives such as Gaze directions, Facial action units (AU), etc. as low-dimensional multi-channel time series data, which can then be used to create two sequence descriptors. The first calculates the sequence-level statistics of the behaviour primitives and the second casts the problem as a Convolutional Neural Network problem operating on a spectral representation of the multichannel behaviour signals. The results of depression detection (binary classification) and severity estimation (regression) experiments conducted on the AVEC 2016 DAIC-WOZ database show that both methods achieved significant improvement compared to the previous state of the art in terms of the depression severity estimation.","inCitations":["04ef4cffe697f0d14a417452dc1ef06d274f53fb"],"pmid":"","title":"Human Behaviour-Based Automatic Depression Analysis Using Hand-Crafted Statistics and Deep Learned Spectral Features","journalPages":"158-165","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00032"],"entities":["Algorithm","Audio signal processing","Baseline (configuration management)","Binary classification","Climate Forecast System","Convolution","Convolutional neural network","Diagnostically Acceptable Irreversible Compression","Electroencephalography","Experiment","Feature extraction","Integrated Woz Machine","Map","Overfitting","Sequence logo","Time series"],"journalVolume":"","outCitations":["1c5d7d02a26aa052ecc47d301de4929083e5d320","8d18262c8083d28937db10436d59b31fccd5c866","3390e85af6b33ea4cda511041444bb97d1cf9151","706b1123217febf934ee5c33b4af27507a85771a","6bc43977fb11cceed0b9aa55b23c6dd29dd9a132","863996000fab53f76e73c083b3c6d14eea5efd76","0c81586c45656082b41b8c061ced0d0755573ea1","2a0563f6b76ff45424afe68ea581184ac342a60c","2fda461869f84a9298a0e93ef280f79b9fb76f94","96578785836d7416bf2e9c154f687eed8f93b1e4","3ef71aee72d959306105224db9d0e3c3fd62de51","839104964d04c505a827ae854e1251271968c7f7","93221e14e6ce639908c9983e02e960bc2e5939ea","129db881247616e7529a27c33d6030243545cb23","dcedec883a2d1335de2932b73bb44e551d5609c9","0db3492c116eb277de26f3b729309c35c627af46","6bea3774d05252c7e105897da89da758da750374","0b6082ce579225b6113c0984713a3cca8d350ff1","8eadb772d9feba2fbf7e1d49eea6386e9213cf17","659fc2a483a97dafb8fb110d08369652bbb759f9","ef628fd5a4c0575791355bfaf16453eb6cb6f612","04f175b22aa3046f29c5400813e1cb0214f60d40","58c45859350b7e9fc2dc6676e318e8f526073f5f","fed90d84dc199d7bd996caddd1cab26ded89de98","314b7d87f77f58e428ec56479fb9939e33a9beda","1499d4c60ebc643173c124406603a8d25ec7d811","b829d53befb725877381a81edfc7c9ae33b385bd","9bf0629b82c0d5bfa3ed0fe518cbd2da08715ad0"],"id":"1e5c92f1300661a527ba8fe8db33e508cbfd5f61","s2Url":"https://semanticscholar.org/paper/1e5c92f1300661a527ba8fe8db33e508cbfd5f61","authors":[{"name":"Siyang Song","ids":["46200063"]},{"name":"Linlin Shen","ids":["1687690"]},{"name":"Michel F. Valstar","ids":["1795528"]}],"doi":"10.1109/FG.2018.00032"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206582","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In this paper we propose a method for monocular visual-inertial odometry that utilizes image edges as measurements. In contrast to previous feature-based approaches, the proposed method does not employ any assumption on the geometry of the scene (e.g., it does not assume straight lines). It can thus use measurements from all image areas with significant gradient, similarly to direct semi-dense methods. However, in contrast to direct semi-dense approaches, the proposed method's measurement model is invariant to linear changes in the image intensity. The novel edge parameterization and measurement model we propose explicitly account for the fact that edge points can only provide useful information in the direction of the image gradient. We present both Monte-Carlo simulations, as well as results from real-world experimental testing, which demonstrate that the proposed edge-based approach to visual-inertial odometry is consistent, and outperforms the point-based one.","inCitations":["a2525e6a30041663e9bc6e85b21605d539ee1dd3"],"pmid":"","title":"Edge-based visual-inertial odometry","journalPages":"6670-6677","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206582"],"entities":["Algorithm","Experiment","Image gradient","Monte Carlo method","Odometry","Semiconductor industry","Simulation"],"journalVolume":"","outCitations":["8f63923aa81173dcc60ba01ba7fb21a37b9683b0","1a998506899da3bc703455bde45114bd4c228947","7633c7470819061477433fdae15c64c8b49a758b","1dcae5e5e9fc1b4c0315b6e68f19b9bb2d9dba05","7603a56de5d426a732b880cdd6ad9e0686f10c44","a56ed0bc2d69094e351a044ae8bc64ca0da691f8","39687b17b5448be8de18ccbae0b571f17f00880c","3b5cff5f874b7c7056d4ac58fc976458c2ab95a7","53226f929c8affbf49c1bdb4cbe5e8ff1b05e0d4","2cbc2797efe322f8ac286d90cbc2c4852f0f6bde","050ef94ece6838a1b9ee0b108d6faaf670ae35b8","bd10e6a3e5663f39e7f671a8869db4af902a0b5e","22e4344164398282c8958b808dd1e77e2e1d8a97","0584f9d8b368d3f1a2ec34fd5e240056f875c4a2","02a78a9fd1a577fc471c82b15f7730673578c94e","55bc43bc2b34acf3ab0cf0a4ef901ef5b786baf1","896dc52d0fdf23e45d7fe029b09cdd81feefaf09","0a202f1dfc6991a6a204eaa5e6b46d6223a4d98a","65caf9290f4ea6ace241c335cfad9601b4394eaa","90d06b6857ea5d1a614c9ceedb164241ed84f60c","c13cb6dfd26a1b545d50d05b52c99eb87b1c82b2","0be0c13803cd08e81b7adaada537e91222eb1491","461f3e050b879124e4caac40385606a8580165a7","badf73b0415e03bbd86ef1abcc3d7ef3da8f6ef9","c6a89bbbfc5915a7e771df5fcb4a968cecbade90","289e8386dc69651047acc55f6a753a9a42ed772e","a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e","4dfe471596d4f4158f2c123964a83849cb29dea8","064b64894d3d3b019e3b85abfc1de274272b9201","2fc894bdfbe36c502df8d9efb3ca6eb2d4a3c56d","31864e13a9b3473ebb07b4f991f0ae3363517244","2e7975fb0351638bd2646a217e5885ae56ca5cff","1dc1ccd81047bff2fc3d375b3147786b1fb399fc","3ae415eb413e359a63a85490f9b7d87a3b641050","7ef197525d18e2fbd1ef5a313776d4a7d65803ab"],"id":"99ce503e0265847b3a84c3623c78a7b916916ad4","s2Url":"https://semanticscholar.org/paper/99ce503e0265847b3a84c3623c78a7b916916ad4","authors":[{"name":"Hongsheng Yu","ids":["2964296"]},{"name":"Anastasios I. Mourikis","ids":["1751869"]}],"doi":"10.1109/IROS.2017.8206582"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206335","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"In robotics, vision sensors are used to estimate the poses of objects in the environment. However, it is a fundamental problem that the estimated poses are not always accurate enough for a given robotic task. Proper sensor placement can mitigate this problem. We present a method which can predict the pose uncertainties in the Iterative Closest Point (ICP) algorithm, which is often used as the last critical pose refinement step in a pose estimation system. With our method we thus provide a crucial tool needed for the optimization of a robust pose estimation system. Our method relies on the generation of synthetic depth images in a Monte Carlo simulation. In this paper we demonstrate our method for depth sensors which rely on Kinect v1 like technology. We evaluate our method using real depth sensor recordings from the publicly available BigBird dataset. The evaluation shows that the uncertainty predictions of our method are in better correspondence with real world experimental results than the state of the art analytical method.","inCitations":["8d4ee3537253cbe5156394e413354f57172eefbe"],"pmid":"","title":"Prediction of ICP pose uncertainties using Monte Carlo simulation with synthetic depth images","journalPages":"4640-4647","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206335"],"entities":["3D pose estimation","Algorithm","Image sensor","Iterative closest point","Iterative method","Kinect","Mathematical optimization","Monte Carlo method","Nvidia 3D Vision","Range imaging","Refinement (computing)","Robot","Robotics","Robust optimization","Simulation","Synthetic data","Synthetic intelligence"],"journalVolume":"","outCitations":["8913a5b7ed91c5f6dec95349fbc6919deee4fc75","a8a618363b8dee8037df9133668ec8dcd532ee4e","0fcad2f00357887951b3fdfaaf9e0109857c0a5b","0c50a857b3299bb7b7c9c0bb5e0beb5e994763a0","bb8123b5f19c05e63be57e8439ebab483174eac4","f0548b3b539f7f1a8af43936cd66b85b70990aa1","e97279b4e44326d98a792be9a3420a9371800555","5f0d86c9c5b7d37b4408843aa95119bf7771533a","3f6c951401c31daa80c2c3d27f6e3b176ba6365e","5dfced93c3256ecc9a68d2f29808a363d9bf1ad4","d1db5c78ce2107e21624b5774a4275b8ec4d3edd","500eb93a9a39bb437cd6220bb8ebb985838fb92c"],"id":"78b99e448f74df1defe1273962d0016ef87e384e","s2Url":"https://semanticscholar.org/paper/78b99e448f74df1defe1273962d0016ef87e384e","authors":[{"name":"Thorbjørn Mosekjær Iversen","ids":["2020492"]},{"name":"Anders Glent Buch","ids":["1789313"]},{"name":"Dirk Kraft","ids":["39450819"]}],"doi":"10.1109/IROS.2017.8206335"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593736","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"While there has been significant progress in solving the problems of image pixel labeling, object detection and scene classification, existing approaches normally address them separately. In this paper, we propose to tackle these problems from a bottom-up perspective, where we simply need a semantic segmentation of the scene as input. We employ the DeepLab architecture, based on the ResNet deep network, which leverages multi-scale inputs to later fuse their responses to perform a precise pixel labeling of the scene. This semantic segmentation mask is used to localize the objects and to recognize the scene, following two simple yet effective strategies. We evaluate the benefits of our solutions, performing a thorough experimental evaluation on the NYU Depth V2 dataset. Our approach achieves a performance that beats the leading results by a significant margin, defining the new state of the art in this benchmark for the three tasks comprising the scene understanding: semantic segmentation, object detection and scene categorization.","inCitations":[],"pmid":"","title":"In pixels we trust: From Pixel Labeling to Object Localization and Scene Categorization","journalPages":"355-361","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593736","http://arxiv.org/abs/1807.07284","https://arxiv.org/pdf/1807.07284v1.pdf"],"entities":[],"journalVolume":"","outCitations":["1e0f00de4902f33d48612f8fb5384366ce7654d4","014ad0ec0fac206d5a9f02afeda047e177bf6743","009fba8df6bbca155d9e070a9bd8d0959bc693c2","9201bf6f8222c2335913002e13fbac640fc0f4ec","9b16ff0e8ac87c5c4994e9025216df6e399375e5","722fcc35def20cfcca3ada76c8dd7a585d6de386","2c03df8b48bf3fa39054345bafabfeff15bfd11d","60326087fc907a58ba52598acf52bab42d1a96e0","c5a6118d2aaa3e74a0d9dae5b33899cd66f7869a","136b9952f29632ab3fa2bbf43fed277204e13cb5","322a7dad274f440a92548faa8f2b2be666b2d01f","a5307e2ffdfe34264385c26ea998a05656170179","148686aebefff0a7fb3f80024f765ef4b06d2efc","20a78d3145279dcd799cd7a856ae2714f4863a16","8eaade06b75dac977574d05ce1058b86df059d47","22ef6475b220356f8547e9980c437f55ac2ad45c","9360ce51ec055c05fd0384343792c58363383952","cab372bc3824780cce20d9dd1c22d4df39ed081a","b705ca751a947e3b761e2305b41891051525d9df","060444554286b4016e409e3d61b3dc39339e2b79","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","07f77ad9c58b21588a9c6fa79ca7917dd58cca98","bf5f67ebbe41f2fb1726a7c3c0be707366d5a4fb","ef83df34e693a28603eeed498090425a4357b267","7d39d69b23424446f0400ef603b2e3e22d0309d6","098abe1eee6208b693daafe7183f017f9e71e409","061356704ec86334dbbc073985375fe13cd39088","0c3d7a871915838ed5e126978c0d9f69f969b6a8","7d54fe9c008733f1f19eec2a0b0eddf7a646ab52"],"id":"01444cefc7c2a611672c1ecf101424b521524997","s2Url":"https://semanticscholar.org/paper/01444cefc7c2a611672c1ecf101424b521524997","authors":[{"name":"Carlos Herranz-Perdiguero","ids":["51136073"]},{"name":"Carolina Redondo-Cabrera","ids":["3296857"]},{"name":"Roberto Javier López-Sastre","ids":["2941882"]}],"doi":"10.1109/IROS.2018.8593736"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206595","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"A truly autonomous mobile robot have to solve the SLAM problem (i.e. simultaneous map building and pose estimation) in order to navigate in an unknown environment. Unfortunately, a universal solution for the problem hasn't been proposed yet. The tinySLAM algorithm that has a compact and clear code was designed to solve SLAM in an indoor environment using a noisy laser scanner. This paper introduces the vinySLAM method that enhances tinySLAM with the Transferable Belief Model to improve its robustness and accuracy. Proposed enhancements affect scan matching and occupancy tracking keeping simplicity and clearness of the original code. The evaluation on publicly available datasets shows significant robustness and accuracy improvements.","inCitations":["71d0dc0ca3ebac4a688b1953956fe2aee1633810"],"pmid":"","title":"VinySLAM: An indoor SLAM method for low-cost platforms based on the Transferable Belief Model","journalPages":"6770-6776","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206595"],"entities":["Algorithm","Autonomous robot","Cartography","Computation","Discrepancy function","Hoc (programming language)","Loss function","Map","Mobile robot","Particle filter","Robot Operating System","Simultaneous localization and mapping"],"journalVolume":"","outCitations":["6f24dfa0cb1556983d0f7be8233085f82be5e9e5","040f9907a894a7c828489fd67ea4188e0dcf0884","95d375e06f1268d0d61a17317b232c7dac82ceab","92f7c3c1999d6be7291b1c24eeef98344f4f7edd","0d82ac80275283c3dd26aca9e629ee6a9ca8a07a","a4a5cfee0e5482714b6f6eaca3ec7a595056cea1","124c33a1984d77124959ce4bd44687afdbe16e35","77ba260424afef22853779f0a10137b48d6332f4","5ea297d0ab8914a324831c219097a4f9b1be2237","39da3a83e62853c911c1aa52001576191765bcde","0a6c8ba367c803366ca1eb7e8a1125cdd5d42fd2","0df9a304101921ef633e2a278044c7cd3d5f1da9","579735c1e5b2b0ae7fb42fcb9e2433f3118afd20","61d234dd4f7b733e5acf2550badcf1e9333b6de1","66349b4c7ef94b4d0f36fd1c862d616d16611854","baefc5400875071f7870d306d3b1e5afa0776263","72933db9c09a69b9d6cc6f8699f3aabc1fc75104","d957d5f92750df278bf295d78d18c41a4edb60f1","68d27f581feee95c5623693b85f77db40f22de14","bfde965e330d488af0cfe87b13069c71970b2d2a"],"id":"79165268558d27210efacf6900e6413b2aa79cc8","s2Url":"https://semanticscholar.org/paper/79165268558d27210efacf6900e6413b2aa79cc8","authors":[{"name":"Arthur Huletski","ids":["2185614"]},{"name":"Dmitriy Kartashov","ids":["40105522"]},{"name":"Kirill Krinkin","ids":["2047484"]}],"doi":"10.1109/IROS.2017.8206595"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202221","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We propose an approach to vision-based pose estimation using object recognition and identity. Whereas feature based scene recognition and pose estimation methods are well established as effective means for estimating motion and recognizing locations, feature-based methods depend critically on the detection of common local features from one view of a scene to another. We focus on place recognition and pose change estimation in the context of large changes in viewing position, even to the extent that no common surfaces are seen between the two views. Our approach is based on using object identities and their inter-relationship to compute pose change. An important secondary outcome of our method is that it simultaneously infers the 3D poses of objects in the scene that are used as features. Such an object-based approach is inspired by a vast literature on human perception and has the potential for great robustness, albeit at the expense of accuracy. We propose a formulation of the problem using pairwise contextual constraints and develop an efficient algorithmic solution. We validate the approach and quantify its performance using the publicly available TUM SLAM dataset [1].","inCitations":["bbf3559d54a9743e8e812a307d2a4ec21c5e32b4","b00bb8b4488e1a877d5b4a7b02d5e2008843bcdd"],"pmid":"","title":"Context-coherent scenes of objects for camera pose estimation","journalPages":"655-660","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202221","http://www.cim.mcgill.ca/~mrl/pubs/jimmy/iros2017_context_relevance.pdf"],"entities":["3D pose estimation","Coherence (physics)","Color depth","Gaussian blur","Ground truth","Image resolution","Natural language","Object-based language","Outline of object recognition","Robotic mapping","Servo","Simultaneous localization and mapping","Visual odometry"],"journalVolume":"","outCitations":["24fef57dec21d45112cd80c97f3e4f15c93d79df","1a998506899da3bc703455bde45114bd4c228947","af1b67e9421e50e78f2dd2123e37bd8146a2f34d","89bf8f9b83bf52c5a28dd6bfa9010a8eec3f6408","0839efad00a12b2369b2832c9c822c38dcd14d90","b2acd11626efe9b1f77118eb7ed5748aaa968233","5eb4c55740165defacf08329beaae5314d7fbfe6","5c3e3c9eb353c2662333b2cafbb67bfce4baa371","5aea5f37c8d97189ad38db84d8ab201c0d1817c7","4bcaff3735371f462c4e185953667f416f223eb9","551a1f99911a8deebeb0a667f5002a5297588304","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","cb6a1213c115d3cd94ecf3aa057220a6cf2b9555","8b977431ddbff04d63d25f38ab759885cb306533","90c26eca18194e23cfd3c3bbf341b133e4bf5f6b","21a1654b856cf0c64e60e58258669b374cb05539","00514ba3949302705b3b88af5eeef2d05cf8497d"],"id":"200ef1ffab122a6b3d5ae7f807c2cbb2350ca905","s2Url":"https://semanticscholar.org/paper/200ef1ffab122a6b3d5ae7f807c2cbb2350ca905","authors":[{"name":"Jimmy Li","ids":["2666951"]},{"name":"David Meger","ids":["2462512"]},{"name":"Gregory Dudek","ids":["1798721"]}],"doi":"10.1109/IROS.2017.8202221"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206586","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper proposes a single-shot approach for recognising clothing categories from 2.5D features. We propose two visual features, BSP (B-Spline Patch) and TSD (Topology Spatial Distances) for this task. The local BSP features are encoded by LLC (Locality-constrained Linear Coding) and fused with three different global features. Our visual feature is robust to deformable shapes and our approach is able to recognise the category of unknown clothing in unconstrained and random configurations. We integrated the category recognition pipeline with a stereo vision system, clothing instance detection, and dual-arm manipulators to achieve an autonomous sorting system. To verify the performance of our proposed method, we build a high-resolution RGBD clothing dataset of 50 clothing items of 5 categories sampled in random configurations (a total of 2,100 clothing samples). Experimental results show that our approach is able to reach 83.2% accuracy while classifying clothing items which were previously unseen during training. This advances beyond the previous state-of-the-art by 36.2%. Finally, we evaluate the proposed approach in an autonomous robot sorting system, in which the robot recognises a clothing item from an unconstrained pile, grasps it, and sorts it into a box according to its category. Our proposed sorting system achieves reasonable sorting success rates with single-shot perception.","inCitations":["8fb1b5e8d107779b966e461d274eca6eae201941","3046b29279a3620140a4fb47720591d83c05b8c1"],"pmid":"","title":"Single-shot clothing category recognition in free-configurations with application to autonomous clothes sorting","journalPages":"6699-6706","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206586","http://eprints.gla.ac.uk/146761/7/146761.pdf","https://export.arxiv.org/pdf/1707.07157","https://arxiv.org/pdf/1707.07157v1.pdf","http://arxiv.org/abs/1707.07157"],"entities":[],"journalVolume":"","outCitations":["ff9f166d16029afae110df8ae0b15de8b0068f8e","33fad977a6b317cfd6ecd43d978687e0df8a7338","b0aff580dd02c6dd847de76aabda175917f074fd","732708b4e744bb9dd962d433b64ab83756783550","53b7b642187f73d06dac440355d260f110f5836c","0b7c1bcd0289058b5dfc0d3ff114972712bc7f1a","67029b874d5d1389dd91d2a5a442e9733b805f9f","47c3c0273c010115cd1d5ee90210937f47658d4e","1a998506899da3bc703455bde45114bd4c228947","80bcdd5d82f03e4d2bca28cbc1399424dac138f9","18f0b9a90e3e00a9d5a95a75f1c6689e4b0264bd","140875551c37222a5a214dc42fe1c76937da928b","10df9f1126be6a93c8c8e847e768aea7db8df4dc","6e1e8d501a70f26ab09dac0a2afd889f046d0ad8","47598c6267a065ad0f9226c0a130728fedf18b81","0f16f6f478b5c788dce466eb50e36c612273c36e","74f42e43e6d5094985ded5ea04ddee8486dd6571","2fab1a93f9e2aff60bf4e2f58ea86d93e020ed38","13dd25c5e7df2b23ec9a168a233598702c2afc97","b91180d8853d00e8f2df7ee3532e07d3d0cce2af","16dfa3aae5bdc99f81dadaa6fc89c233bca4a52e","99e4ea3146994769cf721551124f56f81ceebc06","a4ef2b9b5d71c83ff887680800b0b58feae42d35","7fa2605676c589a7d1a90d759f8d7832940118b5","0404ba243ecbb8efc6bcb07a754b6f8770856131","c1ced85995eaa0d904e25c77b6919b66f168be64"],"id":"7710195e00ff4018820bb8f35c2f3825957136da","s2Url":"https://semanticscholar.org/paper/7710195e00ff4018820bb8f35c2f3825957136da","authors":[{"name":"Li Sun","ids":["49755228"]},{"name":"Gerardo Aragon-Camarasa","ids":["2412785"]},{"name":"Simon Rogers","ids":["1829979"]},{"name":"Rustam Stolkin","ids":["1815998"]},{"name":"J. Paul Siebert","ids":["1982689"]}],"doi":"10.1109/IROS.2017.8206586"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202152","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Robotic-assisted minimally invasive surgical systems suffer from one major limitation which is the lack of interaction forces feedback. The restricted sense of touch hinders the surgeons' performance and reduces their dexterity and precision during a procedure. In this work, we present a sensory substitution approach that relies on visual stimuli to transmit the tool-tissue interaction forces to the operating surgeon. Our approach combines a 3D diffeomorphic deformation mapping with a generative model to precisely label the force level. The main highlights of our approach are that the use of diffeomorphic transformation ensures anatomical structure preservation and the label assignment is based on a parametric form of several mixture elements. We performed experimentations on both ex-vivo and in-vivo datasets and offer careful numerical results evaluating our approach. The results show that our solution has an error measure less than 1mm in all directions and an average labeling error of 2.05%. It can also be applicable to other scenarios that require force feedback such as microsurgery, knot tying or needle-based procedures.","inCitations":["5c8f4ec029c453507e8d81b3d08e9b9adcc8bc9b","eda5a3e8a582a99fe637fd5fd908be89f861aae2"],"pmid":"","title":"Sight to touch: 3D diffeomorphic deformation recovery with mixture components for perceiving forces in robotic-assisted surgery","journalPages":"160-165","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202152","https://upcommons.upc.edu/bitstream/handle/2117/114875/IROS2017fv%5Baviles%5D.pdf?isAllowed=y&sequence=1"],"entities":[],"journalVolume":"","outCitations":["7f663b9069cc07faf854cd20e902cd2a1116c750","d2648ce6386ffddd69c4ad4dd3071b3a21651d26","e06d32cc30bc8983d9c746e6d6b7afb0c9cba9f5","d986718bc16d51cbecab71248b8b95cd7e552e34","069cbdcccb2362787573ed2f8b8e8d8df8f844fe","5c07b87c0707c4395f82dcd02fd6934552728e7b","72bc0602b51114d84edfcda9f41721e26664ee40","177db0937b2592621153184634dd1e606e068972","02ab92692487c254f90cbb30cbd2bf3b179b9ec8","3a3d5fcc4933b65d29b60672681bbbdbab921b17","07381457cde16c1f592b4333baa6416d0b997ecf","7e9988b6f087d3d042b7069e6b8383710db4c2da","07f16e70673881c5bbfc58eb1425d83925ce9cbe","83f4416722f6383a221e4df085f3935471fc1eaf","7309968f873bbf5d14641a1318351577610eddc0","7fd1fbacb6ced72251470cdce51e97aae023c519","f669c126c3e1f4e6968cdcc0d0ee63ee2b24e9d9","2f7541056c3f7ae7cef7c50c0ddb5f868592e16d","13a2b03fcd047707b2fbf4d84a42c6817cd5df9e"],"id":"5d7f840af50d6ff5b2e1c3c27b3f29dbf576b28b","s2Url":"https://semanticscholar.org/paper/5d7f840af50d6ff5b2e1c3c27b3f29dbf576b28b","authors":[{"name":"Angelica I. Avilés","ids":["40108145"]},{"name":"Samar M. Alsaleh","ids":["1775505"]},{"name":"Alicia Casals","ids":["1809424"]}],"doi":"10.1109/IROS.2017.8202152"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545461","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Incremental Structure-from-Motion (SfM) technique is the most prevalent way for image-based reconstruction, but its robustness is highly relying on each camera registration, where a false calibration could make everything following fail. In this paper, we propose a voting-based incremental SfM approach to improve upon the camera registration process. First, the degree of closeness between cameras is used as the vote to determine which cameras are going to register. Then, for each camera, two methods are simultaneously used to estimate the camera pose, and the number of inliers is used as the vote to determine which pose is more accurate. Finally, by estimating the priori global camera rotations from the view-graph, the camera poses that are consistent with the priori camera rotations are considered as getting double votes and preferentially kept. After all these prioritized cameras are calibrated, the other cameras are then incrementally registered. Compared to the state-of-the-art incremental SfM approaches, extensive experiments demonstrate that our system performs similarly or better in terms of reconstruction efficiency, while achieves a better robustness and accuracy. Especially for the ambiguous datasets, our system has a better potential to reconstruct them.11This work was supported in part by the Natural Science Foundation of China (61703397), and in part by the National Key R&D Program of China (2016YFB0502002).","inCitations":[],"pmid":"","title":"Voting-based Incremental Structure-from-Motion","journalPages":"1929-1934","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545461"],"entities":[],"journalVolume":"","outCitations":["06457708d8dbc8da403c659f436e0a1c68262f96","4b0b5e9150a5ad22e8772e8783f52487055d2d0a","81d507f86c1dad38a316a1c6a0965e53f1c1115d","2692039d0fea0b75acf38055568501a81b870ebd","219f49c8c281e72c5c88b8a9c61ccf94a255a6ae","203b08161df4847bad19ad951356dfd4f8958a5b","ad7c163793d4c09f29b621f165868ad912ec2dd6","ce7f6418ac420547446b2c056d22f8d32298587d","24e15827d7595c9b31a9246e3cc598f50c5b5b19","849250eacfc2ab643db83091f4a1d6cb5169f794","98424c1cc26395f380231597244725b16dba57f2","295e9b2ddb3c9c7d029a99d5b877bf3824949f1b","4bb0ea475f7686a56f1f0f34f13143de352188d9","1485b72151d53895a1e251d2e42d84f78c1a5009","af9b5c41a04163db53aa404a733e08e76f876c83","26eba12181f97fccb4232f87b68896dbeb34f877","3f84141c6b08963e32ac4b036bb1f628be93c165","1030d2dbb76128a6fadd425a9bf0a515b2a8364c","d63aeefc6c9b23c27debfb45f1a5dec6a3bc907d","1f48eaf7c34a0faa9f533ed457b5e8e86cf1a15a","0fe41d4253d07c72155ca55c30aabee282f3786b","5515ff72ea239b112e1e387c937371fb461f53e0","92ee1816733c09da195ef7ffe2addaf5a895ff08","bc968c9ab49dc3c5043f23427896fc907761c2b0","1d01cc143c6465b218ec6480fd76f9b1612d9f70","71759b5ec708493103898b3bcf5785d08402eb53","646e3b00e37d024325d0474f792c0d7775926d1e","91d1e372a2114d206f89d5ff2b87c07c3983ed2b","255fc56f362f0f83325236c5fba8f8103ba05f56","73622e2c0ff607c79cd168527d49b50c3a09a233","1c8db24b313a2cadd399362c55fbfeff90b1f844","44b48aaf130632e8774775f3e569b8ff1990b1f8","5e70bb805cb99904504f12b3fa32ddaae066323c","6ed0083ff42ac966a6f37710e0b5555b98fd7565","98c770bad0658e009a88f3fde14ec456eeca9bcd"],"id":"9b4ce5608545a13779ad5424aaea7609710bdab4","s2Url":"https://semanticscholar.org/paper/9b4ce5608545a13779ad5424aaea7609710bdab4","authors":[{"name":"Hainan Cui","ids":["34905465"]},{"name":"Shuhan Shen","ids":["1891336"]},{"name":"Wei Gao","ids":["46207764"]}],"doi":"10.1109/ICPR.2018.8545461"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546164","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Various greedy algorithms have been developed for sparse signal recovery in recent years. However, most of them utilize the $\\ell_{2}$ norm based loss function and sensitive to non-Gaussian noises and outliers. This paper proposes a Cauchy matching pursuit (CauchyMP) algorithm for robust sparse representation and classification. By leveraging a Cauchy estimator based loss function, the proposed approach can robustly learn the sparse representation of noisy data corrupted by various severe noises. As a greedy algorithm, CauchyMP is also computationally efficient. We also develop a CauchyMP based classifier for robust classification with application to face recognition. The experiments on the datasets with gross corruptions demonstrate the efficacy and robustness of CauchyMP for learning robust sparse representation.","inCitations":[],"pmid":"","title":"Cauchy Matching Pursuit for Robust Sparse Representation and Classification","journalPages":"694-698","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546164"],"entities":[],"journalVolume":"","outCitations":["00498a081478924c86597dc5d252562b545c7df7","075bc988728788aa033b04dee1753ded711180ee","c6207645e4b1cc91e8fad537e2c0637dab102f21","39a5f2ab4474b8532aaccb68e3b705e5100a628e","18c84e6b5f1d6da3c670454db3f0fa61266ab1e3","1c16f6e06c7b1ac5934af4a94e28a0bec90f6c1f","1b65af0b2847cf6edb1461eda659f08be27bc76d","2ad0ee93d029e790ebb50574f403a09854b65b7e","9af121fbed84c3484ab86df8f17f1f198ed790a0","2071f3ee9ec4d17250b00626d55e47bf75ae2726","7ffa7a36e5414a0f2b16b1d8f93442ab15e2235d","6dd8d8be00376ac760dc92f9c5f20520872c5355","0b6e98a6a8cf8283fd76fe1100b23f11f4cfa711","0cf75b4b77ae4c8a8cc05c1d556285a40e8eb74c","5facbc5bd594f4faa0524d871d49ba6a6e956e17","a373912847b4d7f5a07c36f9ec963578f3dbab7b","8521d4c6178d1c0238f6d431fed0fbff0c74eaa1","9c71340beae089f45319aef01886fdaa768c082a"],"id":"84ce5efc0d190011cbe3f62fb3532a31a50fe0d7","s2Url":"https://semanticscholar.org/paper/84ce5efc0d190011cbe3f62fb3532a31a50fe0d7","authors":[{"name":"Yulong Wang","ids":["3154834"]},{"name":"Cuiming Zou","ids":["2888882"]},{"name":"Yuan Yan Tang","ids":["1687046"]},{"name":"Luoqing Li","ids":["1685566"]}],"doi":"10.1109/ICPR.2018.8546164"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593831","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a feature encoding method of complex 3D objects for high-level semantic features. Recent approaches to object recognition methods become important for semantic simultaneous localization and mapping (SLAM). However, there is a lack of consideration of the probabilistic observation model for 3D objects, as the shape of a 3D object basically follows a complex probability distribution. Furthermore, since the mobile robot equipped with a range sensor observes only a single view, much information of the object shape is discarded. These limitations are the major obstacles to semantic SLAM and view-independent loop closure using 3D object shapes as features. In order to enable the numerical analysis for the Bayesian inference, we approximate the true observation model of 3D objects to tractable distributions. Since the observation likelihood can be obtained from the generative model, we formulate the true generative model for 3D object with the Bayesian networks. To capture these complex distributions, we apply a variational auto-encoder. To analyze the approximated distributions and encoded features, we perform classification with maximum likelihood estimation and shape retrieval.","inCitations":["86585bd7288f41a28eeda883a35be6442224110a","20c71ee8275969a7df881de69b8d8baf88f1d120"],"pmid":"","title":"A Variational Feature Encoding Method of 3D Object for Probabilistic Semantic SLAM","journalPages":"3605-3612","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1808.10180v1.pdf","http://arxiv.org/abs/1808.10180","https://doi.org/10.1109/IROS.2018.8593831"],"entities":[],"journalVolume":"","outCitations":["0e6cb161465d3305142b440937116894f8a91671","10760d7db3dbb5b114d4468f218c2baf469f8917","0233f4e12db890dac1d06b5593c3ae7205d721a6","8db9926d0a6e4e26e0a5cc8661931ed09e68ba63","9ae252d3b0821303f8d63ba9daf10030c9c97d37","7d39d69b23424446f0400ef603b2e3e22d0309d6","fb4915611ad787c74d44ebb53f886e5802204593","1e87605300245778aa5b899e5c962e1053f7534e","059ff0ee2e0ac95d5cc54fd20ded2d33e5d5dbd0","0f88de2ae3dc2ec1371d1e9f675b9670902b289f","e23222907f95c1fcdc87dc3d3cd93edeaa56fa66","30801beeb4436ce1f15e641b74a3daae836b0a0d","8a937906e2a4235a6509926322f3de88340fd4dc","0348cc294883adf24b48c00f50df9870983317d2","354c60f54b0d7c774ebcedc2098196d6ce2c65ec","97af6ee72b183af777d9146fbfe5eccc57b1cbd4","32e4610bc6aec577f8c660a8c82ac90b23488fbb","5c3e3c9eb353c2662333b2cafbb67bfce4baa371","24091189b5e80d89b9708fab56a3cd7ba7107d12","4065274c7e9878dac7f59767a1a9ba14c7569f50","1c3d7d5a9a9eafa084913b8c83a013c81d50479f","10bf5a7fdf7039dbe12ea293cd50391a5adda08b","ea9d2a2b4ce11aaf85136840c65f3bc9c03ab649","2e4e83ec31b43595ee7160a7bdb5c3a7dc4a1db2","cd9e72031e2823bf622763621f99de87f821cfbd","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","061356704ec86334dbbc073985375fe13cd39088","5a1b5d31905d8cece7b78510f51f3d8bbb063063","3f1c6749edfaf4f89bac38b2d15a0493bd9aa253"],"id":"5f19f81cd6be211eb0cf272e87e632907aa89b57","s2Url":"https://semanticscholar.org/paper/5f19f81cd6be211eb0cf272e87e632907aa89b57","authors":[{"name":"Haoping Yu","ids":["49514567"]},{"name":"B. H. Lee","ids":["49132226"]}],"doi":"10.1109/IROS.2018.8593831"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546057","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Color demosaicking and image denoising each plays an important role in digital cameras. Conventional model-based methods often fail around the areas of strong textures and produce disturbing visual artifacts such as aliasing and zippering. Recently developed deep learning based methods were capable of obtaining images of better qualities though at the price of high computational cost, which make them not suitable for real-time applications. In this paper, we propose a lightweight convolutional neural network for joint demosaicking and denoising (JDD) problem with the following salient features. First, the densely connected network is trained in an end-to-end manner to learn the mapping from the noisy low-resolution space (CFA image) to the clean high-resolution space (color image). Second, the concept of deep residue learning and aggregated residual transformations are extended from image denoising and classification to JDD supporting more efficient training. Third, the design of our end-to-end network architecture is inspired by a rigorous analysis of JDD using sparsity models. Experimental results conducted for both demosaicking-only and JDD tasks have shown that the proposed method performs much better than existing state-of-the-art methods (i.e., higher visual quality, smaller training set and lower computational cost).","inCitations":[],"pmid":"","title":"Lightweight Deep Residue Learning for Joint Color Image Demosaicking and Denoising","journalPages":"127-132","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546057"],"entities":["Algorithmic efficiency","Aliasing","Artificial neural network","Biological Neural Networks","Color image","Computation","Convolutional neural network","Deep learning","Demosaicing","Digital camera","End-to-end principle","Image resolution","Inspiration function","Morphologic artifacts","Network architecture","Noise reduction","Real-time transcription","Small","Sparse matrix","Test set","Texture mapping","Visual artifact","cell transformation"],"journalVolume":"","outCitations":["69e3a905cdcce3b126c645df2deb7838e43b5135","03a5b2aac53443e6078f0f63b35d4f95d6d54c5d","fc684e2e880df46018407f44d45c38d1fee7ec67","7f6b3a361be0d7109b343d00caaceed12e70a773","7ba5d3808e117e7a68dc40331ce1d483ceeedcb2","d003c167e9e3c8ee9f6ba6f96e298a5646e39b41","de8d30f9c59be0c235ae2de7da77993e54f9f91f","471d4b17def36e35719c1ef7a64ee2689aa459d1","0171bdeb1c6e333287be655c667cfba5edb89b76","3ce337feb2816269d49979b0a3c0c8770e67320a","4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5","4f6fd87f9057a2298599a02c1f0e9ccf12ac1025","99dcdeec6351cf1b35eaa6b6f75915f3480d80dd","2c99836b98c8397e26812e3af426a502e0ccb369","867f1a262a704ef4cabe84899310370182dd598f","87c280d0dc204ca5db0d325991a21c211aeec866","e6b8c00b42c95638cac67172660b9b9e2b915914","ba18247cd3ce9f711eecc7296f1c3561dbfb6cc2","272216c1f097706721096669d85b2843c23fa77d","002c43f8eefe17368437431c7c5b48d75b662a0d","2c03df8b48bf3fa39054345bafabfeff15bfd11d","24f184bee3059534af42624c95f86477566e0dff","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","56f52f4564b1d90ee66d894b649d1a6ae8472211","501d99e392783e4acafb220136d32ea68a921282","36fe91183ec9e30925d05c74ce887bf641c010cd"],"id":"ca76a2b3140fe445e533c1d32250960bc7788fb0","s2Url":"https://semanticscholar.org/paper/ca76a2b3140fe445e533c1d32250960bc7788fb0","authors":[{"name":"Tao Huang","ids":["49081715"]},{"name":"F. F. Wu","ids":["46583279"]},{"name":"Weisheng Dong","ids":["2872774"]},{"name":"Guangming Shi","ids":["3878949"]},{"name":"Xin Li","ids":["48568672"]}],"doi":"10.1109/ICPR.2018.8546057"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593514","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Tree species identification using bark images is a challenging problem that could prove useful for many forestry related tasks. However, while the recent progress in deep learning showed impressive results on standard vision problems, a lack of datasets prevented its use on tree bark species classification. In this work, we present, and make publicly available, a novel dataset called BarkNet 1.0 containing more than 23,000 high-resolution bark images from 23 different tree species over a wide range of tree diameters. With it, we demonstrate the feasibility of species recognition through bark images, using deep learning. More specifically, we obtain an accuracy of 93.88% on single crop, and an accuracy of 97.81 % using a majority voting approach on all of the images of a tree. We also empirically demonstrate that, for a fixed number of images, it is better to maximize the number of tree individuals in the training database, thus directing future data collection efforts.","inCitations":[],"pmid":"","title":"Tree Species Identification from Bark Images Using Convolutional Neural Networks","journalPages":"1075-1081","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1803.00949v2.pdf","https://doi.org/10.1109/IROS.2018.8593514","http://www2.ift.ulaval.ca/~pgiguere/papers/tree-species-identification.pdf","http://arxiv.org/abs/1803.00949"],"entities":[],"journalVolume":"","outCitations":["02d171437ec135b14106e4f3fd13d10183940d69","b36a5bb1707bb9c70025294b3a310138aae8327a","3edd3fea5d527c7ff2436ea2d59a25304147ba24","4bf0ca857419da52bc82bcd6197675c42ed7a638","877000fd60a5d2e54088b0a99123f7fa7867d4d9","6f9d40ebe1fdabe3c9de908fadc4e192f89a54db","a1be53dead395b2d83a4009bec76729fce95af83","29693058229d331a489c5ed2ed408ff185336939","e10b86506d54280caff626ea83d6ed904ab71bb5","2a57cf3e15b20708595737a499a5f6fedc7fac94","90c26eca18194e23cfd3c3bbf341b133e4bf5f6b","b16aa75f9c9014df617eb3aec15bc5f3fda84898","59abbc3175e002b3e6cffdeb8ff934467519ad71","4d959ce5c1abfa8fa9ed57a40e78c41fd9ffad04","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","95ea564bd983129ddb5535a6741e72bb1162c779","27ce4b367bf52854efa10adab729b484dcf315aa","7d0518a957fc6dc6fad6a4beef5fd19b5df89420","bb35ef89addbbc28d960bc0cab70d8a29fdf6eee","8333d9d868ba2a28995067cf8c4ce1dc9972e029","21a1075143b65f98e978d5ee92e4fa63ac1b5d27","5a83061de32a3940a96eb3d65f25d005bec8156f","e4c19e4fcd1be1e7003b1c4825502f3292fce514","2c03df8b48bf3fa39054345bafabfeff15bfd11d","59da3fc6f47c19d7950a7f98e7cc750f0bd55861","3ac070e09855a78f0eb276774ee5fde960f64fba","d92c47cc4d84d04e6acf968205f0d3833e84a83c","b48ec679a01efaed2404e884b2b36cf395bde50a","3ba179bceb9692d4d21109d0b87b120195761148"],"id":"e6afe8e54dc97f7773ce919eab2d08d07bb3dc18","s2Url":"https://semanticscholar.org/paper/e6afe8e54dc97f7773ce919eab2d08d07bb3dc18","authors":[{"name":"Mathieu Carpentier","ids":["5143730"]},{"name":"Philippe Giguère","ids":["2310695"]},{"name":"Jonathan Gaudreault","ids":["3357595"]}],"doi":"10.1109/IROS.2018.8593514"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545452","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this work we introduce a cross modal image retrieval system that allows both text and sketch as input modalities for the query. A cross-modal deep network architecture is formulated to jointly model the sketch and text input modalities as well as the the image output modality, learning a common embedding between text and images and between sketches and images. In addition, an attention model is used to selectively focus the attention on the different objects of the image, allowing for retrieval with multiple objects in the query. Experiments show that the proposed method performs the best in both single and multiple object image retrieval in standard datasets.","inCitations":["ddcb77d09e4e9e2a948f9ffe7eaa5554dceb8ce3"],"pmid":"","title":"Learning Cross-Modal Deep Embeddings for Multi-Object Image Retrieval using Text and Sketch","journalPages":"916-921","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1804.10819v1.pdf","http://arxiv.org/abs/1804.10819","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545452"],"entities":["Artificial neural network","Experiment","Image retrieval","Long short-term memory","Modal logic","Modality (human\u2013computer interaction)","Network architecture","Network model","Sketch","Test set","Text-based (computing)","Titan Rain"],"journalVolume":"","outCitations":["b69be1416f374815ff1d6573f976d996aaf125bc","e40f36cea9a7620165543d5c77a3b898dbd83b7f","0a10d64beb0931efdc24a28edaa91d539194b2e2","f4268671d089b90cb52fe10bc77499cceb66d00e","8a336adeb5115f2e70d65dfd9e7feed6e894fb3a","108961c7366e36825ffed94ac9eab603e05b6bc6","0e0900b88c33b671be5dd2ded9885b6526d6b429","1c72e6edd500987f8898d0b76df8b1d1570ebb87","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","6721e26e65c6b72dafae74fd1e7a6f2e6023a312","c3c878c0657bb15e41c1b3da58d97c6e19dfe798","0674c1e2fd78925a1baa6a28216ee05ed7b48ba0","e8db8b3ae77c09e0b882560b06fbfb4b4690792e","146f6f6ed688c905fb6e346ad02332efd5464616","1b89cc4a54c375b7f57b90641c2b585894a29633","071b16f25117fb6133480c6259227d54fc2a5ea0","272216c1f097706721096669d85b2843c23fa77d","d96585b5d9eb2bf503d0c34c3a3decd287228ca7","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","2d309e539c69a93148dea14b6be1ed2083decd51","29aa3dc15450e6eb46c34f30f0e224e5ea16615e","318f477dde073eb5361ddf3f275ec3a7721b1690","11da2d589485685f792a8ac79d4c2e589e5f77bd","4991785cb0e6ee3d0b7823b59e144fb80ca3a83e","4a31ca27b987606ae353b300488068b5240633ee","582c87ef9e98c24694c83eb03853eb96a4d84809","4aa4069693bee00d1b0759ca3df35e59284e9845","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","d6837519e7f028e83140ac2585fe84ebe4aca18a","549d55a06c5402696e063ce36b411f341a64f8a9","cb3e81b912f24e66d91509e8ab41d09b522a397a","38b3cae6ba1b98d6bc6f88d903916dac888cb951","061356704ec86334dbbc073985375fe13cd39088"],"id":"e9ac7adc8db3030707102efa07c06bd38aea233e","s2Url":"https://semanticscholar.org/paper/e9ac7adc8db3030707102efa07c06bd38aea233e","authors":[{"name":"Sounak Dey","ids":["2166891"]},{"name":"Anjan Dutta","ids":["39083167"]},{"name":"Suman K. Ghosh","ids":["39937691"]},{"name":"Ernest Valveny","ids":["2864362"]},{"name":"Josep Lladós","ids":["1712904"]},{"name":"Umapada Pal","ids":["1740569"]}],"doi":"10.1109/ICPR.2018.8545452"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593627","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Fusing visual information with inertial measurements for state estimation has aroused major interests in recent years. However, combining a robust estimation with computational efficiency remains challenging, specifically for low-cost aerial vehicles in which the quality of the sensors and the processor power are constrained by size, weight and cost. In this paper, we present an innovative filter for stereo visual inertial odometry building on: i) the recently introduced stereo multistate constraint Kalman filter; ii) the invariant filtering theory; and iii) the unscented Kalman filter (UKF) on Lie groups. Our solution combines accuracy, robustness and versatility of the UKF. We then compare our approach to state-of-art solutions in terms of accuracy, robustness and computational complexity on the EuRoC dataset and a challenging MAV outdoor dataset.","inCitations":["02c777b3683abcea10f6cd4e7aba1501bfe23f9b"],"pmid":"","title":"Unscented Kalman Filter on Lie Groups for Visual Inertial Odometry","journalPages":"649-655","s2PdfUrl":"","pdfUrls":["https://hal.archives-ouvertes.fr/hal-01735542v1/document","https://hal.archives-ouvertes.fr/hal-01735542/document","https://doi.org/10.1109/IROS.2018.8593627"],"entities":[],"journalVolume":"","outCitations":["c3cd8e3a42f0907e9d2eeb1fca2af7fc8fe66d8f","46034781da5be336f426e2ad16af224776483ae9","02a78a9fd1a577fc471c82b15f7730673578c94e","b4467251482485f8ec0341bb16046f933f07d06b","00e05b249c40ce6f3bdee7832956cba7780cfd39","a6374ee834201b2c1ac3cf8c0097f503def431df","215fe6ebcf44a364edca5e9a19ff79d3360d0e86","94be7be782eeb3361661f50c863dfd608c80124d","56a4a8a7937f0dd2d38e4d2903f0baf4fb189102","84c6c752e9295a39544ab2ecb2d360ae820e9ab1","6d6a68095c9769553831ce2f670481e256688013","281c6a02097e94c88d67078b2c9a4a6da29e1872","8f8cb206adae769d694dde0b4772cbc2dc9d3067","18c7a8ad9e69299a8928e9696d50316a2133d83a","86854374c13516a8ad0dc28ffd9cd4be2bca9bfc","6f7892e29b76ba107ca3e6a923ce56dd3a7cd580","2437aa608757108343241b0bcdfb71d44f8bee57","9da965529ee3da77178ed99cf13d97be0fa85f6e","bd221902f8fc5928d3d612f66351430ca4ba373e","72f91795be752849d86300ae1d7a89dec24afb8b","0be0c13803cd08e81b7adaada537e91222eb1491","4e26e488c02b3647e0f1566760555ebe5d002558","7200dce06a410959760aca287c53557830f43fcf","2e7975fb0351638bd2646a217e5885ae56ca5cff","b9059384edc6b80f5155288941ff6f5528771d13"],"id":"addf69b052c501ddcdac4c5d7d7dd8be02dcd052","s2Url":"https://semanticscholar.org/paper/addf69b052c501ddcdac4c5d7d7dd8be02dcd052","authors":[{"name":"Martin Brossard","ids":["40584142"]},{"name":"Silvère Bonnabel","ids":["40158525"]},{"name":"Axel Barrau","ids":["25602662"]}],"doi":"10.1109/IROS.2018.8593627"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545251","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"This work targets Dance Style Recognition in videos as an application of Human Action Recognition. We propose a novel Spatio-Temporal Laban Feature descriptor (STLF) for dance style recognition based on Laban theory. Laban Movement Analysis has become increasingly popular as a language to describe, index and record human motion. We only exploit motion features and body-pose information without encoding the appearance. The model is tested on some action recognition benchmarks and ICD, a challenging dataset of YouTube dance videos. Unlike other works, where Laban based features have been used in constrained environments, with static camera, sensors and no background noise, we employ STLF on videos in unconstrained and natural settings. It is robust to camera jitter, zoom variations and other acquisition conditions and is computationally cheap. It performs comparable or better than the state-of-the-art.","inCitations":[],"pmid":"","title":"Spatio-Temporal Laban Features for Dance Style Recognition","journalPages":"2911-2916","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545251"],"entities":["Epilepsy, Temporal Lobe","Human\u2013computer interaction","Kinesiology","Silo (dataset)","sensor (device)","videocassette"],"journalVolume":"","outCitations":["070874b011f8eb2b18c8aa521ad0a7a932b4d9ad","f2164be321986c4552e2d8a04d3af2931c859f85","2dc9b005e936c9c303386caacc8d41cabdb1a0a1","c9d01b85da1ff5545328c714d463e9d1f48c86b8","9bf6416b1c1b8f83ee3a9495f5102889e34c42ce","2d83ba2d43306e3c0587ef16f327d59bf4888dc3","b5f2846a506fc417e7da43f6a7679146d99c5e96","794f2471b7350a9cff39ae58408296116a554d83","7a9459345d95680872ad95ef75ff82f9fc7c2b41","4f490cdd43843b3acafdc2490d5f70cfacb9c726","d95d696c5ceeb42fcee858c44845eb3181c497de","7293a0ad28186cff51ed7abd0209abed3d6d2613","21bc1abbad482bab92576c7e74d2036523284a32","39185b82ac410028358b87fad5df776232b7a3b0","99285d5dbb46e71034bc426870ab186f2e330c95","25f5df29342a04936ba0d308b4d1b8245a7e8f5c","b480f6a3750b4cebaf1db205692c8321d45926a2","a8e8f3c8d4418c8d62e306538c9c1292635e9d27","fd9e63c29aa41c0b20bb9588bac1bd32e6eb77f9","44f23600671473c3ddb65a308ca97657bc92e527","83f2974f5b990095f66ab26c7abd8cdf620a38d4","36c473fc0bf3cee5fdd49a13cf122de8be736977","aac934f2eed758d4a27562dae4e9c5415ff4cdb7","c0d5c3aab87d6e8dd3241db1d931470c15b9e39d","58ea2fa0580b2117618be6e1cc9658a5c9531dba","48186494fc7c0cc664edec16ce582b3fcb5249c0","02c75d3a7ab1fcf7276c9bfe24a8ac45e12e3379","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc"],"id":"e65de721e68f498b6e1c63373c6df60a12533cc0","s2Url":"https://semanticscholar.org/paper/e65de721e68f498b6e1c63373c6df60a12533cc0","authors":[{"name":"Swati Dewan","ids":["51936306"]},{"name":"Shubham Agarwal","ids":["2982258"]},{"name":"Navjyoti Singh","ids":["2204983"]}],"doi":"10.1109/ICPR.2018.8545251"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546058","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Cascade shape regression (CSR) methods predict facial landmarks by iteratively updating an initial shape and are state-of-the-art. The initial shape always limits the result and causes local optimum, which is usually obtained from the average face or by randomly picking a face from the training set. In this paper, we propose a CNN-based initial method for CSR. Convolution neural network provides a highly robust initial shape estimation, while the following CSR algorithm fine-tunes the initialization rapidly to achieve higher accuracy. Furthermore, CNN-based initial approach is proposed to get 68-point initial shape, which is calculated from convolutional network 5-point result by the radial basis function interpolation with thin-plate splines (RBF-TPS). Extensive experiments demonstrate that CSR methods are sensitive to the initialization and proposed approach gets favorable results compared to state-of-the-art algorithms and achieves real-time performance.","inCitations":[],"pmid":"","title":"A fast Cascade Shape Regression Method based on CNN-based Initialization","journalPages":"3037-3042","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546058"],"entities":[],"journalVolume":"","outCitations":["5b0bf1063b694e4b1575bb428edb4f3451d9bf04","57ebeff9273dea933e2a75c306849baf43081a8c","e5533c70706109ee8d0b2a4360fbe73fd3b0f35d","8d4f12ed7b5a0eb3aa55c10154d9f1197a0d84f3","2ea6a93199c9227fa0c1c7de13725f918c9be3a4","2a84f7934365f05b6707ea0ac225210f78e547af","8a3c5507237957d013a0fe0f082cab7f757af6ee","0a6d344112b5af7d1abbd712f83c0d70105211d0","4836b084a583d2e794eb6a94982ea30d7990f663","023be757b1769ecb0db810c95c010310d7daf00b","62e913431bcef5983955e9ca160b91bb19d9de42","f57a21cf4addbc029e002929ddf920c9938a4ec3","2fda461869f84a9298a0e93ef280f79b9fb76f94","079115cfe27c6858ffe0113aaa30d860c34c2974","2a4153655ad1169d482e22c468d67f3bc2c49f12","2724ba85ec4a66de18da33925e537f3902f21249","1922ad4978ab92ce0d23acc4c7441a8812f157e5","86c053c162c08bc3fe093cc10398b9e64367a100","272216c1f097706721096669d85b2843c23fa77d","084bd02d171e36458f108f07265386f22b34a1ae","b62628ac06bbac998a3ab825324a41a11bc3a988","8bf83ea8014745fd16436ff2cb860037b8f67fa0","1824b1ccace464ba275ccc86619feaa89018c0ad","0ea96fe9ce2912e96a1d431962b3796576f9fad7","88a323dc70de8788a9cdd415f7c65fc3d3a8c7cb","cbe021d840f9fc1cb191cba79d3f7e3bbcda78d3","044d9a8c61383312cdafbcc44b9d00d650b21c70","95f12d27c3b4914e0668a268360948bce92f7db3","22e2066acfb795ac4db3f97d2ac176d6ca41836c","03f98c175b4230960ac347b1100fbfc10c100d0c","140438a77a771a8fb656b39a78ff488066eb6b50","0e986f51fe45b00633de9fd0c94d082d2be51406"],"id":"3333f2f667c4fa7310a7afbe9bd012bc818e5660","s2Url":"https://semanticscholar.org/paper/3333f2f667c4fa7310a7afbe9bd012bc818e5660","authors":[{"name":"Pengcheng Gao","ids":["2679752"]},{"name":"Jian Xue","ids":["48181305"]},{"name":"Ke Lu","ids":["1875882"]},{"name":"Yanfu Yan","ids":["29883307"]}],"doi":"10.1109/ICPR.2018.8546058"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545109","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Network embedding is to learn effective low-dimensional vector representations for nodes in a network and has attracted considerable attention in recent years. To date, existing methods mainly focus on network structure information and cannot leverage abundant label information, which is potentially valuable in learning better vector representations. Due to the noise and incompleteness of label information, it is intractable to integrate label information into the vector representations in a partially labeled network. To address this issue, we investigate the effects of label information based on the label homophily. Briefly, label homophily can not only drive nodes sharing similar labels to be connected to each other, but also produce a division of a network into densely-connected, homogeneous parts that are weakly connected to each other. Furthermore, we propose a novel Label Homophily Oriented Network Embedding (LHONE) model to make the best of label homophily by converting a partially labeled network to two bipartite networks, and learning vector representations combined with a Gaussian mixture model (GMM). Extensive experiments on two real-world networks demonstrate the effectiveness of LHONE compared to state-of-the-art network embedding approaches.","inCitations":[],"pmid":"","title":"LHONE: Label Homophily Oriented Network Embedding","journalPages":"665-670","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545109"],"entities":[],"journalVolume":"","outCitations":["0e7c75fc648ab84a15698921c241795d9ab55f1f","20bb300eb3400f1af766110ff51feada78170674","a0775fab3b20f2dc0ac2f2626552d046b6afc225","0a10d64beb0931efdc24a28edaa91d539194b2e2","1c059493904b2244d2280b8b4c0c7d3ca115be73","18ca2837d280a6b2250024b6b0e59345601064a7","d3e0d596efd9d19b93d357565a68dfa925dce2bb","41285e207f2bfb861d3aee7c1557a9b8525745af","6ef7a980c4dd8c718c0606487adf538a1a40e2fa","44044556dae0e21cab058c18f704b15d33bd17c5","1562d6e590aac6e754ce750190ef17060c4a1e93","14de052485d4245d5e240d39c62ae0e014ec9e09","1eb6538c7fa6a41923869924a55c95036f06c9a0","c0af91371f426ff92117d2ccdadb2032bec23d2c","0834e74304b547c9354b6d7da6fa78ef47a48fa8","41a08affbb5c847d6f2a82d5ae4be9ed5df52353","aad9c378dc1485957ae8d661596462b2ec16ab24","1e58e93d6f69c6fbfdb5213921f841c68e427a63","8ba7631515d5e7e0c451af1c4772507f41540a5e","0f16f6f478b5c788dce466eb50e36c612273c36e","86fdc3ebc3dd2179a191be275830f09aac510823","d0b7c8828f0fca4dd901674e8fb5bd464a187664","c8cee328b1774c2d38bea10f9fe9d081d8074307","1a67622ca58aa851afe36ad6c6e78f9fb9d691d2","26e8f3cc46b710a013358bf0289df4de033446ba"],"id":"a5be05994c43dcbf5d9837ddbebe02c01b187265","s2Url":"https://semanticscholar.org/paper/a5be05994c43dcbf5d9837ddbebe02c01b187265","authors":[{"name":"Le Zhang","ids":["47058950"]},{"name":"Xiang Li","ids":["1737850"]},{"name":"Ji Xiang","ids":["49236124"]},{"name":"Ying Qi","ids":["50065031"]}],"doi":"10.1109/ICPR.2018.8545109"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594295","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The usage of articulated tools for autonomous robots is still a challenging task. One of the difficulties is to automatically estimate the tool's kinematics model. This model cannot be obtained from a single passive observation, because some information, such as a rotation axis (hinge), can only be detected when the tool is being used. Inspired by a baby using its hands while playing with an articulated toy, we employ a dual arm robotic setup and propose an interactive manipulation strategy based on visual-tactile servoing to estimate the tool's kinematics model. In our proposed method, one hand is holding the tool's handle stably, and the other arm equipped with tactile finger flips the movable part of the articulated tool. An innovative visuo-tactile servoing controller is introduced to implement the flipping task by integrating the vision and tactile feedback in a compact control loop. In order to deal with the temporary invisibility of the movable part in camera, a data fusion method which integrates the visual measurement of the movable part and the fingertip's motion trajectory is used to optimally estimate the orientation of the tool's movable part. The important tool's kinematic parameters are estimated by geometric calculations while the movable part is flipped by the finger. We evaluate our method by flipping a pivoting cleaning head (flap) of a wiper and estimating the wiper's kinematic parameters. We demonstrate that the flap of the wiper is flipped robustly, even the flap is shortly invisible. The orientation of the flap is tracked well compared to the ground truth data. The kinematic parameters of the wiper are estimated correctly.","inCitations":[],"pmid":"","title":"Estimating an Articulated Tool's Kinematics via Visuo-Tactile Based Robotic Interactive Manipulation","journalPages":"6938-6944","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594295","https://pub.uni-bielefeld.de/download/2930580/2930581/root.pdf"],"entities":[],"journalVolume":"","outCitations":["bece353d72890f7e049c719ebe0cc82a2875379a","d0a23b70897cfea52154cd1b57c4db940dc9f1a8","24bcb200832359f133508fb693fc449fdaa80f60","a0037dbbec287151c4dbf23175c7e3e98c80b1c5","474b5dd4c0a92939a3dede3cb989bed930971b3d","35c7161a17c3ea82ebfdfaa6cfeded55f67bc176","7e2b68c06e4c5c0b469dc7ace12b8bec34831dde","4ad1434904092da2b008e1a8cc4e3325843fbd29","fe35da336864ed6b526e5ad4da39dfaab71c81ed","1580511a1ed28a1ef5b076633e97edf2d4527505","80e1c3faa26ec18f6a276c22b31a1563f0e432a0","42bd0d82204d66c1421844e87f5924cc433cae49","4d90834c939762f5f0c4bff8e76e95d7efe714a2","1b6dab8814201312dc3442782b95ab8dadebb7e3","fa635c9140799c5026ccfd444275d4ec29dfac8e","0458cec30079a53a2b7726a14f5dd826b9b39bfd","1ac57ce991d834c37d8ed7bba3163f1fc59f4916","4faecd562031d977eeef07637bd1d64a8258fe75","40e6e4a2091d44d6922a68f84eb3c7ae226a967d","d924a13592d56d6be99935c36cdbc00c8950bbb1","a069a67eceed6903c538eeecae48df21877f0d06","63fb17d46b0098767eec804c9d3260e42fe1b2d3","11be09f50d7b9f0140446270bd7e77dfb1c289a1","3af93fb2571816ff84fc7ab5855f7a5ad09292d4","8fa9c9568d8de9cd3536d6f99d99fe957d45e0a1","7b12132322ffc03e28a4b9d17cec25e0ec495c0e","2b590d49db6fd26f3371cfb21b3bcce8d3c50446"],"id":"5ef0713dec42a673809c053b680d7f2028696ca1","s2Url":"https://semanticscholar.org/paper/5ef0713dec42a673809c053b680d7f2028696ca1","authors":[{"name":"Qiang Li","ids":["40422451"]},{"name":"André Ückermann","ids":["20789548"]},{"name":"Robert Haschke","ids":["2112168"]},{"name":"Helge Ritter","ids":["30258243"]}],"doi":"10.1109/IROS.2018.8594295"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206128","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This paper presents a robot-assisted system to obtain elastic information of a moving tissue using a 2-D ultrasound probe actuated by a 6 degrees of freedom robotic arm. The proposed method combines ultrasound image-based visual servoing, force control and non-rigid motion estimation. We present how the motion estimation, while the force control and visual sevoing are enabled, is useful to compute the strain map of the tissue. Ex-vivo experiments performed on a moving abdominal phantom demonstrate the efficiency and robustness of this methodology to compute the strain information of a tissue in motion.","inCitations":["d56e903face1b31dd029ea36fae843b6ed0faac5","5aa11206b05d1e2214ec1f5e49a003880c25b6d9"],"pmid":"","title":"Strain estimation of moving tissue based on automatic motion compensation by ultrasound visual servoing","journalPages":"2941-2946","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206128"],"entities":["Elastography","Experiment","Motion compensation","Motion estimation","Phantom reference","Region of interest","Robot","Robotic arm","Video-in video-out","Visual servoing"],"journalVolume":"","outCitations":["fd5aa4f476054509e47c26c369fee2bf17928eea","2bfc95f5c919c3f634f44724280e9a035d309cfd","c856d200dad6a9ed4f41382808d0e0d401526e10","22a7fdd74a7a7dc49ba7635dc49769954613661d","c3b0fb5e8b0d9201379c15509047758f3b5cf99d","60273e9c368c3f3708c66ffc1f9ddf2287338b69","00153233c50507c0c06ac44e21f7214feda6747a","e3428b87c51959b57bb5a5d966c35c27bb5ec103","aac5c92faafeb0cf70a7003c4b86221df17d4e04","3187698505336f77e68964280f910406e777fce0","a7800696aa157b6bcdc0af2e14e675cc27fbb4e8","1be57c8120efc81396680c6eb1d397955ad49173","792b6139b559828746585478e51151a32b273f30","4cecafd044b692036b881b357f5436ae2c702c7e","f9dc81c1a4394814a6789100902139b7e4f27361","45a3e0e9359142025698090919538bd7e7d4228d","01fe06ed247b7afe9d3edba918733fa41e5186ad"],"id":"4daeb4ebb5dc27e04b1cce4f1e822a0d16e6fccf","s2Url":"https://semanticscholar.org/paper/4daeb4ebb5dc27e04b1cce4f1e822a0d16e6fccf","authors":[{"name":"Pedro A. Patlan-Rosales","ids":["8154281"]},{"name":"Alexandre Krupa","ids":["1730910"]}],"doi":"10.1109/IROS.2017.8206128"}
{"doiUrl":"","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":[],"year":2018,"text":"Many of the solutions proposed for the object manipulation problem are based on the knowledge of the object features. The approach proposed in this paper intends to provide a simple geometrical approach to securely manipulate an unknown object based only on tactile and kinematic information. The tactile and kinematic data obtained during the manipulation is used to recognize the object shape (at least the local object curvature), allowing to improve the grasping forces when this information is added to the manipulation strategy. The approach has been fully implemented and tested using the Schunk Dexterous Hand (SDH2). Experimental results are shown to illustrate the efficiency of the approach.","inCitations":[],"pmid":"","title":"Improving Grasping Forces During the Manipulation of Unknown Objects","journalPages":"3490-3495","s2PdfUrl":"","pdfUrls":[],"entities":[],"journalVolume":"","outCitations":[],"id":"cc857f811558c87aa00737d7c62139bcd2975975","s2Url":"https://semanticscholar.org/paper/cc857f811558c87aa00737d7c62139bcd2975975","authors":[{"name":"Andr&#x00E9;s Monta&#x00F1;o","ids":[]},{"name":"Ra&#x00FA;l Su&#x00E1;rez","ids":[]}],"doi":""}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206215","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"This work formulates the multi-vehicle lane change motion planning task as a centralized optimal control problem, which is beneficial in being generic and complete. However, a direct solution to this optimal control problem is numerically intractable due to the dimensionality of the collision-avoidance constraints and nonlinearity of the vehicle kinematics. A progressively constrained dynamic optimization (PCDO) method is proposed to facilitate the numerical solving process of this complicated problem. PCDO guarantees to efficiently obtain an optimum to the original optimal control problem via solving a sequence of simplified problems which gradually judge and reserve only the active collision-avoidance constraints. A first-regularization-then-action strategy, together with the look-up table technique, is developed for online solutions. At the regularization stage, the vehicles form a standard formation by linear acceleration/deceleration only. At the action stage, the vehicles execute lane change motions computed offline and recorded in the look-up table. This makes online motion planning feasible because 1) the computational complexity at the regularization stage scales linearly rather than exponentially with the vehicle number; and 2) online computation at the action stage is fully avoided through data extraction from the look-up table.","inCitations":["108e80761b59259c676dded6a2f42d95c9d0fe05","966d26108ab2968ae9909d90d4b064b30c19758b"],"pmid":"","title":"Optimal control-based online motion planning for cooperative lane changes of connected and automated vehicles","journalPages":"3689-3694","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206215"],"entities":["Automation","Autonomous robot","Biomimetics","Centralized computing","Computation","Computational complexity theory","Dynamic programming","Knowledge-based systems","Line search","Lookup table","Mathematical optimization","Motion planning","Nonlinear programming","Nonlinear system","Numerical analysis","Online and offline","Optimal control","Robotics","Search algorithm","Trajectory optimization"],"journalVolume":"","outCitations":["e449b7140ca6564fc414903b715023030e2c20c2","d0208686dd42a786da3831f315ef28d96671b543","7d681e9e1cdd58f9c2cfe77aaa8cfd0a75790100","ad8a2780a7678532ef7a713bdf312536d0e8c85f","70bb8fe178f24bb14131205804bee53e01d8cbf0","c79d4a4b38d99004609178d24ebae208c3c05842","578e19860f7009744ac19d333d864a024a86e1d2","6738cf1109150c8d8da74471e11d2221ab2178a3","1a19ac73375d989ee12b1b0a6829820d3c18ac68","6d06ae8dcd80d02591ca340b26ae5ac9cf1fd874","1e062d01104606d9fef2c20f035302afefc2cc38","0a316502eeca2516fb1ac4b0f5c87ffcfe134db1","a94eebcbd03279d1a6bad3cdca7050bfe761049c","3375f5e091a5084a72eaca9cc62ce27ca18896bd","0ed8ab8a9ac23d6bac59c983b0028ddd1b0e3229"],"id":"df6f657fb409f714cd9218d2adb21248c8c2c914","s2Url":"https://semanticscholar.org/paper/df6f657fb409f714cd9218d2adb21248c8c2c914","authors":[{"name":"Bai Li","ids":["1710717"]},{"name":"Youmin Zhang","ids":["1741984"]},{"name":"Yuming Ge","ids":["1761630"]},{"name":"Zhijiang Shao","ids":["2622980"]},{"name":"Pu Li","ids":["48982329"]}],"doi":"10.1109/IROS.2017.8206215"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594298","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Due to the widespread use of industrial robots in market, its application has extended to welding, painting, and freight handling. And tool coordinate calibration is regularly modified after tool replacement due to collision accident or routine maintenance. After tool replacement, operators often rebuild tool coordinates. This is the traditional mode of operation in the current industrial practices. However, smart factory will make artificial intelligence method replace manual method. This paper presents a system independent method for automatic calibration of the tool coordinate system which is faster, simpler, cheaper and more effective than the manual method. The proposed method required images to be captured using two \u201ceye to hand\u201d cameras and one \u201ceye in hand\u201d camera. Tool position data is then acquired through CamShift and MeanShift algorithm for image trajectory tracking along with coordinate system conversion, several methods like PCA, LDA can deal with the vision data. Optimal Deep Neural Network (DNN) method error compensation of a robot allows the tool to automatically run with the calibration system functions. We have developed a 6 degrees of freedom(DoF) industrial robot for this experiment. Nine different kinds of DNN models are built and finally with suitable tool coordinate error compensation for the current robot, tool calibration can be achieved adaptively and efficiently.","inCitations":[],"pmid":"","title":"Automated Tool Coordinate Calibration System of an Industrial Robot","journalPages":"5592-5597","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594298"],"entities":[],"journalVolume":"","outCitations":[],"id":"e8987b6abe78d46506ac391030111854f55c98da","s2Url":"https://semanticscholar.org/paper/e8987b6abe78d46506ac391030111854f55c98da","authors":[{"name":"Ren C. Luo","ids":[]},{"name":"Hao Wang","ids":[]}],"doi":"10.1109/IROS.2018.8594298"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.75","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Facial aging is a complicated process which usually affects the facial appearance (e.g., wrinkles). Variations of facial appearance pose a big challenge to the automatic face recognition problem. How to eliminate the influence of aging factors to the verification performance is a very challenging problem. Multi-task learning has provided a principled framework for jointly learning multiple related tasks to improve generalization performance. In this paper, we leverage this powerful technique to improve the task of cross-age face verification. We present an end-to-end learning framework for cross-age face verification by designing a multi-task deep neural network architecture that exploits the intrinsic low-dimensional representation shared between the tasks of face verification and age estimation. We show that the algorithm effectively balances feature sharing and feature exclusion between the two given tasks. We evaluate the proposed framework on two standard benchmarks. Experimental results demonstrate that our algorithm has significant improvement over the state-of-theart (2.2% EER on MORPH and 7.8% EER on FG-NET, by more than 50.0% and 59.70% performance gain respectively).","inCitations":["cd6aaa37fffd0b5c2320f386be322b8adaa1cc68","0249c0824a5c9e0db85ed5b5d0a056ae606c6e39","997c7ebf467c579b55859315c5a7f15c1df43432","a94b832facb57ea37b18927b13d2dd4c5fa3a9ea"],"pmid":"","title":"Unleash the Black Magic in Age: A Multi-Task Deep Neural Network Approach for Cross-Age Face Verification","journalPages":"596-603","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.75"],"entities":["Algorithm","Artificial neural network","Benchmark (computing)","Computer multitasking","Deep learning","End-to-end principle","Enhanced entity\u2013relationship model","Facial electromyography","Facial recognition system","Job control (Unix)","Loss function","Multi-task learning","Network architecture"],"journalVolume":"","outCitations":["3edb0fa2d6b0f1984e8e2c523c558cb026b2a983","96c7dde2cb7c49de99083b1933669f09a784991d","289919021222236377fee58a39276bd261c81b0f","2cbb4a2f8fd2ddac86f8804fd7ffacd830a66b58","14ce7635ff18318e7094417d0f92acbec6669f1c","695df73f2f4bca1e406622e8734e720332e4013b","019e471667c72b5b3728b4a9ba9fe301a7426fb2","722fcc35def20cfcca3ada76c8dd7a585d6de386","8a3c5507237957d013a0fe0f082cab7f757af6ee","67db50715cf08e06d6fe7588c205c85d5656714f","7220090ec024ed61d2d8cfc022a5bc670b8e27ac","604a281100784b4d5bc1a6db993d423abc5dc8f0","997c7ebf467c579b55859315c5a7f15c1df43432","15fbb5fc3bdd692a6b2dd737cce7f39f7c89a25c","2b0b2a174adaf6e33bab87ac3d5fa31903760fb8","6bfb0f8dd1a2c0b44347f09006dc991b8a08559c","89002a64e96a82486220b1d5c3f060654b24ef2a","0794ca8c60425dfdbdbc5d793e6fa3f01321bc35","b47a3c909ee9b099854619054fd00e200b944aa9","05aba481e8a221df5d8775a3bb749001e7f2525e","6d5e12ee5d75d5f8c04a196dd94173f96dc8603f","20aa8348cf4847b9f72fe8ddbca8a2594ea23856","4b519e2e88ccd45718b0fc65bfd82ebe103902f7","1d3dd9aba79a53390317ec1e0b7cd742cba43132","c44c84540db1c38ace232ef34b03bda1c81ba039","1c93b48abdd3ef1021599095a1a5ab5e0e020dd5","c5b393da3c3f7f1b3d4f4ca4d34c78eb475046ba","9e28243f047cc9f62a946bf87abedb65b0da0f0a","162ea969d1929ed180cc6de9f0bf116993ff6e06","13e415ed39f406f1a7c687cb55b6129b1c40ddd5","1c5a5d58a92c161e9ba27e2dfe490e7caaee1ff5","92e464a5a67582d5209fa75e3b29de05d82c7c86","177bc509dd0c7b8d388bb47403f28d6228c14b5c","ab29f97e7180b284be7003fbc6725566ea0cd459","7a134dd9c8b1ff7c3cf607aab744e6fdac532b9b","217a21d60bb777d15cd9328970cab563d70b5d23","0f112e49240f67a2bd5aaf46f74a924129f03912","3342018e8defb402896d2133cda0417e49f1e9aa","2c3f31d058f268afbc29940a94207d8800d02ba9","853bd61bc48a431b9b1c7cab10c603830c488e39","1d696a1beb42515ab16f3a9f6f72584a41492a03","af12a79892bd030c19dfea392f7a7ccb0e7ebb72","0c741fa0966ba3ee4fc326e919bf2f9456d0cd74","cdfe859fbe15e40847399dbad0f0c5a258ab7854","1f41a96589c5b5cee4a55fc7c2ce33e1854b09d6","00791f2d67f9a6fdb77b669450fa9f25c275fd56","7755ded70d23839e9f9eb348605c0a4dc336e95d","19d583bf8c5533d1261ccdc068fdc3ef53b9ffb9","f96bda0053733a8c7c1333227a6df23e55457c33","6ab33fa51467595f18a7a22f1d356323876f8262","27a3728e586e333a768f91eb56ded3bbf34fbe3d","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","9055b155cbabdce3b98e16e5ac9c0edf00f9552f","061356704ec86334dbbc073985375fe13cd39088"],"id":"97f3d35d3567cd3d973c4c435cdd6832461b7c3c","s2Url":"https://semanticscholar.org/paper/97f3d35d3567cd3d973c4c435cdd6832461b7c3c","authors":[{"name":"Xiaolong Wang","ids":["1709719"]},{"name":"Yin Zhou","ids":["3030962"]},{"name":"Deguang Kong","ids":["37583154"]},{"name":"Jon Currey","ids":["2858407"]},{"name":"Dawei Li","ids":["49620929"]},{"name":"Jiayu Zhou","ids":["5426025"]}],"doi":"10.1109/FG.2017.75"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545883","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Pattern Recognition techniques have been successfully exploited for the biomedical analysis of NMR spectra. In this context, it is crucial to derive a suitable representation for the data: among others, a successful line of research exploits the Bag of Words representation (called here \u201cBag of Peaks\u201d). However, despite its success, the Bag of Peaks paradigm has not been fully explored: for example, appropriate probabilistic models (such as topic models) can further distill the information contained in the Bag of Words, allowing for more interpretable and accurate solutions for the task-at-hand. This paper is aimed at filling this gap, by investigating the usefulness of topic models in the analysis of NMR spectra. In particular, we first introduce an unsupervised approach, based on topic models, that performs soft biclustering of NMR spectra-this kind of unsupervised analysis being new in the NMR literature. Second, we show that descriptors extracted from topic models can be successfully employed for classification of NMR samples: compared to the original Bag of Words, we prove that our descriptors provide higher accuracies. Finally, we perform an empirical evaluation involving a complex dataset of spectra derived from fruits, and two datasets of medical NMR spectra: our analysis confirms the suitability of such models in the NMR spectra analysis.","inCitations":[],"pmid":"","title":"Mining NMR Spectroscopy Using Topic Models","journalPages":"3784-3789","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545883"],"entities":[],"journalVolume":"","outCitations":["df01d2dede1a243b9b0eb26c27246bc13705d930","b0633a27494850559eb7fd98369be9662c34c8a7","dc218acc4ca46b4ce2a9f813b40a0ca91c6788ab","38c2c7b9063f21369f7a6a13e1359f101cc13ae6","03b4a3dbbc13f60fe331820cf068a55e486ae71c","dc8f89865ad9c9b6e643abc296ec5000ccdb16ee","7db9db128faf3fc7dcff02f00e62dfcb2109919e","899242516f30e8fc22f3aea4e1eddffdc765538d","edb5770ee3826b60468014c3f0598416329d3054","49af3e80343eb80c61e727ae0c27541628c7c5e2","3c578193f74589b04fe6d48d34432f0f860a85d7","75831cdce0100087678fa1d6832e776f14256b32","7314be5cd836c8f06bd1ecab565b00b65259eac6","88e3227a425a83690473507a9886b4c63a814206","06db21401811329e4c53a77a6a10dbb8380e1975","a4cb156b83f9d50c6bfe288a69603df5c00ea173","0814877233fb406b0ba3c4d3cda8e1dee0746576","29f3a031cfdcb44f3b19b8458adffd28ff1b2a9a","f436331ab66c545ad10fc7d0adafe2fb31377ee0","089ea5528d0e230156efbc21b4f9f8a3791ff4d4","c06bccec70ab0a10054b31e93a498d142dc2554c","6e565a52c6743e928ca8d3298e4dd58065587d43","87c37ed1913111c72f59fea96df6aaa26813925a","5bb38d87b6ebb4d44b689836c2de0156851311c9","c42962561eb759346b60bb89a04115a296ca9662","15d2aa6511bd0a8de5cb690bf406d90eef902ff1"],"id":"62b6cdb0ebf0ecb0594f0739cc83138b7b2e1ff4","s2Url":"https://semanticscholar.org/paper/62b6cdb0ebf0ecb0594f0739cc83138b7b2e1ff4","authors":[{"name":"Manuele Bicego","ids":["1722190"]},{"name":"Pietro Lovato","ids":["1739292"]},{"name":"Marco DeBona","ids":["52196407"]},{"name":"Flavia Guzzo","ids":["3791097"]},{"name":"Michael Assfalg","ids":["3263448"]}],"doi":"10.1109/ICPR.2018.8545883"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593997","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper we tackle the problem of finding the source of a gaseous leak with a robot in a three-dimensional (3-D) physical space. The proposed method extends the operational range of the probabilistic Infotaxis algorithm [1] into 3-D and makes multiple improvements in order to increase its performance in such settings. The method has been tested systematically through high-fidelity simulations and in a wind tunnel emulating realistic conditions. The impact of multiple algorithmic and environmental parameters has been studied in the experiments. The algorithm shows good performance in various environmental conditions, particularly in high wind speeds and different source release rates.","inCitations":[],"pmid":"","title":"Design and Performance Evaluation of an Infotaxis-Based Three-Dimensional Algorithm for Odor Source Localization","journalPages":"1413-1420","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593997"],"entities":[],"journalVolume":"","outCitations":[],"id":"5c8f58c75e548c6b2d02c40eeb865e2cea1c7975","s2Url":"https://semanticscholar.org/paper/5c8f58c75e548c6b2d02c40eeb865e2cea1c7975","authors":[{"name":"Julian Ruddick","ids":[]},{"name":"Ali Marjovi","ids":[]},{"name":"Faezeh Rahbar","ids":[]},{"name":"Alcherio Martinoli","ids":[]}],"doi":"10.1109/IROS.2018.8593997"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593892","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a method for motion learning aimed at the execution of autonomous household chores by service robots in real environments. For robots to act autonomously in a real environment, it is necessary to define the appropriate actions for the environment. However, it is difficult to define these actions manually. Therefore, body motions that are common to multiple actions are defined as motion primitives. Complex actions can then be learned by combining these motion primitives. For learning motion primitives, we propose a reference-point and object-dependent Gaussian process hidden semi-Markov model (RPOD-GP-HSMM). For verification, a robot is teleoperated to perform the actions included in several domestic household chores. The robot then learns the associated motion primitives from the robot's body information and object information.","inCitations":[],"pmid":"","title":"Learning and Generation of Actions from Teleoperation for Domestic Service Robots*This work was supported by JST, CREST","journalPages":"8184-8191","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593892"],"entities":[],"journalVolume":"","outCitations":[],"id":"ed2d88d7c8ad91736e9d4ed715ed9db261b02045","s2Url":"https://semanticscholar.org/paper/ed2d88d7c8ad91736e9d4ed715ed9db261b02045","authors":[{"name":"Kensuke Iwata","ids":[]},{"name":"Tatsuya Aoki","ids":[]},{"name":"Takato Horii","ids":[]},{"name":"Tomoaki Nakamura","ids":[]},{"name":"Takayuki Nagai","ids":[]}],"doi":"10.1109/IROS.2018.8593892"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205960","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Deep reinforcement learning has drawn much attention in robot control since it enables agents to learn control policies from very high dimensional states such as raw images. On the other hand, its dependency upon the availability of a significant quantity of training samples and its fragility in learning makes it difficult to apply for real world robot tasks. To alleviate these issues we propose Deep Dynamic Policy Programming (DDPP), which combines the sample efficiency and smooth policy updates of dynamic policy programming with the contemporary deep reinforcement learning framework. The effectiveness of the proposed method is first demonstrated in a simulation of the robot arm control problem, with comparison to Deep Q-Networks. As validation on a real robot system, DDPP also successfully learned the flipping of a handkerchief with a NEXTAGE humanoid robot using a reduced number of learning samples, whereas Deep Q-Networks failed to learn the task.","inCitations":["29b042a09016af72d7e51adb3061d3230093b84e","9b050aeaa780af6533978b38beae73ff2f5f8626"],"pmid":"","title":"Deep dynamic policy programming for robot control with raw images","journalPages":"1545-1550","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205960"],"entities":["Algorithm","Atari","Baseline (configuration management)","Bellman equation","Gradient","Humanoid robot","Kullback\u2013Leibler divergence","Policy-based design","Q-learning","Raw image format","Reinforcement learning","Robot control","Robotic arm","Simulation"],"journalVolume":"","outCitations":["6eabf6e67c29778265bc9fef3b58b2756c739c83","620412c443e85a1fc2f9ab950ddfada8d18d63b4","39f63dbdce9207b87878290c0e3983e84cfcecd9","182bcd5847d7850180458e4430add819c19b6d88","0a1164be2c441994cf9793132e4578a1db0ec744","14318685b5959b51d0f1e3db34643eb2855dc6d9","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","70f350506b58940d33fedda7a76eb542fcbf1abe","d228030bded6fb16d1bfb645fb6c1cd7f8633e54","340f48901f72278f6bf78a04ee5b01df208cc508","3b2a39c3806996aaaf57bbe99bd67de0e2c529af","dd90dee12840f4e700d8146fb111dbc863a938ad","1de0884635d3844f30d6397480292b04bd9a6af1","52e7ca3df4647c45d88416f3eb65836dd991ae27","6e87dd75d08ecfffa188c6e5cf2eb129d2cc10ea","1df9dca1044817a387609e6faa5de83cba2d2fcb","13b58f3108709dbbed5588759bc0496f82a261c4","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","15a679be4631d4256ff3f92f9720b22bac1fd1d1","1c78d3c154fabf1bc42b0f8a2b563de238dd49a4","e78f3223bf67c45c7fe45301ff8400b438c99e1d","1554d6787b88b24cde34f398a330e1ad4d30b634","178631e0f0e624b1607c7a7a2507ed30d4e83a42","3b9732bb07dc99bde5e1f9f75251c6ea5039373e"],"id":"168a0302d52777219bdfaee340c091f2706c5bf9","s2Url":"https://semanticscholar.org/paper/168a0302d52777219bdfaee340c091f2706c5bf9","authors":[{"name":"Yoshihisa Tsurumine","ids":["31991119"]},{"name":"Yunduan Cui","ids":["1824476"]},{"name":"Eiji Uchibe","ids":["1773761"]},{"name":"Takamitsu Matsubara","ids":["3248224"]}],"doi":"10.1109/IROS.2017.8205960"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594175","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this manuscript, a NDVI point cloud generator tool based on low-cost active RGB-D sensor is presented. Taking advantage of currently available ROS point cloud generation tools and RGB-D sensor technology (like Microsoft Kinect), that includes an inbuilt active IR camera and a RGB camera, 3D NDVI maps can be quickly and easily generated for vegetation monitoring purposes. When using low-cost sensors for vegetation index estimation, it is necessary to apply a rigorous methodology for extracting reliable information. In this paper, the methodology for NDVI generation using a low-cost sensor as well as experiments to evaluate its performance is presented. The experiments performed show that it is possible to obtain a reliable NDVI point cloud from a Kinect V2.","inCitations":[],"pmid":"","title":"NDVI Point Cloud Generator Tool Using Low-Cost RGB-D Sensor","journalPages":"7860-7865","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594175"],"entities":[],"journalVolume":"","outCitations":[],"id":"e012143c4d812a6f10a347a3e9c1c6284b905ac8","s2Url":"https://semanticscholar.org/paper/e012143c4d812a6f10a347a3e9c1c6284b905ac8","authors":[{"name":"David Calero","ids":[]},{"name":"E. Fernandez","ids":[]},{"name":"M. Eulàlia Parés","ids":[]},{"name":"E. Angelats","ids":[]}],"doi":"10.1109/IROS.2018.8594175"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206113","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Position-based proportional derivative (PD) controllers are known to be able to render compliant behaviors to a robot, and they are usually used in conjunction with a friction compensator to improve control performance. Existing methods are effective when applied to actuation systems with mechanical transmissions; however, they cannot be applied to actuation systems that use fluid transmissions owing to the characteristics resulting from fluid parameters. To solve this problem, we propose a stability-guaranteed PD control method that incorporates two observers: one for observing friction and the other is for observing flexible joint effects due to fluid compliance and internal leakage. This allows robots with fluid transmissions to asymptotically converge to the desired position. We verified the proposed approach by conducting simulations and experiments.","inCitations":[],"pmid":"","title":"Disturbance-observer-based PD control of electro-hydrostatically actuated flexible joint robots","journalPages":"2821-2828","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206113"],"entities":["Characteristic impedance","Converge","Experiment","Information","Interaction","MAC address","Observer (quantum physics)","Robot","Simulation","Spectral leakage"],"journalVolume":"","outCitations":["f9e37638ae1d470ac182a4d34684337c929c5ef3","a8887e1e7541910c7d1dc2310beb04fa5faf2ba3","6bf3aca4b0f1f34bc43d0df6ccf371d86c10079b","8dbc8f00eb6da14563296fd8febdcc3d6f44149f","124377eb35c8617bc7c4893636fb68fe784d131d","3090ea5ef45338dd6625fd1ebc4698b111ceff3f","fb82dd108c9f805864194c1ef2588bb1fbe02365","8b3539e359f4dd445c0651ebcb911c034d1ffc5b","2afbf3abe701b2879a3001c7a753608f516b9b74","a928785114c2e309800bcea0dd4a2c87aa0d9a8b","35250ab36e1184e41a256fd0e5ce7ab8b1f006a9"],"id":"5e5c3056832e9c6aaac263aa5f2a8193ca18a88f","s2Url":"https://semanticscholar.org/paper/5e5c3056832e9c6aaac263aa5f2a8193ca18a88f","authors":[{"name":"Woongyong Lee","ids":["2762525"]},{"name":"Min Jun Kim","ids":["35466073"]},{"name":"Wan Kyun Chung","ids":["1710051"]}],"doi":"10.1109/IROS.2017.8206113"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202225","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The torso of a humanoid robot is a fundamental part of its kinematic structure because it defines the reachable workspace, supports the entire upper-body and can be used to control the position of the center of mass. The majority of the torso joints are designed exploiting serial or differential mechanisms, while parallel kinematic structures are less used mainly because of their greater design complexity. This paper describes the design and construction of a 4 degrees of freedom (DoF) torso for our new humanoid robot. Three degrees of freedom, namely roll, pitch and heave, have been implemented using a 3 DoF parallel kinematic structure, while the fourth DoF, namely yaw, has been implemented with a rotational joint on top of the parallel structure. The design has been optimized to reduce the cost and the volume of the system. A first prototype of the torso has been constructed and validated with respect to our design requirements. Eventually, experimental tests have been conducted to assess the functionality of the proposed system.","inCitations":["c479aa129e2db7ac1d67b136b036b375ff64f9a5"],"pmid":"","title":"A parallel kinematic mechanism for the torso of a humanoid robot: Design, construction and validation","journalPages":"681-688","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202225"],"entities":["Humanoid robot","Inverse kinematics","Pitch (music)","Prototype","Repeatability","Requirement","Visual servoing","Workspace","Yaws"],"journalVolume":"","outCitations":["022dc10a5661c1392841cfea3f0eb174aa73535e","f710201366d7c59c5b8e3916856b0351031c33ea","9b7422cb2c4054bfc6a4315ee0e23b83d6636547","614bc7247b636062d4a658ddc10cefc1d5931cbb","afba83157286c9892a8715511b6b9d0ce99ac506","c479aa129e2db7ac1d67b136b036b375ff64f9a5","59ac8334e466b402163306f4d39f95e6acf5a2a9","352c94bdfc3a63f9e1bc4e5fab59025268c1d8dd","38034493656903332acf537ee3b0a90880076303","ee27696ad0fe9a0cff2ec77477972feef7bc96f3","61bd01c58f3a017314f195ac2441a96ef8df4d6c"],"id":"98832256758e759b92849d2ac3cf38338e1686be","s2Url":"https://semanticscholar.org/paper/98832256758e759b92849d2ac3cf38338e1686be","authors":[{"name":"Luca Fiorio","ids":["1727273"]},{"name":"Alessandro Scalzo","ids":["34851539"]},{"name":"Lorenzo Natale","ids":["1700995"]},{"name":"Giorgio Metta","ids":["1698471"]},{"name":"Alberto Parmiggiani","ids":["1686361"]}],"doi":"10.1109/IROS.2017.8202225"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206285","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present a micro aerial vehicle (MAV) system, built with inexpensive off-the-shelf hardware, for autonomously following trails in unstructured, outdoor environments such as forests. The system introduces a deep neural network (DNN) called TrailNet for estimating the view orientation and lateral offset of the MAV with respect to the trail center. The DNN-based controller achieves stable flight without oscillations by avoiding overconfident behavior through a loss function that includes both label smoothing and entropy reward. In addition to the TrailNet DNN, the system also utilizes vision modules for environmental awareness, including another DNN for object detection and a visual odometry component for estimating depth for the purpose of low-level obstacle detection. All vision systems run in real time on board the MAV via a Jetson TX1. We provide details on the hardware and software used, as well as implementation details. We present experiments showing the ability of our system to navigate forest trails more robustly than previous techniques, including autonomous flights of 1 km.","inCitations":["b84d6e0cbace95b30888996f23f1dac2be778a3f","6a1fadb3b94eb22d22e4c68b3803246abc0dec73","fb1732a1476798c42a0123aaf127036bf8daef09","5fc54daa49c25c08683840b8a98fb3c719609e54","74de35195c714298f1c01502d87c635b3e693208","aa4942b256228b7279e1ceaa30e4f957c5a08b36","fda560820cdc08a02d54e1b7ee5ee6d4f5eb66c8","ebb889f16c2a1ced1e846ab04dc33f89f207343b","bfc9195d186558334269bc2b7eb453f3b179a144","07e865957c61c639303ba8ac1a0fe72c116406db","606956bcc5c7b0b34bdaa39a54113ad83f48ab93","2d5a838da3cb3f60487fa7ce918960f0dc16c8c1","e6afe8e54dc97f7773ce919eab2d08d07bb3dc18","b847ddf7603d70cfdbea044525c8e0c00401d952","92dca3910333489c16d348958f6ecf717ba9ae60","ffa00d89e85410438d1409e301cf8cc3aa533f1a","6aa15635d6c424d2fb132b6366d54b38fa81ed20","e6d8fd848f0e427cd7e3ec8ee1523a4dfcc42765","7f2e317fee30b8f812ccb81414684ce86fef3c39","6e2e62bad0fe5f3a0d231bfeb8a6e72cad7cc8ad","c533e33190b8183c7fe1a4c6b4180f2e179812ef","7d63cd7883a352ae61a9992ed0a943c70d76f207","d2793fe6484eb0d3a002b888a73ff2f9224849a3","5206b953d03e0443904915542151cfd8b1537cf7","6a04fb965e41530a0ffe3b5eb07f1a87e011df58","8cb06a100b9b043e04cc473c7a558296f3f13051","73d6ef161b87265e2186bd66a3a1a96c09d92334","b6f2ce690f41593e3839a13059718df4f6c5f82b","225f0a0a2e7898bd11db40b4fa7e92409a8c1ae1","4966198e5d61b9632ebcef77d51afdf57735ac0b","5ea89c5ab1938f90096170c9205bb7ca8fea7e82","79d53382135180579fe13ed2044fb13ad5e19542","0a3eb73fc96513635f67a162b04a1de100671f1c","e2da236bc522907c1dc823d7ec670e2392e53e12","a68387aa88ba9ae57b34350e3c05c78aa402f512","a233f839c83b526c232df2b197980ff5404495c3","4b560da88bd3731a0d94a838146d653dbc58e51b"],"pmid":"","title":"Toward low-flying autonomous MAV trail navigation using deep neural networks for environmental awareness","journalPages":"4241-4247","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1705.02550v3.pdf","http://research.nvidia.com/sites/default/files/pubs/2017-09_Toward-Low-Flying-Autonomous//TrailMAV-IROS2017.pdf","https://doi.org/10.1109/IROS.2017.8206285","http://arxiv.org/abs/1705.02550"],"entities":["Aerial photography","Artificial neural network","Autonomous robot","Deep learning","Experiment","High- and low-level","Lateral thinking","Loss function","Motorola Canopy","Object detection","Rectifier (neural networks)","Robotic mapping","Robust control","Smoothing","Tegra","Vii","Visual odometry"],"journalVolume":"","outCitations":["933282df1b96a7b0829dc71029a19dfd8f60de06","0919b3ab82ff8a052490389f217c480c273f2b92","b56ef8e5791a3852864c787ae013b77b10ea1998","62564ca10f008bf10ea1a446378587432c373b86","54584fa817611e9b24492eff04b81a0e834258e1","579e2e12c1e46e8636206eb5028ecf04bf5c205c","620412c443e85a1fc2f9ab950ddfada8d18d63b4","27962faaafbf01092da03130550a70e097e1dd9f","0f0d11429e5aaecbc9fce8445afaa3bad7a74888","a4c069cdbd821ae8edb7d6a2e4778f30a87e2c3c","06feba1ffd596b41884cea6e8ef0da89b6dd2233","421e6c7247f41c419a46212477d7b29540cbf7b1","4a965ee6dcfc1ab12170af762e1d08e11b60ddb8","7bc9ba75c75c190c3ccb4f6899b0efd31cb0266c","20a78d3145279dcd799cd7a856ae2714f4863a16","5ce030f1650145a103527e883e7a9d9a25c45547","034169a2650ae4148fe3c2a038dbe50459a03b48","21a1654b856cf0c64e60e58258669b374cb05539","16e1f2669b9a9ecb71ea4e0ed581e2c24cb55f79","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a","0b8759d61e93b809df16d9fe9010d2a2d7241c74","1dcae5e5e9fc1b4c0315b6e68f19b9bb2d9dba05","a56ed0bc2d69094e351a044ae8bc64ca0da691f8","876ef047c98a640b4153cf6f2314751aa2334d2c","625e2f3229d75dc6ec346961efe485617dd3e048","59db04c4cb3917136554ced5d26a0f71f5c02736","1751e6cc3b2b8ad03ae6c1cde82b2fd2fc94fca4","0626908dd710b91aece1a81f4ca0635f23fc47f3","2c03df8b48bf3fa39054345bafabfeff15bfd11d","7d39d69b23424446f0400ef603b2e3e22d0309d6","95fc4e78a08c2782c5988097025c5843aef8b618","17d72c2802f7d60a2b1a31b7b8df59233dacdcfc","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","1554d6787b88b24cde34f398a330e1ad4d30b634","10535afd96a5061d1f0f096af611227bc70e5fc5","061356704ec86334dbbc073985375fe13cd39088"],"id":"3edd3fea5d527c7ff2436ea2d59a25304147ba24","s2Url":"https://semanticscholar.org/paper/3edd3fea5d527c7ff2436ea2d59a25304147ba24","authors":[{"name":"Nikolai Smolyanskiy","ids":["2887475"]},{"name":"Alexey Kamenev","ids":["38686818"]},{"name":"Jeffrey Smith","ids":["50110890"]},{"name":"Stanley T. Birchfield","ids":["2238841"]}],"doi":"10.1109/IROS.2017.8206285"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593759","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents the design of a vision-based object grasping and motion control architecture for a mobile manipulator system. The optimal grasping areas of the object are estimated using the partial point cloud acquired from an onboard RGB-D sensor system. The reach-to-grasp motion of the mobile manipulator is handled via a Nonlinear Model Predictive Control scheme. The controller is formulated accordingly in order to allow the system to operate in a constrained workspace with static obstacles. The goal of the proposed scheme is to guide the robot's end-effector towards the optimal grasping regions with guaranteed input and state constraints such as occlusion and obstacle avoidance, workspace boundaries and field of view constraints. The performance of the proposed strategy is experimentally verified using an 8 Degrees of Freedom KUKA Youbot in different reach-to-grasp scenarios.","inCitations":[],"pmid":"","title":"A Model Predictive Control Approach for Vision-Based Object Grasping via Mobile Manipulator","journalPages":"1-6","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593759"],"entities":[],"journalVolume":"","outCitations":[],"id":"04b5e033a340817b2caf194eba2557c61cfc1372","s2Url":"https://semanticscholar.org/paper/04b5e033a340817b2caf194eba2557c61cfc1372","authors":[{"name":"Michalis Logothetis","ids":[]},{"name":"George C. Karras","ids":[]},{"name":"Shahab Heshmati-Alamdari","ids":[]},{"name":"Panagiotis Vlantis","ids":[]},{"name":"Kostas J. Kyriakopoulos","ids":[]}],"doi":"10.1109/IROS.2018.8593759"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593672","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Motion planners have been recently developed that provide path quality guarantees for robots with dynamics. This work aims to improve upon their efficiency, while maintaining their properties. Inspired by informed search principles, one objective is to use heuristics. Nevertheless, comprehensive and fast spatial exploration of the state space is still important in robotics. For this reason, this work introduces Dominance-Informed Regions (DIR), which express both whether parts of the space are unexplored and whether they lies along a high quality path. Furthermore, to speed up the generation of a successful successor state, which involves collision checking or physics-based simulation, a proposed strategy generates the most promising successor in an informed way, while maintaing properties. Overall, this paper introduces a new informed and asymptotically optimal kinodynamic motion planner, the Dominance-Informed Region Tree (DIRT). The method balances exploration-exploitation tradeoffs without many explicit parameters. It is shown to outperform sampling-based and search-based methods for robots to significant dynamics.","inCitations":[],"pmid":"","title":"Efficient and Asymptotically Optimal Kinodynamic Motion Planning via Dominance-Informed Regions","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593672","https://www.cs.rutgers.edu/~kb572/pubs/iros_dirt.pdf"],"entities":[],"journalVolume":"","outCitations":["0e2486886522703446ffb826b9e53725bb052915","031ae4445789f2709dacee005469042af9fcaf6d","896d54a5af77fa8fea3bb1178d7454d0483bbafd","0faa04d41b3484314ad547b65e01be3152f177b2","6e2f4794bf38e46a92ef2f24275a0b19e02bee33","90d6b4338a9c7775d9ef71f811014c3c07d09e43","05f7d57c99a06ff3888921bf4a7b738f908b2ae3","0d8f8733768607c21e75d85ccae8c409efc6f76a","e20c517f63c8b61df93596430c928d93233457a9","2d323cc0162c90cac29e62fbf4704bf2ce149643","a1a8dad0b9e7c7676228b5b85ae07cacce05ee2e","3df788ee29d9630b2575b8f424384e34f58405b6","978e3bbd5a2e7ef9d627ad2077317d5c0b872301","ef03ac66934ff031c642e595bce832620e617c4b","19cf2a7eaa29b3b4448ab56d1e17f54a0ace0548","088e21d6931729e005e6a622c6a92695e3c9dce8","308f29607b4022f35eb7d864ffcb607621f0b538","67aaf4e84f11066d0db26a62723ade94625aac0a","65309ace2cee63e779965f9fb6467e9822987f41","12118c5bb693ad6e0b15c3e655f4fd3cb367e22e","bfd9962a72b5e5927ecfceda5c42d5fef883b22b","058abb98e5a12c8d230e8e8099f82c413370f499","0417d0b262457872310ca8c0f58c36c5806c9d61"],"id":"c74018a2eb6e0cfd942e9ee38bf0fb7a74ae371c","s2Url":"https://semanticscholar.org/paper/c74018a2eb6e0cfd942e9ee38bf0fb7a74ae371c","authors":[{"name":"Zakary Littlefield","ids":["3188435"]},{"name":"Kostas E. Bekris","ids":["1739036"]}],"doi":"10.1109/IROS.2018.8593672"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206238","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Robot software controllers are often concurrent and time critical, and requires modern engineering approaches for validation and verification. With this motivation, we have developed a tool and techniques for graphical modelling with support for automatic generation of underlying mathematical definitions for model checking. It is possible to check automatically both general properties, like absence of deadlock, and specific application properties. We cater both for timed and untimed modelling and verification. Our approach has been tried in examples used in a variety of robotic applications.","inCitations":["50fc22bf8192ee58b42b080ff3d67b569f6ffc10","1ceb00b68661734fabef453f706d4f2690a967e9","0c44d839c70f8c7f571c19f020394827b8ba392f","a153285a43faf400ba6f9dfc20d96f3658648b2e","6d4903904424d0ffd3b8b18da61febe19b7bd24e"],"pmid":"","title":"Automatic property checking of robotic applications","journalPages":"3869-3876","s2PdfUrl":"","pdfUrls":["https://www-users.cs.york.ac.uk/~alcc/publications/papers/MRLCT17.pdf","https://doi.org/10.1109/IROS.2017.8206238"],"entities":["Alpha algorithm","Augusto Sampaio","Autonomous robot","Communicating sequential processes","Deadlock","Expectation propagation","Finite-state machine","Graphical user interface","Jim Woodcock","Model checking","PRISM (surveillance program)","Relay","Robot software","Software product line","State diagram","Unified Modeling Language","Verification and validation"],"journalVolume":"","outCitations":["727a69488880db80c28140ea2e3ab0036e8c5bad","4ba9b64a696e111332b6c73f2ac0ec9fdcc442c5","f82b730e3bc3bc7160eb32e1e191c638c63a5160","72bc90e7cdc76b6741bbd1c89c85d28bbd033502","c622687ccb7959f5bddcd0f98df74471c1d25f75","175a8ad42b7c618fc090662e1e995075de5ab261","d2c72dd7df5d349b7daf6d10cd25de5983f4020e","bad08c4df57a908ecc806e256d1d93e7ec96c850","49994a97b54b48f7709326209ba3ba827807e655","1604b66a62684173d25fc62b57ec97d64f958310","58e9dda22b94500cb5e3b77b77031d3dfcf1ea99","b6ca37b036a5b4bb4b14764debe148ee49307605","93bba3ab7af69de624144e78481c6ee0757a876c","a406b4ff1759858ea10667d9e7fa7ae04f7f8367","c9ab6986686ccae97b13290ae9f4fc696c2ec475","142855a5422ff1522f1de3e406b1caac93c525fd","ad82083dd1b97230f63f3e3df26c302de85753ea","1752abd2a0ba3704a2085aad604efaabc9282924","6c1a4985e119f025ba32549d2e2621735b3abe1a","d63ce72f1396a563c24f7b43648b8607a026775c","bceab7e5163908d5e8016b2a77f8b612c45b7213","405dca4ab3e6f4c583ef6429c96342064136183b","f2be61e92d56a2948f2beba2c5c9f1048d16f47f","75c2f98711a4af6037c52e58446fa24c3b413352"],"id":"67ff2e507dcde81c6c2abb0203d179a0aadae098","s2Url":"https://semanticscholar.org/paper/67ff2e507dcde81c6c2abb0203d179a0aadae098","authors":[{"name":"Alvaro Miyazawa","ids":["2559989"]},{"name":"Pedro Ribeiro","ids":["48485689"]},{"name":"Wei Li","ids":["47113208"]},{"name":"Ana Cavalcanti","ids":["1734212"]},{"name":"Jonathan Timmis","ids":["1715578"]}],"doi":"10.1109/IROS.2017.8206238"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594153","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"An MRI-actuated catheter is a novel robotic catheter system that utilizes the MR scanner for both remote steering and catheter tracking. In order to develop the mathematical model and the planning algorithm of the catheter in parallel to the MR tracking system, an alternative catheter tracking method is needed. This paper presents a catheter tracking algorithm based on the particle filter and the catadioptric camera system. The motion model of the particle filter is based on the quasi-static kinematics of the catheter. The measurement model calculates the weights of the particles according to the normalized crosscorrelation of the segmented image from camera and a virtual rendering of the catheter. The efficacy of the tracking algorithm is demonstrates via experimental results.","inCitations":[],"pmid":"","title":"State Estimation for MRI-Actuated Cathers via Catadioptric Stereo Camera","journalPages":"1795-1800","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594153"],"entities":[],"journalVolume":"","outCitations":[],"id":"31b075cb25684f68ac7acc545c3a1a52515de87e","s2Url":"https://semanticscholar.org/paper/31b075cb25684f68ac7acc545c3a1a52515de87e","authors":[{"name":"Tipakorn Greigarn","ids":[]},{"name":"Russell C. Jackson","ids":[]},{"name":"Murat Cenk Cavusoglu","ids":[]}],"doi":"10.1109/IROS.2018.8594153"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593658","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper presents a soft actuator embedded with conductive liquid metal and shape memory epoxy (SME) which function together to enable self-sensing, tunable mechanical degrees of freedom (DoF), and variable stiffness. We embedded thermoplastic shape memory epoxy in the bottom portion of the actuator. Different sections of the SME could be selectively softened by an implanted conductive silver yarn located at different positions. When an electric current passes through the conductive silver yarn, it induces a phase transition that changes the epoxy from stiff state to compliant state. Each section of SME could be softened within 5 s by applying a current of 200 mA to the silver yarn. To acquire the strain curvature, eGaIn was infused into a microchannel surrounding the chambers of the soft actuator. A spiral-shaped eGaIn sensor was also attached to the tip of the actuator to perceive the contact with reliable dynamic force response. Systematic experiments were performed to characterize the stiffness, tunable DoF, and sensing property. We show the ability of the soft composite actuator to support a weight of 200g at the tip (as a cantilever) while maintaining the shape and the ability to recover its original shape after large bending deformation. In particular, seven different motion patterns could be achieved under the same pneumatic pressure of the actuator due to selectively heating the SME sections. A gripper which was fabricated by assembling two actuators to a base was able to grasp the weight up to 56 times of a single actuator through an appropriate motion pattern. For demonstration purposes, the gripper was used to grasp various objects by adjusting the DoF and stiffness with real-time feedback of the bending strain and the contact force.","inCitations":[],"pmid":"","title":"A Variable Degree-of-Freedom and Self-Sensing Soft Bending Actuator Based on Conductive Liquid Metal and Thermoplastic Polymer Composites","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593658"],"entities":[],"journalVolume":"","outCitations":[],"id":"9c971405fbcef72bd6df25091ab7147919ebf065","s2Url":"https://semanticscholar.org/paper/9c971405fbcef72bd6df25091ab7147919ebf065","authors":[{"name":"Yufei Hao","ids":[]},{"name":"Zemin Liu","ids":[]},{"name":"Zhexin Xie","ids":[]},{"name":"Xi Fang","ids":[]},{"name":"Tianmiao Wang","ids":[]},{"name":"Li Wen","ids":[]}],"doi":"10.1109/IROS.2018.8593658"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593858","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we present a model set for designing human-robot collaboration (HRC) experiments. It targets a common scenario in HRC, which is the collaborative assembly of furniture, and it consists of a combination of standard components and custom designs. With this work, we aim at reducing the amount of work required to set up and reproduce HRC experiments, and we provide a unified framework to facilitate the comparison and integration of contributions to the field. The model set is designed to be modular, extendable, and easy to distribute. Importantly, it covers the majority of relevant research in HRC, and it allows tuning of a number of experimental variables that are particularly valuable to the field. Additionally, we provide a set of software libraries for perception, control and interaction, with the goal of encouraging other researchers to proactively contribute to our work.","inCitations":["1eca4e34f720e51ae434bc555d56dcbfc9c120ce","853d5712a3f9c5fa7e0fa39bc5067a21914822b0","966f9ee69c03404652461451a8fb3f39cd0abf74"],"pmid":"","title":"The HRC Model Set for Human-Robot Collaboration Research","journalPages":"1845-1852","s2PdfUrl":"","pdfUrls":["https://alecive.github.io/papers/2018_Zeylikman_IROS_hrc_model_set.pdf","https://doi.org/10.1109/IROS.2018.8593858","https://arxiv.org/pdf/1710.11211v2.pdf","https://export.arxiv.org/pdf/1710.11211","http://arxiv.org/abs/1710.11211"],"entities":[],"journalVolume":"","outCitations":["ed504eb31a4046a7c8889c2e821e12d55e6af5bb","38a62849b31996ff3d3595e505227973e9e3a562","cd2638addc6f003f6dbd8549a48412e4877b2993","c14d3eb270a2de4feb5fb5965a85e994840a1dad","afcfc70faf267848f3774364d0e547605556b4b2","a2fb6b6b52406a9979a7738d64de280596acb26f","d45eaee8b2e047306329e5dbfc954e6dd318ca1e","a63d77edf8313db0385bfc6c6124567a776987ff","53609ec16767b6b4d75ba27c01e07073969a1dae","a2657d4bf74990b16bdbbb9dc2e3304d576ec76d","180ddc453c109c10e1b743b87c2f3089c6bdae73","9b0ae635f1d3c2771920eb59a80aec35a8bce58a","2037b6bf21edb594c20086b253a0179367865dff","7900f0a0c8753ad504381a3bbeb5bfe4eb76f635","2f51d9d2c165344ac333914bd0936966651d24e3","851a3fa8ae320fe91734e8c560591b8e222805f4","f4a8c764d937231cebad6e53464c8bc35171a6b1","b607c20a89eb2799dd513aeb4976dbfa6645d538","694b15f6822631959923ab8b1d69552b100d46a3","02d5608844909b7df470b3a79f595d78496b9fc0","1c45f57d195cc7a90766c2c0786494a5dbeb7ce4","e8563bb2c74a5b012a5fb6b8ff7abbaf573e30be","224ed8825a9fa8459ddc66d6032aedf4f4b6e358","29090dd077faf0ae54b7f5b70c1151309c5d8e3e","537427f717ef43d7a093203d1395da6890fbd623","41c0fc6f291b227159d672f7dd7ca6fa8ddbb071","a556478fc2cb9038baab557021603db717ae21bd","044794da4f7c613eb19d3c147b011cb5f52c2c6f","2d90537b709ae92e07856b93aab70614ac5af561","6ef8ef22a073a642855061b54fcb96f705448c3d","ad3cd8ab307718f7a2ec0adb7839a89f46b4015b","cdd1fdb136acc7a1e61fef2c4d3cf6790a13daec","9b0f223652c4d49b321da62d82f9916827d597bf","47b2b4e204e3d007a95ae314b7d3c7b993274c54","8d78c8dba9fd5e457fdd301dfca75b0c9b4959b2","86428fc6eb917dd914a6837f49dbbde5fc37c6d4","6f12683e049d1e810414a80aa5689db12d062f08","14430b0f7d27c8aeffc82de30c190e7652125d23","4a2bf3d125c7073d2ba55c28a6466419b787fa8b","33a91348e0f51486ddce3b4e0589f56c09298ea2","1e34ae16144b0f8fd36ee31eb409ab3d0108f3f0","5855ccdf69da752bef292f415a295739c7253e4d","3a413210768ba401e2299e83174557e891ea27eb"],"id":"9b644794de7f1a546dd490ef14f6302be549d8cb","s2Url":"https://semanticscholar.org/paper/9b644794de7f1a546dd490ef14f6302be549d8cb","authors":[{"name":"Sofya Zeylikman","ids":["27716833"]},{"name":"Sarah Widder","ids":["21088768"]},{"name":"Alessandro Roncone","ids":["3077150"]},{"name":"Olivier Mangin","ids":["2566334"]},{"name":"Brian Scassellati","ids":["1792053"]}],"doi":"10.1109/IROS.2018.8593858"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594408","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Object proposal that detects candidate bounding boxes of objects in images is an effective way of accelerating object recognition in the robot/computer vision area. We propose an accurate and fast object proposal method using depth images. Existing proposal methods can be roughly divided into two categories: window scoring and object region extraction. The window scoring methods usually have higher efficiency than object region extraction methods. The previous methods using RGB images detect an excessive number of boxes due to edges of texture objects. These methods also may misdetect overlapping objects as one candidate bounding box. To tackle these problems, we propose a novel and effective objectness measure using depth images. The proposed method evaluates objectness by using depth boundary density difference between inner and outer regions of a candidate bounding box. We also consider the uniformity of the outer boundary density in a candidate bounding box to divide overlapping objects into individual candidate bounding boxes. Our reasonable assumption here is that the depth boundary of an object has a closed loop. Our experiments show significant performance gains over existing RGB and RGB-D object proposal methods on the challenging toy-dataset [1] of complex crowded scenes.","inCitations":[],"pmid":"","title":"Unsupervised Object Proposal Using Depth Boundary Density and Density Uniformity","journalPages":"7866-7871","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594408"],"entities":[],"journalVolume":"","outCitations":[],"id":"5e56cc79be1cc488024c42e2311daada2ff646af","s2Url":"https://semanticscholar.org/paper/5e56cc79be1cc488024c42e2311daada2ff646af","authors":[{"name":"Takashi Hosono","ids":[]},{"name":"Shuhei Tarashima","ids":[]},{"name":"Jun Shimamura","ids":[]},{"name":"Tetsuya Kinebuchi","ids":[]}],"doi":"10.1109/IROS.2018.8594408"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545584","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Vehicle re-identification (re-ID) remains an unproblematic problem due to the complicated variations in vehicle appearances from multiple camera views. Most existing algorithms for solving this problem are developed in the fully-supervised setting, requiring access to a large number of labeled training data. However, it is impractical to expect large quantities of labeled data because the high cost of data annotation. Besides, re-ranking is a significant way to improve its performance when considering vehicle re-ID as a retrieval process. Yet limited effort has been devoted to the research of re-ranking in the vehicle re-ID. To address these problems, in this paper, we propose a semi-supervised learning system based on the Convolutional Neural Network (CNN) and re-ranking strategy for Vehicle re-ID. Specifically, we adopt the structure of Generative Adversarial Network (GAN) to obtain more vehicle images and enrich the training set, then a uniform label distribution will be assigned to the unlabeled samples according to the Label Smoothing Regularization for Outliers (LSRO), which regularizes the supervised learning model and improves the performance of re-ID. To optimize the re-ID results, an improved re-ranking method is exploited to optimize the initial rank list. Experimental results on publically available datasets, VeRi-776 and VehicleID, demonstrate that the method significantly outperforms the state-of-the-art.","inCitations":[],"pmid":"","title":"Joint Semi-supervised Learning and Re-ranking for Vehicle Re-identification","journalPages":"278-283","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545584"],"entities":[],"journalVolume":"","outCitations":["4996e64c24cef33d0f7e5a2b1c3baf00e51493e6","e1c54710f8a00655c6be63b059f7151b05d020fe","86eab1845deb3614233360b6bc33ce1ff074458e","ce9643fbfcb9c3156c7b26b5c92ec3bc67111202","47117be4efd4e299bcf904fd2601ec1f3af34cba","92166eb883b0505040c2d61c758985e5ec051f83","1eb5711f76e3fede6ae1cc4c5f4cf23ed91ac129","1b2c6fb85e7e776b533fda63f92fabb04ed5e887","4cb36aea73a328da8ffcdc616407bae3c908aa07","027f6e9b9c196747d2cc7fad42d07940f00486bb","2fa4b37f91667970150481c37a1f4e294a49b7f0","063f78c20405158d87114a8aef1bb7557230bd89","5a603ab4b6353fc244361930c28723b3bc091f4b","783089b28fe3a6568165cf2cfa56657a44e24d9b","45a33bddf460554b7c1f550aa382d63345a20704","35756f711a97166df11202ebe46820a36704ae77","4954fa180728932959997a4768411ff9136aac81","9fede3a43d6b63a8d5988cb7f52d74bba8d164cd","4cf343cbd28343ff44fa15e4160c58a2953df5d1","ccf934a335793fe416b0115183783d2c355b64ed","0626908dd710b91aece1a81f4ca0635f23fc47f3","2c03df8b48bf3fa39054345bafabfeff15bfd11d","abc8638968909ab0fdfbf1049009082df554a49e","1e432547edf1f0c0cf77d03a106ae5771fd5c0d7","11fdff97f4511ae3d3691cfdeec5a19fa04db6ef","798d9840d2439a0e5d47bcf5d164aa46d5e7dc26"],"id":"91ddf662748051390bb3356380568fc9447a48ed","s2Url":"https://semanticscholar.org/paper/91ddf662748051390bb3356380568fc9447a48ed","authors":[{"name":"Fangyu Wu","ids":["5145427"]},{"name":"Shiyang Yan","ids":["3491491"]},{"name":"Jeremy S. Smith","ids":["33830793"]},{"name":"Bailing Zhang","ids":["46824190"]}],"doi":"10.1109/ICPR.2018.8545584"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593434","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In an autonomous vehicle setting, we propose a method for the estimation of a semantic grid, i.e. a bird's eye grid centered on the car's position and aligned with its driving direction, which contains high-level semantic information about the environment and its actors. Each grid cell contains a semantic label with divers classes, as for instance {Road, Vegetation, Building, Pedestrian, Car\u2026}. We propose a hybrid approach, which combines the advantages of two different methodologies: we use Deep Learning to perform semantic segmentation on monocular RGB images with supervised learning from labeled groundtruth data. We combine these segmentations with occupancy grids calculated from LIDAR data using a generative Bayesian particle filter. The fusion itself is carried out with a deep neural network, which learns to integrate geometric information from the LIDAR with semantic information from the RGB data. We tested our method on two datasets, namely the KITTI dataset, which is publicly available and widely used, and our own dataset obtained with our own platform, equipped with a LIDAR and various sensors. We largely outperform baselines which calculate the semantic grid either from the RGB image alone or from LIDAR output alone, showing the interest of this hybrid approach.","inCitations":[],"pmid":"","title":"Semantic Grid Estimation with a Hybrid Bayesian and Deep Neural Network Approach","journalPages":"888-895","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593434"],"entities":[],"journalVolume":"","outCitations":[],"id":"c543eddd8817524fb941e96c81946f967dd68976","s2Url":"https://semanticscholar.org/paper/c543eddd8817524fb941e96c81946f967dd68976","authors":[{"name":"Özgür Erkent","ids":[]},{"name":"Christian Wolf","ids":[]},{"name":"Christian Laugier","ids":[]},{"name":"David Sierra González","ids":[]},{"name":"Victor Romero-Cano","ids":[]}],"doi":"10.1109/IROS.2018.8593434"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206279","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present an approach for estimating the absolute poses of a swarm Micro Autonomous Underwater Vehicles (μAUVs) by decomposing the problem into few absolute position estimations and many relative pose estimations. As power constraints are critical to small mobile robots, we develop an extension of active marker pose estimation using color information to solve the marker correspondence problem, and show that this approach is more energy efficient than reflective estimation approaches. We show the feasibility of this approach by localizing a robot navigating in an underwater test tank environment. Detailed analysis is presented characterizing the noise and error properties when estimating robot poses from fixed on-board markers. Moreover, we provide comparisons in power and computational cost for other popular methods of underwater localization.","inCitations":[],"pmid":"","title":"Low-cost monocular localization with active markers for micro autonomous underwater vehicles","journalPages":"4181-4188","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206279"],"entities":["Acoustic cryptanalysis","Algorithmic efficiency","Autonomous robot","Color space","Computation","Connected-component labeling","Control theory","Correspondence problem","Embedded system","Human error","Image noise","Internationalization and localization","Mobile robot","On-board data handling","Pixel","Swarm","Xfig"],"journalVolume":"","outCitations":["8b70acbf660110b731fa7ac66ad3af9ba2edca26","042305fdca6fa3efa785c77bd1d72bf9cabbd993","960e4fcb417a2b2679adb14ef0f9e600fc655367","d02f030ae7a081a09d4c38b9c00a82d4ad90ddc6","3ad006050e84a274bd9ee6128b275ec6f283783e","590219e9892d1e049aab81d7a10d2019f23da1e7","efbadd0be06c554ad1c8f26713591f26428c1443","a99162af01db38bbc5e11f785c423c63bd57caa3","205bced5235598a19ccdced779cf98bdfbcdcbe5","a3ccc3d564751e0b288961af84df246c49e4569b","219f49c8c281e72c5c88b8a9c61ccf94a255a6ae"],"id":"391114db490013fc23593b625f15951c5abe15ad","s2Url":"https://semanticscholar.org/paper/391114db490013fc23593b625f15951c5abe15ad","authors":[{"name":"Austin D. Buchan","ids":["20431516"]},{"name":"Eugen Solowjow","ids":["2419277"]},{"name":"Daniel-André Duecker","ids":["14784469"]},{"name":"Edwin Kreuzer","ids":["31474271"]}],"doi":"10.1109/IROS.2017.8206279"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546221","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Recently, self-normalizing neural networks have been proposed with a scaled version of exponential linear units (SELUs), which can force neuron activations automatically converge towards zero mean and unit variance without use of batch normalization. As the negative part of SELUs is an exponential function, it is computationally intensive. In this paper, we introduce self-normalizing piecewise linear units (SPeLUs) for fast approximation of SELUs, adopting piecewise linear functions instead of the exponential part. Various possible shapes are discussed for piecewise linear units with stable self-normalizing properties. Experiments show that SPeLUs can provide an efficient and fast alternative to SELUs, with almost similar classification performance over MNIST, CIFAR-10 and CIFAR-10 datasets. With SPeLUs, we also show that batch normalization can be simply neglected for constructing deep neural nets, which could be advantageous for fast implementation of deep neural networks.","inCitations":[],"pmid":"","title":"Piecewise Linear Units for Fast Self-Normalizing Neural Networks","journalPages":"429-434","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546221"],"entities":[],"journalVolume":"","outCitations":["424a6e62084d919bfc2e39a507c263e5991ebdad","009fba8df6bbca155d9e070a9bd8d0959bc693c2","0a2a4b2627121ac71b5953572f61cbbd290209c7","3ba179bceb9692d4d21109d0b87b120195761148","a538b05ebb01a40323997629e171c91aa28b8e2f","83174a52f38c80427e237446ccda79e2a9170742","0b8759d61e93b809df16d9fe9010d2a2d7241c74","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","41951953579a0e3620f0235e5fcb80b930e6eee3","367f2c63a6f6a10b3b64b8729d601e69337ee3cc","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","97fb4e3d45bb098e27e0071448b6152217bd35a5","e9fac1091d9a1646314b1b91e58f40dae3a750cd"],"id":"634031824bd777698d6b27dd08468ca708559b2e","s2Url":"https://semanticscholar.org/paper/634031824bd777698d6b27dd08468ca708559b2e","authors":[{"name":"Yuanyuan Chang","ids":["2692250"]},{"name":"Xiaofu Wu","ids":["34885692"]},{"name":"Suofei Zhang","ids":["2819904"]}],"doi":"10.1109/ICPR.2018.8546221"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206600","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We compare five policies for teams of mobile robots to share a limited number of charging points while exploring a structured, unknown environment. Since it is infeasible to precompute an optimized schedule due to a limited time horizon, an energy-aware planner is used for adaptively deciding when and where to recharge. The coordination between robots is based on market economy giving them the flexibility to incorporate multiple objectives into a single cost function. The presented policies are evaluated through simulations with different robot team sizes on different maps. We measure the exploration gain in explored area and the exploration cost in traveled distance over time. We compare the market-based approach to a simple first-come, first-served approach. Results demonstrate the applicability for multi-robot exploration and show the strengths and weaknesses of each policy.","inCitations":[],"pmid":"","title":"Coordinated recharging of mobile robots during exploration","journalPages":"6809-6816","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206600"],"entities":["Greedy algorithm","Loss function","Map","Mobile robot","Rechargeable battery","Scheduling (computing)","Simulation"],"journalVolume":"","outCitations":["ef7bf242f7e33c74c35b8d025e22a457cc7af45f","0633f1b321384b445c6c2f9058d817e8b41fdbfc","74009eb02b174ce5b35ba64edfb45acafc6af4b3","96e714f211e9a47f1ddae54a567de9326ad3922c","5b5332e79aefa3b913d42a434b8ddb09b31b5b2e","3dfd0d32e1937af5ed732693e442c1d5febb0290","06fb88d6409b26742751aa3374bcdb640d647ccf","305917e0b5799ef496a1e24a052965424d3e6c13","64443f43e3d9eafb115cc283562c45630aac2432","8b091cb31626f70f575bfba634c8588c975cb331","75127cffd0e962ad09a90da55ac30f1b2341e384","2cc85de6c80639d3204ae0d4f247296cf6d9999d","f0123a3aec38c19cf84c215fbfcc58f4d6895f48","321f9c4da5a330ae91b64fcc27b196485cbe0dec","6e2a1c535c497df4a5dfea8cd04fff82e3435e2c","280b126f5aa3ca4bddfd84f43341799260642cb4"],"id":"492da20198e92451d173832d9f0111a7b8768964","s2Url":"https://semanticscholar.org/paper/492da20198e92451d173832d9f0111a7b8768964","authors":[{"name":"Micha Rappaport","ids":["47404905"]},{"name":"Christian Bettstetter","ids":["1712611"]}],"doi":"10.1109/IROS.2017.8206600"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545279","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"DLRC is an extension of LRC that extends LRC from conventional still image based method to the image set based method. DLRC has a demonstrated better performance on image set classification. However, when the image sets of different objects are not linear separable, or when the linear regression axes of class-specific samples of different classes have an intersection, DLRC may be failed for well classifying the image sets. In this paper, a new classification method, kernel dual linear regression classification (KDLRC), is proposed. KDLRC is a nonlinear version of DLRC and can overcome the drawback of DLRC. KDLRC first embeds the input data into a high-dimensional Hilbert space, then in the kernel space, the data become easier to classify. Extensive experiments on four well-known databases prove that the performance of KDLRC is better than that of DLRC and several state-of-the-art classifiers.","inCitations":[],"pmid":"","title":"Kernel Dual Linear Regression for Face Image Set Classification","journalPages":"1542-1547","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545279"],"entities":["Class","Classification","Database","Dual","Dual-Energy X-Ray Absorptiometry","Experiment","Hilbert space","Intersection of set of elements","Kernel (operating system)","Linear IgA Bullous Dermatosis","Nevus sebaceous","Nonlinear system","Physical object","User space"],"journalVolume":"","outCitations":["7cee973bd210032ef53d3ab2c4cb71d656a27853","a5d4cc596446517dfaa4d92276a12d5e1c0a284c","49de5abe5f111328fd3fa48c2a0a5c68362eaaa9","075bc988728788aa033b04dee1753ded711180ee","d1128415f741c970bbfd149c5ce6317d8abdbf8e","24cb375a998f4af278998f8dee1d33603057e525","5c7019fdbcc1a89d4a75ba88565d9ea8324579f4","6582f4ec2815d2106957215ca2fa298396dde274","cd373ffe31fd02cce35131cf0e3b7dcc2922ac33","06aab105d55c88bd2baa058dc51fa54580746424","8213dbed4db44e113af3ed17d6dad57471a0c048","d17a954a3d04ac8f887ec01fcadc8aa42a9cff01","ab734bac3994b00bf97ce22b9abc881ee8c12918","a018e597608609018f3f11c4b8cda0cb2c6aff1e","1ab39085672b72634ff35fb00d795a6768fb90d0","45eb8ae2c68bc515a1f105e135eea747cfd5bbd0","1d0cb1dd4b14e026b7767fa05bb34e5c84cf3d39","47a2252fdb2c5dae498a61dd73bfd48a8c5bb4d0","464159a30f6a469916e4bb5efde42a95e8f2bd10","7ea3b08e6af11989a70e5699501c56dc423f6b69","7e64d5ccb28bcec57d4210dd707ceccb247aaf93","6b9e47cd85cf07178e58bdced02f3ad013904f58","cadb9a014a4c5bbec57aaf30391f472fa4b69b4d","c0323bb0aa1782a14f3860158689578bc3b8f5e3","006ff99363137e8ae1ad621e6b0160080b77a1ae","09370d132a1e238a778f5e39a7a096994dc25ec1","7d3f6dd220bec883a44596ddec9b1f0ed4f6aca2","683ec608442617d11200cfbcd816e86ce9ec0899","1270044a3fa1a469ec2f4f3bd364754f58a1cb56","599cc88971d2f5b375ff23b6342f17855e01791c","6204776d31359d129a582057c2d788a14f8aadeb"],"id":"c57494a6433980e645416c983bcdc5a4fef73ed0","s2Url":"https://semanticscholar.org/paper/c57494a6433980e645416c983bcdc5a4fef73ed0","authors":[{"name":"Xizhan Gao","ids":["2681557"]},{"name":"Quansen Sun","ids":["31085136"]},{"name":"Haitao Xu","ids":["47995498"]},{"name":"Yanmeng Li","ids":["48514671"]}],"doi":"10.1109/ICPR.2018.8545279"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205984","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Aerial manipulation tasks necessitate a reliable trajectory planning algorithms to perform complicated tasks. This paper provides a method of developing the locally optimal trajectory for aerial manipulation in constrained environments. We first show differential flatness of the aerial manipulation system when the inertial effect due to equipped robotic arm is compensated with the aid of a robust controller. To find the locally optimal path, we parameterize flat outputs as polynomials of time, and formulate a sequential quadratic programming (SQP) problem. Given a convex mesh representation of environment, we obtain a collision-free trajectory in almost real time, by imposing constraints based on a signed distance metric. We also conduct an experiment of operating an object located in a confined space, which validates effectiveness of the proposed algorithm.","inCitations":["e55923fea9eb921ec7b26f25d952468679e6eaf2","6e7cf178242a8b36c092793d3e9b70c138d1afc6"],"pmid":"","title":"Locally optimal trajectory planning for aerial manipulation in constrained environments","journalPages":"1719-1724","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205984"],"entities":["Aerial photography","Algorithm","Coefficient","Convex optimization","Local optimum","Mathematical optimization","Point cloud","Polynomial","Robot","Robotic arm","Robust control","Sequential quadratic programming","Voxel"],"journalVolume":"","outCitations":["a353f74b74da4791703ae6d24d9c503be692ccfb","a2c31661d997c371500cf005d0e0d093dcc45c6c","4741f71553f1b924ac1f62745bf0b0769674d815","8c7e62df4b1e2c44734c901682508d9203662eb7","114465c1a8d507ce729718c3b03885fce151fc20","a0fe90b7d0d791616bac61a9d1e45b9f7633c409","80c3f33f08c56789f9aa9050e5b8ddadec52a2e6","2376078d13761387cabb933798b93a706c2ea7ef","0058ab13855b9c0635ef12738fb810d862828355","640c4f3e5d7b479dab191adce9834b8f32e49eaf","f8afdd3fa0d7763ccdb878a458283550835d2eba","beca967ea2f3df56869c3542958d57cacf19c41a","a06fcb8aaa4f0fdbd4230c4ab6efacddee511ad9","26c25edbcc7224ac8ce80ab5874bf912d78c37f2","6a4d0a8ac2556fd1fd92d7abdd751e8dc28e9de5","7bbe0bd97e8b15cb61fa0168aade78faeb406fca","2e83b6f1d6da2694dd029597911599c03b690afc","5dea60549b709d4fd73a2b42e77d686e989c08f5","684bcd66a887a72fed83f077154083b420b8286d"],"id":"a38952721a16ef2ab85ddd8d2c8a6125cc36c183","s2Url":"https://semanticscholar.org/paper/a38952721a16ef2ab85ddd8d2c8a6125cc36c183","authors":[{"name":"Hoseong Seo","ids":["31509184"]},{"name":"Hyoun Jin Kim","ids":["49717577"]}],"doi":"10.1109/IROS.2017.8205984"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593574","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper considers the combination of temporal logic (TL) specifications and local objective functions to create online, multiagent, motion plans. These plans are guaranteed to satisfy a persistent mission TL specification and locally optimize an objective function (e.g. in this paper, a cost based on information entropy). The presented approach decouples the two tasks by assigning sub-teams of agents to fulfill the TL specification, while unassigned agents optimize the objective function locally. This paper also presents a novel decoupling of the classic product automaton based approach while maintaining satisfaction guarantees. We also qualitatively show that optimality loss in the local greedy minimization due to the TL constraints can be approximated based on specification complexity. This approach is evaluated with a set of simulations and an experiment of 6 robots with real sensors.","inCitations":[],"pmid":"","title":"Distributed Sensing Subject to Temporal Logic Constraints","journalPages":"4862-4868","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593574"],"entities":[],"journalVolume":"","outCitations":[],"id":"17ab35118764e589e43115a6f67f71f750dde211","s2Url":"https://semanticscholar.org/paper/17ab35118764e589e43115a6f67f71f750dde211","authors":[{"name":"Zachary T. Serlin","ids":[]},{"name":"Kevin Leahy","ids":[]},{"name":"Roberto Tronl","ids":[]},{"name":"Calin Beita","ids":[]}],"doi":"10.1109/IROS.2018.8593574"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206497","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The autonomous measurement of tree traits, such as branching structure, branch diameters, branch lengths, and branch angles, is required for tasks such as robotic pruning of trees as well as structural phenotyping. We propose a robotic vision system called the Robotic System for Tree Shape Estimation (RoTSE) to determine tree traits in field settings. The process is composed of the following stages: image acquisition with a mobile robot unit, segmentation, reconstruction, curve skeletonization, conversion to a graph representation, and then computation of traits. Quantitative and qualitative results on apple trees are shown in terms of accuracy, computation time, and robustness. Compared to ground truth measurements, the RoTSE produced the following estimates: branch diameter (mean-squared error 0.99 mm), branch length (mean-squared error 45.64 mm), and branch angle (mean-squared error 10.36 degrees). The average run time was 8.47 minutes when the voxel resolution was 3 mm3.","inCitations":["d7529060e2bb14af4256ef15ef9275e8cb0982ce","89276080d080cf03d37e57570d8b2e197cecefb8","1f5b0caec136778decf7689cb090cb1911c2263f","13284a97bac0da1636380a1ee88c11f08cf895d4","7143770ba633df5f7e41d0e67b55148983954fe8"],"pmid":"","title":"A robotic vision system to measure tree traits","journalPages":"6005-6012","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206497","http://arxiv.org/abs/1707.05368","https://arxiv.org/pdf/1707.05368v2.pdf"],"entities":["Autonomous robot","Computation","Graph (abstract data type)","Ground truth","Larry Laffer","Lawrence M. Breed","Mean squared error","Mobile robot","Online and offline","Real life","Robustness (computer science)","Run time (program lifecycle phase)","Supercomputer","Time complexity","Trellis quantization","Voxel"],"journalVolume":"","outCitations":["11dfb746ef4a520ceb2ab1ffa064118de000c20f","a45dbb9bc3997aece2c4f2b6e7c4665eb2b5b9da","16853869023edd837c9f72d9116fbb511966a6b2","ca1e493e548276b10f65b0cd82b9e39dcfaa0f09","93d983728a71c2b121495729788cc7c1871e3f66","1aebbf41899d92568e24e446d0f11ce1a4b546e1","ec7c70892895b91b7ad12e53f5285e75676400bb","c3e87989e88b12d1ed67b8953874580fe2c05f00","761e5ea2bc84eb2942fe6bb8bd2c69914504305f","3bb490808f3c7692684a60b3e3997b935957fc06","974d1d5a3c692c187142924d85b4feb73240abb0","c5338dc6c2e9377dd0b85878109556ddf08f1eac","1a3e542b908e6af7923b04d4738e45e5bac10dcb","8d4dff7435cdab4db940a0855258fdc8493e7064","a0aaedd253249059252937b53d98b2c958a7ac3c"],"id":"4ee7c4862cf6dbb3551ab02e00ce9d966b23d801","s2Url":"https://semanticscholar.org/paper/4ee7c4862cf6dbb3551ab02e00ce9d966b23d801","authors":[{"name":"Amy Tabb","ids":["2865636"]},{"name":"Henry Medeiros","ids":["2767912"]}],"doi":"10.1109/IROS.2017.8206497"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205994","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Wearable robotic systems which aid or enhance human performance are the subject of substantial ongoing research efforts. Soft exosuits to assist with walking represent one class of system that leverage textiles and apparel to provide a lightweight and nonrestrictive means to interface to the lower extremity and apply assistive joint torques via a cable that applies a force across a biological joint. Current embodiments of the soft exosuit use off-the-shelf load cells attached to the distal end of a Bowden cable to measure and control the delivered force. As these systems evolve, we envision replacing all rigid components with compliant, textile-based components. Here we present a new force sensing concept suitable for exosuit applications. The approach is based on micro-machined carbon fiber composite structures encapsulated in elastomer materials as the transducer element, and high-strength, stiff textiles as the load bearing element. The transduction mechanism uses the Poisson effect in the encapsulating elastomer to create electrical contact between the carbon fiber structures. This method allows the sensor to be sensitive in tension while remaining insensitive to bending deformation. We fabricate a prototype sensor capable of detected forces up to 300 N, yet weighing just 2.15 g. The compliant nature of the materials used in fabrication allow the sensor to be flexible. The sensor output is qualitatively very repeatable, yet exhibits moderate drift at peak load. However this drift begins to stabilize over the test duration. These preliminary results demonstrate the promising potential of this sensor technology for soft exosuit systems.","inCitations":["c5be428978cf3e37de303c6541d34e307d409c58","3ab7dd8c258a6acdb07eb3dc5792cac911d02514","8b71437e962a67cf5ae30097ea5ec46fe2f4174a"],"pmid":"","title":"Hybrid carbon fiber-textile compliant force sensors for high-load sensing in soft exosuits","journalPages":"1798-1803","s2PdfUrl":"","pdfUrls":["http://micro.seas.harvard.edu/papers/Araromi_2017_HybridCompliantForceSensorsForhigh-loadInSoftExosuits.pdf","https://doi.org/10.1109/IROS.2017.8205994"],"entities":["Human reliability","Load profile","Powered exoskeleton","Prototype","Robot","Sensor","Transducer","Transduction (machine learning)","Wearable computer"],"journalVolume":"","outCitations":["8a0df7b97021bcf9d91d8891e7a242136f2bd594","e62f47d4c92f087d1c968e2e898a4d7eca2d78ab","e9ea3587b002ecadea8848baf739aa25b952b9c4","b980ae538e12185ec1c8910557d23da00286527c","2f87f521ee24be32538ae5f176fbbef306e6d20c","d0461105dfe2893535b129192bdbe9df5b43267f","4dd369b955caeea322803d8669555c3caef45235","1c13e92593fa6637037dd94246abaa329f93459f","793359742815e8e0e8af6cf4d2c4515569e6eedf","83208efd3a37fabc6a217a07cf852c4d3002aac7","ff9d79955484e0e1d8f1b01acec7d778191cfcca","59814833ec8106b84918cfddcf17a3932fb31f43","173c44a59e6004e114b8eb8748607765d81afbf5"],"id":"23fc514cf81d762a04cf0212d4ebb94a07927e1a","s2Url":"https://semanticscholar.org/paper/23fc514cf81d762a04cf0212d4ebb94a07927e1a","authors":[{"name":"Oluwaseun A. Araromi","ids":["5475430"]},{"name":"Conor J. Walsh","ids":["2184037"]},{"name":"Robert J. Wood","ids":["1706886"]}],"doi":"10.1109/IROS.2017.8205994"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593887","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Suction is an effective method for picking various objects because it makes trajectory planning and control easy. However, suction has not been used due to misalignment and leakage of suction air when handling a variety of shapes. We therefore develop a hand to handle these characteristics. First, we model the vacuum pump and pad characteristics to allow evaluation of momentum and suction force in the case of leakage. Utilizing this, we select a configuration suitable for the items in the Amazon Robotics Challenge 2017. In addition, we design a mechanism for switching from suction to pinching for grasping items that cannot be sucked. Moreover, robust pinching is made possible by equipping the fingertips with a passive linear motion mechanism. In the Amazon Robotics Challenge 2017, it was shown possible to stably grasp items with irregularities and items with large moments. Furthermore, items that cannot be grasped by suction can also be grasped robustly by switching to the pinching mechanism.","inCitations":[],"pmid":"","title":"A Gripper System for Robustly Picking Various Objects Placed Densely by Suction and Pinching","journalPages":"6093-6098","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593887"],"entities":[],"journalVolume":"","outCitations":[],"id":"4b63f8e3800765130f512a3b6512871b6cdc8e9d","s2Url":"https://semanticscholar.org/paper/4b63f8e3800765130f512a3b6512871b6cdc8e9d","authors":[{"name":"Hideichi Nakamoto","ids":[]},{"name":"Masashi Ohtake","ids":[]},{"name":"Kazuma Komoda","ids":[]},{"name":"Atsushi Sugahara","ids":[]},{"name":"Akihito Ogawa","ids":[]}],"doi":"10.1109/IROS.2018.8593887"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202309","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Agile MAVs are required to operate in cluttered, unstructured environments at high speeds and low altitudes for efficient data gathering. Given the payload constraints and long range sensing requirements, cameras are the preferred sensing modality for MAVs. The computation burden of using cameras for obstacle sensing has forced the state of the art methods to construct world representations on a per frame basis, leading to myopic decision making. In this paper we propose a long range perception and planning approach using cameras. By utilizing FPGA hardware for disparity calculation and image space to represent obstacles, our approach and system design allows for construction of long term world representation whilst accounting for highly non-linear noise models in real time. We demonstrate these obstacle avoidance capabilities on a quadrotor flying through dense foliage at speeds of up to 4 m/s for a total of 1.6 hours of autonomous flights. The presented approach enables high speed navigation at low altitudes for MAVs for terrestrial scouting.","inCitations":["9057a9536a761e194691a23a983d6d5f0aab681f","7c530519666e8908ccda9e8130c78ddf19fb161a","2bfdc2f72ba9f543d4a06eb511f49293591c9d6a"],"pmid":"","title":"DROAN \u2014 Disparity-space representation for obstacle AvoidaNce","journalPages":"1324-1330","s2PdfUrl":"","pdfUrls":["http://ri.cmu.edu/wp-content/uploads/2017/08/root1.pdf","http://ri.cmu.edu/wp-content/uploads/2017/08/cmuthesis_gd.pdf","https://doi.org/10.1109/IROS.2017.8202309"],"entities":["Agile software development","Autonomous robot","Binocular disparity","Computation","Course (navigation)","Field-programmable gate array","Library (computing)","Nonlinear system","Obstacle avoidance","Requirement","Systems design","Terrestrial television","While"],"journalVolume":"","outCitations":["1c63279c8643c0edb2af450d59b28f8dbdf65f1d","8ac317fd410e0110cafbd0004d917f909976128d","36515a1049836e095ca192424bfd59e1501ce9cd","a717150b8c7a464bf8130c772db8b38499d79c23","f1dab2ad43935b9f98b1e368c40f179033a09e11","2452861b54918b36e4afe913aa46502c0557e1cf","78b4777451f8c01e604237f8c800ac0fd6a5216b","79ba9b1e1b16f166da84e206b71bbb4e6b9be9c5","a696a817cbde92e4d5c4a216875831e74ee6c30c","7a564f5ca6dc361000b358ca2dc85fb93a871825","45d3e574ee9aaa8a5440a583926eab1a615b7835","92f66d26dc52d4b9bde8684520a3dddfb3bd8d36","bee6bc8da99540117b49d333546dc7932fa0952f"],"id":"40d679dc40e5737251734d329c72649e459338b5","s2Url":"https://semanticscholar.org/paper/40d679dc40e5737251734d329c72649e459338b5","authors":[{"name":"Geetesh Dubey","ids":["2032994"]},{"name":"Sankalp Arora","ids":["3248107"]},{"name":"Sebastian Scherer","ids":["32634992"]}],"doi":"10.1109/IROS.2017.8202309"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593645","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a probabilistic collision threat assessment algorithm for autonomous driving at road intersections that assesses a given traffic situation at an intersection reliably and robustly for an autonomous vehicle to cross the intersection safely, even in the face of violation vehicles (that is, vehicles in violation of traffic rules at the intersection). To this end, the proposed algorithm employs a detailed digital map to predict future paths of observed vehicles and then utilizes the predicted future paths to identify potential threats (vehicles) and potential collision areas, regardless of whether observed vehicles are obeying traffic rules at the intersection. Next, by means of Bayesian networks and time window filtering under an independent and distributed reasoning structure, it assesses the potential threats regarding the possibility of collision reliably and robustly, even under uncertain and incomplete noise data. Then, it has been tested and evaluated through in-vehicle testing on a closed urban test road under traffic conditions inclusive of non-violation and violation vehicles. In-vehicle testing results show that the performance of the proposed algorithm is sufficiently reliable to be used in decision-making for autonomous driving at intersections in terms of reliability and robustness, even in the face of violation vehicles.","inCitations":[],"pmid":"","title":"Probabilistic Collision Threat Assessment for Autonomous Driving at Road Intersections Inclusive of Vehicles in Violation of Traffic Rules","journalPages":"4499-4506","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593645"],"entities":[],"journalVolume":"","outCitations":[],"id":"735442655ef7c56f9987ce1e7d9cff23f49a2492","s2Url":"https://semanticscholar.org/paper/735442655ef7c56f9987ce1e7d9cff23f49a2492","authors":[{"name":"Samyeul Noh","ids":[]}],"doi":"10.1109/IROS.2018.8593645"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594344","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We present a human-subjects study approach that supports the analysis of the manipulation performance of robotic hands that have the same morphology but different actuation and compliance. Specifically, we use this approach to analyze three different types of hands (one underactuated, one fully actuated, one fully actuated with compliant distal joints) as they are used to perform two manipulation tasks. The first task uses a power grasp (spraying with a spray bottle), the second a precision grasp (tracing a line on a bowl with a pen). We show that compliance in the distal joints significantly improves performance and task completion. We also show that humans choose significantly different poses for the same task when using a fully-actuated versus underactuated hand, which also results in superior task performance. Our results suggest that humans use a combination of under-actuated and fully-actuated techniques, which when used on robotic systems would also improve their performance on manipulation tasks.","inCitations":[],"pmid":"","title":"Using human studies to analyze capabilities of underactuated and compliant hands in manipulation tasks","journalPages":"2949-2954","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594344"],"entities":[],"journalVolume":"","outCitations":[],"id":"bff18565b9715582d0f2d4aeed7e7b109c7bff1b","s2Url":"https://semanticscholar.org/paper/bff18565b9715582d0f2d4aeed7e7b109c7bff1b","authors":[{"name":"John Morrow","ids":[]},{"name":"Ammar Kothari","ids":[]},{"name":"Yi Herng Ong","ids":[]},{"name":"Nathan Harlan","ids":[]},{"name":"Ravi Balasubramanian","ids":[]},{"name":"Cindy Grimm","ids":[]}],"doi":"10.1109/IROS.2018.8594344"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593387","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Catheter/guidewire manipulation in endovascu-lar intervention procedures are associated with risks of injury on vessel wall and embolization. Determination of catheter/guidewire-vessel interaction contact forces can improve the navigation process safety and efficiency which prevent injuries in both manual and robotic vascular interventions. This study proposes a sensor-less sensing solution to estimate multiple contact point forces at the side of catheter/guidewire exerted on the vasculature. This goal is achieved by using image feedback of catheter-vessel interaction and numerical finite element modeling (FEM). Real-time image processing algorithms are implemented to track interaction contact points on catheter/guidewire. Image-based deflection measurement and contact points tracking data are given to a nonlinear finite element beam model to estimate the forces. The variable equivalent bending modulus of the guidewire is found through a series of three-point-bending tests. To directly measure contact point forces, an experimental platform is prepared which simulates catheter/guidewire-vessel interaction with two, three and four contact points. The effectiveness of the proposed approach is tested in six scenarios in which force estimation accuracy of more than 87.9% is achieved. The proposed approach can be applied to various types of under-actuated catheter/guidewire in endovascular intervention procedures. This study proves that multiple catheter/guidewire side contact forces can be estimated by using the deflected shape and equivalent bending modulus property without embedding any force sensor.","inCitations":[],"pmid":"","title":"A Sensor-less Catheter Contact Force Estimation Approach in Endovascular Intervention Procedures*","journalPages":"2100-2106","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593387"],"entities":[],"journalVolume":"","outCitations":[],"id":"2151d3ac0a463d17c18868c0b0b4a50e757aaa51","s2Url":"https://semanticscholar.org/paper/2151d3ac0a463d17c18868c0b0b4a50e757aaa51","authors":[{"name":"Masoud Razban","ids":[]},{"name":"Javad Dargahi","ids":[]},{"name":"Benoit Boulet","ids":[]}],"doi":"10.1109/IROS.2018.8593387"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206391","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We present a system for creating object models from RGB-D views acquired autonomously by a mobile robot. We create high-quality textured meshes of the objects by approximating the underlying geometry with a Poisson surface. Our system employs two optimization steps, first registering the views spatially based on image features, and second aligning the RGB images to maximize photometric consistency with respect to the reconstructed mesh. We show that the resulting models can be used robustly for recognition by training a Convolutional Neural Network (CNN) on images rendered from the reconstructed meshes. We perform experiments on data collected autonomously by a mobile robot both in controlled and uncontrolled scenarios. We compare quantitatively and qualitatively to previous work to validate our approach.","inCitations":["950cfcbaafad1e2aaae43728fe499d8a4c90f6ec","13388622dee8204987729d9983e42bd0a57f848f","24f07dbb2abe3dc8f6d2ad473dac31109550259a"],"pmid":"","title":"Autonomous meshing, texturing and recognition of object models with a mobile robot","journalPages":"5071-5078","s2PdfUrl":"","pdfUrls":["http://www.diva-portal.org/smash/get/diva2:1147195/FULLTEXT01.pdf","https://doi.org/10.1109/IROS.2017.8206391"],"entities":["Convolutional neural network","Display resolution","Emoticon","Experiment","Mathematical optimization","Mobile robot","Point cloud","Super-resolution imaging","Synthetic data","Texture mapping","Uncontrolled format string"],"journalVolume":"","outCitations":["a8a618363b8dee8037df9133668ec8dcd532ee4e","0191eed95c6bf8cb4b2cbb3b22942d39dc1c969e","20035f36d8d21207170ca2a6c15623ca056a8641","0b7e30218edc55b2fb0d85e5d49d9acd2eaeec9e","19ec2006b1ad38f2f2b2da3f94fb74fc594e1225","6137bc38172b9e0f0f5fe28a1276e6bc73115b10","b75419a5530cf1ba417078142c05d9b043ad56db","b4488bb6787bd2546ba25efe069f6a20db08c06a","3b2697d76f035304bfeb57f6a682224c87645065","41984604bf962ad33c8c07dec557945671246b74","cc1d935abeed32368ab7e78bd39ffa18e160f312","087d87eb6fc37245b7867cde27235c2f23574949","3408d4aae185abee38ff3d8e47e32d351462d19f","2fcf7e718a300a222017912255574a750f052f11","2e17630fd79058d4d4e1628eb81a2f3ccaa20296","12b0713eb6ec54a6d8f6a40bfe512947d42962e7","74ece1eea3372af82047e1fbfcadc8a42e12405f","0626908dd710b91aece1a81f4ca0635f23fc47f3","655b14a3a3033182a27aa01d5cb16b99f0419a70","2c03df8b48bf3fa39054345bafabfeff15bfd11d"],"id":"4df911aba8c24a32b24308e51694f7bb0027ef65","s2Url":"https://semanticscholar.org/paper/4df911aba8c24a32b24308e51694f7bb0027ef65","authors":[{"name":"Rares Ambrus","ids":["1829964"]},{"name":"Nils Bore","ids":["2180336"]},{"name":"John Folkesson","ids":["3248522"]},{"name":"Patric Jensfelt","ids":["1770066"]}],"doi":"10.1109/IROS.2017.8206391"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.64","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"We propose a ConvNet model for predicting 2D human body poses in an image. The model regresses a heatmap representation for each body keypoint, and is able to learn and represent both the part appearances and the context of the part configuration. We make the following three contributions: (i) an architecture combining a feed forward module with a recurrent module, where the recurrent module can be run iteratively to improve the performance; (ii) the model can be trained end-to-end and from scratch, with auxiliary losses incorporated to improve performance; (iii) we investigate whether keypoint visibility can also be predicted. The model is evaluated on two benchmark datasets. The result is a simple architecture that achieves performance on par with the state of the art, but without the complexity of a graphical model stage (or layers).","inCitations":["332339c32d41cc8176d360082b4d9faa90dadffa","45e7ddd5248977ba8ec61be111db912a4387d62f","ed173a39f4cd980eef319116b6ba39cec1b37c42","2d2569aed63250b8e2dc305e28edee613a3cdbac","61cdc6f0dda2aa42f0a8cbda9ca82e1f6c64ce3c","54ed052738ca0f4570c74931857b3275fca9993b","e22e848d3bbe8e75bba09203b70bdb1324dad471","30b74c53bd7a9b364920e5074b52b3f737a71c89","cd87fea30b68ad1c9ebcb71a224c53cde3516adb","971c76d8d896ae575a6c3f17b035914b5b65bc45","8dc55e20135d9c24af2ea57c7aa213814cfc2bf0","ccf74a4aa3f136a93627352f9b90de5ff4964962","b999364980e4c21d9c22cc5a9f14501432999ca4","20b038c50cc7148dfb364e2de51cde120c907c9f","45e8ef229fae18b0a2ab328037d8e520866c3c81","db44c8572d082fdcb62d899eb1f292acbf05cbb2","2d069885943862c4ba59910b5ea46496c63c2dee","1deb7f96fc92d5c9e04d2cbb277473fee878e144","f568eff0b3d8b9ae527e6b4483e2bc2ce5fd01bb","c3da60330497cb948ce1741f1624347ee7acfceb","81eb804756f27d08f2d193d1074e58e1c5d263ca","0209389b8369aaa2a08830ac3b2036d4901ba1f1","a345fc597b15c26f3f2823ccd5aac0d4c976279e","56d3df5ce2ffb695728c091252087979be31f0c7","303e7b842a5f4210f83e5fea9f764767d546c9a3","8189e4f5fc09ae691c77bbd0d4e09b8853b02edf","ebd28f04c2ab1e61430d309ecbf7c832173d65a5","64bd5878170bfab423bc3fc38d693202ef4ba6b6","2707f0f56b63c6ac472c37d3e40388e433e752b5","fa747db22e9e6cd7a64019eec6e0dd53e94be4b3","f26a8dcfbaf9f46c021c41a3545fcfa845660c47","571a29d324cbacd036d8a30ffd0f586ba128a10e","3adbf4ed5e4e3f59afb7509119667c8701c7cf37","100c452a508711c11ab643d52e54c441cf58403e","f57176165d2731fc40ef795e2babb061fac09112","a92b5234b8b73e06709dd48ec5f0ec357c1aabed","3884b78a06ccfde3249c16ac450b5254d033126a","6111832ed676ad0789d030577c87d4a539242bd3","4543052aeaf52fdb01fced9b3ccf97827582cef5","0a3651647cb44b87ec8373ab4a1b53e2ac352bc2","3d2891950f1b76f783a9ba77b3c55b8e68b95fbe","1f4aa88107d7c4b91b1436b721b7630b93ce7d06","6dda02b42e417a033b8398f9ed8ef84750c90fdc","6c38ab65df4a1bf546f1426e8a7f2f5cb5f765d3","0cb5a80d9ddc9876c73b6013fa927a8202736349","0b2c68dac4625771e23c5432acb938592e525276","0b5519f76fc8e31ecf9931f00184aee86694e3a4","790fb309a30e3d96efdeb5b16a29a79b7a67aa24","0a81810af97e8ab5b8c483209b4d0ff7210436f9","5ad0e283c4c2aa7b9985012979835d0131fe73d8","e4f032ee301d4a4b3d598e6fa6cffbcdb9cdfdd1","68edd8add77c99c60d50aa10ed94173e84307319","53695e2dfec6737d585dd4e53757f73d18bd5e01","d38af10096aa90dfccd7e4cec9757900bf6958bd","3254692e2794ef8c8f96374aadb27c3f3926492e","85d8c16ccd76e2eec303f98f2d1ab239dc3947a2","606e920681b6bd2910a1cccda2403ba7e361a3a9","2031b062f4c41f43a32835430b1d55a422baa564","4209f140d64ce6fb891eb6ada26eaeb40af123e2","9eec12c62b44d40909bb0f8dde325654c83f29e1","69f097ceab54d580e7cc8eb52ee79ad2182c2686","6c8420b40eb13ae209f72125d339963805f4ff90","1efaa128378f988965841eb3f49d1319a102dc36","16ee8df3c0202a2756117ab0686cffda65362a4c","2bf88f01fb9c2c9c227b354504b2695e340efeec","3aea679168c72c6df7ead45d4f7f1fd7f3680a11","1b73bd672c6abe6918f91812f4334db23189d1d6","3bb670b2afdcc45da2b09a02aac07e22ea7dbdc2","db6017d7f86188484f7927c3844f27088cb396ba","4065d038ecbda579a0791aaf46fc62bbcba5b1f3","e19fbcd13f6b42ece8cc95de638c83a470eafd7a","0f4724cc069609a9544ca7d9a429b52cfe89c182"],"pmid":"","title":"Recurrent Human Pose Estimation","journalPages":"468-475","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1605.02914v2.pdf","http://www.robots.ox.ac.uk/~vgg/publications/2017/Belagiannis17/belagiannis17.pdf","http://www.robots.ox.ac.uk/~vgg/publications/2017/Belagiannis17/belagiannis17.pdf.pdf","https://arxiv.org/pdf/1605.02914v3.pdf","http://arxiv.org/abs/1605.02914","https://au.arxiv.org/pdf/1605.02914.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2017.64","http://arxiv.org/pdf/1605.02914v1.pdf"],"entities":["3D pose estimation","Benchmark (computing)","Convolutional neural network","End-to-end principle","Graphical model","Heat map"],"journalVolume":"","outCitations":["8283cc4fd5218e2a818324214c79682bc684626d","2e3f24d9cdba1d0343248a81c13bca96db123c21","719da2a0ddd38e78151e1cb2db31703ea8b2e490","2830fb5282de23d7784b4b4bc37065d27839a412","188d823521b7d00abd6876f87509938dddfa64cf","195d331c958f2da3431f37a344559f9bce09c0f7","2d02cf53bc0f2d919b89bec8f9160b50916bb625","0cfab1c2839ddacc19bc9af2e821d5c5fd4f28c1","5793b25e2492d47f5faf9b93b8c0fe36802de8b6","00f17fca3cf3ab4262edde3626e6230a89ff1a1f","14318685b5959b51d0f1e3db34643eb2855dc6d9","3325860c0c82a93b2eac654f5324dd6a776f609e","0e4d1eb8f1ea8484795e6683d63f6b66811324b1","22db07c472b2d7bc7704b8c2bbd8f620e2e68ca9","3615bbdd4fe81acd9e5d166af731b5556b19a2cd","26c9e57116061594ef843141a6a8bc49759f766c","25a7ea26685c856a10f47243e10c5f8be384d320","098a0bd7c948e9c94704ac5e8c768c8d430e1842","08847df8ea5b22c6a2d6d75352ef6270f53611de","148a5fa66480afa7744409cde659f79c7c9b3fdc","ec7cd3fff8bdbbe7005bc8d6b7f6b87d72aac2d9","1827de6fa9c9c1b3d647a9d707042e89cf94abf0","6f37b6da734c2a13d2860bc5a23e809148b39bd7","6801c8ea1fcb2f76799234a9a81c6199dd61b24c","016dd886d5cb01c55a0204e2988274cf9417b564","34d21e6bf1586715e233248dee787afc66a1fbe7","06f7e0aee7fc5807ab862432a4e5ade2cda73c4b","573c11e7e00389a033787984223ced536e15c904","4e4746094bf60ee83e40d8597a6191e463b57f76","02a88a2f2765b17c9ea76fe13148b4b8a9050b95","270f029b03ee1bdfeae4ff4c5167b450d185a981","25f5df29342a04936ba0d308b4d1b8245a7e8f5c","0ba6f4fb548d8289fb42d68ac64d55f9e3a274ca","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","052b1d8ce63b07fec3de9dbb583772d860b7c769","1b2c6fb85e7e776b533fda63f92fabb04ed5e887","061356704ec86334dbbc073985375fe13cd39088","3f3a483402a3a2b800cf2c86506a37f6ef1a5332"],"id":"2dd46b83a1cf5c7c811a462728d9797c270c2cb4","s2Url":"https://semanticscholar.org/paper/2dd46b83a1cf5c7c811a462728d9797c270c2cb4","authors":[{"name":"Vasileios Belagiannis","ids":["1882784"]},{"name":"Andrew Zisserman","ids":["1688869"]}],"doi":"10.1109/FG.2017.64"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594252","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Skin surface wounds due to burns, surgeries and chronic illness affect millions of people worldwide. Tissue engineering has become an increasingly popular treatment, but it is a highly manual process. Increasing the automation in tissue engineering could increase the rate of treatment for patients and improve outcomes. We present an initial investigation into an automated in-situ treatment. In our proposed method, a 3D machine vision system detects a skin wound to be treated and then determines the 3D point set corresponding to the wound. The 3D point set is then passed to path planning algorithm for a robot manipulator to move an ink-jet nozzle over the wound and fill the cavity with quick-curing/gelling fluids such collagen and other biomaterials and cell growth promoters. This paper details initial results and experimental validation of each of the proposed steps.","inCitations":[],"pmid":"","title":"A Robot System for Automated Wound Filling with Jetted Materials","journalPages":"1789-1794","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594252"],"entities":[],"journalVolume":"","outCitations":[],"id":"cff939d10959fda966f51ba1161fdbda16616523","s2Url":"https://semanticscholar.org/paper/cff939d10959fda966f51ba1161fdbda16616523","authors":[{"name":"Bashir Hosseini Jafari","ids":[]},{"name":"Namhyung Lee","ids":[]},{"name":"Rachael Thompson","ids":[]},{"name":"Jackson Schellhorn","ids":[]},{"name":"Bogdan Antohe","ids":[]},{"name":"Nicholas R. Gans","ids":[]}],"doi":"10.1109/IROS.2018.8594252"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546302","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"How to best understand and interpret the decisions of deep neural networks is a crucial topic, as the impact of intelligent deep network systems is prevalent in many applications. We propose a superpixel based method to interpret and explain the results of black-box deep networks in the widely-applied image classification tasks. We perform probabilistic prediction difference analysis upon one or more superpixels clustered from image pixels. Our method generates a superpixel score map visualization that can provide rich interpretation regarding image components. Such interpretation provides supportive/unsupportive likelihood of image regions upon the decisions performed by the black-box classifier. We compare our method against state-of-art pixelwise interpretation methods over the latest deep neural network classifiers on the ImageNet dataset. Results show that our method produces more consistent interpretations in less computation time. Our method also supports interactive interpretation, where users can acquire explanations on specified regions through a convenient interface for a prompt reaction.","inCitations":[],"pmid":"","title":"Explain Black-box Image Classifications Using Superpixel-based Interpretation","journalPages":"1640-1645","s2PdfUrl":"","pdfUrls":["https://www.albany.edu/faculty/mchang2/files/2018_08_ICPR_InterpretDNN.pdf","https://www.albany.edu/faculty/mchang2/files/2018_08_ICPR_InterpretDNN_poster.pdf","http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546302"],"entities":[],"journalVolume":"","outCitations":["dbf777403156adda2d551a973379edd3e2bc5aaa","dbddd2b8dfc0efdaa895f1bb0f84b74f3f0abd30","8c4c723a74fe479c2b8af7d911817377dd6d85c9","266b5b038750e1ab1311e38554e4c2c8ba6564fd","376b078694f0c183e4832900debda4dfed021a9a","3b2697d76f035304bfeb57f6a682224c87645065","5636dca44384240ce9aff2b10b78458cd3c2f450","8ac2eb5ae591218a5e9bb56ffe59e6889c97bdeb","056713e422a0753c5eb1733d73e9f8185e2015d4","14b5e8ba23860f440ea83ed4770e662b2a111119","14318685b5959b51d0f1e3db34643eb2855dc6d9","24ab513a04791708b56699f253b5d3315cc7fb4e","1f8f64382681fb92da07bac2d7b5bdd136c6f1fe","39138cc9431cdd95ad5ea7aa074946c9bd086b24","501d99e392783e4acafb220136d32ea68a921282","2c03df8b48bf3fa39054345bafabfeff15bfd11d","061356704ec86334dbbc073985375fe13cd39088"],"id":"80f2664ad917c6206262457d3175dcdd700cbf63","s2Url":"https://semanticscholar.org/paper/80f2664ad917c6206262457d3175dcdd700cbf63","authors":[{"name":"Yi Wei","ids":["49019561"]},{"name":"Ming-Ching Chang","ids":["39643145"]},{"name":"Yiming Ying","ids":["38954213"]},{"name":"Ser-Nam Lim","ids":["51281726"]},{"name":"Siwei Lyu","ids":["1794837"]}],"doi":"10.1109/ICPR.2018.8546302"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00072","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Social robots are now part of human society, destined for schools, hospitals, and homes to perform a variety of tasks. To engage their human users, social robots must be equipped with the essential social skill of facial expression communication. Yet, even state-of-the-art social robots are limited in this ability because they often rely on a restricted set of facial expressions derived from theory with well-known limitations such as lacking naturalistic dynamics. With no agreed methodology to objectively engineer a broader variance of more psychologically impactful facial expressions into the social robots' repertoire, human-robot interactions remain restricted. Here, we address this generic challenge with new methodologies that can reverse-engineer dynamic facial expressions into a social robot head. Our data-driven, user-centered approach, which combines human perception with psychophysical methods, produced highly recognizable and human-like dynamic facial expressions of the six classic emotions that generally outperformed state-of-art social robot facial expressions. Our data demonstrates the feasibility of our method applied to social robotics and highlights the benefits of using a data-driven approach that puts human users as central to deriving facial expressions for social robots. We also discuss future work to reverse-engineer a wider range of socially relevant facial expressions including conversational messages (e.g., interest, confusion) and personality traits (e.g., trustworthiness, attractiveness). Together, our results highlight the key role that psychology must continue to play in the design of social robots.","inCitations":["67ad22cd25095c2602d46ce4cb110b46bb5d1e81"],"pmid":"","title":"Reverse Engineering Psychologically Valid Facial Expressions of Emotion into Social Robots","journalPages":"448-452","s2PdfUrl":"","pdfUrls":["http://eprints.gla.ac.uk/159228/7/159228.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2018.00072"],"entities":["Interaction","Reverse engineering","Robotics","Social robot","Trust (emotion)","User-centered design"],"journalVolume":"","outCitations":["74bf7de4a5e4b9af9c615dcd0698648fc66b4f45","5db87645bd7c5d943b156f1fd38b41dcf3cff889","adca5a4cd8c0741ed241bb556a6b6646d2e30ac3","4880434b8efc9a73db45b47b89f04df533420b51","d1708075f2042e20cad45e014b5353fad9cd2d04","74052224879bbdf1323c3bde8ad2cac85912bdd5","f47e7253f0763579c6c045cd3fa5b34b0697f254","bcc142fa6ef82d97e807500f0d4b13239ea89f7e","a288d505c8a14c2383a2aaecd5bf0dbb1cace79d","34b4c12146e6aab1309bfe91a86ff3fa76d1677a","9a9a7d06d71d2aeb8a950aeb844b6c82eaf21ab6","4d9a02d080636e9666c4d1cc438b9893391ec6c7","353fe6fd56ecaf186a5d3e3364b4eb06d98bfdb7","65c9b2cdf9ba8ba5696170a82fb5465ff1c3d4ea","128dac550f872b297c95ebda9285ec98da9a925c","496b959c9fc6d67c5ee5043da1cc500096e0c293","21dff32ff0f6b1244ce9c4aad2f468889748fbb2","2b736624b79e43d9296cdad936fc189f67f85398","76ef81e98214069e248740db7bd621536b91c544","a41d3f3793b6d5bec740df35932a38960c986eac","0be253dcc5547acf377062c0f2b4f455ebf04063","bf2437acde72b1b1aec33fd44fdc60f96ce78f6f"],"id":"00cb35cadee5c1ea11bd4c37cdfd234859c69617","s2Url":"https://semanticscholar.org/paper/00cb35cadee5c1ea11bd4c37cdfd234859c69617","authors":[{"name":"Chaona Chen","ids":["6416348"]},{"name":"Oliver G. B. Garrod","ids":["48522841"]},{"name":"Jiayu Zhan","ids":["4152104"]},{"name":"Jonas Beskow","ids":["1826819"]},{"name":"Philippe G. Schyns","ids":["2287417"]},{"name":"Rachael E. Jack","ids":["48991162"]}],"doi":"10.1109/FG.2018.00072"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.29","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Face detection in the wild is a challenging vision problem due to large variations and unpredictable ambiguities commonly existed in real world images. Whilst introducing powerful but complex models is often computationally inefficient, using hand-crafted features is hence problematic. In this paper, we propose a nested CNN-cascade learning algorithm that adopts shallow neural network architectures that allow efficient and progressive elimination of negative hypothesis from easy to hard via self-learning discriminative representations from coarse to fine scales. The face detection problem is considered as solving three sub-problems: eliminating easy background with a simple but fast model, then localising the face region with a soft-cascade, followed by precise detection and localisation by verifying retained regions with a deeper and stronger model. The face detector is trained on the AFLW dataset following the standard evaluation procedure, and the method is tested on four other public datasets, i.e. FDDB, AFW, CMU-MIT and GENKI. Both quantitative and qualitative results on FDDB and AFW are reported, which show promising performances on detecting faces in unconstrained environment.","inCitations":["a28adc7e175e03b59a3bec8cdc0172670db55e62","b9c262838db89597a7f9c0cb937cb7d671ab4544"],"pmid":"","title":"Nested Shallow CNN-Cascade for Face Detection in the Wild","journalPages":"165-172","s2PdfUrl":"","pdfUrls":["https://cronfa.swan.ac.uk/Record/cronfa32108/Download/0032108-27052017151854.pdf","http://doi.ieeecomputersociety.org/10.1109/FG.2017.29"],"entities":[],"journalVolume":"","outCitations":["04661729f0ff6afe4b4d6223f18d0da1d479accf","daa7e0cafee740ba91fdba581dea4d273153802c","1606b1475e125bba1b2d87bcf1e33b06f42c5f0d","b0c512fcfb7bd6c500429cbda963e28850f2e948","15cf63f8d44179423b4100531db4bb84245aa6f1","aa23d33983b1abd2d8a677040eb875e93c478a7f","a5f35880477ae82902c620245e258cf854c09be9","009fba8df6bbca155d9e070a9bd8d0959bc693c2","81b7dcaef4a53daab41658a4d1e97972d04b3384","3b2697d76f035304bfeb57f6a682224c87645065","ca6ca00a66795fd6c051364a375bda9901758937","2fd1c99edbb3d22cec4adc9ba9319cfc2360e903","16fc1065c296840cb0f8ca62601aa17b7f0a02bf","887567782cb859ecd339693589056903b0071353","21d4258394a9c8f0ea15f0792d67f7e645720ff6","25d7da85858a4d89b7de84fd94f0c0a51a9fc67a","2f5ae4d6cd240ec7bc3f8ada47030e8439125df2","961a5d5750f18e91e28a767b3cb234a77aac8305","b74493cce2de626739cb1e4cabff8bd5cd0ee0fe","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","8ade5d29ae9eac7b0980bc6bc1b873d0dd12a486","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","04fa47f1d3983bacfea1e3c838cf868f9b73dc58","2f04ba0f74df046b0080ca78e56898bd4847898b","0e986f51fe45b00633de9fd0c94d082d2be51406","9a0024fbad7fcda8af1d241064f0948a8365b969","174930cac7174257515a189cd3ecfdd80ee7dd54","c94b3a05f6f41d015d524169972ae8fd52871b67","7c953868cd51f596300c8231192d57c9c514ae17","0289786e0d5edf663c586c8552ec3708eff62331","4ab10174a4f98f7e2da7cf6ccfeb9bc64c8e7da8","abe9f3b91fd26fa1b50cd685c0d20debfb372f73","76f02eca773414828271da315b379efa9e1f57fa","a53fe4347da39dcf61ac37cee66c945e79a5052e","5e0f8c355a37a5a89351c02f174e7a5ddcb98683","373f76633cc1f6c7a421e31c989842021a52fca4","6748023a06e096a763d17a2e0be878a1ba73eca8","75da1df4ed319926c544eefe17ec8d720feef8c0","a74251efa970b92925b89eeef50a5e37d9281ad0"],"id":"1b331be4b5f0b2023c4ca135683fb5ea99e78247","s2Url":"https://semanticscholar.org/paper/1b331be4b5f0b2023c4ca135683fb5ea99e78247","authors":[{"name":"Jingjing Deng","ids":["6248353"]},{"name":"Xianghua Xie","ids":["2168049"]}],"doi":"10.1109/FG.2017.29"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545238","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a new efficient superpixel-based feature extraction and fusion method on hyperspectral and LiDAR data. Such important factor that the adjacent pixels belong to the same class with high probability is taken into consideration in our method, which means each superpixel can be regarded as a small region consisting of a number of pixels with similar spectral characteristics. In order to represent each superpixel well, we use our Gabor-wavelet-based feature extraction approach instead of morphological APs. A feature selection and fusion process has also been used to reduce the redundancy among Gabor features and make the fused feature more discriminative. The results on the several real dataset indicate that the proposed method provides state-of-the-art classification results, respectively, even when only few samples, i.e., only three samples per class, are labeled.","inCitations":[],"pmid":"","title":"Superpixel-Based Feature Extraction and Fusion Method for Hyperspectral and LiDAR Classification","journalPages":"764-769","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545238"],"entities":[],"journalVolume":"","outCitations":["2d61ff467309c187ae288ce678fbc25edf4fdd30","7df5d095903b7d81f7572c720ebdad79e43b6fb2","0e1431fa42d76c44911b07078610d4b9254bd4ce","df6b864fe4477c5d18e7ee872e19f1755595cbe9","cb1de9461440318dc2520d785fca11a9c5811784","bad222b351fdd9bf65f0a5490f3ec6ee94d7b35a","f028cb94e180a62d62003fe8482051be7292904b","79eb61c015c02fec633b4b5fd33fb3855ecc0b9f","d0d970c28c973af862217f42e4a16cfb456671cf"],"id":"1a5be8b11194b5309810807b4771b83413a5e788","s2Url":"https://semanticscholar.org/paper/1a5be8b11194b5309810807b4771b83413a5e788","authors":[{"name":"Sen Jia","ids":["1805367"]},{"name":"Meng Zhang","ids":["39982497"]},{"name":"Junjian Xian","ids":["51902091"]},{"name":"Jiayue Zhuang","ids":["52197009"]},{"name":"Qiang Huang","ids":["49321457"]}],"doi":"10.1109/ICPR.2018.8545238"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.126","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"An emerging topic in face recognition is matching between facial images acquired from different sensing modalities, referred to as heterogeneous face recognition. Heterogeneous face recognition has the potential to provide key capabilities for the commercial sector as well as for law enforcement, intelligence gathering, and the military, especially in challenging unconstrained settings. However, the difficulty in heterogeneous face recognition is compounded by phenomenology differences between modalities, giving rise to significant facial appearance variations due to the modality gap. In this paper, we focus on a subset of heterogeneous face recognition and present a succinct review of recent work on infrared-to-visible face recognition.","inCitations":[],"pmid":"","title":"Heterogeneous Face Recognition: Recent Advances in Infrared-to-Visible Matching","journalPages":"883-890","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.126"],"entities":["Facial recognition system","Modality (human\u2013computer interaction)"],"journalVolume":"","outCitations":["d80519408e329d6e23f1a642e5362acbee7a9664","589b30ebdb76659ce5d3a19cd9fa0e7a3466d85d","c6d6193c8f611331c8178c3857f9ef92607a4507","319a27b8559e2301c9dbf0462bcf0ca6ad3449ad","4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5","065ff8006dc81a20f2a30cfe31458413ddb2ac5e","362ba8317aba71c78dafca023be60fb71320381d","2a62d0cca2fabf1d6f6ee15e4c14cef415b657d1","9530be236c0bf9c32d39ff35571a496cbd266e01","1ccffaca596c42f42b573924e48b94a5661ff3a0","3f0253832a302730a6d569f4435f9615db321b22","04a73a1d34f6074e6ad7de575c75fee0757031cd","fa00cc84b7e722f80b0ad96bcd553d5f93b24a05","8c7811c029905f4f3e9f31e925634a42e413f6d8","aa914999f26ed180dbf5a5e472e5d3c15d0ecd5b","6cce5ccc5d366996f5a32de17a403341db5fddc6","eddf036049ee24f49d8f23e677d0932c2724589a","55fdff2881d43050a8c51c7fdc094dbfbbe6fa46","a8748a79e8d37e395354ba7a8b3038468cb37e1f","61010eeb7721310f12400e1513efc719ac7db16b","36119c10f75094e0568cae8256400c94546d973b","0d3f27c30228367ef3b1d5116689aff1bf333f64","4c28ca360096d1ebf9a0c1d5ac10bedfa35819b4","f9a2079dedf8f0205d00bbc05cb36d82308a4ba5","8796463a68509e83f45afc2137c276b689bff412","3a1c3307f57ef09577ac0dc8cd8b090a4fe8091f","a6c2791754e845155f074347bde76ba0e385dd7f","b734ce54a94701c4b7b3254269a85b133e89645b","5db20c37c13cfd712f396937446f341b1be8df72","6ea4f04b60a6e954b6dbce69e57ae587669f0c1c","36e4578e29adacc5b44edd3bf9f2a77561b0f2e0","62da5876fbc5b6abe467891fc71b68173e6ad061","853bd61bc48a431b9b1c7cab10c603830c488e39","b632d47eb7421a3d622b0f1ceb009e4415ccc84d","59e9ef8b61182acace9e37f41f9c2a03db69c15b","f3971e8a9b9cf8a0312b3d6b7963dfd58b2931fa","c51fbd2574e488e486483e39702a3d7754cc769b"],"id":"e7144f5c19848e037bb96e225d1cfd961f82bd9f","s2Url":"https://semanticscholar.org/paper/e7144f5c19848e037bb96e225d1cfd961f82bd9f","authors":[{"name":"Shuowen Hu","ids":["2423536"]},{"name":"Nathaniel Short","ids":["14458808"]},{"name":"Benjamin S. Riggan","ids":["2480650"]},{"name":"Matthew Chasse","ids":["40428909"]},{"name":"M. Saquib Sarfraz","ids":["4241648"]}],"doi":"10.1109/FG.2017.126"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594019","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Studying and understanding the computational primitives of our neural system requires for a diverse and complementary set of techniques. In this work, we use the Neuro-robotic Platform (NRP)to evaluate the vestibulo ocular cerebellar adaptatIon (Vestibulo-ocular reflex, VOR)mediated by two STDP mechanisms located at the cerebellar molecular layer and the vestibular nuclei respectively. This simulation study adopts an experimental setup (rotatory VOR)widely used by neuroscientists to better understand the contribution of certain specific cerebellar properties (i.e. distributed STDP, neural properties, coding cerebellar topology, etc.)to r-VOR adaptation. The work proposes and describes an embodiment solution for which we endow a simulated humanoid robot (iCub)with a spiking cerebellar model by means of the NRP, and we face the humanoid to an r-VOR task. The results validate the adaptive capabilities of the spiking cerebellar model (with STDP)in a perception-action closed-loop (r- VOR)causing the simulated iCub robot to mimic a human behavior.","inCitations":[],"pmid":"","title":"Exploring Vestibulo-Ocular Adaptation in a Closed-Loop Neuro-Robotic Experiment Using STDP. A Simulation Study","journalPages":"1-9","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594019"],"entities":[],"journalVolume":"","outCitations":[],"id":"7ebcad326e8c17b9ff93487a14e4b44ae7061b37","s2Url":"https://semanticscholar.org/paper/7ebcad326e8c17b9ff93487a14e4b44ae7061b37","authors":[{"name":"Francisco Naveros","ids":[]},{"name":"Jesás Alberto Garrido","ids":[]},{"name":"Angelo Arleo","ids":[]},{"name":"Eduardo Ros Vidal","ids":[]},{"name":"Niceto R. Luque","ids":[]}],"doi":"10.1109/IROS.2018.8594019"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594157","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper describes a system to map a ground-plane, and to subsequently use the map for localization of a mobile robot. The robot has a downward-facing camera, and works on a variety of ground textures including general texture like tarmac, man-made designs like carpet, and rectilinear textures like indoor tiles or outdoor slabs. Such textures provide a basis for measuring relative motion (i.e. computer mouse functionality). But the goal here is the more challenging one of absolute localization. The paper describes a complete working pipeline to build a globally consistent map of a given ground-plane and subsequently to localize within this map at real-time. Two algorithms are described. The first is a feature-based approach which is general to any ground plane texture. The second algorithm takes advantage of the extra constraints available for common rectilinear textures like indoor tiling, paving slabs, and laid brickwork. Quantitative and qualitative experimental results are shown for mapping and localization on a variety of ground-planes.","inCitations":[],"pmid":"","title":"StreetMap - Mapping and Localization on Ground Planes using a Downward Facing Camera","journalPages":"1672-1679","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594157"],"entities":[],"journalVolume":"","outCitations":[],"id":"60076a567229dcf88f5665af6a19f4bce2afc8bc","s2Url":"https://semanticscholar.org/paper/60076a567229dcf88f5665af6a19f4bce2afc8bc","authors":[{"name":"Xu Chen","ids":[]},{"name":"Anurag Sai Vempati","ids":[]},{"name":"Paul A. Beardsley","ids":[]}],"doi":"10.1109/IROS.2018.8594157"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593762","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"How can high dimensional robots learn general sets of skills from experience in the real world? Many previous approaches focus on maximizing a single utility function and require large datasets of experience to do this, something that is not possible to collect outside of simulation as every data point is expensive both in time and in a potential wear down of the robot. This paper addresses this question using a newly developed framework called Finite Element Goal Babbling (FEGB). FEGB is an online learning method that aims at providing general control over some measurable feature, in contrast to optimizing it to some given utility function. It generalizes standard goal babbling by breaking down the full learning problem into local sub-problems, and combining it with a planner that learns how to navigate between these subproblems. We test FEGB using a real humanoid robot Nao, and find that it could quickly learn to robustly control its body orientation. After only 20\u201330 minutes of training, the robot could freely move into any body orientation between lying on either side and on its back. Rapid learning of body orientation control in high dimensional real robots is largely an unexplored field of robotics, and although many challenges remain, FEGB shows a feasible approach to the problem.","inCitations":[],"pmid":"","title":"Online Learning of Body Orientation Control on a Humanoid Robot Using Finite Element Goal Babbling","journalPages":"4091-4098","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593762"],"entities":[],"journalVolume":"","outCitations":[],"id":"119c01446f9dcace382113e3032442dbe7031c7c","s2Url":"https://semanticscholar.org/paper/119c01446f9dcace382113e3032442dbe7031c7c","authors":[{"name":"Pontus Loviken","ids":[]},{"name":"Nikolas Hemion","ids":[]},{"name":"Alban Laflaquière","ids":[]},{"name":"Michael Spranger","ids":[]},{"name":"Angelo Cangelosi","ids":[]}],"doi":"10.1109/IROS.2018.8593762"}
{"doiUrl":"","venue":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","journalName":"2017 3rd International Conference on Pattern Recognition and Image Analysis (IPRIA)","sources":[],"year":2017,"text":"With the assumption that natural images contain considerable amount of self-similarity, non-local means image de-noising uses patches similarity in order to filter noisy images. Although the output of the Non local means algorithm is very desirable in removing the low level of noise, when the noise increases, the performance deteriorates. This is because the similarity cannot be evaluated perfectly through noisy patches. To solve this problem, in the proposed approach, the similarity evaluation for each patch is performed based on the structural information. This is due to the fact that the HVS (Human Visual System) is highly adopted to extract structural information from a viewing scene. In this paper, a modified non-local means filter is introduced in order to find better similar patches especially in the case of high level of noise. The experimental results show appropriate performance of the presented algorithm both visually and quantitatively.","inCitations":[],"pmid":"","title":"Non-local means denoising based on SVD basis images","journalPages":"206-210","s2PdfUrl":"","pdfUrls":[],"entities":["Algorithm","CNS disorder","Distance","Entity Name Part Qualifier - adopted","High-level programming language","Human body weight","Human visual system model","NL (complexity)","Noise reduction","Non-local means","Patch (computing)","Self-similarity","Singular value decomposition","Snowflake vitreoretinal degeneration","vetispiradiene synthase activity"],"journalVolume":"","outCitations":["0c03ddd72323362bbe45ac99f6b868e482d51829","74cf1618f9469f33b6b59485bfb533a7de40c772","cc53a721320b202503050afe38623643f0784f99","4af1da2b308865e0642d5b81238a91050e0f0d31","267dca58c46d5d8b42d755d4b13f818f67b78675","4c5934eed30ff48f6e0e5baeeee9ff01ceb276bb","3e3b86751060b6d73ba631345bc2ac210ad288d7","74a122d308dc39f261a2c8fd9f6d5ffe3179aa9f","0c2705ca3977d4930ece5327468c3452b06d44e6","52bba8a6f6ac79796d8d93d012197d22def599ff","1294cb51cc2e4d2745c286109e391290c2914bf7","04f7769cf0908101af532b02428928baccb2c4d3","a9291aab826488eeeed46000a6d7d05d735204c4","d7e80709074cd4da6d99961076cd815c0a02b78e","3ab78ff4f233b970d26858df625108b6c1d86ad8","577adfe4e936dbd45a20ca761c69b58c4e9d8326","29a2706df4d788de9b12123f490916a9bc2d6c10","4c9cec89a2c9c8173ee53ab4cda2c021421eb7a5","7e29e8b0bfc4fccda9f2e7435d8f528fda3bf84a","46ab1da601ac2d543cd1c561a2f7cce6c1db8181"],"id":"4364a7e7df6d43a0bfb617e8ba951b8e49dae2e7","s2Url":"https://semanticscholar.org/paper/4364a7e7df6d43a0bfb617e8ba951b8e49dae2e7","authors":[{"name":"Marzieh Seyedebrahim","ids":["20738404"]},{"name":"Azadeh Mansouri","ids":["2972300"]}],"doi":""}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593904","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"Walking ability is critically important for pediatric health, well-being, and independence. Children with cerebral palsy (CP), the most prevalent cause of pediatric physical disability, often present pathological gait patterns that negatively impact walking capacity. Reduced function of the muscles surrounding the ankle joint in those with CP also greatly increases the energy cost of transport leading to reduce mobility. Ankle-foot-orthoses show limited effectiveness for clinically relevant improvement in gait mechanics, while orthopedic surgery, muscle injections and physical therapy are unable to completely restore gait function. While wearable exoskeletons hold promise for gait rehabilitation, appropriately controlling the timing and magnitude of powered assistance across individuals and conditions remains a considerable challenge. This work seeks to address this challenge through the design and initial clinical verification of a simple ankle exoskeleton control scheme designed to reduce the metabolic cost of transport during walking in an individual with CP. Preliminary experimental results from instrumented gait analysis following 5 training visits demonstrated a 45% increase in positive ankle power and a 16% reduction in net metabolic rate during walking with the exoskeleton providing powered plantar-flexion assistance compared to walking without the exoskeleton. Future work will expand this investigation to a larger cohort of individuals with CP and across additional modes of locomotion.","inCitations":["158ddea2854961669b9a0b140072c80522c5f643"],"pmid":"","title":"Verification of a Robotic Ankle Exoskeleton Control Scheme for Gait Assistance in Individuals with Cerebral Palsy","journalPages":"4673-4678","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593904"],"entities":[],"journalVolume":"","outCitations":[],"id":"4b47568a7640fa7161f47d9870a5cfd68b684639","s2Url":"https://semanticscholar.org/paper/4b47568a7640fa7161f47d9870a5cfd68b684639","authors":[{"name":"Gian Maria Gasparri","ids":[]},{"name":"Michael Owen Bair","ids":[]},{"name":"Robert Patrick Libby","ids":[]},{"name":"Zachary Forest Lerner","ids":[]}],"doi":"10.1109/IROS.2018.8593904"}
{"doiUrl":"https://doi.org/10.1109/FG.2017.119","venue":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","journalName":"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)","sources":["DBLP"],"year":2017,"text":"Gender classification is a fundamental and important application in computer vision, and it has become a research hotspot. Real-world applications require gender classification in unconstrained conditions where traditional methods are not appropriate. This paper proposes a Deep Convolutional Neural Network for feature extraction together with fully-connected layers for metric learning. A Siamese network is built for similarity measuring to promote the performance of classification. Extensive experiments on several databases demonstrate that a significant improvement can be obtained for gender classification tasks in both constrained and unconstrained conditions.","inCitations":[],"pmid":"","title":"Metric-Promoted Siamese Network for Gender Classification","journalPages":"961-966","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2017.119"],"entities":["Benchmark (computing)","Computer vision","Convolutional neural network","Database","Experiment","Feature extraction","Java HotSpot Virtual Machine","Statistical classification"],"journalVolume":"","outCitations":["d38b32d91d56b01c77ef4dd7d625ce5217c6950b","0b7c1bcd0289058b5dfc0d3ff114972712bc7f1a","4e061a302816f5890a621eb278c6efa6e37d7e2f","3619a9b46ad4779d0a63b20f7a6a8d3d49530339","247df1d4fca00bc68e64af338b84baaecc34690b","370b5757a5379b15e30d619e4d3fb9e8e13f3256","10d6b12fa07c7c8d6c8c3f42c7f1c061c131d4c5","1be498d4bbc30c3bfd0029114c784bc2114d67c0","8213dbed4db44e113af3ed17d6dad57471a0c048","3d276cfc78c09b434377090a768d26b1ec4afbfe","91d6d25512e3297830b7d8b330f311656db09831","09ee392c7542c0b00ce5df9be3f69b44975cde3b","2704a9af1b368e2b68b0fe022b2fd48b8c7c25cc","0f0fcf041559703998abf310e56f8a2f90ee6f21","81b235d6f0f147d8ae8bb90c6f6e80bbecbc4525","4971439e282db841aa7f82c07ccccd356f4a22a8","2cbb4a2f8fd2ddac86f8804fd7ffacd830a66b58","60cd4ba089d0b078cdac0db311099493b55624d8","14318685b5959b51d0f1e3db34643eb2855dc6d9","4ff4c27e47b0aa80d6383427642bb8ee9d01c0ac","985dccc81905d207c84acd51e0b0d10caceab963","659fc2a483a97dafb8fb110d08369652bbb759f9","b73cdb60b2fe9fb317fca4fb9f5e1106e13c2345","7a76c45fdaaa2756233d00b4b1f2e3a580df9870","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","b1290dff343ae4980e3e853055ad9a5b9116238b","852ff0d410a25ebb7936043a05efe2469c699e4b","061356704ec86334dbbc073985375fe13cd39088","25c3c7baf770d9706745f0a2d26f20653c979504","f4003cbbff3b3d008aa64c76fed163c10d9c68bd"],"id":"b8b9cef0938975c5b640b7ada4e3dea6c06d64e9","s2Url":"https://semanticscholar.org/paper/b8b9cef0938975c5b640b7ada4e3dea6c06d64e9","authors":[{"name":"Yipeng Huang","ids":["2576333"]},{"name":"Shuying Liu","ids":["50152570"]},{"name":"Jiani Hu","ids":["23224233"]},{"name":"Weihong Deng","ids":["1774956"]}],"doi":"10.1109/FG.2017.119"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594195","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This article studies imitation learning policies that encode task compliance to provide teleoperation assistance for remote manufacturing. The central challenge is how to handle off-nominal situations, such as out-of-sequence work or unplanned obstacles, since the assistance has not been trained to handle such scenarios. In such cases, there is potential for the assistance to degrade-rather than improve-operator performance. This work proposes a method that exploits the learned task compliance to classify persistent human actions as off-nominal, and attenuate assistance in these regions. Applied to a hole-cleaning task with $n$ = 11 subjects, the proposed method shows up to 17% reduction in task completion time and up to 68% reduction in forces in off-nominal situations as compared to assistance without the method. Additionally, the method retains the performance improvements of assistance in nominal operating regimes.","inCitations":[],"pmid":"","title":"Managing Off-Nominal Events in Shared Teleoperation with Learned Task Compliance","journalPages":"5509-5516","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594195"],"entities":[],"journalVolume":"","outCitations":[],"id":"410c1d555141f6d0eec9265c41f74b94957584e6","s2Url":"https://semanticscholar.org/paper/410c1d555141f6d0eec9265c41f74b94957584e6","authors":[{"name":"Parker Owan","ids":[]},{"name":"Joseph Garbini","ids":[]},{"name":"Santosh Devasia","ids":[]}],"doi":"10.1109/IROS.2018.8594195"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546088","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Object detection is a core problem in computer vision and pattern recognition. In this paper, we study the problem of learning an effective object detector using weakly-annotated images (i.e., only the image level annotation is given) and a small proportion of fully-annotated images (i.e., bounding box level annotation is given) with curriculum learning. Our method is built upon Faster R-CNN. Different from previous weakly-supervised object detectors which rely on hand-craft object proposals, the proposed method learns a region proposal network using weakly- and semi-supervised training data. And the weakly-labeled images are fed into the deep network in a meaningful order which illustrates from easy to gradually more complex examples with curriculum learning. We name the Faster R-CNN trained using Weakly- And Semi-Supervised data with Curriculum Learning as WASSCL R-CNN. The WASSCL R-CNN is validated on the PASCAL VOC 2007 benchmark, and obtains 90% of a fully-supervised Faster R-CNN's performance (measured using mAP) with only 15% of fully-supervised annotations together with weak supervision. The results show that the proposed learning framework can significantly reduce the labeling efforts for obtaining reliable object detectors.","inCitations":[],"pmid":"","title":"Weakly- and Semi-supervised Faster R-CNN with Curriculum Learning","journalPages":"2416-2421","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546088"],"entities":[],"journalVolume":"","outCitations":["06ca83cc7def5b0d582f4d933057c4370a6345d7","1c7d38f68fe1150895a186e30b60c02dd89a676a","1c0e8c3fb143eb5eb5af3026eae7257255fcf814","05357b8c05b5bc020e871fc330a88910c3177e4d","7d39d69b23424446f0400ef603b2e3e22d0309d6","5c4e20c089ab36c32d3d1ad5c5c37486d0cfec0e","521768f7772163bc7c57ae2c9855889abb747fbb","15caf136368f918f62508f963a2eb0424f07df5f","16fc1065c296840cb0f8ca62601aa17b7f0a02bf","2a4744bf5acfd888f0dcda3057367b0e0eccaa29","04eda7eee3e0282de50e54554f50870dd17defa1","7771807cd05f78a4591f2d0b094ddd3e0bd5339a","e4f725a14bb87fd54495e36750cbc3e12a2fc49e","804eecda772857604566935070b6d3d8644b628e","2f84d432e46ed1253764e238e3038c9c791790e7","121503705689f46546cade78ff62963574b4750b","5960c496739ace3c86728a4529f8dc3409884107","0f891006a55e09f8dace79dd7f8041543bbf3722","424561d8585ff8ebce7d5d07de8dbf7aae5e7270","3dd2f70f48588e9bb89f1e5eec7f0d8750dd920a","0fd1bffb171699a968c700f206665b2f8837d953","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc"],"id":"0028744f2ccb5973c8976fdbe706044f1b6f3bec","s2Url":"https://semanticscholar.org/paper/0028744f2ccb5973c8976fdbe706044f1b6f3bec","authors":[{"name":"Jiasi Wang","ids":["3768940"]},{"name":"Xinggang Wang","ids":["2443233"]},{"name":"Wenyu Liu","ids":["46641540"]}],"doi":"10.1109/ICPR.2018.8546088"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206010","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"We propose a method that allows for dexterous manipulation of an object by exploiting contact with an external surface. The technique requires a compliant grasp, enabling the motion of the object in the robot hand while allowing for significant contact forces to be present on the external surface. We show that under this type of grasp it is possible to estimate and control the pose of the object with respect to the surface, leveraging the trade-off between force control and manipulative dexterity. The method is independent of the object geometry, relying only on the assumptions of type of grasp and the existence of a contact with a known surface. Furthermore, by adapting the estimated grasp compliance, the method can handle unmodelled effects. The approach is demonstrated and evaluated with experiments on object pose regulation and pivoting against a rigid surface, where a mechanical spring provides the required compliance.","inCitations":["3f386b98c5ab9b0ddbd1c72f09f03423c42df648"],"pmid":"","title":"Dexterous manipulation with compliant grasps and external contacts","journalPages":"1913-1920","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206010","http://www.diva-portal.org/smash/get/diva2:1147199/FULLTEXT01.pdf"],"entities":["Apache Axis","Experiment","Extended Kalman filter","GRASP","Linear model","Pivot table","Robot end effector","Side effect (computer science)"],"journalVolume":"","outCitations":["4ad1434904092da2b008e1a8cc4e3325843fbd29","9d67020a47afc1366172a2c7d8d51b9e3f2dc06c","42a5ff36e08a69f59bf5a3f4bea9d6a0a612cdf1","7f2a14e09263deef5c5b0b1379a3f410c31a2c90","1984615d50057fbc269af182ec83c6c40b12ac67","6e8fea2966ff645bc9eb313b6567783633ea4501","a4fc50a9e136d64ac32926e5bcf09857da71337c","7a76697bcf441f1123e5018cbabca46b6ab48df2","1153691476241aed35de896998fd129589537a45","3271a7d2f89c745758e2d0763aab2befb030d854","1526a9220539f5c5353947f9ccf310caf082dfcb","a11139ef44108280c6e17f9dc43042b77826b655","aaa83a3a7e35e2e1ec2c914ee131124588fafaa2","01337d8533455ccbd801cdfa437a369c51ca9ba5","3b195d8b0e67fe694671ed688fbc19e4a03d8e1a","8a5c81665824d2c38687708249dfc2f3dae607e3","29a5789140f8e3900b742464fda451e9cc5c86d2","6f9e7fac92565bec2ae9da2b0d7c04a4741cdeae","3066aeb0bda592614a4fc0106c43c88465f8e37a","e5e6b13c985120030640235397fe48bd9448d70a","69cfb4d256399f029f3914efc438f44e8fdaba97","412a0bb5a3baa91b62053d82c562bc172df0439f","8b0894303dd19ab3deb9e3d7a66d124265d739b9","16211e22b8bd55379dcaf01b58fb2e332b30d3d8","ff6edd2fde1d38a01294c974611355b34e99a0dd","f203bb97671aafb719a6fc67b33bf207b7fb71b7","31cb25628e54dbcb147c0ef6d4391bff273313ca"],"id":"cdb513ee6328653bca2d63d4650f01d7f1ee72b8","s2Url":"https://semanticscholar.org/paper/cdb513ee6328653bca2d63d4650f01d7f1ee72b8","authors":[{"name":"Diogo Almeida","ids":["39194579"]},{"name":"Yiannis Karayiannidis","ids":["1778250"]}],"doi":"10.1109/IROS.2017.8206010"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206034","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"A robust state estimator is presented by fusing the aerodynamic model of multi-rotor UAVs with measurements from optical flow and other low-cost sensors such as IMU, magnetometer, and ultrasonic sensor. Due to the particular aerodynamics of multi-rotor UAVs, the body velocity in the rotor plane is able to be measured by the accelerometer. We therefore propose a novel state estimator by fully exploring the characteristic of aerodynamics of multi-rotor UAV. Our state estimator is fast and easy to be implemented. We have tested our estimator with different platforms in different scenes. Experimental results show that our estimator performs robustly in low light conditions where existing methods usually fail.","inCitations":[],"pmid":"","title":"An aerodynamic model-aided state estimator for multi-rotor UAVs","journalPages":"2164-2170","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206034"],"entities":["Coefficient","Computational complexity theory","Extended Kalman filter","Optical flow","R.O.T.O.R.","Sensor","Unmanned aerial vehicle","Velocity (software development)"],"journalVolume":"","outCitations":["2cc1c41984b5c2232cdce3846a227e476e1768d1","579e2e12c1e46e8636206eb5028ecf04bf5c205c","93a8b161cb18a981424275fe20aa7c4a67c2be30","ecb84ba6f0f2a1236ab4317e8871035c3a842931","51fea461cf3724123c888cb9184474e176c12e61","837a35eee58c7ed1f626315e0ab820ce9a06a8cb","362f9af442e7242ec2f0b19a83ede6fa07637d37","1099983c747a8773c1572a3226373ce4521107b1","f56ef87ec866d5749a12d308d8413c1911e4d927","aca864792bb96b0258f638fc216741b5a6a8ca9e","63d9ba08e9be64b653c8c0c5eb6fe5d84774e4ca","10cfa5bfab3da9c8026d3a358695ea2a5eba0f33","18eb1f6987a983e273fb51191b6a4893283c5fe5","2295fda13d5118808557d04310b0e176a1341063","38a26c53a39f69735e73bb15a91b0e028018af15","5e6ee8cf9cee533860206493b2be3fd25912c60b","440d54508e16c642904f22ce0e48188ca36e0bca","0be0c13803cd08e81b7adaada537e91222eb1491","563e656203f29f0cbabc5cf0611355ba79ae4320","99c7ff2ed7a9c1be545bd80d10a7b8508174f107","b1e5907533b7bf87f0900e7ed9c0955568c07977","2aec35a5912f05a3d502123c66ed2daa2c711e22","c8965cc5c62a245593dbc679aebdf3338bb945fc","230ad73e8bd1d3268d56c66a83442d24176b864d","efd9888a7f6f1cb609d6632127b0c6f32c2e0ea6"],"id":"a5e354a40358ed105fc63e534e55f6c576a37a2c","s2Url":"https://semanticscholar.org/paper/a5e354a40358ed105fc63e534e55f6c576a37a2c","authors":[{"name":"Rongzhi Wang","ids":["2304128"]},{"name":"Danping Zou","ids":["2561213"]},{"name":"Changqing Xu","ids":["46748095"]},{"name":"Ling Pei","ids":["37694039"]},{"name":"Peilin Liu","ids":["1752791"]},{"name":"Wenxian Yu","ids":["1809966"]}],"doi":"10.1109/IROS.2017.8206034"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594441","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"The present study highlights the benefits of using well-controlled experimental designs, grounded in experimental psychology research and objective neuroscientific methods, for generating progress in human-robot interaction (HRI) research. More specifically, we aimed at implementing a well-studied paradigm of attentional cueing through gaze (the so-called \u201cjoint attention\u201d or \u201cgaze cueing\u201d) in an HRI protocol involving the iCub robot. Similarly to documented results in gaze-cueing research, we found faster response times and enhanced event-related potentials of the EEG signal for discrimination of cued, relative to uncued, targets. These results are informative for the robotics community by showing that a humanoid robot with mechanistic eyes and human-like characteristics of the face is in fact capable of engaging a human in joint attention to a similar extent as another human would do. More generally, we propose that the methodology of combining neuroscience methods with an HRI protocol, contributes to understanding mechanisms of human social cognition in interactions with robots and to improving robot design, thanks to systematic and well-controlled experimentation tapping onto specific cognitive mechanisms of the human, such as joint attention.","inCitations":["77d7782ebf03520af966c01deeda497086701b80"],"pmid":"","title":"Neuroscientifically-Grounded Research for Improved Human-Robot Interaction","journalPages":"3403-3408","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594441"],"entities":[],"journalVolume":"","outCitations":[],"id":"737fbf53d1ba9b33cbb111221a4855cf60dac302","s2Url":"https://semanticscholar.org/paper/737fbf53d1ba9b33cbb111221a4855cf60dac302","authors":[{"name":"Kyveli Kompatsiari","ids":[]},{"name":"Jairo Pérez-Osorio","ids":[]},{"name":"Davide De Tommaso","ids":[]},{"name":"Giorgio Metta","ids":[]},{"name":"Agnieszka Wykowska","ids":[]}],"doi":"10.1109/IROS.2018.8594441"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206467","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Tendon-driven mechanisms used in robotic surgery exhibit strong nonlinearities, particularly a static backlash-like hysteresis, in their motion transmission behavior. In this paper, an extension of a previously developed model is proposed that allows for estimation of angular displacements in multi-DOF tendon-driven devices where special attention is given to the coupling effect between DOFs. The proposed model consists of the conventional coupling matrix and a novel elongation matrix which compensates for the coupled hysteretic effect. The model is applied to the problem of position estimation in three DOFs (one pitch and two grasping DOFs) of a da Vinci® surgical instrument. As a further extension, a preliminary dynamic model is also suggested to deal with high-frequency inputs. According to the experimental results obtained, the proposed quasi-static model can describe the transmission behavior with goodness-of-fit of 76\u201392 per cent, and the estimates are improved by 35\u201372 per cent in terms of the RMSE for the proposed dynamic model as compared to the conventional rigid model.","inCitations":[],"pmid":"","title":"A motion transmission model for multi-DOF tendon-driven mechanisms with hysteresis and coupling: Application to a da Vinci® instrument","journalPages":"5764-5769","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206467"],"entities":["AngularJS","Encoder","Experiment","Hysteresis","Mathematical model","Multi Language Virtual Machine","Robot"],"journalVolume":"","outCitations":["49b7aa50601a7c98006502afa80e637e56b06f5d","ea362f290e7c276ddcd2a1921b2a6447842c217b","c0e5ab35ce45219281ae5ff70dfa073a34415709","6bce6481ec360e70126994fd1b06e6c799a55f85","257ac72b34db7fd31ffaab67ed896d796e585f37","657a7b0602ec2a61de1304831b4fb8400606f288","8634b048d81cee79e92c911c19225d13fb0c6506"],"id":"459f3438ead423441de6b387bab9e29a52f6f952","s2Url":"https://semanticscholar.org/paper/459f3438ead423441de6b387bab9e29a52f6f952","authors":[{"name":"Farshad Anooshahpour","ids":["3277552"]},{"name":"Peyman Yadmellat","ids":["1992187"]},{"name":"Ilia G. Polushin","ids":["9379916"]},{"name":"Rajnikant V. Patel","ids":["1710391"]}],"doi":"10.1109/IROS.2017.8206467"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00037","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Positive interpersonal relationships require shared understanding along with a sense of rapport. A key facet of rapport is mirroring and convergence of facial expression and body language, known as nonverbal synchrony. We examined nonverbal synchrony in a study of 29 heterosexual romantic couples, in which audio, video, and bracelet accelerometer were recorded during three conversations. We extracted facial expression, body movement, and acoustic-prosodic features to train neural network models that predicted the nonverbal behaviors of one partner from those of the other. Recurrent models (LSTMs) outperformed feed-forward neural networks and other chance baselines. The models learned behaviors encompassing facial responses, speech-related facial movements, and head movement. However, they did not capture fleeting or periodic behaviors, such as nodding, head turning, and hand gestures. Notably, a preliminary analysis of clinical measures showed greater association with our model outputs than correlation of raw signals. We discuss potential uses of these generative models as a research tool to complement current analytical methods along with real-world applications (e.g., as a tool in therapy).","inCitations":["cb34aca4beb4ed9ed2f2765731157aeeba035fc5"],"pmid":"","title":"Generative Multimodal Models of Nonverbal Synchrony in Close Relationships","journalPages":"195-202","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00037"],"entities":["Acoustic cryptanalysis","Artificial neural network","Disk mirroring","Feedforward neural network","Generative model","Multimodal interaction","Virtual synchrony"],"journalVolume":"","outCitations":[],"id":"b7af0013f297cb10de5768835ba73148bcafbb4d","s2Url":"https://semanticscholar.org/paper/b7af0013f297cb10de5768835ba73148bcafbb4d","authors":[{"name":"Joseph F. Grafsgaard","ids":["1931563"]},{"name":"Nicholas D. Duran","ids":["3068289"]},{"name":"Ashley K. Randall","ids":["2968287"]},{"name":"Chun Tao","ids":["28891223"]},{"name":"Sidney D'Mello","ids":["47406458"]}],"doi":"10.1109/FG.2018.00037"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546090","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In the context of Automatic Speech Recognition (ASR), improving the noise robustness remains an intractable task. Speech enhancement, combined with Generative Adversarial Networks (GAN), such as SEGAN, has effective performance in denoising raw waveform speech signals. Instead of waveforms, using Mel filterbank spectra in GAN is proposed, which has better performance in the task of ASR. However, these techniques will still miss useful information when GAN is used in them. In this paper, we investigate to protect the useful information in GAN, and propose a novel model, called Discriminator Generator Classifier-GAN (DGC-GAN). While normal GAN combining just two networks will lead the model to denoising rather than recognition, DGC-GAN has another network called classifier, which is an ASR system that will tune GAN to be recognized easier. By adding a classifier into previous GAN to get DGC-GAN, we achieve 29.1% Phone Error Rate (PER) relative improvement in a tiny dataset and 47.4% PER relative improvement in a large dataset.","inCitations":[],"pmid":"","title":"Mutual-optimization Towards Generative Adversarial Networks For Robust Speech Recognition","journalPages":"2699-2704","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546090"],"entities":["Automated system recovery","Automatic speech recognition","Discriminator","Distributed garbage collection","Filter bank","Generative adversarial networks","Glycyrrhiza uralensis","MELORHEOSTOSIS, ISOLATED","Mathematical optimization","Mel-frequency cepstrum","Noise reduction","Radionuclide Generators","Silo (dataset)","Speech enhancement","Waveform","dystrophin-associated glycoprotein complex","gosha-jinki-gan"],"journalVolume":"","outCitations":["307acb91ebc6333f044359aff9284bbe0d48e358","70dc18bb6607e408ec1cd3f71c0fdac3534c288d","326a0914dcdf7f42b5e1c2887174476728ca1b9d","4c6821c0e2fcd3d34d1d40ee082d47e298323d9f","238dcd217b11a0f0507bd9fecc266f4a02620f11","b22458b7194458bb8c8a3af4f1d8e8aa4c5b34cb","46aca9fd693cda49f7f02d575efaee0977f078c7","a538b05ebb01a40323997629e171c91aa28b8e2f","c0d387cfc583f97fc16b122d10394a1dd2abd88d","b554da42487697cb0d01a4146858e966c1d2404f","94de4d89445a22b53dcc7dba76905f1817fc409d","8660642c37be1eaeca0d22598558249ac47d767d","ba753286b9e2f32c5d5a7df08571262e257d2e53","8648dbfff9662fa9c62a95622712dd2951b5b3a3","6de2b1058c5b717878cce4e7e50d3a372cc4aaa6","3ba179bceb9692d4d21109d0b87b120195761148","0bbc35bdbd643fb520ce349bdd486ef2c490f1fc","35756f711a97166df11202ebe46820a36704ae77","072fd0b8d471f183da0ca9880379b3bb29031b6a","9ed8e2f6c338f4e0d1ab0d8e6ab8b836ea66ae95","eb00bdcc7a0bbdc018acf15810f3bd96c56601aa"],"id":"d4054cdf807f59ae644d3c81787f5752a1f182f6","s2Url":"https://semanticscholar.org/paper/d4054cdf807f59ae644d3c81787f5752a1f182f6","authors":[{"name":"Ke Ding","ids":["48800995"]},{"name":"Ne Luo","ids":["51455843"]},{"name":"Yanyan Xu","ids":["48615794"]},{"name":"Dengfeng Ke","ids":["36213393"]},{"name":"Kaile Su","ids":["1724626"]}],"doi":"10.1109/ICPR.2018.8546090"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8593838","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"We present a novel approach for interactive auditory object analysis with a humanoid robot. The robot elicits sensory information by physically shaking visually indistinguishable plastic capsules. It gathers the resulting audio signals from microphones that are embedded into the robotic ears. A neural network architecture learns from these signals to analyze properties of the contents of the containers. Specifically, we evaluate the material classification and weight prediction accuracy and demonstrate that the framework is fairly robust to acoustic real-world noise.","inCitations":[],"pmid":"","title":"Deep Neural Object Analysis by Interactive Auditory Exploration with a Humanoid Robot","journalPages":"284-289","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8593838","http://arxiv.org/abs/1807.01035","https://arxiv.org/pdf/1807.01035v2.pdf"],"entities":[],"journalVolume":"","outCitations":["706b0eadc7136608230778ed04fa9a262dcd4e4a","01a885f259adebb89586d6ff4e87c170525f893e","91c441dd9cfd2877ac66a05c2c693a5917a8e846","9bcb27598a4abdc4ec76b8742d3036710adbacf0","0b3cfbf79d50dae4a16584533227bb728e3522aa","cf510111bcac6bf69d4eccea7baa9716b0974406","b3416a5f7339c7e83d68ba1d00d00576880a8f04","06ae2d9650be9fc5a17f907bb1368ccea014c52b","45ec3eb5b019395f13e58533d9147c774f7f09a9","6ef28af882e408ff63f83ca670392a008d203fbc","e0c9cde206dfe319e02f475d7a26e885842a7475","603ce1556f074fe26caf13d0b206708cba3de686","8023147f471f133dc5c9b32c9ffc539fcce4a4e1","8d8e9bde0bfc823e7e5231416b8a32ea0fd25ff6","51d2ce2641097e1c8d0111cbf8f1f5c65701f855","fdd40554bfcc9fcc532f8266df638ea1bb0858f7","3f5e8f884e71310d7d5571bd98e5a049b8175075","9d3f4aa0918c1cefd5cf4b22baa6528d2a42410f","2d9e3f53fcdb548b0b3c4d4efb197f164fe0c381","b4ab2555d5690e8e6fb1cf23c995a120181698a6","1c9da6cef6b1be9c116b26dd52c341c0adcf7db2","4879da25c9a0a1e03d9f153ccdc69a12d8d0dafc","8e6d6c45566c542338f42ad86d2f07a7f1ad7cae","ba5bc241a9e2c9dccc4a861b302dadd816f1525c"],"id":"509363e5c70334c25e6ac2a3807b22601ece4f09","s2Url":"https://semanticscholar.org/paper/509363e5c70334c25e6ac2a3807b22601ece4f09","authors":[{"name":"Manfred Eppe","ids":["2236890"]},{"name":"Matthias Kerzel","ids":["2991958"]},{"name":"Erik Strahl","ids":["2317555"]},{"name":"Stefan Wermter","ids":["1736513"]}],"doi":"10.1109/IROS.2018.8593838"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8546063","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"In this paper, we propose a small-footprint wake-up-word speech recognition (WUWSR) system based on long short-term memory (LSTM) recurrent neural network, and we design a novel back-end calibration scoring method named modified zero normalization (MZN). First, LSTM is trained to predict posterior probability of context-dependent state. Next, MZN is adopted to transfer posterior probability to normalized score, which is then converted to confidence score by dynamic programming. Finally, a certain wake-up-word is recognized according to the confidence score. This WUWSR system can recognize multiple wake-up words and change wake-up words flexibly. This system can guarantee low latency by omitting decoding network. Equal error rate (EER) is adopted as the evaluation metric. Experimental results show that the proposed LSTM-based system achieves 33.33% relative improvement compared with a baseline system based on deep feed-forward neural network. Combining the front-end LSTM acoustic model with back-end MZN method, our WUWSR system can achieve 51.92% relative improvement.","inCitations":[],"pmid":"","title":"Recurrent Neural Network Based Small-footprint Wake-up-word Speech Recognition System with a Score Calibration Method","journalPages":"3222-3227","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8546063"],"entities":[],"journalVolume":"","outCitations":["c204729049d70ac8e0217c9680d377727e45616d","a3b0ef4d6aa2a52db421be84b902b4c9bfca336e","0b00efa192ba2b8e87b0ea02330fe1881ed1457d","c4756dcc7afc2f09d61e6e4cf2199d9f6dd695cc","47e4346e697c2c4054d2439d173dba2e97ac4327","db97a0bfcaac723fd468d85b912739d44d167859","d20af5f8edeb93cffdeafdf763964e3d0af45448","64b34fbb35528daaaf6027c65845313b7a441312","f3daba7ddda31ed821af0980182b270def8918ce","d2f24c3f9c86329b38327997ada98d117293ba85","080644c599b813b920053a760d744d8abd615250","06bae254319f8d39e80c7254c841787b45baf820","40acfbe3e2c095cc792f9493befba779323d5f95","d2740ebd37a35ecf6f7d713558308eeb28311961","c85d46a94768bdcf7ffcb844b47c5b8e8e8234a3","8f4c9d62482eb7989cba846c223b3a1ec098949c","09b6ee10aa1a3e15de6f937dd0dc8940d13c4771","9fca2af9a0e3f2c5c3ed47abb3ebd21b7265ac2b","47dbd770adf14ab9f28b3549f5d2912c87332bec","df50c6e1903b1e2d657f78c28ab041756baca86a","310ff1354c8dd99fb68faa2874c7943d98420368"],"id":"88ebd29ee983653795c459e803f7d3098a134866","s2Url":"https://semanticscholar.org/paper/88ebd29ee983653795c459e803f7d3098a134866","authors":[{"name":"Chenxing Li","ids":["2131479"]},{"name":"Lei Zhu","ids":["46812936"]},{"name":"Shuang Xu","ids":["48923222"]},{"name":"Peng Gao","ids":["50735378"]},{"name":"Bo Xu","ids":["40377923"]}],"doi":"10.1109/ICPR.2018.8546063"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594184","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"This paper explores the estimation of user attention in the setting of a cooperative handheld robot \u2014 a robot designed to behave as a handheld tool but that has levels of task knowledge. We use a tool-mounted gaze tracking system, which, after modelling via a pilot study, we use as a proxy for estimating the attention of the user. This information is then used for cooperation with users in a task of selecting and engaging with objects on a dynamic screen. Via a video game setup, we test various degrees of robot autonomy from fully autonomous, where the robot knows what it has to do and acts, to no autonomy where the user is in full control of the task. Our results measure performance and subjective metrics and show how the attention model benefits the interaction and preference of users.","inCitations":[],"pmid":"","title":"I Can See Your Aim: Estimating User Attention from Gaze for Handheld Robot Collaboration","journalPages":"3897-3904","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594184","http://arxiv.org/abs/1810.06404","https://arxiv.org/pdf/1810.06404v1.pdf","https://export.arxiv.org/pdf/1810.06404"],"entities":[],"journalVolume":"","outCitations":["1af1ada3d7232a1bb9b8bdc7e29fb5842dacf033","5a331f846879b9e85eeef7c4eec4bb480a4b1383","eb1e52ac5cfe6522579e5b73aba34ac40375c462","517ac6c483ebcb1a0676180d1cbbeed0fde82029","e1219ae13de1f971876a45ef3ad9ccf240fc146e","854452a4a1651aa75da20f37d02a83f360871a32","446f68668b34124691f9a38f0d8a626e1225aabb","6eb1882161ffb1b1a37f3bd20c52e6ecf738cb04","311fc36480ce19e45400d1905a21e8687aeb0fa0","41fb40a942f6d2cc59021d393d5283462a0f7f1f","9ea00cd33b9a000ab79ad02eb6c190f33307feaf","4273cf4214fb035bc76ea28b0ca12b35a8d3a0db","966dfdd0b0bb6b50416559d35a2561c0b96d9a9e","2c1e797b781b788637d6dea4274622e5800b80a3","915b9c5bfc04016ee1c1785c694579ddfe7fcab1","69cc07ae98423e8bf7123ea12d9fbf2fc0ba84e4","c6f2196ed7fc4f7fdf9f3472e9b13d815a96c03e"],"id":"329a886cf8b49b29add39b3152fb56f71618d43d","s2Url":"https://semanticscholar.org/paper/329a886cf8b49b29add39b3152fb56f71618d43d","authors":[{"name":"Janis Stolzenwald","ids":["32205094"]},{"name":"Walterio W. Mayol-Cuevas","ids":["1731214"]}],"doi":"10.1109/IROS.2018.8594184"}
{"doiUrl":"https://doi.org/10.1109/ICPR.2018.8545413","venue":"2018 24th International Conference on Pattern Recognition (ICPR)","journalName":"2018 24th International Conference on Pattern Recognition (ICPR)","sources":["DBLP"],"year":2018,"text":"Image matching is widely used in visual-based navigation systems, most of which simply assume the ideal inputs without considering the degradation of the real world, such as image blur. In presence of such situation, the traditional matching methods first resort to image restoration and then perform image matching with the restored image. However, by treating the restoration and matching separately, the accuracy of image matching will be reduced by the defective output of the image restoration. In this paper, we propose a joint image restoration and matching method based on distance-weighted sparse representation (JRM-DSR), which utilizes the sparse representation prior to exploit the correlation between restoration and matching. This prior assumes that the blurry image, if correctly restored, can be well represented as a sparse linear combination of the dictionary constructed by the reference image. In order to achieve more accurate matching results to help restoration, we consider both local and sparse information and adopt distance-weighted sparse representation to obtain better representation coefficients. By iteratively restoring the input image in pursuit of the sparest representation, our approach can achieve restoration and matching simultaneous, and these two tasks can benefit greatly from each other. matching, we give a coarse to fine matching strategy to further improve the matching accuracy. Experiments demonstrate the effectiveness of our method compared with conventional methods.","inCitations":[],"pmid":"","title":"Joint Image Restoration and Matching Based on Distance-Weighted Sparse Representation","journalPages":"2498-2503","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/ICPR.2018.8545413"],"entities":["ANSI escape code","Circuit restoration","Coefficient","Dictionary [Publication Type]","Elegant degradation","Experiment","Gaussian blur","Image registration","Image restoration","MATCHING","Sparse approximation","Sparse matrix"],"journalVolume":"","outCitations":["6584cdd5ac482759a7d5f75f45b690e92fbe6185","57bb0d2fadcf4d7d11ff94a6df366ae84b9835b5","1080d910e3a35e25b0ccce9f8b5117090bbfb119","1d9e8248ec8b333f86233bb0c4a88060776f51b1","44b401731987fb97d4b3b3ff0314cc309ebd7655","1a998506899da3bc703455bde45114bd4c228947","8b01d5b0ff35a765c63eb59b9132f0fbb6ba5006","075bc988728788aa033b04dee1753ded711180ee","494b2d31637ba2b28ac1fa5eec0915ad00e313af","9b928c0c7f5e47b4480cb9bfdf3d5b7a29dfd493","490020c0d4fa1eb85fe353add5713e49f08c628d","5c894ff93e40997dfad3df5f724cf435d17ec48e","f82120ecd784667cb3622c9af7b8cfe8c06ea623","6b2e3c9b32e92dbbdd094d2bd88eb60a80c3083d","266bf8847801ff302c6f91f899f36269807317ee","31864e13a9b3473ebb07b4f991f0ae3363517244","2c1a11a72bf252220dda106c2eecc391ec2742ed","2b5ca2eedccb2b4d7fc62eab1043a7d50f411032","1e360df8e0da9da7338ba6acfaf85f3891376538","887a7c782b81952a5f0b85c4e7c8bf936f11096f","464dbb687cc87ae784fdd0f7314bae4c0c9f57ec","25fc7c60a47546eb8c5d1404df366ef625004641","e56c90ceb022ebcf6f2a9d83349b34402639d54d"],"id":"d8501ad486a73153d0a80a718d68065d7fff6f3d","s2Url":"https://semanticscholar.org/paper/d8501ad486a73153d0a80a718d68065d7fff6f3d","authors":[{"name":"Yuanjie Shao","ids":["2195861"]},{"name":"Nong Sang","ids":["1707161"]},{"name":"Changxin Gao","ids":["40115662"]},{"name":"Wei Lin","ids":["38836600"]}],"doi":"10.1109/ICPR.2018.8545413"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00029","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"We present an approach for unsupervised training of CNNs in order to learn discriminative face representations. We mine supervised training data by noting that multiple faces in the same video frame must belong to different persons and the same face tracked across multiple frames must belong to the same person. We obtain millions of face pairs from hundreds of videos without using any manual supervision. Although faces extracted from videos have a lower spatial resolution than those which are available as part of standard supervised face datasets such as LFW and CASIA-WebFace, the former represent a much more realistic setting, e.g. in surveillance scenarios where most of the faces detected are very small. We train our CNNs with the relatively low resolution faces extracted from video frames collected, and achieve a higher verification accuracy on the benchmark LFW dataset cf. hand-crafted features such as LBPs, and even surpasses the performance of state-of-the-art deep networks such as VGG-Face, when they are made to work with low resolution input images.","inCitations":[],"pmid":"","title":"Unsupervised Learning of Face Representations","journalPages":"135-142","s2PdfUrl":"","pdfUrls":["https://arxiv.org/pdf/1803.01260v1.pdf","http://arxiv.org/abs/1803.01260","http://doi.ieeecomputersociety.org/10.1109/FG.2018.00029"],"entities":["Benchmark (computing)","Image resolution","Unsupervised learning"],"journalVolume":"","outCitations":["03563dfaf4d2cfa397d3c12d742e9669f4e95bab","aefc7c708269b874182a5c877fb6dae06da210d4","6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","14ce7635ff18318e7094417d0f92acbec6669f1c","370b5757a5379b15e30d619e4d3fb9e8e13f3256","53e14bb909ef71388c8ca189c9db84b52af2db44","64e216c128164f56bc91a33c18ab461647384869","a94783b5f57bfb44a432af043d309b226954ad3b","b9c830332608afbbf452a20afeff38ca5aac2e27","a8a8f17582a5ba873eef2526ef4774813b19bfac","025720574ef67672c44ba9e7065a83a5d6075c36","187480101af3fb195993da1e2c17d917df24eb23","132e3d3b5cfc2f59db6ed69ac1eac4a1ee6dca71","0d30860edf7dd5362436e6fd5262c618e33573d5","013cd20c0eaffb9cab80875a43086e0c3224fe20","6c11626ae08706e6185fceff0a6d05e4bfd6bd06","167f07b9d2babb8920acfa320ab04ee2758b5db6","42269d0438c0ae4ca892334946ed779999691074","3619a9b46ad4779d0a63b20f7a6a8d3d49530339","49209adc42ab37b69d3a9553151a9edfa2d0fe72","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","1d696a1beb42515ab16f3a9f6f72584a41492a03","582edc19f2b1ab2ac6883426f147196c8306685a","19d583bf8c5533d1261ccdc068fdc3ef53b9ffb9","162ea969d1929ed180cc6de9f0bf116993ff6e06","213d7af7107fa4921eb0adea82c9f711fd105232","12a376e621d690f3e94bce14cd03c2798a626a38","84574aa43a98ad8a29470977e7b091f5a5ec2366"],"id":"96f0e7416994035c91f4e0dfa40fd45090debfc5","s2Url":"https://semanticscholar.org/paper/96f0e7416994035c91f4e0dfa40fd45090debfc5","authors":[{"name":"Samyak Datta","ids":["19200118"]},{"name":"Gaurav Sharma","ids":["39396475"]},{"name":"C. V. Jawahar","ids":["1694502"]}],"doi":"10.1109/FG.2018.00029"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206542","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Achieving object exploration with passive vision and active touch has been under investigation for thirty years. We build upon recent progress in biomimetic active touch that combines perception via Bayesian evidence accumulation with controlling the tactile sensor using perceived stimulus location. Here, passive vision is combined with active touch by providing a visual prior for each perceptual decision, with the precision of this prior setting the relative contribution of each modality. The performance is examined on an edge following task using a tactile fingertip (the TacTip) mounted on a robot arm. We find that the quality of exploration is a U-shaped function of the relative contribution of vision and touch; moreover, multi-modal performance is more robust, completing the contour when touch alone fails. The overall system has several parallels with biological theories of perception, and thus plausibly represents a robot model of visuo-tactile exploration in humans.","inCitations":[],"pmid":"","title":"Object exploration using vision and active touch","journalPages":"6363-6370","s2PdfUrl":"","pdfUrls":["http://www.lepora.com/publications/yang_lepora2017iros.pdf","https://doi.org/10.1109/IROS.2017.8206542"],"entities":["Biomimetics","Modal logic","Modality (human\u2013computer interaction)","Parallels Desktop for Mac","Robotic arm","Tactile sensor","Theory","Tree accumulation"],"journalVolume":"","outCitations":["a9489bec2c5d6728a57166bf5de45ecef6b0e5fe","828001ab5f84c1209d7a3db3f170134ac248f593","42bd0d82204d66c1421844e87f5924cc433cae49","935852665deab249685b28ee706f7c3c0cd4164f","c96bad70db489701ade007b365fe215478303003","17ccb414fa4732ec50e2f777feb96864d338b3cc","b25e9d944cb02a53b1ca0c64a60fabafcc845855","a03710c3c6545b0f3d4399b68664f9ea7cdb349b","3138569648f22f130737fd2998e1000bdb9cff96","765ff34607c8ab442ff3510f594b88978bcbcac9","077c2620f1a35409393046dfe5b4e5f65cb8aefc","93d949bf9fe90d3495f491a0d970713be8a94fc4","35e03bf99ed0b6e72633131507e1527b474db094","5789932a08b33f5e3e3ca0c774e373fba67e63b9","31be30d263acf72a7505ebb115cb704905660bd6","fad57c71d7231507da98b5f7f160cace9946307e","7063e1e337a6efbdebdf22cb0de8dad542f455d3","244602166c65425a4ab10a7b599cf275f56dfc37","4b814e9d09ff72279030960df5718041b8c1b50c","6f97847847bb4dd7f2194dd4add14e2a30904624","851eaccc47a8636e4cca00b98e21eb9c2b7cd52e","15f203497d030162b2e0c5a8b539b9130e100ecf","84468b2fdf73471a4b985c073756086d6bd88a8c","5b69f06478f227a3b8ac809c52666f049c7d40bc","de5e72495e73521141516896e26a06129555cfdf","488f6fda6e4dbf210898c3acbe2a56ffcf9f9cfc","246b9ed99e5b412051e50cfcd0c01bea5d183f59","fe72ca651135c91717a8d8819a7b1c4d09fbe93f","6e3a73ea83d5d4129b774e023b894bc8d2d5db03"],"id":"3021fe87ba100e58516076d72f90d55df89e5375","s2Url":"https://semanticscholar.org/paper/3021fe87ba100e58516076d72f90d55df89e5375","authors":[{"name":"Chuanyu Yang","ids":["6172832"]},{"name":"Nathan F. Lepora","ids":["2467565"]}],"doi":"10.1109/IROS.2017.8206542"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8202132","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Depth estimation from a single image is very challenging due to the inherent ambiguity of mapping a color image to a depth map. Previous work tackles this problem by exploiting various levels of features with multi-scale deep convolutional neural networks. However, most of the local geometric structure related monocular depth cues are lost when being propagated through convolutional neural network. Moreover, the error of depth cues related to local geometric structures is not considered in the loss function. In this work, we propose the GeoCueDepth convolutional neural network to exploit local geometric structure cues and propose a training loss that takes the geometric error into consideration, which significantly improve the performance of depth prediction in both accuracy and sharpness. Experiments show that the proposed method achieves 0.122 average relative error and 0.078 square relative error on the NYU Depth v2 data set, which outperforms state-of-the-art monocular depth estimation approaches.","inCitations":[],"pmid":"","title":"GeoCueDepth: Exploiting geometric structure cues to estimate depth from a single image","journalPages":"17-22","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8202132"],"entities":["Approximation error","Artificial neural network","Autostereogram","Color image","Convolutional neural network","Deep learning","Depth map","Depth perception","Experiment","Gradient","Ground truth","Loss function"],"journalVolume":"","outCitations":["9201bf6f8222c2335913002e13fbac640fc0f4ec","d5dd3c0e2d18c14fb0a6407195d2ca5e8e781a91","a51cbfd637d0b756afa03202f869c88e27e1cb00","344e99659aa1a705b17dc82cad69ac36f8d6b843","8c4c723a74fe479c2b8af7d911817377dd6d85c9","9f9ad7715a27eb9c5d958ebc4184ce6d1635dd59","84c01c9760cd294718bd7c4b4c93596db1e5e068","597edc3174dc9f26badd67c4e81d0e8a58f9dbb3","2a0fa09b22a870e513edafa75d7b19a40ebc72af","293b47fc6c1d83e72484debf7b4faadc5d75cce7","13f85dae967a9c80f408f599086013ef9dbd1ad8","06feba1ffd596b41884cea6e8ef0da89b6dd2233","7449cff9a37e1c64080d97b6971f94c29b31fd30","3c9ab02fa6b883c3b6f53b0332acf221ef1fa809","2909953f5f988fe4d0756536840074779e9313d3","2315fc6c2c0c4abd2443e26a26e7bb86df8e24cc","3e7ffb5658cf99968633ede18785c5cfdd6aa9eb","29cd8e824e047695f47954633457271ae0c09e3b","df028190efdd1e78bb79195b0627670511e9a5fa","061356704ec86334dbbc073985375fe13cd39088","c1e4420ddc71c4962e0ba26287293a25a774fb6e","7be8a483ad69aed9d56640ad1c667af2387c3f99","286adff6eff2f53e84fe5b4d4eb25837b46cae23","722fcc35def20cfcca3ada76c8dd7a585d6de386","2c03df8b48bf3fa39054345bafabfeff15bfd11d","20f6ff260aefe840cd7cd07b06d71863546d746d","0b9af9b0ac87fafd9d7747d8047df38ee58dc647","39d187585d7df69c330e2bdd5f9c0c41ad99eba0","bf5f67ebbe41f2fb1726a7c3c0be707366d5a4fb"],"id":"161f820bab8317c72c3feffcbd4578e464e38ebe","s2Url":"https://semanticscholar.org/paper/161f820bab8317c72c3feffcbd4578e464e38ebe","authors":[{"name":"Yiming Zeng","ids":["48722906"]},{"name":"Yu Hu","ids":["49994650"]},{"name":"Shice Liu","ids":["32758259"]},{"name":"Qiankun Tang","ids":["31431435"]},{"name":"Jing Ye","ids":["2408310"]},{"name":"Xiaowei Li","ids":["20132046"]}],"doi":"10.1109/IROS.2017.8202132"}
{"doiUrl":"https://doi.org/10.1109/FG.2018.00114","venue":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","journalName":"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","sources":["DBLP"],"year":2018,"text":"Being able to produce facial expressions (FEs) that are adequate given a social context is key to harmonious social development, particularly in the case of children plagued with autism spectrum disorder (ASD). In this paper, we introduce JEMImE, a serious game solution that aims at teaching children how to produce FEs. JEMImE is based on a FE recognition module that is learned on a large video corpus of children performing FEs. This module is validated and incorporated through multiple scenarios of gradual difficulty, ranging from a training phase where children have to perform the FEs on request, with or without an avatar model, to an in-context phase that involves many emotion-eliciting social situations with virtual characters.","inCitations":[],"pmid":"","title":"JEMImE: A Serious Game to Teach Children with ASD How to Adequately Produce Facial Expressions","journalPages":"723-730","s2PdfUrl":"","pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/FG.2018.00114"],"entities":["Expression (computer science)","List of Code Lyoko episodes"],"journalVolume":"","outCitations":["2138b37bfced70599d26dfccbf93a8e7a4b7ad85","217ab8c91940077ede7b10328b6de0dcc9aee206","8d548e8d85d7f4126aafecea2b56207511ce881b","832e1d128059dd5ed5fa5a0b0f021a025903f9d5","03f98c175b4230960ac347b1100fbfc10c100d0c","84e2be16fc0226af8429f2e2adf8fc0efceec8a7","bd1808d6914440fc781f467ea95bc8dfa5eb5f22","2ad009994070c46e289ede82497fe72299bc6db3","3c91cd425d83cc92ee429f5d5187d7bf7fabd892","48e71c7d49d8d95ffde8363be7be0ff7c30dca8d","757d8f3871aee43f44fa4d2d84d11a84fdc9b8f2","790aa543151312aef3f7102d64ea699a1d15cb29","13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986","b47ea4d5b0040d85181925bda74da4ab5303768f","24a4a4ac94813680dca447bcfaebe71aac96f338","a3a6e3cadfed3c0a520e4417fc27da561324fbc6","79a32a53732103239cb4bc3cf31dbb950b92e765","12a376e621d690f3e94bce14cd03c2798a626a38","d0450305dc7406b22b9db6c748e42d14b3b2c460","7e507370124a2ac66fb7a228d75be032ddd083cc","33082dad6b9b681792cedb2d03121b0b11658c1d","497d46649af7dab664cdb9d47242df6dc06b1a48","4e499138269356a32fc9adc70d116579d78192af"],"id":"f7b7ec84c30f121421a7634229a9c0770d0e1ff2","s2Url":"https://semanticscholar.org/paper/f7b7ec84c30f121421a7634229a9c0770d0e1ff2","authors":[{"name":"Arnaud Dapogny","ids":["3190846"]},{"name":"Charline Grossard","ids":["8756234"]},{"name":"Stephanie Hun","ids":["7737732"]},{"name":"Sylvie Serret","ids":["6312012"]},{"name":"Jeremy Bourgeois","ids":["50075632"]},{"name":"Hedy Jean-Marie","ids":["46233391"]},{"name":"Pierre Foulon","ids":["48223216"]},{"name":"Huaxiong Ding","ids":["2421673"]},{"name":"Liming Chen","ids":["34086868"]},{"name":"Séverine Dubuisson","ids":["1701986"]},{"name":"Ouriel Grynszpan","ids":["2791712"]},{"name":"David Cohen","ids":["14129357"]},{"name":"Kevin Bailly","ids":["2521061"]}],"doi":"10.1109/FG.2018.00114"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206537","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Force-sensing system represents one of the vital components of robotic systems for physical interaction performances. This system is facilitated by the force controllable actuators. However, to make a force controllable actuator, it is still a challenging subject for the majority of the robotic system. This paper proposes a novel POWERPACK unit integrated a torque sensor, a harmonic drive and a motor for enabling the force control. The torque sensing element is based on the capacitance sensing to achieve a compact and a simple structure. In this research, reveals the practical details of the POWERPACK and evaluates the performance of the actuator unit.","inCitations":[],"pmid":"","title":"A novel POWERPACK for robotic application, integrated torque sensor, harmonic drive and motor","journalPages":"6325-6330","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206537"],"entities":["Capacitive sensing","Channel I/O","Embedded system","Fundamental interaction","Kinetic data structure","Performance","Robot","Sensor"],"journalVolume":"","outCitations":["49bc63b1c5d4e7d5b4e1d5d393a2988b1a293932","0fcda137ecc4a8033bb0b5159c2a6659b027abcf","0050b67643e64cc01e39b71a509579295478059d","9969ff0b19914bc8487b6abb1d5312ce538a7284","f0f98d201fa041f013095896dd095eeb4becb5f5","0c0564c0a9a8167f03cdbcf0dcdc703a3d299acb","7154f8c4a3d29f41dff4f08073d7826fc15d11d9","e8ead4682d6a75502c6bb9fbfe2898b5953f8fec","469693f04eed06f51068f19b675e5239817233ae","34569b8a6c2d7429642dabc3bda532fe1d3b3a24","7db022345bf776a3aae11393b57335165869eda8","d04c3ab918aad5f2b3fcec971db5cbe81de72d3e","92f8d0d6e5ff48ff55a848bde0b011f4476bd000","95f8db06ea79dba6bf6baf6ae4f099d94116abfc"],"id":"97fe62ebcf6327d11da47d811ab8ed5eb1cf331a","s2Url":"https://semanticscholar.org/paper/97fe62ebcf6327d11da47d811ab8ed5eb1cf331a","authors":[{"name":"Yong Bum Kim","ids":["2051329"]},{"name":"Uikyum Kim","ids":["8130510"]},{"name":"Dong-Yeop Seok","ids":["7190353"]},{"name":"JinHo So","ids":["40232975"]},{"name":"Yoon Haeng Lee","ids":["2628293"]},{"name":"Hyoukryeol Choi","ids":["1685707"]}],"doi":"10.1109/IROS.2017.8206537"}
{"doiUrl":"https://doi.org/10.1109/IROS.2018.8594310","venue":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2018,"text":"As autonomous robots are increasingly being introduced in real-world environments operating for long periods of time, the difficulties of long-term mapping are attracting the attention of the robotics research community. This paper proposes a full SLAM system capable of handling the dynamics of the environment across a single or multiple mapping sessions. Using the pose graph SLAM paradigm, the system works on local maps in the form of 2D point cloud data which are updated over time to store the most up-to-date state of the environment. The core of our system is an efficient ICP-based alignment and merging procedure working on the clouds that copes with non-static entities of the environment. Furthermore, the system retains the graph complexity by removing out-dated nodes upon robust inter- and intra-session loop closure detections while graph coherency is preserved by using condensed measurements. Experiments conducted with real data from longterm SLAM datasets demonstrate the efficiency, accuracy and effectiveness of our system in the management of the mapping problem during long-term robot operation.","inCitations":[],"pmid":"","title":"Efficient Long-term Mapping in Dynamic Environments","journalPages":"153-160","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2018.8594310"],"entities":[],"journalVolume":"","outCitations":[],"id":"8322e639dda10325feaf45b9b4a50e9137f1f1b0","s2Url":"https://semanticscholar.org/paper/8322e639dda10325feaf45b9b4a50e9137f1f1b0","authors":[{"name":"Maria Teresa Lazaro","ids":[]},{"name":"Roberto Capobianco","ids":[]},{"name":"Giorgio Grisetti","ids":[]}],"doi":"10.1109/IROS.2018.8594310"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8206176","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"Model Predictive Control is becoming more and more present in robotic applications. It has been successfully used in control of humanoid robots to adjust positions of the footsteps in order to satisfy stability constraints. In this paper we show how to adapt such scheme for a quadruped robot utilizing a static walking. The MPC is used to provide a center of mass projection reference, keeping it within support polygons to ensure stability. User is given freedom in choosing the desired dynamical behavior of the reference. The proposed control framework is tested in simulation and on a real quadruped robot. An emphasize is put on generality of such approach which is independent of gait parameters.","inCitations":[],"pmid":"","title":"Model predictive control based framework for CoM control of a quadruped robot","journalPages":"3372-3378","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8206176"],"entities":["Humanoid robot","Parameter (computer programming)","Simulation"],"journalVolume":"","outCitations":["6abf9d33fc37651238d5144413bb3ae280f4a29f","2fbc78e2c13d5136bc9d21a45cd3eca23a5e272d","3f751fc1c7f47db39e48ed536c37166e2c55d52e","863f9193b60d677eacc5a08896a8eb6720e8ddd4","8c11def05d9701db9a78589e2289e12f8262b928","beca967ea2f3df56869c3542958d57cacf19c41a","28be5bc73866ca06be55056405937b328c82e05d","537f4b4464f255d3d7a2d31e2a416cba1a877bd6","661fbd589db7a022a1b64a0d126c8f623cb470c5","41787f9bedcf872d5a9aa948a6cb6a49b3f4b96a","9e096a74385cb81315b070ed46b92aca0fa06a6d","b5cbe697951fbe600ea2e8dede5a45a0ac630037","3b979cca666dcb0d7f2cfb49223227772458c38b","0e261657a6a7e2287c51bf5968a37949f6b97d49","ea08da42ede3cacf384080afaca7e43dd63de8e8"],"id":"dc46d7b135e21f46577806f0aa8e82acca059c71","s2Url":"https://semanticscholar.org/paper/dc46d7b135e21f46577806f0aa8e82acca059c71","authors":[{"name":"Tomislav Horvat","ids":["1943916"]},{"name":"Kamilo Melo","ids":["3246052"]},{"name":"Auke Jan Ijspeert","ids":["8038419"]}],"doi":"10.1109/IROS.2017.8206176"}
{"doiUrl":"https://doi.org/10.1109/IROS.2017.8205948","venue":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","journalName":"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","sources":["DBLP"],"year":2017,"text":"The da Vinci Research Kit (DVRK) is a telerobotic surgical research platform endowed with an open controller that allows position, velocity and current control. We consider the problem of modelling and identification of both the Patient Side Manipulators (PSMs) and of the Master Tool Manipulators (MTMs) of the platform. This problem is relevant when realistic dynamic simulations have to be performed using standard software tools, but also for the design of model-based control laws, and for the implementation of sensorless strategies for collision detection or contact force estimation. A LMI-based approach is used for the identification of the robot dynamics in order to guarantee the physical feasibility of the parameters that is not ensured by standard least-squares methods. The identified models are validated experimentally.","inCitations":["0f13e38c8367ba8a94ee4e7caba648b6adeee08c","d4228448bc086f48941b8ef8841da378c21623cf","c21884a26085b2f6330704ea9bc35b8eb4248e1a","29bcba8d0db57b1ba8d88c2188d8cafb11cd2ce6","d073a0fc2403d4036fe0f4385972105dd1e92730","1c8d436444369da2bff7c601196456c83a52e2cd","55f43d2f0929d31b26ea113c2a02a94ba9902ce5"],"pmid":"","title":"Modelling and identification of the da Vinci Research Kit robotic arms","journalPages":"1464-1469","s2PdfUrl":"","pdfUrls":["https://doi.org/10.1109/IROS.2017.8205948"],"entities":["Coat of arms","Collision detection","Experiment","Least squares","Linear matrix inequality","Log management","Mathematical model","Midwoofer-tweeter-midwoofer","Nonlinear system","Optimality criterion","Robot","Simulation","System identification","Telerobotics","Velocity (software development)","Vii"],"journalVolume":"","outCitations":["6860e16465a9f7e6985449cb20836a5f482f1095","3a73595a1675fcdeb1da541485a19b7299ee8285","fa677e824b989b36c6b227cf1192e462925cd962","70d96ba3a722c58102271647e1904236bf8e84c5","1230c17063420ce294ff3dfc44d62885f7977daf","8e3bd2d48226033aba2ad41bddc4b0ff1a2333a0","29bcba8d0db57b1ba8d88c2188d8cafb11cd2ce6","9d9df2110ce50e484ce1574d5048890b05216e2a","595529b5e0087a482388e82996a19a298c5bcf2d","fd97d1757d88f8c3576836615e47c8e021d97369","140cd82d13c23c6822fc26f7a66d6a85f3dbb7ec","d329ae5080b4f5e0f6364d7e2a73d4a5d2aafc1e"],"id":"481add0c55ad9dcf348398f7ead578775b1ef61d","s2Url":"https://semanticscholar.org/paper/481add0c55ad9dcf348398f7ead578775b1ef61d","authors":[{"name":"Giuseppe Andrea Fontanelli","ids":["39686267"]},{"name":"Fanny Ficuciello","ids":["1773159"]},{"name":"Luigi Villani","ids":["38073952"]},{"name":"Bruno Siciliano","ids":["1783131"]}],"doi":"10.1109/IROS.2017.8205948"}